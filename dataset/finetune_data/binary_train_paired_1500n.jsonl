{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _test_setup_modules(authn=\"authn_testing\",authz=\"authz_allowall\",pxe_once=1):\n\n    # rewrite modules.conf so we know we can use the testing module\n    # for xmlrpc rw testing (Makefile will put the user value back)\n    \n    import yaml\n    import Cheetah.Template as Template\n\n    MODULES_TEMPLATE = \"installer_templates/modules.conf.template\"\n    DEFAULTS = \"installer_templates/defaults\"\n    fh = open(DEFAULTS)\n    data = yaml.load(fh.read())\n    fh.close()\n    data[\"authn_module\"] = authn\n    data[\"authz_module\"] = authz\n    data[\"pxe_once\"] = pxe_once\n    \n    t = Template.Template(file=MODULES_TEMPLATE, searchList=[data])\n    open(\"/etc/cobbler/modules.conf\",\"w+\").write(t.respond())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_users(self, email):\n        \"\"\"Given an email, return matching user(s) who should receive a reset.\n\n        This allows subclasses to more easily customize the default policies\n        that prevent inactive users and users with unusable passwords from\n        resetting their password.\n        \"\"\"\n        active_users = UserModel._default_manager.filter(**{\n            '%s__iexact' % UserModel.get_email_field_name(): email,\n            'is_active': True,\n        })\n        return (u for u in active_users if u.has_usable_password())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def reset_metric_variables(self) -> None:\n    \"\"\"Reset metric variable values to their initial values.\"\"\"\n    self._session.run(self._reset_variables_op)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n    OP_REQUIRES_OK(\n        ctx, functor::UpperBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def repercent_broken_unicode(path):\n    \"\"\"\n    As per section 3.2 of RFC 3987, step three of converting a URI into an IRI,\n    repercent-encode any octet produced that is not part of a strictly legal\n    UTF-8 octet sequence.\n    \"\"\"\n    while True:\n        try:\n            path.decode()\n        except UnicodeDecodeError as e:\n            # CVE-2019-14235: A recursion shouldn't be used since the exception\n            # handling uses massive amounts of memory\n            repercent = quote(path[e.start : e.end], safe=b\"/#%[]=:;$&()+,!?*@'~\")\n            path = path[: e.start] + repercent.encode() + path[e.end :]\n        else:\n            return path\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "export function getSessionUrl(baseUrl: string, id: string): string {\n  return URLExt.join(baseUrl, SESSION_SERVICE_URL, id);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_model_version_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    name = request_dict.get(\"name\")\n    version = request_dict.get(\"version\")\n    artifact_uri = _get_model_registry_store().get_model_version_download_uri(name, version)\n\n    if _is_servable_proxied_run_artifact_root(artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=artifact_uri,\n            relative_path=request_dict[\"path\"],\n        )\n    else:\n        artifact_repo = get_artifact_repository(artifact_uri)\n        artifact_path = request_dict[\"path\"]\n\n    return _send_artifact(artifact_repo, artifact_path)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def delete_service(self, context, service_id):\n        service_ref = self.catalog_api.get_service(context, service_id)\n        if not service_ref:\n            raise exception.ServiceNotFound(service_id=service_id)\n        self.catalog_api.delete_service(context, service_id)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def wrapper(self, *args, **kwargs):\n        try:\n            result = method(self, *args, **kwargs)\n        except web.HTTPError as e:\n            status = e.status_code\n            message = e.log_message\n            self.log.warn(message)\n            self.set_status(e.status_code)\n            self.finish(json.dumps(dict(message=message)))\n        except Exception:\n            self.log.error(\"Unhandled error in API request\", exc_info=True)\n            status = 500\n            message = \"Unknown server error\"\n            t, value, tb = sys.exc_info()\n            self.set_status(status)\n            tb_text = ''.join(traceback.format_exception(t, value, tb))\n            reply = dict(message=message, traceback=tb_text)\n            self.finish(json.dumps(reply))\n        else:\n            return result\n    return wrapper"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create(self, request, *args, **kwargs):\n        \"\"\"Create a new StockItem object via the API.\n\n        We override the default 'create' implementation.\n\n        If a location is *not* specified, but the linked *part* has a default location,\n        we can pre-fill the location automatically.\n        \"\"\"\n        user = request.user\n\n        # Copy the request data, to side-step \"mutability\" issues\n        data = OrderedDict()\n        data.update(request.data)\n\n        quantity = data.get('quantity', None)\n\n        if quantity is None:\n            raise ValidationError({\n                'quantity': _('Quantity is required'),\n            })\n\n        try:\n            part = Part.objects.get(pk=data.get('part', None))\n        except (ValueError, Part.DoesNotExist):\n            raise ValidationError({\n                'part': _('Valid part must be supplied'),\n            })\n\n        # Set default location (if not provided)\n        if 'location' not in data:\n            location = part.get_default_location()\n\n            if location:\n                data['location'] = location.pk\n\n        # An expiry date was *not* specified - try to infer it!\n        if 'expiry_date' not in data and part.default_expiry > 0:\n            data['expiry_date'] = datetime.now().date() + timedelta(days=part.default_expiry)\n\n        # Attempt to extract serial numbers from submitted data\n        serials = None\n\n        # Check if a set of serial numbers was provided\n        serial_numbers = data.get('serial_numbers', '')\n\n        # Assign serial numbers for a trackable part\n        if serial_numbers:\n\n            if not part.trackable:\n                raise ValidationError({\n                    'serial_numbers': [_(\"Serial numbers cannot be supplied for a non-trackable part\")]\n                })\n\n            # If serial numbers are specified, check that they match!\n            try:\n                serials = extract_serial_numbers(serial_numbers, quantity, part.getLatestSerialNumberInt())\n\n                # Determine if any of the specified serial numbers already exist!\n                existing = []\n\n                for serial in serials:\n                    if part.checkIfSerialNumberExists(serial):\n                        existing.append(serial)\n\n                if len(existing) > 0:\n\n                    msg = _(\"The following serial numbers already exist\")\n                    msg += \" : \"\n                    msg += \",\".join([str(e) for e in existing])\n\n                    raise ValidationError({\n                        'serial_numbers': [msg],\n                    })\n\n            except DjangoValidationError as e:\n                raise ValidationError({\n                    'quantity': e.messages,\n                    'serial_numbers': e.messages,\n                })\n\n        if serials is not None:\n            \"\"\"If the stock item is going to be serialized, set the quantity to 1.\"\"\"\n            data['quantity'] = 1\n\n        # De-serialize the provided data\n        serializer = self.get_serializer(data=data)\n        serializer.is_valid(raise_exception=True)\n\n        with transaction.atomic():\n\n            # Create an initial StockItem object\n            item = serializer.save()\n\n            if serials:\n                # Assign the first serial number to the \"master\" item\n                item.serial = serials[0]\n\n            # Save the item (with user information)\n            item.save(user=user)\n\n            if serials:\n                for serial in serials[1:]:\n\n                    # Create a duplicate stock item with the next serial number\n                    item.pk = None\n                    item.serial = serial\n\n                    item.save(user=user)\n\n                response_data = {\n                    'quantity': quantity,\n                    'serial_numbers': serials,\n                }\n\n            else:\n                response_data = serializer.data\n\n            return Response(response_data, status=status.HTTP_201_CREATED, headers=self.get_success_headers(serializer.data))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def respond_error(self, context, exception):\n        context.respond_server_error()\n        stack = traceback.format_exc()\n        return \"\"\"\n        <html>\n            <body>\n\n                <style>\n                    body {\n                        font-family: sans-serif;\n                        color: #888;\n                        text-align: center;\n                    }\n\n                    body pre {\n                        width: 600px;\n                        text-align: left;\n                        margin: auto;\n                        font-family: monospace;\n                    }\n                </style>\n\n                <img src=\"/ajenti:static/main/error.jpeg\" />\n                <br/>\n                <p>\n                    Server error\n                </p>\n                <pre>\n%s\n                </pre>\n            </body>\n        </html>\n        \"\"\" % stack\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, database_url):\n        self._ss = SAML2SessionStore(database_url=database_url)\n        self.user = None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def cpu_profile(\n        self, pid: int, format=\"flamegraph\", duration: float = 5, native: bool = False\n    ) -> (bool, str):\n        if format == \"flamegraph\":\n            extension = \"svg\"\n        else:\n            extension = \"txt\"\n        profile_file_path = (\n            self.profile_dir_path / f\"{format}_{pid}_cpu_profiling.{extension}\"\n        )\n        cmd = (\n            f\"py-spy record \"\n            f\"-o {profile_file_path} -p {pid} -d {duration} -f {format}\"\n        )\n        if sys.platform == \"linux\" and native:\n            cmd += \" --native\"\n        if await _can_passwordless_sudo():\n            cmd = \"sudo -n \" + cmd.replace(\"py-spy\", \"$(which py-spy)\")\n        process = await asyncio.create_subprocess_shell(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            shell=True,\n        )\n        stdout, stderr = await process.communicate()\n        if process.returncode != 0:\n            return False, _format_failed_pyspy_command(cmd, stdout, stderr)\n        else:\n            return True, open(profile_file_path, \"rb\").read()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_context(self, name, value, attrs):\n        context = super().get_context(name, value, attrs)\n        rel_to = self.rel.model\n        if rel_to in self.admin_site._registry:\n            # The related object is registered with the same AdminSite\n            related_url = reverse(\n                'admin:%s_%s_changelist' % (\n                    rel_to._meta.app_label,\n                    rel_to._meta.model_name,\n                ),\n                current_app=self.admin_site.name,\n            )\n\n            params = self.url_parameters()\n            if params:\n                related_url += '?' + '&amp;'.join('%s=%s' % (k, v) for k, v in params.items())\n            context['related_url'] = mark_safe(related_url)\n            context['link_title'] = _('Lookup')\n            # The JavaScript code looks for this class.\n            context['widget']['attrs'].setdefault('class', 'vForeignKeyRawIdAdminField')\n        else:\n            context['related_url'] = None\n        if context['widget']['value']:\n            context['link_label'], context['link_url'] = self.label_and_url_for_value(value)\n        else:\n            context['link_label'] = None\n        return context\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_user_roles(self, context, user_id, tenant_id=None):\n        \"\"\"Get the roles for a user and tenant pair.\n\n        Since we're trying to ignore the idea of user-only roles we're\n        not implementing them in hopes that the idea will die off.\n\n        \"\"\"\n        if tenant_id is None:\n            raise exception.NotImplemented(message='User roles not supported: '\n                                                   'tenant ID required')\n\n        user = self.identity_api.get_user(context, user_id)\n        if user is None:\n            raise exception.UserNotFound(user_id=user_id)\n        tenant = self.identity_api.get_tenant(context, tenant_id)\n        if tenant is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        roles = self.identity_api.get_roles_for_user_and_tenant(\n                context, user_id, tenant_id)\n        return {'roles': [self.identity_api.get_role(context, x)\n                          for x in roles]}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& indices = context->input(0);\n    const Tensor& values = context->input(1);\n    const Tensor& shape = context->input(2);\n    const Tensor& weights = context->input(3);\n    bool use_weights = weights.NumElements() > 0;\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(indices.shape()),\n                errors::InvalidArgument(\n                    \"Input indices must be a 2-dimensional tensor. Got: \",\n                    indices.shape().DebugString()));\n\n    if (use_weights) {\n      OP_REQUIRES(\n          context, weights.shape() == values.shape(),\n          errors::InvalidArgument(\n              \"Weights and values must have the same shape. Weight shape: \",\n              weights.shape().DebugString(),\n              \"; values shape: \", values.shape().DebugString()));\n    }\n\n    OP_REQUIRES(context, shape.NumElements() != 0,\n                errors::InvalidArgument(\n                    \"The shape argument requires at least one element.\"));\n\n    bool is_1d = shape.NumElements() == 1;\n    auto shape_vector = shape.flat<int64_t>();\n    int num_batches = is_1d ? 1 : shape_vector(0);\n    int num_values = values.NumElements();\n\n    for (int b = 0; b < shape_vector.size(); b++) {\n      OP_REQUIRES(context, shape_vector(b) >= 0,\n                  errors::InvalidArgument(\n                      \"Elements in dense_shape must be >= 0. Instead got:\",\n                      shape.DebugString()));\n    }\n\n    OP_REQUIRES(context, num_values == indices.shape().dim_size(0),\n                errors::InvalidArgument(\n                    \"Number of values must match first dimension of indices.\",\n                    \"Got \", num_values,\n                    \" values, indices shape: \", indices.shape().DebugString()));\n\n    const auto indices_values = indices.matrix<int64_t>();\n    const auto values_values = values.flat<T>();\n    const auto weight_values = weights.flat<W>();\n\n    auto per_batch_counts = BatchedMap<W>(num_batches);\n\n    T max_value = 0;\n\n    OP_REQUIRES(context, num_values <= indices.shape().dim_size(0),\n                errors::InvalidArgument(\n                    \"The first dimension of indices must be equal to or \"\n                    \"greather than number of values. ( \",\n                    indices.shape().dim_size(0), \" vs. \", num_values, \" )\"));\n    OP_REQUIRES(context, indices.shape().dim_size(1) > 0,\n                errors::InvalidArgument(\"The second dimension of indices must \"\n                                        \"be greater than 0. Received: \",\n                                        indices.shape().dim_size(1)));\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      int batch = is_1d ? 0 : indices_values(idx, 0);\n      if (batch >= num_batches) {\n        OP_REQUIRES(context, batch < num_batches,\n                    errors::InvalidArgument(\n                        \"Indices value along the first dimension must be \",\n                        \"lower than the first index of the shape.\", \"Got \",\n                        batch, \" as batch and \", num_batches,\n                        \" as the first dimension of the shape.\"));\n      }\n      const auto& value = values_values(idx);\n      if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {\n        if (binary_output_) {\n          per_batch_counts[batch][value] = 1;\n        } else if (use_weights) {\n          per_batch_counts[batch][value] += weight_values(idx);\n        } else {\n          per_batch_counts[batch][value]++;\n        }\n        if (value > max_value) {\n          max_value = value;\n        }\n      }\n    }\n\n    int num_output_values = GetOutputSize(max_value, maxlength_, minlength_);\n    OP_REQUIRES_OK(context, OutputSparse<W>(per_batch_counts, num_output_values,\n                                            is_1d, context));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_valid_filename(s):\n    \"\"\"\n    Return the given string converted to a string that can be used for a clean\n    filename. Remove leading and trailing spaces; convert other spaces to\n    underscores; and remove anything that is not an alphanumeric, dash,\n    underscore, or dot.\n    >>> get_valid_filename(\"john's portrait in 2004.jpg\")\n    'johns_portrait_in_2004.jpg'\n    \"\"\"\n    s = str(s).strip().replace(' ', '_')\n    return re.sub(r'(?u)[^-\\w.]', '', s)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def main():\n    global logger\n    result = 0\n\n    parser = argparse.ArgumentParser(description='Keycloak REST client',\n                    prog=prog_name,\n                    epilog=verbose_help.format(prog_name=prog_name),\n                    formatter_class=argparse.RawDescriptionHelpFormatter)\n\n    parser.add_argument('-v', '--verbose', action='store_true',\n                        help='be chatty')\n\n    parser.add_argument('-d', '--debug', action='store_true',\n                        help='turn on debug info')\n\n    parser.add_argument('--show-traceback', action='store_true',\n                        help='exceptions print traceback in addition to '\n                             'error message')\n\n    parser.add_argument('--log-file',\n                        default='/tmp/{prog_name}.log'.format(\n                            prog_name=prog_name),\n                        help='log file pathname')\n\n    parser.add_argument('--permit-insecure-transport',  action='store_true',\n                        help='Normally secure transport such as TLS '\n                        'is required, defeat this check')\n\n    parser.add_argument('--tls-verify', action=TlsVerifyAction,\n                        default=True,\n                        help='TLS certificate verification for requests to'\n                        ' the server. May be one of case insenstive '\n                        '[true, yes, on] to enable,'\n                        '[false, no, off] to disable.'\n                        'Or the pathname to a OpenSSL CA bundle to use.'\n                        ' Default is True.')\n\n    group = parser.add_argument_group('Server')\n\n    group.add_argument('-s', '--server',\n                       required=True,\n                       help='DNS name or IP address of Keycloak server')\n\n    group.add_argument('-a', '--auth-role',\n                       choices=AUTH_ROLES,\n                       default='root-admin',\n                       help='authenticating as what type of user (default: root-admin)')\n\n    group.add_argument('-u', '--admin-username',\n                       default='admin',\n                       help='admin user name (default: admin)')\n\n    group.add_argument('-p', '--admin-password',\n                       required=True,\n                       help='admin password')\n\n    group.add_argument('--admin-realm',\n                       default='master',\n                       help='realm admin belongs to')\n\n    cmd_parsers = parser.add_subparsers(help='available commands')\n\n    # --- realm commands ---\n    realm_parser = cmd_parsers.add_parser('realm',\n                                          help='realm operations')\n\n    sub_parser = realm_parser.add_subparsers(help='realm commands')\n\n    cmd_parser = sub_parser.add_parser('server_info',\n                                       help='dump server info')\n    cmd_parser.set_defaults(func=do_server_info)\n\n    cmd_parser = sub_parser.add_parser('list',\n                                       help='list realm names')\n    cmd_parser.set_defaults(func=do_list_realms)\n\n    cmd_parser = sub_parser.add_parser('create',\n                                       help='create new realm')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_create_realm)\n\n    cmd_parser = sub_parser.add_parser('delete',\n                                       help='delete existing realm')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_delete_realm)\n\n    cmd_parser = sub_parser.add_parser('metadata',\n                                       help='retrieve realm metadata')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_get_realm_metadata)\n\n    # --- client commands ---\n    client_parser = cmd_parsers.add_parser('client',\n                                           help='client operations')\n\n    sub_parser = client_parser.add_subparsers(help='client commands')\n\n    cmd_parser = sub_parser.add_parser('list',\n                                       help='list client names')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n\n    cmd_parser.set_defaults(func=do_list_clients)\n\n    cmd_parser = sub_parser.add_parser('create',\n                                       help='create new client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-m', '--metadata', type=argparse.FileType('rb'),\n                            required=True,\n                            help='SP metadata file or stdin')\n    cmd_parser.set_defaults(func=do_create_client)\n\n    cmd_parser = sub_parser.add_parser('register',\n                                       help='register new client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-m', '--metadata', type=argparse.FileType('rb'),\n                            required=True,\n                            help='SP metadata file or stdin')\n    cmd_parser.add_argument('--initial-access-token', required=True,\n                            help='realm initial access token for '\n                            'client registeration')\n    cmd_parser.set_defaults(func=do_register_client)\n\n    cmd_parser = sub_parser.add_parser('delete',\n                                       help='delete existing client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-c', '--client-name', required=True,\n                            help='client name')\n    cmd_parser.set_defaults(func=do_delete_client)\n\n    cmd_parser = sub_parser.add_parser('test',\n                                       help='experimental test used during '\n                                       'development')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-c', '--client-name', required=True,\n                            help='client name')\n    cmd_parser.set_defaults(func=do_client_test)\n\n    # Process command line arguments\n    options = parser.parse_args()\n    configure_logging(options)\n\n    if options.permit_insecure_transport:\n        os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'\n\n    try:\n        anonymous_conn = KeycloakAnonymousConnection(options.server,\n                                                     options.tls_verify)\n\n        admin_conn = KeycloakAdminConnection(options.server,\n                                             options.auth_role,\n                                             options.admin_realm,\n                                             ADMIN_CLIENT_ID,\n                                             options.admin_username,\n                                             options.admin_password,\n                                             options.tls_verify)\n    except Exception as e:\n        if options.show_traceback:\n            traceback.print_exc()\n        print(six.text_type(e), file=sys.stderr)\n        result = 1\n        return result\n\n    try:\n        if options.func == do_register_client:\n            conn = admin_conn\n        else:\n            conn = admin_conn\n        result = options.func(options, conn)\n    except Exception as e:\n        if options.show_traceback:\n            traceback.print_exc()\n        print(six.text_type(e), file=sys.stderr)\n        result = 2\n        return result\n\n    return result\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def read_config(self, config, **kwargs):\n        # FIXME: federation_domain_whitelist needs sytests\n        self.federation_domain_whitelist = None  # type: Optional[dict]\n        federation_domain_whitelist = config.get(\"federation_domain_whitelist\", None)\n\n        if federation_domain_whitelist is not None:\n            # turn the whitelist into a hash for speed of lookup\n            self.federation_domain_whitelist = {}\n\n            for domain in federation_domain_whitelist:\n                self.federation_domain_whitelist[domain] = True\n\n        self.federation_ip_range_blacklist = config.get(\n            \"federation_ip_range_blacklist\", []\n        )\n\n        # Attempt to create an IPSet from the given ranges\n        try:\n            self.federation_ip_range_blacklist = IPSet(\n                self.federation_ip_range_blacklist\n            )\n\n            # Always blacklist 0.0.0.0, ::\n            self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n        except Exception as e:\n            raise ConfigError(\n                \"Invalid range(s) provided in federation_ip_range_blacklist: %s\" % e\n            )\n\n        federation_metrics_domains = config.get(\"federation_metrics_domains\") or []\n        validate_config(\n            _METRICS_FOR_DOMAINS_SCHEMA,\n            federation_metrics_domains,\n            (\"federation_metrics_domains\",),\n        )\n        self.federation_metrics_domains = set(federation_metrics_domains)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def readNextEndLine(self, stream, limit_offset=0):\n        debug = False\n        if debug: print(\">>readNextEndLine\")\n        line = b_(\"\")\n        while True:\n            # Prevent infinite loops in malformed PDFs\n            if stream.tell() == 0 or stream.tell() == limit_offset:\n                raise PdfReadError(\"Could not read malformed PDF file\")\n            x = stream.read(1)\n            if debug: print((\"  x:\", x, \"%x\"%ord(x)))\n            if stream.tell() < 2:\n                raise PdfReadError(\"EOL marker not found\")\n            stream.seek(-2, 1)\n            if x == b_('\\n') or x == b_('\\r'): ## \\n = LF; \\r = CR\n                crlf = False\n                while x == b_('\\n') or x == b_('\\r'):\n                    if debug:\n                        if ord(x) == 0x0D: print(\"  x is CR 0D\")\n                        elif ord(x) == 0x0A: print(\"  x is LF 0A\")\n                    x = stream.read(1)\n                    if x == b_('\\n') or x == b_('\\r'): # account for CR+LF\n                        stream.seek(-1, 1)\n                        crlf = True\n                    if stream.tell() < 2:\n                        raise PdfReadError(\"EOL marker not found\")\n                    stream.seek(-2, 1)\n                stream.seek(2 if crlf else 1, 1) # if using CR+LF, go back 2 bytes, else 1\n                break\n            else:\n                if debug: print(\"  x is neither\")\n                line = x + line\n                if debug: print((\"  RNEL line:\", line))\n        if debug: print(\"leaving RNEL\")\n        return line\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse_index_server(cls, line):\n        \"\"\"\n\n        :param line:\n        :return:\n        \"\"\"\n        matches = URL_REGEX.findall(line)\n        if matches:\n            url = matches[0]\n            return url if url.endswith(\"/\") else url + \"/\"\n        return None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool IsHostMemoryArg(const EagerOperation& op, const NodeDef* node_def,\n                     const Device* op_device, const KernelDef* kernel_def,\n                     const int port_id) {\n  if (op.is_function()) return false;\n  if (node_def == nullptr) return false;\n  if (kernel_def == nullptr || op_device == nullptr) return false;\n  const auto& host_memory_args = kernel_def->host_memory_arg();\n  const OpDef& op_def = OpRegistry::Global()->LookUp(op.Name())->op_def;\n  const int arg_id = OpPortIdToArgId(*node_def, op_def.input_arg(), port_id);\n  return std::find(host_memory_args.begin(), host_memory_args.end(),\n                   op_def.input_arg(arg_id).name()) != host_memory_args.end();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def deferred_init(self) -> None:\n        all_vols = self.asrv.vfs.all_vols\n        have_e2d = self.init_indexes(all_vols, [], False)\n\n        if self.stop:\n            # up-mt consistency not guaranteed if init is interrupted;\n            # drop caches for a full scan on next boot\n            self._drop_caches()\n\n            if self.pp:\n                self.pp.end = True\n                self.pp = None\n\n            return\n\n        if not self.pp and self.args.exit == \"idx\":\n            return self.hub.sigterm()\n\n        Daemon(self._snapshot, \"up2k-snapshot\")\n        if have_e2d:\n            Daemon(self._hasher, \"up2k-hasher\")\n            Daemon(self._sched_rescan, \"up2k-rescan\")\n            if self.mtag:\n                for n in range(max(1, self.args.mtag_mt)):\n                    Daemon(self._tagger, \"tagger-{}\".format(n))\n\n                Daemon(self._run_all_mtp, \"up2k-mtp-init\")\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _is_javascript_scheme(s):\n    if _is_image_dataurl(s):\n        return None\n    return _is_possibly_malicious_scheme(s)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def decrypt_data(token, secret, data):\n    \"\"\" Decrypt the data with the given secret key. \"\"\"\n    if data.startswith(ENCRYPT_MARKER):\n        try:\n            # encrypted data\n            encoded_data = data[len(ENCRYPT_MARKER):]\n            aes_key = generate_aes_key(token, secret)\n            decoded_data = base64.b64decode(encoded_data)\n            iv = decoded_data[:16]\n            encrypted_data = decoded_data[16:]\n            cipher = AES.new(aes_key, AES.MODE_CFB, iv)\n            decrypted_data = cipher.decrypt(encrypted_data)\n            return json.loads(decrypted_data)\n        except:\n            raise DecryptError('data appeared to be corrupted')\n    else:\n        # doesn't appear to be encrypted data\n        return data\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _create_database(self, last_upgrade_to_run):\n        \"\"\"\n        Make sure that the database is created and sets the file permissions.\n        This should be done before storing any sensitive data in it.\n        \"\"\"\n        # Create the tables in the database\n        conn = self._connect()\n        try:\n            with conn:\n                self._create_tables(conn, last_upgrade_to_run)\n        finally:\n            conn.close()\n\n        # Set the file permissions\n        os.chmod(self.filename, stat.S_IRUSR | stat.S_IWUSR)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _on_load_finished(self, ok):\n        if sip.isdeleted(self._widget):\n            # https://github.com/qutebrowser/qutebrowser/issues/3498\n            return\n\n        sess_manager = objreg.get('session-manager')\n        sess_manager.save_autosave()\n\n        if ok and not self._has_ssl_errors:\n            if self.url().scheme() == 'https':\n                self._set_load_status(usertypes.LoadStatus.success_https)\n            else:\n                self._set_load_status(usertypes.LoadStatus.success)\n        elif ok:\n            self._set_load_status(usertypes.LoadStatus.warn)\n        else:\n            self._set_load_status(usertypes.LoadStatus.error)\n\n        self.load_finished.emit(ok)\n\n        if not self.title():\n            self.title_changed.emit(self.url().toDisplayString())\n\n        self.zoom.set_current()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def feed_data(\n        self,\n        data: bytes,\n        SEP: bytes = b\"\\r\\n\",\n        EMPTY: bytes = b\"\",\n        CONTENT_LENGTH: istr = hdrs.CONTENT_LENGTH,\n        METH_CONNECT: str = hdrs.METH_CONNECT,\n        SEC_WEBSOCKET_KEY1: istr = hdrs.SEC_WEBSOCKET_KEY1,\n    ) -> Tuple[List[Tuple[_MsgT, StreamReader]], bool, bytes]:\n\n        messages = []\n\n        if self._tail:\n            data, self._tail = self._tail + data, b\"\"\n\n        data_len = len(data)\n        start_pos = 0\n        loop = self.loop\n\n        while start_pos < data_len:\n\n            # read HTTP message (request/response line + headers), \\r\\n\\r\\n\n            # and split by lines\n            if self._payload_parser is None and not self._upgraded:\n                pos = data.find(SEP, start_pos)\n                # consume \\r\\n\n                if pos == start_pos and not self._lines:\n                    start_pos = pos + 2\n                    continue\n\n                if pos >= start_pos:\n                    # line found\n                    self._lines.append(data[start_pos:pos])\n                    start_pos = pos + 2\n\n                    # \\r\\n\\r\\n found\n                    if self._lines[-1] == EMPTY:\n                        try:\n                            msg: _MsgT = self.parse_message(self._lines)\n                        finally:\n                            self._lines.clear()\n\n                        def get_content_length() -> Optional[int]:\n                            # payload length\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\n                            if length_hdr is None:\n                                return None\n\n                            try:\n                                length = int(length_hdr)\n                            except ValueError:\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            if length < 0:\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            return length\n\n                        length = get_content_length()\n                        # do not support old websocket spec\n                        if SEC_WEBSOCKET_KEY1 in msg.headers:\n                            raise InvalidHeader(SEC_WEBSOCKET_KEY1)\n\n                        self._upgraded = msg.upgrade\n\n                        method = getattr(msg, \"method\", self.method)\n\n                        assert self.protocol is not None\n                        # calculate payload\n                        if (\n                            (length is not None and length > 0)\n                            or msg.chunked\n                            and not msg.upgrade\n                        ):\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            payload_parser = HttpPayloadParser(\n                                payload,\n                                length=length,\n                                chunked=msg.chunked,\n                                method=method,\n                                compression=msg.compression,\n                                code=self.code,\n                                readall=self.readall,\n                                response_with_body=self.response_with_body,\n                                auto_decompress=self._auto_decompress,\n                            )\n                            if not payload_parser.done:\n                                self._payload_parser = payload_parser\n                        elif method == METH_CONNECT:\n                            assert isinstance(msg, RawRequestMessage)\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            self._upgraded = True\n                            self._payload_parser = HttpPayloadParser(\n                                payload,\n                                method=msg.method,\n                                compression=msg.compression,\n                                readall=True,\n                                auto_decompress=self._auto_decompress,\n                            )\n                        else:\n                            if (\n                                getattr(msg, \"code\", 100) >= 199\n                                and length is None\n                                and self.read_until_eof\n                            ):\n                                payload = StreamReader(\n                                    self.protocol,\n                                    timer=self.timer,\n                                    loop=loop,\n                                    limit=self._limit,\n                                )\n                                payload_parser = HttpPayloadParser(\n                                    payload,\n                                    length=length,\n                                    chunked=msg.chunked,\n                                    method=method,\n                                    compression=msg.compression,\n                                    code=self.code,\n                                    readall=True,\n                                    response_with_body=self.response_with_body,\n                                    auto_decompress=self._auto_decompress,\n                                )\n                                if not payload_parser.done:\n                                    self._payload_parser = payload_parser\n                            else:\n                                payload = EMPTY_PAYLOAD\n\n                        messages.append((msg, payload))\n                else:\n                    self._tail = data[start_pos:]\n                    data = EMPTY\n                    break\n\n            # no parser, just store\n            elif self._payload_parser is None and self._upgraded:\n                assert not self._lines\n                break\n\n            # feed payload\n            elif data and start_pos < data_len:\n                assert not self._lines\n                assert self._payload_parser is not None\n                try:\n                    eof, data = self._payload_parser.feed_data(data[start_pos:])\n                except BaseException as exc:\n                    if self.payload_exception is not None:\n                        self._payload_parser.payload.set_exception(\n                            self.payload_exception(str(exc))\n                        )\n                    else:\n                        self._payload_parser.payload.set_exception(exc)\n\n                    eof = True\n                    data = b\"\"\n\n                if eof:\n                    start_pos = 0\n                    data_len = len(data)\n                    self._payload_parser = None\n                    continue\n            else:\n                break\n\n        if data and start_pos < data_len:\n            data = data[start_pos:]\n        else:\n            data = EMPTY\n\n        return messages, self._upgraded, data\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _parse_qsl(qs):\n    r = []\n    for pair in qs.replace(';','&').split('&'):\n        if not pair: continue\n        nv = pair.split('=', 1)\n        if len(nv) != 2: nv.append('')\n        key = urlunquote(nv[0].replace('+', ' '))\n        value = urlunquote(nv[1].replace('+', ' '))\n        r.append((key, value))\n    return r\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def authenticate(self, context, auth=None):\n        \"\"\"Authenticate credentials and return a token.\n\n        Accept auth as a dict that looks like::\n\n            {\n                \"auth\":{\n                    \"passwordCredentials\":{\n                        \"username\":\"test_user\",\n                        \"password\":\"mypass\"\n                    },\n                    \"tenantName\":\"customer-x\"\n                }\n            }\n\n        In this case, tenant is optional, if not provided the token will be\n        considered \"unscoped\" and can later be used to get a scoped token.\n\n        Alternatively, this call accepts auth with only a token and tenant\n        that will return a token that is scoped to that tenant.\n        \"\"\"\n\n        token_id = uuid.uuid4().hex\n        if 'passwordCredentials' in auth:\n            username = auth['passwordCredentials'].get('username', '')\n            password = auth['passwordCredentials'].get('password', '')\n            tenant_name = auth.get('tenantName', None)\n\n            user_id = auth['passwordCredentials'].get('userId', None)\n            if username:\n                user_ref = self.identity_api.get_user_by_name(\n                        context=context, user_name=username)\n                if user_ref:\n                    user_id = user_ref['id']\n\n            # more compat\n            tenant_id = auth.get('tenantId', None)\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                if tenant_ref:\n                    tenant_id = tenant_ref['id']\n\n            try:\n                auth_info = self.identity_api.authenticate(context=context,\n                                                           user_id=user_id,\n                                                           password=password,\n                                                           tenant_id=tenant_id)\n                (user_ref, tenant_ref, metadata_ref) = auth_info\n\n                # If the user is disabled don't allow them to authenticate\n                if not user_ref.get('enabled', True):\n                    LOG.warning('User %s is disabled' % user_id)\n                    raise exception.Unauthorized()\n            except AssertionError as e:\n                raise exception.Unauthorized(e.message)\n\n            token_ref = self.token_api.create_token(\n                    context, token_id, dict(id=token_id,\n                                            user=user_ref,\n                                            tenant=tenant_ref,\n                                            metadata=metadata_ref))\n            if tenant_ref:\n                catalog_ref = self.catalog_api.get_catalog(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'],\n                        metadata=metadata_ref)\n            else:\n                catalog_ref = {}\n\n        elif 'token' in auth:\n            token = auth['token'].get('id', None)\n\n            tenant_name = auth.get('tenantName')\n\n            # more compat\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                tenant_id = tenant_ref['id']\n            else:\n                tenant_id = auth.get('tenantId', None)\n\n            try:\n                old_token_ref = self.token_api.get_token(context=context,\n                                                         token_id=token)\n            except exception.NotFound:\n                raise exception.Unauthorized()\n\n            user_ref = old_token_ref['user']\n\n            # If the user is disabled don't allow them to authenticate\n            current_user_ref = self.identity_api.get_user(\n                                                    context=context,\n                                                    user_id=user_ref['id'])\n            if not current_user_ref.get('enabled', True):\n                LOG.warning('User %s is disabled' % user_ref['id'])\n                raise exception.Unauthorized()\n\n            tenants = self.identity_api.get_tenants_for_user(context,\n                                                             user_ref['id'])\n            if tenant_id:\n                assert tenant_id in tenants\n\n            tenant_ref = self.identity_api.get_tenant(context=context,\n                                                      tenant_id=tenant_id)\n            if tenant_ref:\n                metadata_ref = self.identity_api.get_metadata(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'])\n                catalog_ref = self.catalog_api.get_catalog(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'],\n                        metadata=metadata_ref)\n            else:\n                metadata_ref = {}\n                catalog_ref = {}\n\n            token_ref = self.token_api.create_token(\n                    context, token_id, dict(id=token_id,\n                                            user=user_ref,\n                                            tenant=tenant_ref,\n                                            metadata=metadata_ref,\n                                            expires=old_token_ref['expires']))\n\n        # TODO(termie): optimize this call at some point and put it into the\n        #               the return for metadata\n        # fill out the roles in the metadata\n        roles_ref = []\n        for role_id in metadata_ref.get('roles', []):\n            roles_ref.append(self.identity_api.get_role(context, role_id))\n        logging.debug('TOKEN_REF %s', token_ref)\n        return self._format_authenticate(token_ref, roles_ref, catalog_ref)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def values_from_list(values: List[str]) -> Dict[str, str]:\n        \"\"\" Convert a list of values into dict of {name: value} \"\"\"\n        output: Dict[str, str] = {}\n\n        for i, value in enumerate(values):\n            if value[0].isalpha():\n                key = value.upper()\n            else:\n                key = f\"VALUE_{i}\"\n            if key in output:\n                raise ValueError(f\"Duplicate key {key} in Enum\")\n            output[key] = value\n\n        return output\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, expression, delimiter, **extra):\n        super().__init__(expression, delimiter=delimiter, **extra)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def urlize(text, trim_url_limit=None, nofollow=False, autoescape=False):\n    \"\"\"\n    Converts any URLs in text into clickable links.\n\n    Works on http://, https://, www. links, and also on links ending in one of\n    the original seven gTLDs (.com, .edu, .gov, .int, .mil, .net, and .org).\n    Links can have trailing punctuation (periods, commas, close-parens) and\n    leading punctuation (opening parens) and it'll still do the right thing.\n\n    If trim_url_limit is not None, the URLs in the link text longer than this\n    limit will be truncated to trim_url_limit-3 characters and appended with\n    an ellipsis.\n\n    If nofollow is True, the links will get a rel=\"nofollow\" attribute.\n\n    If autoescape is True, the link text and URLs will be autoescaped.\n    \"\"\"\n    safe_input = isinstance(text, SafeData)\n\n    def trim_url(x, limit=trim_url_limit):\n        if limit is None or len(x) <= limit:\n            return x\n        return '%s...' % x[:max(0, limit - 3)]\n\n    def unescape(text, trail):\n        \"\"\"\n        If input URL is HTML-escaped, unescape it so as we can safely feed it to\n        smart_urlquote. For example:\n        http://example.com?x=1&amp;y=&lt;2&gt; => http://example.com?x=1&y=<2>\n        \"\"\"\n        unescaped = (text + trail).replace(\n            '&amp;', '&').replace('&lt;', '<').replace(\n            '&gt;', '>').replace('&quot;', '\"').replace('&#39;', \"'\")\n        if trail and unescaped.endswith(trail):\n            # Remove trail for unescaped if it was not consumed by unescape\n            unescaped = unescaped[:-len(trail)]\n        elif trail == ';':\n            # Trail was consumed by unescape (as end-of-entity marker), move it to text\n            text += trail\n            trail = ''\n        return text, unescaped, trail\n\n    words = word_split_re.split(force_text(text))\n    for i, word in enumerate(words):\n        if '.' in word or '@' in word or ':' in word:\n            # Deal with punctuation.\n            lead, middle, trail = '', word, ''\n            for punctuation in TRAILING_PUNCTUATION:\n                if middle.endswith(punctuation):\n                    middle = middle[:-len(punctuation)]\n                    trail = punctuation + trail\n            for opening, closing in WRAPPING_PUNCTUATION:\n                if middle.startswith(opening):\n                    middle = middle[len(opening):]\n                    lead = lead + opening\n                # Keep parentheses at the end only if they're balanced.\n                if (middle.endswith(closing)\n                        and middle.count(closing) == middle.count(opening) + 1):\n                    middle = middle[:-len(closing)]\n                    trail = closing + trail\n\n            # Make URL we want to point to.\n            url = None\n            nofollow_attr = ' rel=\"nofollow\"' if nofollow else ''\n            if simple_url_re.match(middle):\n                middle, middle_unescaped, trail = unescape(middle, trail)\n                url = smart_urlquote(middle_unescaped)\n            elif simple_url_2_re.match(middle):\n                middle, middle_unescaped, trail = unescape(middle, trail)\n                url = smart_urlquote('http://%s' % middle_unescaped)\n            elif ':' not in middle and simple_email_re.match(middle):\n                local, domain = middle.rsplit('@', 1)\n                try:\n                    domain = domain.encode('idna').decode('ascii')\n                except UnicodeError:\n                    continue\n                url = 'mailto:%s@%s' % (local, domain)\n                nofollow_attr = ''\n\n            # Make link.\n            if url:\n                trimmed = trim_url(middle)\n                if autoescape and not safe_input:\n                    lead, trail = escape(lead), escape(trail)\n                    trimmed = escape(trimmed)\n                middle = '<a href=\"%s\"%s>%s</a>' % (escape(url), nofollow_attr, trimmed)\n                words[i] = mark_safe('%s%s%s' % (lead, middle, trail))\n            else:\n                if safe_input:\n                    words[i] = mark_safe(word)\n                elif autoescape:\n                    words[i] = escape(word)\n        elif safe_input:\n            words[i] = mark_safe(word)\n        elif autoescape:\n            words[i] = escape(word)\n    return ''.join(words)\nurlize = allow_lazy(urlize, six.text_type)"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    int32_t dim = context->input(0).scalar<int32_t>()();\n    int32_t num_results = context->input(1).scalar<int32_t>()();\n    int32_t skip = context->input(2).scalar<int32_t>()();\n\n    OP_REQUIRES(context, dim >= 1,\n                errors::InvalidArgument(\"dim must be at least one\"));\n    OP_REQUIRES(context, dim <= sobol_data::kMaxSobolDim,\n                errors::InvalidArgument(\"dim must be at most \",\n                                        sobol_data::kMaxSobolDim));\n    OP_REQUIRES(context, num_results >= 1,\n                errors::InvalidArgument(\"num_results must be at least one\"));\n    OP_REQUIRES(context, skip >= 0,\n                errors::InvalidArgument(\"skip must be non-negative\"));\n    OP_REQUIRES(context,\n                num_results < std::numeric_limits<int32_t>::max() - skip,\n                errors::InvalidArgument(\"num_results+skip must be less than \",\n                                        std::numeric_limits<int32_t>::max()));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\n                       0, TensorShape({num_results, dim}), &output));\n    auto output_flat = output->flat<T>();\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n    int num_threads = worker_threads.num_threads;\n    int block_size = std::max(\n        kMinBlockSize, static_cast<int>(std::ceil(\n                           static_cast<float>(num_results) / num_threads)));\n    worker_threads.workers->TransformRangeConcurrently(\n        block_size, num_results /* total */,\n        [&dim, &skip, &output_flat](const int start, const int end) {\n          CalculateSobolSample<T>(dim, end - start /* num_results */, skip,\n                                  start, output_flat);\n        });\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def save(request, name):\n  if 'change' not in getPermissions(request.user):\n    return json_response( dict(error=\"Must be logged in with appropriate permissions to save\") )\n  # Deserialize and reserialize as a validation step\n  state = str( json.dumps( json.loads( request.POST['state'] ) ) )\n\n  try:\n    dashboard = Dashboard.objects.get(name=name)\n  except Dashboard.DoesNotExist:\n    dashboard = Dashboard.objects.create(name=name, state=state)\n  else:\n    dashboard.state = state\n    dashboard.save()\n\n  return json_response( dict(success=True) )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "std::vector<int64_t> CastPyArg2Longs(PyObject* obj,\n                                     const std::string& op_type,\n                                     ssize_t arg_pos) {\n  std::vector<int64_t> value;\n  if (PyList_Check(obj)) {\n    Py_ssize_t len = PyList_Size(obj);\n    PyObject* item = nullptr;\n    for (Py_ssize_t i = 0; i < len; i++) {\n      item = PyList_GetItem(obj, i);\n      if (PyObject_CheckLongOrToLong(&item)) {\n        value.emplace_back((int64_t)PyLong_AsLongLong(item));\n      } else {\n        PADDLE_THROW(platform::errors::InvalidArgument(\n            \"%s(): argument (position %d) must be \"\n            \"list of int, but got %s at pos %d\",\n            op_type,\n            arg_pos + 1,\n            ((PyTypeObject*)item->ob_type)->tp_name,  // NOLINT\n            i));\n      }\n    }\n  } else if (PyTuple_Check(obj)) {\n    Py_ssize_t len = PyTuple_Size(obj);\n    PyObject* item = nullptr;\n    for (Py_ssize_t i = 0; i < len; i++) {\n      item = PyTuple_GetItem(obj, i);\n      if (PyObject_CheckLongOrToLong(&item)) {\n        value.emplace_back((int64_t)PyLong_AsLongLong(item));\n      } else {\n        PADDLE_THROW(platform::errors::InvalidArgument(\n            \"%s(): argument (position %d) must be \"\n            \"list of int, but got %s at pos %d\",\n            op_type,\n            arg_pos + 1,\n            ((PyTypeObject*)item->ob_type)->tp_name,  // NOLINT\n            i));\n      }\n    }\n  } else if (PySequence_Check(obj)) {\n    Py_ssize_t len = PySequence_Size(obj);\n    PyObject* item = nullptr;\n    for (Py_ssize_t i = 0; i < len; i++) {\n      item = PySequence_GetItem(obj, i);\n      if (PyObject_CheckLongOrToLong(&item)) {\n        value.emplace_back((int64_t)PyLong_AsLongLong(item));\n      } else {\n        PADDLE_THROW(platform::errors::InvalidArgument(\n            \"%s(): argument (position %d) must be \"\n            \"list of int, but got %s at pos %d\",\n            op_type,\n            arg_pos + 1,\n            ((PyTypeObject*)item->ob_type)->tp_name,  // NOLINT\n            i));\n      }\n    }\n  } else if (obj == Py_None) {\n    return {};\n  } else if (PyObject_CheckLongOrToLong(&obj)) {\n    return {(int64_t)PyLong_AsLongLong(obj)};\n  } else {\n    PADDLE_THROW(platform::errors::InvalidArgument(\n        \"%s(): argument (position %d) must be \"\n        \"list or tuple, but got %s\",\n        op_type,\n        arg_pos + 1,\n        ((PyTypeObject*)obj->ob_type)->tp_name));  // NOLINT\n  }\n\n  return value;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void pointZZ_pMul(PointZZ_p * rop, const PointZZ_p * point, const mpz_t scalar, const CurveZZ_p * curve) {\n    PointZZ_p R0, R1, tmp;\n    mpz_inits(R1.x, R1.y, tmp.x, tmp.y, NULL);\n    mpz_init_set(R0.x, point->x);\n    mpz_init_set(R0.y, point->y);\n    pointZZ_pDouble(&R1, point, curve);\n\n    int dbits = mpz_sizeinbase(scalar, 2), i;\n\n    for(i = dbits - 2; i >= 0; i--) {\n        if(mpz_tstbit(scalar, i)) {\n            mpz_set(tmp.x, R0.x);\n            mpz_set(tmp.y, R0.y);\n            pointZZ_pAdd(&R0, &R1, &tmp, curve);\n            mpz_set(tmp.x, R1.x);\n            mpz_set(tmp.y, R1.y);\n            pointZZ_pDouble(&R1, &tmp, curve);\n        }\n        else {\n            mpz_set(tmp.x, R1.x);\n            mpz_set(tmp.y, R1.y);\n            pointZZ_pAdd(&R1, &R0, &tmp, curve);\n            mpz_set(tmp.x, R0.x);\n            mpz_set(tmp.y, R0.y);\n            pointZZ_pDouble(&R0, &tmp, curve);\n        }\n    }\n\n    mpz_init_set(rop->x, R0.x);\n    mpz_init_set(rop->y, R0.y);\n    mpz_clears(R0.x, R0.y, R1.x, R1.y, tmp.x, tmp.y, NULL);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  explicit FractionalMaxPoolOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pooling_ratio\", &pooling_ratio_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"pseudo_random\", &pseudo_random_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n\n    OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                errors::InvalidArgument(\"pooling_ratio field must \"\n                                        \"specify 4 dimensions\"));\n\n    OP_REQUIRES(\n        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n        errors::Unimplemented(\"Fractional max pooling is not yet \"\n                              \"supported on the batch nor channel dimension.\"));\n\n    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed2\", &seed2_));\n    if (deterministic_) {\n      // If both seeds are not set when deterministic_ is true, force set seeds.\n      if ((seed_ == 0) && (seed2_ == 0)) {\n        seed_ = random::New64();\n        seed2_ = random::New64();\n      }\n    } else {\n      OP_REQUIRES(\n          context, (seed_ == 0) && (seed2_ == 0),\n          errors::InvalidArgument(\n              \"Both seed and seed2 should be 0 if deterministic is false.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def authenticate_request(self, request, content):\n        \"\"\"Authenticates a Matrix federation request based on the X-Matrix header\n        XXX: Copied largely from synapse\n\n        :param request: The request object to authenticate\n        :type request: twisted.web.server.Request\n        :param content: The content of the request, if any\n        :type content: bytes or None\n\n        :return: The origin of the server whose signature was validated\n        :rtype: twisted.internet.defer.Deferred[unicode]\n        \"\"\"\n        json_request = {\n            \"method\": request.method,\n            \"uri\": request.uri,\n            \"destination_is\": self.sydent.server_name,\n            \"signatures\": {},\n        }\n\n        if content is not None:\n            json_request[\"content\"] = content\n\n        origin = None\n\n        def parse_auth_header(header_str):\n            \"\"\"\n            Extracts a server name, signing key and payload signature from an\n            authentication header.\n\n            :param header_str: The content of the header\n            :type header_str: unicode\n\n            :return: The server name, the signing key, and the payload signature.\n            :rtype: tuple[unicode]\n            \"\"\"\n            try:\n                params = header_str.split(u\" \")[1].split(u\",\")\n                param_dict = dict(kv.split(u\"=\") for kv in params)\n\n                def strip_quotes(value):\n                    if value.startswith(u\"\\\"\"):\n                        return value[1:-1]\n                    else:\n                        return value\n\n                origin = strip_quotes(param_dict[\"origin\"])\n                key = strip_quotes(param_dict[\"key\"])\n                sig = strip_quotes(param_dict[\"sig\"])\n                return origin, key, sig\n            except Exception:\n                raise SignatureVerifyException(\"Malformed Authorization header\")\n\n        auth_headers = request.requestHeaders.getRawHeaders(u\"Authorization\")\n\n        if not auth_headers:\n            raise NoAuthenticationError(\"Missing Authorization headers\")\n\n        for auth in auth_headers:\n            if auth.startswith(u\"X-Matrix\"):\n                (origin, key, sig) = parse_auth_header(auth)\n                json_request[\"origin\"] = origin\n                json_request[\"signatures\"].setdefault(origin, {})[key] = sig\n\n        if not json_request[\"signatures\"]:\n            raise NoAuthenticationError(\"Missing X-Matrix Authorization header\")\n\n        if not is_valid_hostname(json_request[\"origin\"]):\n            raise InvalidServerName(\"X-Matrix header's origin parameter must be a valid hostname\")\n\n        yield self.verifyServerSignedJson(json_request, [origin])\n\n        logger.info(\"Verified request from HS %s\", origin)\n\n        defer.returnValue(origin)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def __setstate__(self, state):\n    \"\"\"Restore from pickled state.\"\"\"\n    self.__dict__ = state\n    self._lock = threading.Lock()\n    self._descriptor_cache = weakref.WeakKeyDictionary()\n    self._key_for_call_stats = self._get_key_for_call_stats()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def update(self, request, *args, **kwargs):\n        \"\"\"Perform 'update' function and mark this part as 'starred' (or not)\"\"\"\n        if 'starred' in request.data:\n            starred = str2bool(request.data.get('starred', False))\n\n            self.get_object().set_starred(request.user, starred)\n\n        response = super().update(request, *args, **kwargs)\n\n        return response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void CalculateOutputIndexValueRowID(\n      OpKernelContext* context, const RowPartitionTensor& value_rowids,\n      const vector<INDEX_TYPE>& parent_output_index,\n      INDEX_TYPE output_index_multiplier, INDEX_TYPE output_size,\n      vector<INDEX_TYPE>* result) {\n    const INDEX_TYPE index_size = value_rowids.size();\n    result->reserve(index_size);\n    if (index_size == 0) {\n      return;\n    }\n\n    INDEX_TYPE current_output_column = 0;\n    INDEX_TYPE current_value_rowid = value_rowids(0);\n    DCHECK_LT(current_value_rowid, parent_output_index.size());\n    INDEX_TYPE current_output_index = parent_output_index[current_value_rowid];\n    result->push_back(current_output_index);\n    for (INDEX_TYPE i = 1; i < index_size; ++i) {\n      INDEX_TYPE next_value_rowid = value_rowids(i);\n      if (next_value_rowid == current_value_rowid) {\n        if (current_output_index >= 0) {\n          ++current_output_column;\n          if (current_output_column < output_size) {\n            current_output_index += output_index_multiplier;\n          } else {\n            current_output_index = -1;\n          }\n        }\n      } else {\n        current_output_column = 0;\n        current_value_rowid = next_value_rowid;\n        DCHECK_LT(next_value_rowid, parent_output_index.size());\n        current_output_index = parent_output_index[next_value_rowid];\n      }\n      result->push_back(current_output_index);\n    }\n    OP_REQUIRES(context, result->size() == value_rowids.size(),\n                errors::InvalidArgument(\"Invalid row ids.\"));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def build_IR(self, expr, args, kwargs, context):\n        placeholder_node = IRnode.from_list(\n            context.new_internal_variable(BytesT(128)), typ=BytesT(128), location=MEMORY\n        )\n        return IRnode.from_list(\n            [\n                \"seq\",\n                [\"mstore\", placeholder_node, args[0]],\n                [\"mstore\", [\"add\", placeholder_node, 32], args[1]],\n                [\"mstore\", [\"add\", placeholder_node, 64], args[2]],\n                [\"mstore\", [\"add\", placeholder_node, 96], args[3]],\n                [\n                    \"pop\",\n                    [\n                        \"staticcall\",\n                        [\"gas\"],\n                        1,\n                        placeholder_node,\n                        128,\n                        MemoryPositions.FREE_VAR_SPACE,\n                        32,\n                    ],\n                ],\n                [\"mload\", MemoryPositions.FREE_VAR_SPACE],\n            ],\n            typ=AddressT(),\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_annotation(self, annotation, alias, is_summary=False, select=True):\n        \"\"\"Add a single annotation expression to the Query.\"\"\"\n        annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,\n                                                   summarize=is_summary)\n        if select:\n            self.append_annotation_mask([alias])\n        else:\n            self.set_annotation_mask(set(self.annotation_select).difference({alias}))\n        self.annotations[alias] = annotation\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _rpsl_db_query_to_graphql_out(query: RPSLDatabaseQuery, info: GraphQLResolveInfo):\n    \"\"\"\n    Given an RPSL database query, execute it and clean up the output\n    to be suitable to return to GraphQL.\n\n    Main changes are:\n    - Enum handling\n    - Adding the asn and prefix fields if applicable\n    - Ensuring the right fields are returned as a list of strings or a string\n    \"\"\"\n    database_handler = info.context['request'].app.state.database_handler\n    if info.context.get('sql_trace'):\n        if 'sql_queries' not in info.context:\n            info.context['sql_queries'] = [repr(query)]\n        else:\n            info.context['sql_queries'].append(repr(query))\n\n    for row in database_handler.execute_query(query, refresh_on_error=True):\n        graphql_result = {snake_to_camel_case(k): v for k, v in row.items() if k != 'parsed_data'}\n        if 'object_text' in row:\n            graphql_result['objectText'] = remove_auth_hashes(row['object_text'])\n        if 'rpki_status' in row:\n            graphql_result['rpkiStatus'] = row['rpki_status']\n        if row.get('ip_first') is not None and row.get('prefix_length'):\n            graphql_result['prefix'] = row['ip_first'] + '/' + str(row['prefix_length'])\n        if row.get('asn_first') is not None and row.get('asn_first') == row.get('asn_last'):\n            graphql_result['asn'] = row['asn_first']\n\n        object_type = resolve_rpsl_object_type(row)\n        for key, value in row.get('parsed_data', dict()).items():\n            if key == 'auth':\n                value = remove_auth_hashes(value)\n            graphql_type = schema.graphql_types[object_type][key]\n            if graphql_type == 'String' and isinstance(value, list):\n                value = '\\n'.join(value)\n            graphql_result[snake_to_camel_case(key)] = value\n        yield graphql_result\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_safe_url(url, host=None):\n    \"\"\"\n    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to\n    a different host and uses a safe scheme).\n\n    Always returns ``False`` on an empty url.\n    \"\"\"\n    if url is not None:\n        url = url.strip()\n    if not url:\n        return False\n    # Chrome treats \\ completely as /\n    url = url.replace('\\\\', '/')\n    # Chrome considers any URL with more than two slashes to be absolute, but\n    # urlparse is not so flexible. Treat any url with three slashes as unsafe.\n    if url.startswith('///'):\n        return False\n    url_info = urlparse(url)\n    # Forbid URLs like http:///example.com - with a scheme, but without a hostname.\n    # In that URL, example.com is not the hostname but, a path component. However,\n    # Chrome will still consider example.com to be the hostname, so we must not\n    # allow this syntax.\n    if not url_info.netloc and url_info.scheme:\n        return False\n    # Forbid URLs that start with control characters. Some browsers (like\n    # Chrome) ignore quite a few control characters at the start of a\n    # URL and might consider the URL as scheme relative.\n    if unicodedata.category(url[0])[0] == 'C':\n        return False\n    return ((not url_info.netloc or url_info.netloc == host) and\n            (not url_info.scheme or url_info.scheme in ['http', 'https']))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def apt_sync(self, repo):\n\n        \"\"\"\n        Handle copying of http:// and ftp:// debian repos.\n        \"\"\"\n\n        # warn about not having mirror program.\n\n        mirror_program = \"/usr/bin/debmirror\"\n        if not os.path.exists(mirror_program):\n            utils.die(self.logger, \"no %s found, please install it\" % (mirror_program))\n\n        cmd = \"\"                  # command to run\n\n        # detect cases that require special handling\n\n        if repo.rpm_list != \"\" and repo.rpm_list != []:\n            utils.die(self.logger, \"has_rpm_list not yet supported on apt repos\")\n\n        if not repo.arch:\n            utils.die(self.logger, \"Architecture is required for apt repositories\")\n\n        # built destination path for the repo\n        dest_path = os.path.join(\"/var/www/cobbler/repo_mirror\", repo.name)\n\n        if repo.mirror_locally:\n            # NOTE: Dropping @@suite@@ replace as it is also dropped from\n            # from manage_import_debian_ubuntu.py due that repo has no os_version\n            # attribute. If it is added again it will break the Web UI!\n            # mirror = repo.mirror.replace(\"@@suite@@\",repo.os_version)\n            mirror = repo.mirror\n\n            idx = mirror.find(\"://\")\n            method = mirror[:idx]\n            mirror = mirror[idx + 3:]\n\n            idx = mirror.find(\"/\")\n            host = mirror[:idx]\n            mirror = mirror[idx:]\n\n            dists = \",\".join(repo.apt_dists)\n            components = \",\".join(repo.apt_components)\n\n            mirror_data = \"--method=%s --host=%s --root=%s --dist=%s --section=%s\" % (method, host, mirror, dists, components)\n\n            rflags = \"--nocleanup\"\n            for x in repo.yumopts:\n                if repo.yumopts[x]:\n                    rflags += \" %s %s\" % (x, repo.yumopts[x])\n                else:\n                    rflags += \" %s\" % x\n            cmd = \"%s %s %s %s\" % (mirror_program, rflags, mirror_data, dest_path)\n            if repo.arch == \"src\":\n                cmd = \"%s --source\" % cmd\n            else:\n                arch = repo.arch\n                if arch == \"x86\":\n                    arch = \"i386\"       # FIX potential arch errors\n                if arch == \"x86_64\":\n                    arch = \"amd64\"      # FIX potential arch errors\n                cmd = \"%s --nosource -a %s\" % (cmd, arch)\n\n            # Set's an environment variable for subprocess, otherwise debmirror will fail\n            # as it needs this variable to exist.\n            # FIXME: might this break anything? So far it doesn't\n            os.putenv(\"HOME\", \"/var/lib/cobbler\")\n\n            rc = utils.subprocess_call(self.logger, cmd)\n            if rc != 0:\n                utils.die(self.logger, \"cobbler reposync failed\")\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def rollback_to(self):\n        await self.transaction.connection.execute(\n            f\"ROLLBACK TO SAVEPOINT {self.name}\"\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def build_attrs(self, *args, **kwargs):\n        attrs = super().build_attrs(*args, **kwargs)\n\n        accept = attrs.get(\"accept\")\n        response = self.client.generate_presigned_post(\n            self.bucket_name,\n            str(pathlib.PurePosixPath(self.upload_folder, \"${filename}\")),\n            Conditions=self.get_conditions(accept),\n            ExpiresIn=self.expires,\n        )\n\n        defaults = {\n            \"data-fields-%s\" % key: value for key, value in response[\"fields\"].items()\n        }\n        defaults[\"data-url\"] = response[\"url\"]\n        defaults.update(attrs)\n\n        try:\n            defaults[\"class\"] += \" s3file\"\n        except KeyError:\n            defaults[\"class\"] = \"s3file\"\n        return defaults\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _group_or_org_create(context, data_dict, is_org=False):\n    model = context['model']\n    user = context['user']\n    session = context['session']\n    data_dict['is_organization'] = is_org\n\n    upload = uploader.get_uploader('group')\n    upload.update_data_dict(data_dict, 'image_url',\n                            'image_upload', 'clear_upload')\n    # get the schema\n    group_type = data_dict.get('type', 'organization' if is_org else 'group')\n    group_plugin = lib_plugins.lookup_group_plugin(group_type)\n    try:\n        schema = group_plugin.form_to_db_schema_options({\n            'type': 'create', 'api': 'api_version' in context,\n            'context': context})\n    except AttributeError:\n        schema = group_plugin.form_to_db_schema()\n\n    if 'api_version' not in context:\n        # old plugins do not support passing the schema so we need\n        # to ensure they still work\n        try:\n            group_plugin.check_data_dict(data_dict, schema)\n        except TypeError:\n            group_plugin.check_data_dict(data_dict)\n\n    data, errors = lib_plugins.plugin_validate(\n        group_plugin, context, data_dict, schema,\n        'organization_create' if is_org else 'group_create')\n    log.debug('group_create validate_errs=%r user=%s group=%s data_dict=%r',\n              errors, context.get('user'), data_dict.get('name'), data_dict)\n\n    if errors:\n        session.rollback()\n        raise ValidationError(errors)\n\n    group = model_save.group_dict_save(data, context)\n\n    # Needed to let extensions know the group id\n    session.flush()\n\n    if is_org:\n        plugin_type = plugins.IOrganizationController\n    else:\n        plugin_type = plugins.IGroupController\n\n    for item in plugins.PluginImplementations(plugin_type):\n        item.create(group)\n\n    if is_org:\n        activity_type = 'new organization'\n    else:\n        activity_type = 'new group'\n\n    user_id = model.User.by_name(six.ensure_text(user)).id\n\n    activity_dict = {\n        'user_id': user_id,\n        'object_id': group.id,\n        'activity_type': activity_type,\n    }\n    activity_dict['data'] = {\n        'group': ckan.lib.dictization.table_dictize(group, context)\n    }\n    activity_create_context = {\n        'model': model,\n        'user': user,\n        'defer_commit': True,\n        'ignore_auth': True,\n        'session': session\n    }\n    logic.get_action('activity_create')(activity_create_context, activity_dict)\n\n    upload.upload(uploader.get_max_image_size())\n\n    if not context.get('defer_commit'):\n        model.repo.commit()\n    context[\"group\"] = group\n    context[\"id\"] = group.id\n\n    # creator of group/org becomes an admin\n    # this needs to be after the repo.commit or else revisions break\n    member_dict = {\n        'id': group.id,\n        'object': user_id,\n        'object_type': 'user',\n        'capacity': 'admin',\n    }\n    member_create_context = {\n        'model': model,\n        'user': user,\n        'ignore_auth': True,  # we are not a member of the group at this point\n        'session': session\n    }\n    logic.get_action('member_create')(member_create_context, member_dict)\n\n    log.debug('Created object %s' % group.name)\n\n    return_id_only = context.get('return_id_only', False)\n    action = 'organization_show' if is_org else 'group_show'\n\n    output = context['id'] if return_id_only \\\n        else _get_action(action)(context, {'id': group.id})\n\n    return output\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def explain(self, using, format=None, **options):\n        q = self.clone()\n        q.explain_info = ExplainInfo(format, options)\n        compiler = q.get_compiler(using=using)\n        return \"\\n\".join(compiler.explain_query())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _execute_module(self, conn, tmp, module_name, args,\n        async_jid=None, async_module=None, async_limit=None, inject=None, persist_files=False, complex_args=None, delete_remote_tmp=True):\n\n        ''' transfer and run a module along with its arguments on the remote side'''\n\n        # hack to support fireball mode\n        if module_name == 'fireball':\n            args = \"%s password=%s\" % (args, base64.b64encode(str(utils.key_for_hostname(conn.host))))\n            if 'port' not in args:\n                args += \" port=%s\" % C.ZEROMQ_PORT\n\n        (\n        module_style,\n        shebang,\n        module_data\n        ) = self._configure_module(conn, module_name, args, inject, complex_args)\n\n        # a remote tmp path may be necessary and not already created\n        if self._late_needs_tmp_path(conn, tmp, module_style):\n            tmp = self._make_tmp_path(conn)\n\n        remote_module_path = conn.shell.join_path(tmp, module_name)\n\n        if (module_style != 'new'\n           or async_jid is not None\n           or not conn.has_pipelining\n           or not C.ANSIBLE_SSH_PIPELINING\n           or C.DEFAULT_KEEP_REMOTE_FILES\n           or self.su):\n            self._transfer_str(conn, tmp, module_name, module_data)\n\n        environment_string = self._compute_environment_string(conn, inject)\n\n        if \"tmp\" in tmp and ((self.sudo and self.sudo_user != 'root') or (self.su and self.su_user != 'root')):\n            # deal with possible umask issues once sudo'ed to other user\n            self._remote_chmod(conn, 'a+r', remote_module_path, tmp)\n\n        cmd = \"\"\n        in_data = None\n        if module_style != 'new':\n            if 'CHECKMODE=True' in args:\n                # if module isn't using AnsibleModuleCommon infrastructure we can't be certain it knows how to\n                # do --check mode, so to be safe we will not run it.\n                return ReturnData(conn=conn, result=dict(skipped=True, msg=\"cannot yet run check mode against old-style modules\"))\n            elif 'NO_LOG' in args:\n                return ReturnData(conn=conn, result=dict(skipped=True, msg=\"cannot use no_log: with old-style modules\"))\n\n            args = template.template(self.basedir, args, inject)\n\n            # decide whether we need to transfer JSON or key=value\n            argsfile = None\n            if module_style == 'non_native_want_json':\n                if complex_args:\n                    complex_args.update(utils.parse_kv(args))\n                    argsfile = self._transfer_str(conn, tmp, 'arguments', utils.jsonify(complex_args))\n                else:\n                    argsfile = self._transfer_str(conn, tmp, 'arguments', utils.jsonify(utils.parse_kv(args)))\n\n            else:\n                argsfile = self._transfer_str(conn, tmp, 'arguments', args)\n\n            if (self.sudo and self.sudo_user != 'root') or (self.su and self.su_user != 'root'):\n                # deal with possible umask issues once sudo'ed to other user\n                self._remote_chmod(conn, 'a+r', argsfile, tmp)\n\n            if async_jid is None:\n                cmd = \"%s %s\" % (remote_module_path, argsfile)\n            else:\n                cmd = \" \".join([str(x) for x in [remote_module_path, async_jid, async_limit, async_module, argsfile]])\n        else:\n            if async_jid is None:\n                if conn.has_pipelining and C.ANSIBLE_SSH_PIPELINING and not C.DEFAULT_KEEP_REMOTE_FILES and not self.su:\n                    in_data = module_data\n                else:\n                    cmd = \"%s\" % (remote_module_path)\n            else:\n                cmd = \" \".join([str(x) for x in [remote_module_path, async_jid, async_limit, async_module]])\n\n        if not shebang:\n            raise errors.AnsibleError(\"module is missing interpreter line\")\n\n        rm_tmp = None\n        if \"tmp\" in tmp and not C.DEFAULT_KEEP_REMOTE_FILES and not persist_files and delete_remote_tmp:\n            if not self.sudo or self.su or self.sudo_user == 'root' or self.su_user == 'root':\n                # not sudoing or sudoing to root, so can cleanup files in the same step\n                rm_tmp = tmp\n\n        cmd = conn.shell.build_module_command(environment_string, shebang, cmd, rm_tmp)\n        cmd = cmd.strip()\n\n        sudoable = True\n        if module_name == \"accelerate\":\n            # always run the accelerate module as the user\n            # specified in the play, not the sudo_user\n            sudoable = False\n\n        if self.su:\n            res = self._low_level_exec_command(conn, cmd, tmp, su=True, in_data=in_data)\n        else:\n            res = self._low_level_exec_command(conn, cmd, tmp, sudoable=sudoable, in_data=in_data)\n\n        if \"tmp\" in tmp and not C.DEFAULT_KEEP_REMOTE_FILES and not persist_files and delete_remote_tmp:\n            if (self.sudo and self.sudo_user != 'root') or (self.su and self.su_user != 'root'):\n            # not sudoing to root, so maybe can't delete files as that other user\n            # have to clean up temp files as original user in a second step\n                cmd2 = conn.shell.remove(tmp, recurse=True)\n                self._low_level_exec_command(conn, cmd2, tmp, sudoable=False)\n\n        data = utils.parse_json(res['stdout'])\n        if 'parsed' in data and data['parsed'] == False:\n            data['msg'] += res['stderr']\n        return ReturnData(conn=conn, result=data)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static int ReadHuffmanCodeLengths(\n    VP8LDecoder* const dec, const int* const code_length_code_lengths,\n    int num_symbols, int* const code_lengths) {\n  int ok = 0;\n  VP8LBitReader* const br = &dec->br_;\n  int symbol;\n  int max_symbol;\n  int prev_code_len = DEFAULT_CODE_LENGTH;\n  HuffmanCode table[1 << LENGTHS_TABLE_BITS];\n\n  if (!VP8LBuildHuffmanTable(table, LENGTHS_TABLE_BITS,\n                             code_length_code_lengths,\n                             NUM_CODE_LENGTH_CODES)) {\n    goto End;\n  }\n\n  if (VP8LReadBits(br, 1)) {    // use length\n    const int length_nbits = 2 + 2 * VP8LReadBits(br, 3);\n    max_symbol = 2 + VP8LReadBits(br, length_nbits);\n    if (max_symbol > num_symbols) {\n      goto End;\n    }\n  } else {\n    max_symbol = num_symbols;\n  }\n\n  symbol = 0;\n  while (symbol < num_symbols) {\n    const HuffmanCode* p;\n    int code_len;\n    if (max_symbol-- == 0) break;\n    VP8LFillBitWindow(br);\n    p = &table[VP8LPrefetchBits(br) & LENGTHS_TABLE_MASK];\n    VP8LSetBitPos(br, br->bit_pos_ + p->bits);\n    code_len = p->value;\n    if (code_len < kCodeLengthLiterals) {\n      code_lengths[symbol++] = code_len;\n      if (code_len != 0) prev_code_len = code_len;\n    } else {\n      const int use_prev = (code_len == kCodeLengthRepeatCode);\n      const int slot = code_len - kCodeLengthLiterals;\n      const int extra_bits = kCodeLengthExtraBits[slot];\n      const int repeat_offset = kCodeLengthRepeatOffsets[slot];\n      int repeat = VP8LReadBits(br, extra_bits) + repeat_offset;\n      if (symbol + repeat > num_symbols) {\n        goto End;\n      } else {\n        const int length = use_prev ? prev_code_len : 0;\n        while (repeat-- > 0) code_lengths[symbol++] = length;\n      }\n    }\n  }\n  ok = 1;\n\n End:\n  if (!ok) return VP8LSetError(dec, VP8_STATUS_BITSTREAM_ERROR);\n  return ok;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def start(\n        self,\n        measurement_info: MeasurementInfo,\n        service_info: ServiceInfo,\n        configuration_parameter_list: List[ParameterMetadata],\n        output_parameter_list: List[ParameterMetadata],\n        measure_function: Callable,\n        owner: object = None,\n    ) -> str:\n        \"\"\"Start the gRPC server and register it with the discovery service.\n\n        Returns:\n            The insecure port.\n        \"\"\"\n        interceptors: List[grpc.ServerInterceptor] = []\n        if ServerLogger.is_enabled():\n            interceptors.append(ServerLogger())\n        self._server = grpc.server(\n            logging_pool.pool(max_workers=10),\n            interceptors=interceptors,\n            options=[\n                (\"grpc.max_receive_message_length\", -1),\n                (\"grpc.max_send_message_length\", -1),\n            ],\n        )\n        for interface in service_info.provided_interfaces:\n            if interface == _V1_INTERFACE:\n                servicer_v1 = MeasurementServiceServicerV1(\n                    measurement_info,\n                    configuration_parameter_list,\n                    output_parameter_list,\n                    measure_function,\n                    owner,\n                )\n                v1_measurement_service_pb2_grpc.add_MeasurementServiceServicer_to_server(\n                    servicer_v1, self._server\n                )\n            elif interface == _V2_INTERFACE:\n                servicer_v2 = MeasurementServiceServicerV2(\n                    measurement_info,\n                    configuration_parameter_list,\n                    output_parameter_list,\n                    measure_function,\n                    owner,\n                )\n                v2_measurement_service_pb2_grpc.add_MeasurementServiceServicer_to_server(\n                    servicer_v2, self._server\n                )\n            else:\n                raise ValueError(\n                    f\"Unknown interface was provided in the .serviceconfig file: {interface}\"\n                )\n        port = str(self._server.add_insecure_port(\"[::]:0\"))\n        self._server.start()\n        _logger.info(\"Measurement service hosted on port: %s\", port)\n\n        self._service_location = ServiceLocation(\"localhost\", port, \"\")\n        self._registration_id = self._discovery_client.register_service(\n            service_info, self.service_location\n        )\n        return port\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def rsync_sync(self, repo):\n\n        \"\"\"\n        Handle copying of rsync:// and rsync-over-ssh repos.\n        \"\"\"\n\n        if not repo.mirror_locally:\n            utils.die(self.logger, \"rsync:// urls must be mirrored locally, yum cannot access them directly\")\n\n        if repo.rpm_list != \"\" and repo.rpm_list != []:\n            self.logger.warning(\"--rpm-list is not supported for rsync'd repositories\")\n\n        # FIXME: don't hardcode\n        dest_path = os.path.join(self.settings.webdir + \"/repo_mirror\", repo.name)\n\n        spacer = \"\"\n        if not repo.mirror.startswith(\"rsync://\") and not repo.mirror.startswith(\"/\"):\n            spacer = \"-e ssh\"\n        if not repo.mirror.strip().endswith(\"/\"):\n            repo.mirror = \"%s/\" % repo.mirror\n\n        # FIXME: wrapper for subprocess that logs to logger\n        cmd = \"rsync -rltDv --copy-unsafe-links --delete-after %s --delete --exclude-from=/etc/cobbler/rsync.exclude %s %s\" % (spacer, repo.mirror, dest_path)\n        rc = utils.subprocess_call(self.logger, cmd)\n\n        if rc != 0:\n            utils.die(self.logger, \"cobbler reposync failed\")\n        os.path.walk(dest_path, self.createrepo_walker, repo)\n        self.create_local_file(dest_path, repo)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status SparseTensorToCSRSparseMatrixCPUFunctor::operator()(\n    const int64 batch_size, const int num_rows,\n    TTypes<int64>::ConstMatrix indices, TTypes<int32>::Vec batch_ptr,\n    TTypes<int32>::Vec csr_row_ptr, TTypes<int32>::Vec csr_col_ind) {\n  // Validate inputs.\n  if (batch_ptr.size() != batch_size + 1) {\n    return errors::InvalidArgument(\n        \"Expected batch_ptr.size() == batch_size + 1. Got: \", batch_ptr.size(),\n        \" vs. \", batch_size + 1);\n  }\n  if (csr_row_ptr.size() != batch_size * (num_rows + 1)) {\n    return errors::InvalidArgument(\n        \"Expected csr_row_ptr.size() == batch_size * (num_rows + 1). Got: \",\n        csr_row_ptr.size(), \" vs. \", batch_size * (num_rows + 1));\n  }\n\n  const int64 total_nnz = indices.dimension(0);\n  const int rank = indices.dimension(1);\n  if (rank == 2 && batch_size != 1) {\n    return errors::InvalidArgument(\n        \"Expected batch_size == 1 when rank is 2. Got batch_size: \",\n        batch_size);\n  }\n  if (csr_col_ind.size() != total_nnz) {\n    return errors::InvalidArgument(\n        \"Expected csr_col_ind.size() == total_nnz. Got: \", csr_col_ind.size(),\n        \" vs. \", total_nnz);\n  }\n\n  int prev_batch = -1;\n  if (rank == 2) {\n    // For a single batch, the batch_ptrs are {0, total_nnz}.\n    batch_ptr(0) = 0;\n    ++prev_batch;\n\n    for (int64 i = 0; i < total_nnz; ++i) {\n      // For now, the rows pointers store the corresponding row counts.\n      csr_row_ptr(indices(i, 0) + 1) += 1;\n      csr_col_ind(i) = indices(i, 1);\n    }\n  } else {  // rank == 3\n    for (int64 i = 0; i < total_nnz; ++i) {\n      const int cur_batch = indices(i, 0);\n      // For now, the rows pointers store the corresponding row counts.\n      csr_row_ptr(cur_batch * (num_rows + 1) + indices(i, 1) + 1) += 1;\n      csr_col_ind(i) = indices(i, 2);\n\n      // We're at a new batch and might have skipped over empty batches.\n      while (prev_batch < cur_batch) {\n        // The previous batch ends at position i.\n        batch_ptr(prev_batch + 1) = i;\n        ++prev_batch;\n      }\n    }\n  }\n  // Set the last element of batch_ptr and account for trailing empty batches.\n  while (prev_batch < batch_size) {\n    batch_ptr(prev_batch + 1) = total_nnz;\n    ++prev_batch;\n  }\n\n  // Compute the cumulative row counts for each batch.\n  for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n    auto* row_ptr_batch = csr_row_ptr.data() + batch_idx * (num_rows + 1);\n    std::partial_sum(row_ptr_batch, row_ptr_batch + num_rows + 1,\n                     row_ptr_batch);\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def login():\n    user = flask.request.form[\"username\"]\n    password = flask.request.form[\"password\"]\n\n    api = flask.current_app.config[\"PYLOAD_API\"]\n    user_info = api.check_auth(user, password)\n\n    if not user_info:\n        log.error(f\"Login failed for user '{user}'\")\n        return jsonify(False)\n\n    s = set_session(user_info)\n    log.info(f\"User '{user}' successfully logged in\")\n    flask.flash(\"Logged in successfully\")\n\n    return jsonify(s)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _ttl_action(self):\n        # type: () -> TtlActions\n        \"\"\"Determine the correct action to take based on the local resources and TTL.\n\n        :returns: decision\n        :rtype: TtlActions\n        \"\"\"\n        if self._version is None:\n            _LOGGER.debug(\"TTL Expired because no version is known\")\n            return TtlActions.EXPIRED\n\n        time_since_updated = time.time() - self._last_updated\n\n        if time_since_updated < self._version_ttl:\n            return TtlActions.LIVE\n\n        if time_since_updated < self._version_ttl + _GRACE_PERIOD:\n            return TtlActions.GRACE_PERIOD\n\n        _LOGGER.debug(\"TTL Expired because known version has expired\")\n        return TtlActions.EXPIRED\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool ConstantFolding::IsFoldableUncached(\n    const NodeDef& node, const GraphProperties* properties) const {\n  // Folding not applicable to ops with no inputs.\n  if (node.input().empty()) {\n    return false;\n  }\n  // We can only fold nodes if all their inputs are known statically, except in\n  // the case of a merge node that propagate the first inputs that becomes\n  // available, and therefore only requires a single constant input to be\n  // foldable.\n  bool merge_has_constant_input = false;\n  const bool is_merge = IsMerge(node);\n  for (const auto& input : node.input()) {\n    if (IsControlInput(input)) {\n      continue;\n    }\n    const NodeDef* input_node = node_map_->GetNode(input);\n    if (!input_node) {\n      return false;\n    }\n    bool is_const = IsReallyConstant(*input_node);\n    if (is_const) {\n      // Don't fold strings constants for now since this causes problems with\n      // checkpointing.\n      if (input_node->attr().count(\"dtype\") == 0 ||\n          input_node->attr().at(\"dtype\").type() == DT_STRING) {\n        return false;\n      }\n      // Special case: If a Merge node has at least one constant input that\n      // does not depend on a control input, we can fold it.\n      merge_has_constant_input |= !HasControlInputs(*input_node);\n    } else if (!is_merge) {\n      return false;\n    }\n  }\n  if (is_merge && !merge_has_constant_input) return false;\n  if (disable_compressed_tensor_optimization_ &&\n      (IsFill(node) || IsZerosLike(node) || IsOnesLike(node)))\n    return false;\n\n  // If we know the output shapes, make sure that the outputs are small enough\n  // to materialize.\n  if (properties != nullptr && properties->HasOutputProperties(node.name())) {\n    const std::vector<OpInfo::TensorProperties>& input_props =\n        properties->GetInputProperties(node.name());\n    const std::vector<OpInfo::TensorProperties>& output_props =\n        properties->GetOutputProperties(node.name());\n    // Compute total size of inputs.\n    int64_t input_size_bytes = 0;\n    for (const auto& input_prop : input_props) {\n      const PartialTensorShape input_shape(input_prop.shape());\n      if (input_shape.IsFullyDefined()) {\n        input_size_bytes +=\n            input_shape.num_elements() * DataTypeSize(input_prop.dtype());\n      }\n    }\n    for (const auto& output_prop : output_props) {\n      const PartialTensorShape output_shape(output_prop.shape());\n      if (output_shape.IsFullyDefined()) {\n        const int64_t num_bytes =\n            output_shape.num_elements() * DataTypeSize(output_prop.dtype());\n        if (num_bytes > input_size_bytes && num_bytes > kMaxConstantSize) {\n          // Do not fold nodes if the in-memory size of output is too large.\n          // Notice that this is not exactly the same check used in\n          // CreateNodeDef() where the actual encoded size is checked.\n          return false;\n        }\n      }\n    }\n  }\n\n  return true;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render_prefs_panel(self, panelid, action=None, **kwargs):  # @UnusedVariable\n\n        # Handle action\n        form = SshForm()\n        if action == \"add\":\n            self._add_key(action, form)\n        elif action == 'delete':\n            self._delete_key(action, DeleteSshForm())\n\n        # Get SSH keys if file exists.\n        params = {'form': form}\n        try:\n            params[\"sshkeys\"] = [\n                {'title': key.comment or (key.keytype + ' ' + key.key[:18]), 'fingerprint': key.fingerprint}\n                for key in self.app.currentuser.authorizedkeys\n            ]\n        except IOError:\n            params[\"sshkeys\"] = []\n            flash(_(\"Failed to get SSH keys\"), level='error')\n            _logger.warning(\"error reading SSH keys\", exc_info=1)\n\n        return \"prefs_sshkeys.html\", params\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def delete_shelf(shelf_id):\n    cur_shelf = ub.session.query(ub.Shelf).filter(ub.Shelf.id == shelf_id).first()\n    try:\n        delete_shelf_helper(cur_shelf)\n    except InvalidRequestError:\n        ub.session.rollback()\n        log.error(\"Settings DB is not Writeable\")\n        flash(_(\"Settings DB is not Writeable\"), category=\"error\")\n    return redirect(url_for('web.index'))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_headers_finished(self) -> None:\n        message = (MultiPartMessage.HEADERS_FINISHED, b\"\")\n        self.messages.append(message)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  inline Eigen::IndexList<int, Eigen::type2index<1>> NByOne(int n) {\n    Eigen::IndexList<int, Eigen::type2index<1>> ret;\n    ret.set(0, n);\n    return ret;\n  }\n  inline Eigen::IndexList<Eigen::type2index<1>, int> OneByM(int m) {"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def feed_data(\n        self,\n        data: bytes,\n        SEP: _SEP = b\"\\r\\n\",\n        EMPTY: bytes = b\"\",\n        CONTENT_LENGTH: istr = hdrs.CONTENT_LENGTH,\n        METH_CONNECT: str = hdrs.METH_CONNECT,\n        SEC_WEBSOCKET_KEY1: istr = hdrs.SEC_WEBSOCKET_KEY1,\n    ) -> Tuple[List[Tuple[_MsgT, StreamReader]], bool, bytes]:\n        messages = []\n\n        if self._tail:\n            data, self._tail = self._tail + data, b\"\"\n\n        data_len = len(data)\n        start_pos = 0\n        loop = self.loop\n\n        while start_pos < data_len:\n            # read HTTP message (request/response line + headers), \\r\\n\\r\\n\n            # and split by lines\n            if self._payload_parser is None and not self._upgraded:\n                pos = data.find(SEP, start_pos)\n                # consume \\r\\n\n                if pos == start_pos and not self._lines:\n                    start_pos = pos + len(SEP)\n                    continue\n\n                if pos >= start_pos:\n                    # line found\n                    line = data[start_pos:pos]\n                    if SEP == b\"\\n\":  # For lax response parsing\n                        line = line.rstrip(b\"\\r\")\n                    self._lines.append(line)\n                    start_pos = pos + len(SEP)\n\n                    # \\r\\n\\r\\n found\n                    if self._lines[-1] == EMPTY:\n                        try:\n                            msg: _MsgT = self.parse_message(self._lines)\n                        finally:\n                            self._lines.clear()\n\n                        def get_content_length() -> Optional[int]:\n                            # payload length\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\n                            if length_hdr is None:\n                                return None\n\n                            # Shouldn't allow +/- or other number formats.\n                            # https://www.rfc-editor.org/rfc/rfc9110#section-8.6-2\n                            if not length_hdr.strip(\" \\t\").isdecimal():\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            return int(length_hdr)\n\n                        length = get_content_length()\n                        # do not support old websocket spec\n                        if SEC_WEBSOCKET_KEY1 in msg.headers:\n                            raise InvalidHeader(SEC_WEBSOCKET_KEY1)\n\n                        self._upgraded = msg.upgrade\n\n                        method = getattr(msg, \"method\", self.method)\n                        # code is only present on responses\n                        code = getattr(msg, \"code\", 0)\n\n                        assert self.protocol is not None\n                        # calculate payload\n                        empty_body = status_code_must_be_empty_body(code) or bool(\n                            method and method_must_be_empty_body(method)\n                        )\n                        if not empty_body and (\n                            (length is not None and length > 0)\n                            or msg.chunked\n                            and not msg.upgrade\n                        ):\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            payload_parser = HttpPayloadParser(\n                                payload,\n                                length=length,\n                                chunked=msg.chunked,\n                                method=method,\n                                compression=msg.compression,\n                                code=self.code,\n                                readall=self.readall,\n                                response_with_body=self.response_with_body,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                            if not payload_parser.done:\n                                self._payload_parser = payload_parser\n                        elif method == METH_CONNECT:\n                            assert isinstance(msg, RawRequestMessage)\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            self._upgraded = True\n                            self._payload_parser = HttpPayloadParser(\n                                payload,\n                                method=msg.method,\n                                compression=msg.compression,\n                                readall=True,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                        elif not empty_body and length is None and self.read_until_eof:\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            payload_parser = HttpPayloadParser(\n                                payload,\n                                length=length,\n                                chunked=msg.chunked,\n                                method=method,\n                                compression=msg.compression,\n                                code=self.code,\n                                readall=True,\n                                response_with_body=self.response_with_body,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                            if not payload_parser.done:\n                                self._payload_parser = payload_parser\n                        else:\n                            payload = EMPTY_PAYLOAD\n\n                        messages.append((msg, payload))\n                else:\n                    self._tail = data[start_pos:]\n                    data = EMPTY\n                    break\n\n            # no parser, just store\n            elif self._payload_parser is None and self._upgraded:\n                assert not self._lines\n                break\n\n            # feed payload\n            elif data and start_pos < data_len:\n                assert not self._lines\n                assert self._payload_parser is not None\n                try:\n                    eof, data = self._payload_parser.feed_data(data[start_pos:], SEP)\n                except BaseException as exc:\n                    if self.payload_exception is not None:\n                        self._payload_parser.payload.set_exception(\n                            self.payload_exception(str(exc))\n                        )\n                    else:\n                        self._payload_parser.payload.set_exception(exc)\n\n                    eof = True\n                    data = b\"\"\n\n                if eof:\n                    start_pos = 0\n                    data_len = len(data)\n                    self._payload_parser = None\n                    continue\n            else:\n                break\n\n        if data and start_pos < data_len:\n            data = data[start_pos:]\n        else:\n            data = EMPTY\n\n        return messages, self._upgraded, data\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_extra(self, select, select_params, where, params, tables, order_by):\n        \"\"\"\n        Add data to the various extra_* attributes for user-created additions\n        to the query.\n        \"\"\"\n        if select:\n            # We need to pair any placeholder markers in the 'select'\n            # dictionary with their parameters in 'select_params' so that\n            # subsequent updates to the select dictionary also adjust the\n            # parameters appropriately.\n            select_pairs = {}\n            if select_params:\n                param_iter = iter(select_params)\n            else:\n                param_iter = iter([])\n            for name, entry in select.items():\n                entry = str(entry)\n                entry_params = []\n                pos = entry.find(\"%s\")\n                while pos != -1:\n                    if pos == 0 or entry[pos - 1] != \"%\":\n                        entry_params.append(next(param_iter))\n                    pos = entry.find(\"%s\", pos + 2)\n                select_pairs[name] = (entry, entry_params)\n            self.extra.update(select_pairs)\n        if where or params:\n            self.where.add(ExtraWhere(where, params), AND)\n        if tables:\n            self.extra_tables += tuple(tables)\n        if order_by:\n            self.extra_order_by = order_by\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def post(self, request, *args, **kwargs):\n        content = request.POST.get(\"content\", \"\")\n        lexer = request.POST.get(\"lexer\", highlight.LEXER_DEFAULT).strip()\n        filename = request.POST.get(\"filename\", \"\").strip()\n        expires = request.POST.get(\"expires\", \"\").strip()\n        response_format = request.POST.get(\"format\", \"default\").strip()\n\n        if not content.strip():\n            return HttpResponseBadRequest(\"No content given\")\n\n        # We need at least a lexer or a filename\n        if not lexer and not filename:\n            return HttpResponseBadRequest(\n                \"No lexer or filename given. Unable to \"\n                \"determine a highlight. Valid lexers are: %s\"\n                % \", \".join(highlight.LEXER_KEYS)\n            )\n\n        # A lexer is given, check if its valid at all\n        if lexer and lexer not in highlight.LEXER_KEYS:\n            return HttpResponseBadRequest(\n                'Invalid lexer \"%s\" given. Valid lexers are: %s'\n                % (lexer, \", \".join(highlight.LEXER_KEYS))\n            )\n\n        # No lexer is given, but we have a filename, try to get the lexer\n        #  out of it. In case Pygments cannot determine the lexer of the\n        # filename, we fallback to 'plain' code.\n        if not lexer and filename:\n            try:\n                lexer_cls = get_lexer_for_filename(filename)\n                lexer = lexer_cls.aliases[0]\n            except (ClassNotFound, IndexError):\n                lexer = config.PLAIN_CODE_SYMBOL\n\n        if expires:\n            expire_options = [str(i) for i in dict(config.EXPIRE_CHOICES)]\n            if expires not in expire_options:\n                return HttpResponseBadRequest(\n                    'Invalid expire choice \"{}\" given. Valid values are: {}'.format(\n                        expires, \", \".join(expire_options)\n                    )\n                )\n            expires, expire_type = get_expire_values(expires)\n        else:\n            expires = datetime.datetime.now() + datetime.timedelta(seconds=60 * 60 * 24)\n            expire_type = Snippet.EXPIRE_TIME\n\n        snippet = Snippet.objects.create(\n            content=content,\n            lexer=lexer,\n            expires=expires,\n            expire_type=expire_type,\n        )\n\n        # Custom formatter for the API response\n        formatter = getattr(self, f\"_format_{response_format}\", None)\n        if callable(formatter):\n            return HttpResponse(formatter(snippet))\n\n        # Otherwise use the default one.\n        return HttpResponse(self._format_default(snippet))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& data = context->input(0);\n    const Tensor& segment_ids = context->input(1);\n    const Tensor& num_segments = context->input(2);\n    if (!UnsortedSegmentReductionDoValidation(this, context, data, segment_ids,\n                                              num_segments)) {\n      return;\n    }\n    const auto segment_flat = segment_ids.flat<Index>();\n    const Index output_rows = internal::SubtleMustCopy(static_cast<Index>(\n        num_segments.dtype() == DT_INT32 ? num_segments.scalar<int32>()()\n                                         : num_segments.scalar<int64>()()));\n    OP_REQUIRES(context, output_rows >= 0,\n                errors::InvalidArgument(\"Input num_segments == \", output_rows,\n                                        \" must not be negative.\"));\n    TensorShape output_shape;\n    output_shape.AddDim(output_rows);\n    for (int i = segment_ids.dims(); i < data.dims(); i++) {\n      output_shape.AddDim(data.dim_size(i));\n    }\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &output));\n    auto output_flat = output->flat_outer_dims<T>();\n    auto data_ptr = data.template flat<T>().data();\n    reduction_functor_(context, output_rows, segment_ids.shape(), segment_flat,\n                       data.NumElements(), data_ptr, output_flat);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def _received_remote_receipt(self, origin: str, content: JsonDict) -> None:\n        \"\"\"Called when we receive an EDU of type m.receipt from a remote HS.\"\"\"\n        receipts = []\n        for room_id, room_values in content.items():\n            # If we're not in the room just ditch the event entirely. This is\n            # probably an old server that has come back and thinks we're still in\n            # the room (or we've been rejoined to the room by a state reset).\n            is_in_room = await self.event_auth_handler.is_host_in_room(\n                room_id, self.server_name\n            )\n            if not is_in_room:\n                logger.info(\n                    \"Ignoring receipt for room %r from server %s as we're not in the room\",\n                    room_id,\n                    origin,\n                )\n                continue\n\n            for receipt_type, users in room_values.items():\n                for user_id, user_values in users.items():\n                    if get_domain_from_id(user_id) != origin:\n                        logger.info(\n                            \"Received receipt for user %r from server %s, ignoring\",\n                            user_id,\n                            origin,\n                        )\n                        continue\n\n                    # Check if these receipts apply to a thread.\n                    data = user_values.get(\"data\", {})\n                    thread_id = data.get(\"thread_id\")\n                    # If the thread ID is invalid, consider it missing.\n                    if not isinstance(thread_id, str):\n                        thread_id = None\n\n                    receipts.append(\n                        ReadReceipt(\n                            room_id=room_id,\n                            receipt_type=receipt_type,\n                            user_id=user_id,\n                            event_ids=user_values[\"event_ids\"],\n                            thread_id=thread_id,\n                            data=data,\n                        )\n                    )\n\n        await self._handle_new_receipts(receipts)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render(self, request):\n        redir = self.base_url + self.basepath\n        request.redirect(redir)\n        return redir\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def parse_type_line(type_line, rcb, loc):\n    \"\"\"Parses a type annotation specified as a comment.\n\n    Example inputs:\n        # type: (Tensor, torch.Tensor) -> Tuple[Tensor]\n        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tensor\n    \"\"\"\n    arg_ann_str, ret_ann_str = split_type_line(type_line)\n\n    try:\n        arg_ann = eval(arg_ann_str, {}, EvalEnv(rcb))  # type: ignore[arg-type] # noqa: P204\n    except (NameError, SyntaxError) as e:\n        raise RuntimeError(\"Failed to parse the argument list of a type annotation\") from e\n\n    if not isinstance(arg_ann, tuple):\n        arg_ann = (arg_ann,)\n\n    try:\n        ret_ann = eval(ret_ann_str, {}, EvalEnv(rcb))  # type: ignore[arg-type] # noqa: P204\n    except (NameError, SyntaxError) as e:\n        raise RuntimeError(\"Failed to parse the return type of a type annotation\") from e\n\n    arg_types = [ann_to_type(ann, loc) for ann in arg_ann]\n    return arg_types, ann_to_type(ret_ann, loc)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _handle_forwarded_room_key_event(\n        self,\n        sender,  # type: str\n        sender_key,  # type: str\n        payload,  # type: Dict[Any, Any]\n    ):\n        # type: (...) -> Union[ForwardedRoomKeyEvent, BadEventType, None]\n        event = ForwardedRoomKeyEvent.from_dict(payload, sender, sender_key)\n\n        if isinstance(event, (BadEvent, UnknownBadEvent)):\n            return event\n\n        if event.algorithm != \"m.megolm.v1.aes-sha2\":\n            logger.error(\n                f\"Error: unsupported forwarded room key of type {event.algorithm}\"\n            )\n            return None\n\n        if event.session_id not in self.outgoing_key_requests:\n            logger.info(\n                \"Ignoring session key we have not requested from device {}.\", sender_key\n            )\n            return None\n\n        key_request = self.outgoing_key_requests[event.session_id]\n\n        if (\n            event.algorithm != key_request.algorithm\n            or event.room_id != key_request.room_id\n            or event.session_id != key_request.session_id\n        ):\n            logger.info(\n                \"Ignoring session key with mismatched algorithm, room_id, or \"\n                \"session id.\"\n            )\n            return None\n\n        content = payload[\"content\"]\n\n        session_sender_key = content[\"sender_key\"]\n        signing_key = content[\"sender_claimed_ed25519_key\"]\n        chain = content[\"forwarding_curve25519_key_chain\"]\n        chain.append(session_sender_key)\n\n        session = Olm._import_group_session(\n            content[\"session_key\"],\n            signing_key,\n            session_sender_key,\n            event.room_id,\n            chain,\n        )\n\n        if not session:\n            return None\n\n        if self.inbound_group_store.add(session):\n            self.save_inbound_group_session(session)\n\n        key_request = self.outgoing_key_requests.pop(key_request.request_id)\n        self.store.remove_outgoing_key_request(key_request)\n        self.outgoing_to_device_messages.append(\n            key_request.as_cancellation(self.user_id, self.device_id)\n        )\n\n        return event\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def to_yaml(self, **kwargs):\n    \"\"\"Returns a yaml string containing the network configuration.\n\n    To load a network from a yaml save file, use\n    `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n\n    `custom_objects` should be a dictionary mapping\n    the names of custom losses / layers / etc to the corresponding\n    functions / classes.\n\n    Arguments:\n        **kwargs: Additional keyword arguments\n            to be passed to `yaml.dump()`.\n\n    Returns:\n        A YAML string.\n\n    Raises:\n        ImportError: if yaml module is not found.\n    \"\"\"\n    if yaml is None:\n      raise ImportError(\n          'Requires yaml module installed (`pip install pyyaml`).')\n    return yaml.dump(self._updated_config(), **kwargs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor* hypothesis_indices;\n    const Tensor* hypothesis_values;\n    const Tensor* hypothesis_shape;\n    const Tensor* truth_indices;\n    const Tensor* truth_values;\n    const Tensor* truth_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_indices\", &hypothesis_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_values\", &hypothesis_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_shape\", &hypothesis_shape));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_indices\", &truth_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_values\", &truth_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_shape\", &truth_shape));\n\n    OP_REQUIRES_OK(\n        ctx, ValidateShapes(ctx, *hypothesis_indices, *hypothesis_values,\n                            *hypothesis_shape, *truth_indices, *truth_values,\n                            *truth_shape));\n\n    TensorShape hypothesis_st_shape;\n    OP_REQUIRES_OK(\n        ctx, TensorShapeUtils::MakeShape(hypothesis_shape->vec<int64>().data(),\n                                         hypothesis_shape->NumElements(),\n                                         &hypothesis_st_shape));\n    TensorShape truth_st_shape;\n    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n                            truth_shape->vec<int64>().data(),\n                            truth_shape->NumElements(), &truth_st_shape));\n\n    // Assume indices are sorted in row-major order.\n    std::vector<int64> sorted_order(truth_st_shape.dims());\n    std::iota(sorted_order.begin(), sorted_order.end(), 0);\n\n    sparse::SparseTensor hypothesis;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *hypothesis_indices, *hypothesis_values,\n                            hypothesis_st_shape, sorted_order, &hypothesis));\n\n    sparse::SparseTensor truth;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *truth_indices, *truth_values, truth_st_shape,\n                            sorted_order, &truth));\n\n    // Group dims 0, 1, ..., RANK - 1.  The very last dim is assumed\n    // to store the variable length sequences.\n    std::vector<int64> group_dims(truth_st_shape.dims() - 1);\n    std::iota(group_dims.begin(), group_dims.end(), 0);\n\n    TensorShape output_shape;\n    for (int d = 0; d < static_cast<int>(group_dims.size()); ++d) {\n      output_shape.AddDim(std::max(hypothesis_st_shape.dim_size(d),\n                                   truth_st_shape.dim_size(d)));\n    }\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"output\", output_shape, &output));\n    auto output_t = output->flat<float>();\n    output_t.setZero();\n\n    std::vector<int64> output_strides(output_shape.dims());\n    output_strides[output_shape.dims() - 1] = 1;\n    for (int d = output_shape.dims() - 2; d >= 0; --d) {\n      output_strides[d] = output_strides[d + 1] * output_shape.dim_size(d + 1);\n    }\n\n    auto hypothesis_grouper = hypothesis.group(group_dims);\n    auto truth_grouper = truth.group(group_dims);\n\n    auto hypothesis_iter = hypothesis_grouper.begin();\n    auto truth_iter = truth_grouper.begin();\n\n    auto cmp = std::equal_to<T>();\n\n    while (hypothesis_iter != hypothesis_grouper.end() &&\n           truth_iter != truth_grouper.end()) {\n      sparse::Group truth_i = *truth_iter;\n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64> g_truth = truth_i.group();\n      std::vector<int64> g_hypothesis = hypothesis_j.group();\n      auto truth_seq = truth_i.values<T>();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n\n      if (g_truth == g_hypothesis) {\n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64{0});\n        output_t(loc) =\n            gtl::LevenshteinDistance<T>(truth_seq, hypothesis_seq, cmp);\n        if (normalize_) output_t(loc) /= truth_seq.size();\n\n        ++hypothesis_iter;\n        ++truth_iter;\n      } else if (g_truth > g_hypothesis) {  // zero-length truth\n        auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                      output_strides.begin(), int64{0});\n        output_t(loc) = hypothesis_seq.size();\n        if (normalize_ && output_t(loc) != 0.0f) {\n          output_t(loc) = std::numeric_limits<float>::infinity();\n        }\n        ++hypothesis_iter;\n      } else {  // zero-length hypothesis\n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64{0});\n        output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n        ++truth_iter;\n      }\n    }\n    while (hypothesis_iter != hypothesis_grouper.end()) {  // zero-length truths\n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64> g_hypothesis = hypothesis_j.group();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                    output_strides.begin(), int64{0});\n      output_t(loc) = hypothesis_seq.size();\n      if (normalize_ && output_t(loc) != 0.0f) {\n        output_t(loc) = std::numeric_limits<float>::infinity();\n      }\n      ++hypothesis_iter;\n    }\n    while (truth_iter != truth_grouper.end()) {  // missing hypotheses\n      sparse::Group truth_i = *truth_iter;\n      std::vector<int64> g_truth = truth_i.group();\n      auto truth_seq = truth_i.values<T>();\n      auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                    output_strides.begin(), int64{0});\n      output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n      ++truth_iter;\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def process_pdus_for_room(room_id):\n            logger.debug(\"Processing PDUs for %s\", room_id)\n            try:\n                yield self.check_server_matches_acl(origin_host, room_id)\n            except AuthError as e:\n                logger.warn(\n                    \"Ignoring PDUs for room %s from banned server\", room_id,\n                )\n                for pdu in pdus_by_room[room_id]:\n                    event_id = pdu.event_id\n                    pdu_results[event_id] = e.error_dict()\n                return\n\n            for pdu in pdus_by_room[room_id]:\n                event_id = pdu.event_id\n                try:\n                    yield self._handle_received_pdu(\n                        transaction.origin, pdu\n                    )\n                    pdu_results[event_id] = {}\n                except FederationError as e:\n                    logger.warn(\"Error handling PDU %s: %s\", event_id, e)\n                    pdu_results[event_id] = {\"error\": str(e)}\n                except Exception as e:\n                    f = failure.Failure()\n                    pdu_results[event_id] = {\"error\": str(e)}\n                    logger.error(\n                        \"Failed to handle PDU %s: %s\",\n                        event_id, f.getTraceback().rstrip(),\n                    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _redirect_safe(self, url, default=None):\n        \"\"\"Redirect if url is on our PATH\n\n        Full-domain redirects are allowed if they pass our CORS origin checks.\n\n        Otherwise use default (self.base_url if unspecified).\n        \"\"\"\n        if default is None:\n            default = self.base_url\n        if not url.startswith(self.base_url):\n            # require that next_url be absolute path within our path\n            allow = False\n            # OR pass our cross-origin check\n            if '://' in url:\n                # if full URL, run our cross-origin check:\n                parsed = urlparse(url.lower())\n                origin = '%s://%s' % (parsed.scheme, parsed.netloc)\n                if self.allow_origin:\n                    allow = self.allow_origin == origin\n                elif self.allow_origin_pat:\n                    allow = bool(self.allow_origin_pat.match(origin))\n            if not allow:\n                # not allowed, use default\n                self.log.warning(\"Not allowing login redirect to %r\" % url)\n                url = default\n        self.redirect(url)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _add_auth_parsers_arguments(parser):\n    parser.add_argument('-a', '--auth-method',\n                        type=bigflow.deploy.AuthorizationType,\n                        default='local_account',\n                        help='One of two authentication method: '\n                             'local_account -- you are using credentials of your local user authenticated in gcloud; '\n                             'vault -- credentials for service account are obtained from Vault. '\n                             'Default: local_account',\n                        choices=list(bigflow.deploy.AuthorizationType),\n    )\n    parser.add_argument('-ve', '--vault-endpoint',\n                        type=str,\n                        help='URL of a Vault endpoint to get OAuth token for service account. '\n                             'Required if auth-method is vault. '\n                             'If not set, will be read from deployment_config.py.'\n                        )\n    parser.add_argument('-vs', '--vault-secret',\n                        type=str,\n                        help='Vault secret token. '\n                             'Required if auth-method is vault.'\n                        )\n    parser.add_argument('-dc', '--deployment-config-path',\n                        type=str,\n                        help='Path to the deployment_config.py file. '\n                             'If not set, {current_dir}/deployment_config.py will be used.')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "export const redirectAnyRequest = response => {\n  const {headers: {location}} = Response.message (response);\n  const original = Response.request (response);\n  const oldUrl = Request.url (original);\n  const newUrl = mergeUrls (oldUrl) (location);\n  return (Request (Request.options (original))\n                  (newUrl)\n                  (Request.body (original)));\n};\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def user_create(context, data_dict):\n    '''Create a new user.\n\n    You must be authorized to create users.\n\n    :param name: the name of the new user, a string between 2 and 100\n        characters in length, containing only lowercase alphanumeric\n        characters, ``-`` and ``_``\n    :type name: string\n    :param email: the email address for the new user\n    :type email: string\n    :param password: the password of the new user, a string of at least 4\n        characters\n    :type password: string\n    :param id: the id of the new user (optional)\n    :type id: string\n    :param fullname: the full name of the new user (optional)\n    :type fullname: string\n    :param about: a description of the new user (optional)\n    :type about: string\n    :param image_url: the URL to an image to be displayed on the group's page\n        (optional)\n    :type image_url: string\n    :param plugin_extras: private extra user data belonging to plugins.\n        Only sysadmin users may set this value. It should be a dict that can\n        be dumped into JSON, and plugins should namespace their extras with\n        the plugin name to avoid collisions with other plugins, eg::\n\n            {\n                \"name\": \"test_user\",\n                \"email\": \"test@example.com\",\n                \"plugin_extras\": {\n                    \"my_plugin\": {\n                        \"private_extra\": 1\n                    },\n                    \"another_plugin\": {\n                        \"another_extra\": True\n                    }\n                }\n            }\n    :type plugin_extras: dict\n\n\n    :returns: the newly created user\n    :rtype: dictionary\n\n    '''\n    model = context['model']\n    schema = context.get('schema') or ckan.logic.schema.default_user_schema()\n    session = context['session']\n\n    _check_access('user_create', context, data_dict)\n\n    upload = uploader.get_uploader('user')\n    upload.update_data_dict(data_dict, 'image_url',\n                            'image_upload', 'clear_upload')\n    data, errors = _validate(data_dict, schema, context)\n\n    if errors:\n        session.rollback()\n        raise ValidationError(errors)\n\n    # user schema prevents non-sysadmins from providing password_hash\n    if 'password_hash' in data:\n        data['_password'] = data.pop('password_hash')\n\n    user = model_save.user_dict_save(data, context)\n    plugins.toolkit.signals.user_created.send(user.name, user=user)\n    # Flush the session to cause user.id to be initialised, because\n    # activity_create() (below) needs it.\n    session.flush()\n\n    activity_create_context = {\n        'model': model,\n        'user': context['user'],\n        'defer_commit': True,\n        'ignore_auth': True,\n        'session': session\n    }\n    activity_dict = {\n        'user_id': user.id,\n        'object_id': user.id,\n        'activity_type': 'new user',\n    }\n    logic.get_action('activity_create')(activity_create_context, activity_dict)\n\n    upload.upload(uploader.get_max_image_size())\n\n    if not context.get('defer_commit'):\n        model.repo.commit()\n\n    # A new context is required for dictizing the newly constructed user in\n    # order that all the new user's data is returned, in particular, the\n    # api_key.\n    #\n    # The context is copied so as not to clobber the caller's context dict.\n    user_dictize_context = context.copy()\n    user_dictize_context['keep_apikey'] = True\n    user_dictize_context['keep_email'] = True\n\n    author_obj = model.User.get(context.get('user'))\n    include_plugin_extras = False\n    if author_obj:\n        include_plugin_extras = author_obj.sysadmin and 'plugin_extras' in data\n    user_dict = model_dictize.user_dictize(\n        user, user_dictize_context,\n        include_plugin_extras=include_plugin_extras\n    )\n\n    context['user_obj'] = user\n    context['id'] = user.id\n\n    model.Dashboard.get(user.id)  # Create dashboard for user.\n\n    log.debug('Created user {name}'.format(name=user.name))\n    return user_dict\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteSpaceToDepthParams*>(node->builtin_data);\n\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n\n  auto data_type = output->type;\n  TF_LITE_ENSURE(context,\n                 data_type == kTfLiteFloat32 || data_type == kTfLiteUInt8 ||\n                     data_type == kTfLiteInt8 || data_type == kTfLiteInt32 ||\n                     data_type == kTfLiteInt64);\n  TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n\n  const int block_size = params->block_size;\n  const int input_height = input->dims->data[1];\n  const int input_width = input->dims->data[2];\n  int output_height = input_height / block_size;\n  int output_width = input_width / block_size;\n\n  TF_LITE_ENSURE_EQ(context, input_height, output_height * block_size);\n  TF_LITE_ENSURE_EQ(context, input_width, output_width * block_size);\n\n  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);\n  output_size->data[0] = input->dims->data[0];\n  output_size->data[1] = output_height;\n  output_size->data[2] = output_width;\n  output_size->data[3] = input->dims->data[3] * block_size * block_size;\n\n  return context->ResizeTensor(context, output, output_size);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def pascal_case(value: str) -> str:\n    return stringcase.pascalcase(_sanitize(value))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _is_unsafe(self, val):\n        '''\n        Our helper function, which will also recursively check dict and\n        list entries due to the fact that they may be repr'd and contain\n        a key or value which contains jinja2 syntax and would otherwise\n        lose the AnsibleUnsafe value.\n        '''\n        if isinstance(val, dict):\n            for key in val.keys():\n                if self._is_unsafe(val[key]):\n                    return True\n        elif isinstance(val, list):\n            for item in val:\n                if self._is_unsafe(item):\n                    return True\n        elif isinstance(val, string_types) and hasattr(val, '__UNSAFE__'):\n            return True\n        return False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def download(self, debug_file_id, project):\n        rate_limited = ratelimits.is_limited(\n            project=project,\n            key=f\"rl:DSymFilesEndpoint:download:{debug_file_id}:{project.id}\",\n            limit=10,\n        )\n        if rate_limited:\n            logger.info(\n                \"notification.rate_limited\",\n                extra={\"project_id\": project.id, \"project_debug_file_id\": debug_file_id},\n            )\n            return HttpResponse({\"Too many download requests\"}, status=403)\n\n        debug_file = ProjectDebugFile.objects.filter(id=debug_file_id).first()\n\n        if debug_file is None:\n            raise Http404\n\n        try:\n            fp = debug_file.file.getfile()\n            response = StreamingHttpResponse(\n                iter(lambda: fp.read(4096), b\"\"), content_type=\"application/octet-stream\"\n            )\n            response[\"Content-Length\"] = debug_file.file.size\n            response[\"Content-Disposition\"] = 'attachment; filename=\"{}{}\"'.format(\n                posixpath.basename(debug_file.debug_id),\n                debug_file.file_extension,\n            )\n            return response\n        except OSError:\n            raise Http404\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def finalize_auth(token, identity_type, cookie_dict_extra=None):\n    \"\"\"\n    Finalises login. Validates provided token, sets cookies\n    and redirects to the final page.\n    :param token: token string\n    :param identity_type:  identity_type e.g. x509, userpass, oidc, saml\n    :param cookie_dict_extra: extra cookies to set, dictionary expected\n    :returns: redirects to the final page or renders a page with an error message.\n    \"\"\"\n    global COOKIES\n    valid_token_dict = validate_webui_token(from_cookie=False, session_token=token)\n    if not valid_token_dict:\n        return render_template(\"problem.html\", msg=\"It was not possible to validate and finalize your login with the provided token.\")\n    try:\n        attribs = list_account_attributes(valid_token_dict['account'], valid_token_dict['vo'])\n        accounts = identity.list_accounts_for_identity(valid_token_dict['identity'], identity_type)\n        accvalues = \"\"\n        for acc in accounts:\n            accvalues += acc + \" \"\n        accounts = accvalues[:-1]\n\n        COOKIES.extend([{'key': 'x-rucio-auth-token', 'value': quote(token)},\n                        {'key': 'x-rucio-auth-type', 'value': quote(identity_type)},\n                        {'key': 'rucio-auth-token-created-at', 'value': str(long(time()))},\n                        {'key': 'rucio-available-accounts', 'value': quote(accounts)},\n                        {'key': 'rucio-account-attr', 'value': quote(dumps(attribs))},\n                        {'key': 'rucio-selected-account', 'value': quote(valid_token_dict['account'])},\n                        {'key': 'rucio-selected-vo', 'value': quote(valid_token_dict['vo'])}])\n\n        if cookie_dict_extra:\n            for key, value in cookie_dict_extra.items():\n                COOKIES.append({'key': key, 'value': value})\n        return redirect_to_last_known_url()\n    except Exception:\n        return render_template(\"problem.html\", msg=\"It was not possible to validate and finalize your login with the provided token.\")\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def link_dialog(request):\n    # list of wiki pages\n    name = request.values.get(\"pagename\", \"\")\n    if name:\n        from MoinMoin import search\n        # XXX error handling!\n        searchresult = search.searchPages(request, 't:\"%s\"' % name)\n\n        pages = [p.page_name for p in searchresult.hits]\n        pages.sort()\n        pages[0:0] = [name]\n        page_list = '''\n         <tr>\n          <td colspan=2>\n           <select id=\"sctPagename\" size=\"1\" onchange=\"OnChangePagename(this.value);\">\n           %s\n           </select>\n          <td>\n         </tr>\n''' % \"\\n\".join(['<option value=\"%s\">%s</option>' % (wikiutil.escape(page), wikiutil.escape(page))\n                 for page in pages])\n    else:\n        page_list = \"\"\n\n    # list of interwiki names\n    interwiki_list = wikiutil.load_wikimap(request)\n    interwiki = interwiki_list.keys()\n    interwiki.sort()\n    iwpreferred = request.cfg.interwiki_preferred[:]\n    if not iwpreferred or iwpreferred and iwpreferred[-1] is not None:\n        resultlist = iwpreferred\n        for iw in interwiki:\n            if not iw in iwpreferred:\n                resultlist.append(iw)\n    else:\n        resultlist = iwpreferred[:-1]\n    interwiki = \"\\n\".join(\n        ['<option value=\"%s\">%s</option>' % (wikiutil.escape(key), wikiutil.escape(key))\n         for key in resultlist])\n\n    # wiki url\n    url_prefix_static = request.cfg.url_prefix_static\n    scriptname = request.script_root + '/'\n    action = scriptname\n    basepage = wikiutil.escape(request.page.page_name)\n    request.write(u'''\n<!--\n * FCKeditor - The text editor for internet\n * Copyright (C) 2003-2004 Frederico Caldeira Knabben\n *\n * Licensed under the terms of the GNU Lesser General Public License:\n *   http://www.opensource.org/licenses/lgpl-license.php\n *\n * For further information visit:\n *   http://www.fckeditor.net/\n *\n * File Name: fck_link.html\n *  Link dialog window.\n *\n * Version:  2.0 FC (Preview)\n * Modified: 2005-02-18 23:55:22\n *\n * File Authors:\n *   Frederico Caldeira Knabben (fredck@fckeditor.net)\n-->\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\">\n<meta name=\"robots\" content=\"index,nofollow\">\n<html>\n <head>\n  <title>Link Properties</title>\n  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n  <meta name=\"robots\" content=\"noindex,nofollow\" />\n  <script src=\"%(url_prefix_static)s/applets/FCKeditor/editor/dialog/common/fck_dialog_common.js\" type=\"text/javascript\"></script>\n  <script src=\"%(url_prefix_static)s/applets/moinFCKplugins/moinlink/fck_link.js\" type=\"text/javascript\"></script>\n  <script src=\"%(url_prefix_static)s/applets/moinFCKplugins/moinurllib.js\" type=\"text/javascript\"></script>\n </head>\n <body scroll=\"no\" style=\"OVERFLOW: hidden\">\n  <div id=\"divInfo\" style=\"DISPLAY: none\">\n   <span fckLang=\"DlgLnkType\">Link Type</span><br />\n   <select id=\"cmbLinkType\" onchange=\"SetLinkType(this.value);\">\n    <option value=\"wiki\" selected=\"selected\">WikiPage</option>\n    <option value=\"interwiki\">Interwiki</option>\n    <option value=\"url\" fckLang=\"DlgLnkTypeURL\">URL</option>\n   </select>\n   <br />\n   <br />\n   <div id=\"divLinkTypeWiki\">\n    <table height=\"100%%\" cellSpacing=\"0\" cellPadding=\"0\" width=\"100%%\" border=\"0\">\n     <tr>\n      <td>\n       <form action=%(action)s method=\"GET\">\n       <input type=\"hidden\" name=\"action\" value=\"fckdialog\">\n       <input type=\"hidden\" name=\"dialog\" value=\"link\">\n       <input type=\"hidden\" id=\"basepage\" name=\"basepage\" value=\"%(basepage)s\">\n       <table cellSpacing=\"0\" cellPadding=\"0\" align=\"center\" border=\"0\">\n        <tr>\n         <td>\n          <span fckLang=\"PageDlgName\">Page Name</span><br>\n          <input id=\"txtPagename\" name=\"pagename\" size=\"30\" value=\"%(name)s\">\n         </td>\n         <td valign=\"bottom\">\n           <input id=btnSearchpage type=\"submit\" value=\"Search\">\n         </td>\n        </tr>\n        %(page_list)s\n       </table>\n       </form>\n      </td>\n     </tr>\n    </table>\n   </div>\n   <div id=\"divLinkTypeInterwiki\">\n    <table height=\"100%%\" cellSpacing=\"0\" cellPadding=\"0\" width=\"100%%\" border=\"0\">\n     <tr>\n      <td>\n       <table cellSpacing=\"0\" cellPadding=\"0\" align=\"center\" border=\"0\">\n        <tr>\n         <td>\n          <span fckLang=\"WikiDlgName\">Wiki:PageName</span><br>\n          <select id=\"sctInterwiki\" size=\"1\">\n          %(interwiki)s\n          </select>:\n          <input id=\"txtInterwikipagename\"></input>\n         </td>\n        </tr>\n       </table>\n      </td>\n     </tr>\n    </table>\n   </div>\n   <div id=\"divLinkTypeUrl\">\n    <table cellspacing=\"0\" cellpadding=\"0\" width=\"100%%\" border=\"0\">\n     <tr>\n      <td nowrap=\"nowrap\">\n       <span fckLang=\"DlgLnkProto\">Protocol</span><br />\n       <select id=\"cmbLinkProtocol\">\n        <option value=\"http://\" selected=\"selected\">http://</option>\n        <option value=\"https://\">https://</option>\n        <option value=\"ftp://\">ftp://</option>\n        <option value=\"file://\">file://</option>\n        <option value=\"news://\">news://</option>\n        <option value=\"mailto:\">mailto:</option>\n        <option value=\"\" fckLang=\"DlgLnkProtoOther\">&lt;other&gt;</option>\n       </select>\n      </td>\n      <td nowrap=\"nowrap\">&nbsp;</td>\n      <td nowrap=\"nowrap\" width=\"100%%\">\n       <span fckLang=\"DlgLnkURL\">URL</span><br />\n       <input id=\"txtUrl\" style=\"WIDTH: 100%%\" type=\"text\" onkeyup=\"OnUrlChange();\" onchange=\"OnUrlChange();\" />\n      </td>\n     </tr>\n    </table>\n    <br />\n   </div>\n  </div>\n </body>\n</html>\n''' % locals())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def logout(request):\n    \"\"\" Clears the session and logs the current user out. \"\"\"\n    request.session.clear()\n    # FIXME(gabriel): we don't ship a view named splash\n    return shortcuts.redirect('splash')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def modify_setting(self, setting_name, value, token):\n        try:\n            self.api.settings().set(setting_name, value)\n            return 0\n        except:\n            return 1\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, hs):\n        self.server_name = hs.hostname\n        self.client = hs.get_http_client()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def renderCell(self, item):\n        value = get_value(item, self.attribute, default=())\n\n        if not isinstance(value, (list, tuple)):\n            value = (value,)\n\n        gtool = getToolByName(plone.api.portal.get(), 'portal_groups')\n        mtool = getToolByName(plone.api.portal.get(), 'portal_membership')\n        principals = []\n        for principal_id in value:\n            user = mtool.getMemberById(principal_id)\n            if user is not None:\n                principals.append(user.getProperty('fullname', None) or user.getId())\n            else:\n                group = gtool.getGroupById(principal_id)\n                if group is not None:\n                    principals.append(group.getProperty('title', None) or group.getId())\n\n        return ', '.join(principals).decode('utf-8')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_paths(base_path: pathlib.Path):\n    data_file = pathlib.Path(str(base_path) + \".data\")\n    metadata_file = pathlib.Path(str(base_path) + \".meta\")\n\n    return data_file, metadata_file\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus ScatterNd(const TfLiteTensor* indices, const TfLiteTensor* updates,\n                       TfLiteTensor* output) {\n  reference_ops::ScatterNd(\n      GetTensorShape(indices), GetTensorData<IndicesT>(indices),\n      GetTensorShape(updates), GetTensorData<UpdatesT>(updates),\n      GetTensorShape(output), GetTensorData<UpdatesT>(output));\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& gradient = ctx->input(0);\n    const Tensor& input = ctx->input(1);\n    Tensor* input_backprop = nullptr;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, input.shape(), &input_backprop));\n\n    OP_REQUIRES(\n        ctx, input.IsSameSize(gradient),\n        errors::InvalidArgument(\"gradient and input must be the same size\"));\n    const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n    const Tensor& input_min_tensor = ctx->input(2);\n    OP_REQUIRES(ctx,\n                input_min_tensor.dims() == 0 || input_min_tensor.dims() == 1,\n                errors::InvalidArgument(\n                    \"Input min tensor must have dimension 1. Recieved \",\n                    input_min_tensor.dims(), \".\"));\n    const Tensor& input_max_tensor = ctx->input(3);\n    OP_REQUIRES(ctx,\n                input_max_tensor.dims() == 0 || input_max_tensor.dims() == 1,\n                errors::InvalidArgument(\n                    \"Input max tensor must have dimension 1. Recieved \",\n                    input_max_tensor.dims(), \".\"));\n    if (axis_ != -1) {\n      OP_REQUIRES(\n          ctx, input_min_tensor.dim_size(0) == depth,\n          errors::InvalidArgument(\"min has incorrect size, expected \", depth,\n                                  \" was \", input_min_tensor.dim_size(0)));\n      OP_REQUIRES(\n          ctx, input_max_tensor.dim_size(0) == depth,\n          errors::InvalidArgument(\"max has incorrect size, expected \", depth,\n                                  \" was \", input_max_tensor.dim_size(0)));\n    }\n\n    TensorShape min_max_shape(input_min_tensor.shape());\n    Tensor* input_min_backprop;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(1, min_max_shape, &input_min_backprop));\n\n    Tensor* input_max_backprop;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(2, min_max_shape, &input_max_backprop));\n\n    if (axis_ == -1) {\n      functor::QuantizeAndDequantizeOneScaleGradientFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(), gradient.template flat<T>(),\n        input.template flat<T>(), input_min_tensor.scalar<T>(),\n        input_max_tensor.scalar<T>(), input_backprop->template flat<T>(),\n        input_min_backprop->template scalar<T>(),\n        input_max_backprop->template scalar<T>());\n    } else {\n      functor::QuantizeAndDequantizePerChannelGradientFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(),\n        gradient.template flat_inner_outer_dims<T, 3>(axis_ - 1),\n        input.template flat_inner_outer_dims<T, 3>(axis_ - 1),\n        &input_min_tensor, &input_max_tensor,\n        input_backprop->template flat_inner_outer_dims<T, 3>(axis_ - 1),\n        input_min_backprop->template flat<T>(),\n        input_max_backprop->template flat<T>());\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def class_instances_from_soap_enveloped_saml_thingies(text, modules):\n    \"\"\"Parses a SOAP enveloped header and body SAML thing and returns the\n    thing as a dictionary class instance.\n\n    :param text: The SOAP object as XML\n    :param modules: modules representing xsd schemas\n    :return: The body and headers as class instances\n    \"\"\"\n    try:\n        envelope = ElementTree.fromstring(text)\n    except Exception as exc:\n        raise XmlParseError(\"%s\" % exc)\n\n    assert envelope.tag == '{%s}Envelope' % soapenv.NAMESPACE\n    assert len(envelope) >= 1\n    env = {\"header\": [], \"body\": None}\n\n    for part in envelope:\n        if part.tag == '{%s}Body' % soapenv.NAMESPACE:\n            assert len(part) == 1\n            env[\"body\"] = instanciate_class(part[0], modules)\n        elif part.tag == \"{%s}Header\" % soapenv.NAMESPACE:\n            for item in part:\n                env[\"header\"].append(instanciate_class(item, modules))\n\n    return env\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, filename):\n        \"\"\"\n        Validate the filename by calling get_valid_name() and return a filename\n        to be passed to the save() method.\n        \"\"\"\n        # `filename` may include a path as returned by FileField.upload_to.\n        dirname, filename = os.path.split(filename)\n        return os.path.normpath(os.path.join(dirname, self.get_valid_name(filename)))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const Tensor& filter_sizes = context->input(1);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(filter_sizes.shape()),\n        errors::InvalidArgument(\n            \"Conv2DBackpropFilter: filter_sizes input must be 1-dim, not \",\n            filter_sizes.dims()));\n    TensorShape filter_shape;\n    const int32* filter_sizes_data = filter_sizes.template flat<int32>().data();\n    for (int i = 0; i < filter_sizes.NumElements(); ++i) {\n      OP_REQUIRES(context, filter_sizes_data[i] >= 0,\n                  errors::InvalidArgument(\"Dimension \", i,\n                                          \" of filter_sizes must be >= 0\"));\n      filter_shape.AddDim(filter_sizes_data[i]);\n    }\n    const TensorShape& input_shape = input.shape();\n\n    EXTRACT_AND_VERIFY_DIMENSIONS(\"DepthwiseConv2DBackpropFilter\");\n    Tensor* filter_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {1}, 0, filter_shape, &filter_backprop));\n\n    // If there is nothing to compute, return.\n    if (out_backprop.shape().num_elements() == 0) {\n      return;\n    }\n\n    // If in_depth==1, this operation is just a standard convolution.\n    // Depthwise convolution is a special case of cuDNN's grouped convolution.\n    bool use_cudnn = std::is_same<Device, GPUDevice>::value &&\n                     (in_depth == 1 ||\n                      (use_cudnn_grouped_conv_ &&\n                       (IsCudnnSupportedFilterSize(/*filter_rows=*/filter_rows,\n                                                   /*filter_cols=*/filter_cols,\n                                                   /*in_depth=*/in_depth,\n                                                   /*out_depth=*/out_depth) ||\n                        OpDeterminismRequired())));\n\n    VLOG(2) << \"DepthwiseConv2dNativeBackpropFilter: \"\n            << \" Input: [\" << batch << \", \" << input_rows << \", \" << input_cols\n            << \", \" << in_depth << \"]; Filter: [\" << filter_rows << \", \"\n            << filter_cols << \", \" << in_depth << \", \" << depth_multiplier\n            << \"]; Output: [\" << batch << \", \" << out_rows << \", \" << out_cols\n            << \", \" << out_depth << \"], stride = \" << stride_\n            << \", pad_rows = \" << pad_top << \", pad_cols = \" << pad_left\n            << \", Use cuDNN: \" << use_cudnn;\n\n    if (use_cudnn) {\n      // Reshape from TF depthwise filter to cuDNN grouped convolution filter:\n      //\n      //                  | TensorFlow       | cuDNN\n      // --------------------------------------------------------------------\n      // filter_out_depth | depth_multiplier | depth_multiplier * group_count\n      // filter_in_depth  | in_depth         | in_depth / group_count\n      //\n      // For depthwise convolution, we have group_count == in_depth.\n      int32_t filter_in_depth = 1;\n      TensorShape shape =\n          TensorShape{filter_rows, filter_cols, filter_in_depth, out_depth};\n      Tensor reshaped_filter(/*type=*/dtype_);\n      OP_REQUIRES(\n          context, reshaped_filter.CopyFrom(*filter_backprop, shape),\n          errors::Internal(\n              \"Failed to reshape filter tensor for grouped convolution.\"));\n\n      // TODO(yangzihao): Send in arbitrary dilation rates after the dilated\n      // conv is supported.\n      launcher_(context, /*use_cudnn=*/true, cudnn_use_autotune_, out_backprop,\n                input,\n                /*row_dilation=*/1, /*col_dilation=*/1, stride_, stride_,\n                padding_, explicit_paddings_, &reshaped_filter, data_format_);\n      return;\n    }\n\n    // For GPU inputs with type half, we cast inputs to float and outputs back\n    // to half, as half implementation is slow and does not use full precision\n    // accumulation in some cases.\n    constexpr bool cast_to_float = std::is_same<T, Eigen::half>::value &&\n                                   std::is_same<Device, GPUDevice>::value;\n    using U = typename std::conditional<cast_to_float, float, T>::type;\n    Tensor casted_out_backprop = out_backprop;\n    Tensor casted_input = input;\n    Tensor casted_filter_backprop = *filter_backprop;\n    const Device& device = context->template eigen_device<Device>();\n    if (cast_to_float) {\n      functor::CastFunctor<Device, float, Eigen::half> cast;\n      OP_REQUIRES_OK(context,\n                     context->allocate_temp(DT_FLOAT, out_backprop.shape(),\n                                            &casted_out_backprop));\n      cast(device, casted_out_backprop.template flat<float>(),\n           out_backprop.template flat<Eigen::half>());\n      OP_REQUIRES_OK(context, context->allocate_temp(DT_FLOAT, input.shape(),\n                                                     &casted_input));\n      cast(device, casted_input.template flat<float>(),\n           input.template flat<Eigen::half>());\n      OP_REQUIRES_OK(context,\n                     context->allocate_temp(DT_FLOAT, filter_backprop->shape(),\n                                            &casted_filter_backprop));\n    }\n\n    auto out_backprop_ptr = casted_out_backprop.template flat<U>().data();\n    auto input_ptr = casted_input.template flat<U>().data();\n    auto filter_backprop_ptr = casted_filter_backprop.template flat<U>().data();\n    LaunchDepthwiseConvBackpropFilterOp<Device, U>()(\n        context, args, out_backprop_ptr, input_ptr, filter_backprop_ptr,\n        data_format_);\n\n    if (cast_to_float) {\n      functor::CastFunctor<Device, Eigen::half, float> cast;\n      const Tensor& casted_filter_backprop_const = casted_filter_backprop;\n      cast(device, filter_backprop->template flat<Eigen::half>(),\n           casted_filter_backprop_const.template flat<float>());\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status GetFirstDimensionSize(OpKernelContext* context, INDEX_TYPE* result) {\n    const Tensor first_partition_tensor =\n        context->input(kFirstPartitionInputIndex);\n    const RowPartitionType first_partition_type = row_partition_types_[0];\n    switch (first_partition_type) {\n      case RowPartitionType::FIRST_DIM_SIZE:\n        *result = first_partition_tensor.scalar<INDEX_TYPE>()();\n        return Status::OK();\n      case RowPartitionType::VALUE_ROWIDS:\n        return errors::InvalidArgument(\n            \"Cannot handle VALUE_ROWIDS in first dimension.\");\n      case RowPartitionType::ROW_SPLITS:\n        *result = first_partition_tensor.shape().dim_size(0) - 1;\n        return Status::OK();\n      default:\n        return errors::InvalidArgument(\n            \"Cannot handle type \",\n            RowPartitionTypeToString(first_partition_type));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static cache_accel_t *read_cache_accel(RBuffer *cache_buf, cache_hdr_t *hdr, cache_map_t *maps) {\n\tif (!cache_buf || !hdr || !hdr->accelerateInfoSize || !hdr->accelerateInfoAddr) {\n\t\treturn NULL;\n\t}\n\n\tut64 offset = va2pa (hdr->accelerateInfoAddr, hdr->mappingCount, maps, cache_buf, 0, NULL, NULL);\n\tif (!offset) {\n\t\treturn NULL;\n\t}\n\n\tut64 size = sizeof (cache_accel_t);\n\tcache_accel_t *accel = R_NEW0 (cache_accel_t);\n\tif (!accel) {\n\t\treturn NULL;\n\t}\n\n\tif (r_buf_fread_at (cache_buf, offset, (ut8*) accel, \"16il\", 1) != size) {\n\t\tR_FREE (accel);\n\t\treturn NULL;\n\t}\n\n\taccel->imagesExtrasOffset += offset;\n\taccel->bottomUpListOffset += offset;\n\taccel->dylibTrieOffset += offset;\n\taccel->initializersOffset += offset;\n\taccel->dofSectionsOffset += offset;\n\taccel->reExportListOffset += offset;\n\taccel->depListOffset += offset;\n\taccel->rangeTableOffset += offset;\n\n\treturn accel;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def on_claim_client_keys(\n        self, query: List[Tuple[str, str, str, int]], always_include_fallback_keys: bool\n    ) -> Dict[str, Any]:\n        log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})\n        results = await self._e2e_keys_handler.claim_local_one_time_keys(\n            query, always_include_fallback_keys=always_include_fallback_keys\n        )\n\n        json_result: Dict[str, Dict[str, Dict[str, JsonDict]]] = {}\n        for result in results:\n            for user_id, device_keys in result.items():\n                for device_id, keys in device_keys.items():\n                    for key_id, key in keys.items():\n                        json_result.setdefault(user_id, {}).setdefault(device_id, {})[\n                            key_id\n                        ] = key\n\n        logger.info(\n            \"Claimed one-time-keys: %s\",\n            \",\".join(\n                (\n                    \"%s for %s:%s\" % (key_id, user_id, device_id)\n                    for user_id, user_keys in json_result.items()\n                    for device_id, device_keys in user_keys.items()\n                    for key_id, _ in device_keys.items()\n                )\n            ),\n        )\n\n        return {\"one_time_keys\": json_result}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def title(self):\n        if base_hasattr(self.context, 'get_full_title'):\n            title = self.context.get_full_title()\n        else:\n            title = self.context.Title()\n        title = title and safe_unicode(title) or u\"\"\n        return title\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def save_cover_from_url(url, book_path):\n    try:\n        if not cli.allow_localhost:\n            # 127.0.x.x, localhost, [::1], [::ffff:7f00:1]\n            ip = socket.getaddrinfo(urlparse(url).hostname, 0)[0][4][0]\n            if ip.startswith(\"127.\") or ip.startswith('::ffff:7f') or ip == \"::1\":\n                log.error(\"Localhost was accessed for cover upload\")\n                return False, _(\"You are not allowed to access localhost for cover uploads\")\n        img = requests.get(url, timeout=(10, 200))      # ToDo: Error Handling\n        img.raise_for_status()\n        return save_cover(img, book_path)\n    except (socket.gaierror,\n            requests.exceptions.HTTPError,\n            requests.exceptions.ConnectionError,\n            requests.exceptions.Timeout) as ex:\n        log.info(u'Cover Download Error %s', ex)\n        return False, _(\"Error Downloading Cover\")\n    except MissingDelegateError as ex:\n        log.info(u'File Format Error %s', ex)\n        return False, _(\"Cover Format Error\")\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    // This call processes inputs 1 and 2 to write output 0.\n    ReshapeOp::Compute(ctx);\n\n    const float input_min_float = ctx->input(2).flat<float>()(0);\n    const float input_max_float = ctx->input(3).flat<float>()(0);\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\n    output_min->flat<float>()(0) = input_min_float;\n\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(2, TensorShape({}), &output_max));\n    output_max->flat<float>()(0) = input_max_float;\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void copy_iov_to_host(void* _mem, struct iovec* host_iov, wasi_iovec_t* wasi_iov, int32_t iovs_len)\n{\n    // Convert wasi memory offsets to host addresses\n    for (int i = 0; i < iovs_len; i++) {\n        host_iov[i].iov_base = m3ApiOffsetToPtr(m3ApiReadMem32(&wasi_iov[i].buf));\n        host_iov[i].iov_len  = m3ApiReadMem32(&wasi_iov[i].buf_len);\n    }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_extra(self, select, select_params, where, params, tables, order_by):\n        \"\"\"\n        Add data to the various extra_* attributes for user-created additions\n        to the query.\n        \"\"\"\n        if select:\n            # We need to pair any placeholder markers in the 'select'\n            # dictionary with their parameters in 'select_params' so that\n            # subsequent updates to the select dictionary also adjust the\n            # parameters appropriately.\n            select_pairs = {}\n            if select_params:\n                param_iter = iter(select_params)\n            else:\n                param_iter = iter([])\n            for name, entry in select.items():\n                entry = str(entry)\n                entry_params = []\n                pos = entry.find(\"%s\")\n                while pos != -1:\n                    if pos == 0 or entry[pos - 1] != \"%\":\n                        entry_params.append(next(param_iter))\n                    pos = entry.find(\"%s\", pos + 2)\n                select_pairs[name] = (entry, entry_params)\n            self.extra.update(select_pairs)\n        if where or params:\n            self.where.add(ExtraWhere(where, params), AND)\n        if tables:\n            self.extra_tables += tuple(tables)\n        if order_by:\n            self.extra_order_by = order_by\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def encode(self, types: Iterable[TypeStr], args: Iterable[Any]) -> bytes:\n        \"\"\"\n        Encodes the python values in ``args`` as a sequence of binary values of\n        the ABI types in ``types`` via the head-tail mechanism.\n\n        :param types: A list or tuple of string representations of the ABI types\n            that will be used for encoding e.g.  ``('uint256', 'bytes[]',\n            '(int,int)')``\n        :param args: A list or tuple of python values to be encoded.\n\n        :returns: The head-tail encoded binary representation of the python\n            values in ``args`` as values of the ABI types in ``types``.\n        \"\"\"\n        validate_list_like_param(types, \"types\")\n        validate_list_like_param(args, \"args\")\n\n        encoders = [self._registry.get_encoder(type_str) for type_str in types]\n\n        encoder = TupleEncoder(encoders=encoders)\n\n        return encoder(args)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_header_end(self) -> None:\n        message = (MultiPartMessage.HEADER_END, b\"\")\n        self.messages.append(message)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def write_data(self, data, filename, shred=True):\n        \"\"\"Write the data bytes to given path\n\n        This is used to write a byte string to a file or stdout. It is used for\n        writing the results of vault encryption or decryption. It is used for\n        saving the ciphertext after encryption and it is also used for saving the\n        plaintext after decrypting a vault. The type of the 'data' arg should be bytes,\n        since in the plaintext case, the original contents can be of any text encoding\n        or arbitrary binary data.\n\n        When used to write the result of vault encryption, the val of the 'data' arg\n        should be a utf-8 encoded byte string and not a text typ and not a text type..\n\n        When used to write the result of vault decryption, the val of the 'data' arg\n        should be a byte string and not a text type.\n\n        :arg data: the byte string (bytes) data\n        :arg filename: filename to save 'data' to.\n        :arg shred: if shred==True, make sure that the original data is first shredded so that is cannot be recovered.\n        :returns: None\n        \"\"\"\n        # FIXME: do we need this now? data_bytes should always be a utf-8 byte string\n        b_file_data = to_bytes(data, errors='strict')\n\n        # get a ref to either sys.stdout.buffer for py3 or plain old sys.stdout for py2\n        # We need sys.stdout.buffer on py3 so we can write bytes to it since the plaintext\n        # of the vaulted object could be anything/binary/etc\n        output = getattr(sys.stdout, 'buffer', sys.stdout)\n\n        if filename == '-':\n            output.write(b_file_data)\n        else:\n            if os.path.isfile(filename):\n                if shred:\n                    self._shred_file(filename)\n                else:\n                    os.remove(filename)\n            with open(filename, \"wb\") as fh:\n                fh.write(b_file_data)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __parse_content_stream(self, stream: StreamType) -> None:\n        # 7.8.2 Content Streams\n        stream.seek(0, 0)\n        operands: List[Union[int, str, PdfObject]] = []\n        while True:\n            peek = read_non_whitespace(stream)\n            if peek == b\"\" or peek == 0:\n                break\n            stream.seek(-1, 1)\n            if peek.isalpha() or peek in (b\"'\", b'\"'):\n                operator = read_until_regex(stream, NameObject.delimiter_pattern)\n                if operator == b\"BI\":\n                    # begin inline image - a completely different parsing\n                    # mechanism is required, of course... thanks buddy...\n                    assert operands == []\n                    ii = self._read_inline_image(stream)\n                    self.operations.append((ii, b\"INLINE IMAGE\"))\n                else:\n                    self.operations.append((operands, operator))\n                    operands = []\n            elif peek == b\"%\":\n                # If we encounter a comment in the content stream, we have to\n                # handle it here.  Typically, read_object will handle\n                # encountering a comment -- but read_object assumes that\n                # following the comment must be the object we're trying to\n                # read.  In this case, it could be an operator instead.\n                while peek not in (b\"\\r\", b\"\\n\"):\n                    peek = stream.read(1)\n            else:\n                operands.append(read_object(stream, None, self.forced_encoding))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "int ossl_dh_compute_key(unsigned char *key, const BIGNUM *pub_key, DH *dh)\n{\n    BN_CTX *ctx = NULL;\n    BN_MONT_CTX *mont = NULL;\n    BIGNUM *z = NULL, *pminus1;\n    int ret = -1;\n\n    if (BN_num_bits(dh->params.p) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n        goto err;\n    }\n\n    if (BN_num_bits(dh->params.p) < DH_MIN_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_SMALL);\n        return 0;\n    }\n\n    ctx = BN_CTX_new_ex(dh->libctx);\n    if (ctx == NULL)\n        goto err;\n    BN_CTX_start(ctx);\n    pminus1 = BN_CTX_get(ctx);\n    z = BN_CTX_get(ctx);\n    if (z == NULL)\n        goto err;\n\n    if (dh->priv_key == NULL) {\n        ERR_raise(ERR_LIB_DH, DH_R_NO_PRIVATE_VALUE);\n        goto err;\n    }\n\n    if (dh->flags & DH_FLAG_CACHE_MONT_P) {\n        mont = BN_MONT_CTX_set_locked(&dh->method_mont_p,\n                                      dh->lock, dh->params.p, ctx);\n        BN_set_flags(dh->priv_key, BN_FLG_CONSTTIME);\n        if (!mont)\n            goto err;\n    }\n\n    /* (Step 1) Z = pub_key^priv_key mod p */\n    if (!dh->meth->bn_mod_exp(dh, z, pub_key, dh->priv_key, dh->params.p, ctx,\n                              mont)) {\n        ERR_raise(ERR_LIB_DH, ERR_R_BN_LIB);\n        goto err;\n    }\n\n    /* (Step 2) Error if z <= 1 or z = p - 1 */\n    if (BN_copy(pminus1, dh->params.p) == NULL\n        || !BN_sub_word(pminus1, 1)\n        || BN_cmp(z, BN_value_one()) <= 0\n        || BN_cmp(z, pminus1) == 0) {\n        ERR_raise(ERR_LIB_DH, DH_R_INVALID_SECRET);\n        goto err;\n    }\n\n    /* return the padded key, i.e. same number of bytes as the modulus */\n    ret = BN_bn2binpad(z, key, BN_num_bytes(dh->params.p));\n err:\n    BN_clear(z); /* (Step 2) destroy intermediate values */\n    BN_CTX_end(ctx);\n    BN_CTX_free(ctx);\n    return ret;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_python_instance_state(self, instance, state):\n        if hasattr(instance, '__setstate__'):\n            instance.__setstate__(state)\n        else:\n            slotstate = {}\n            if isinstance(state, tuple) and len(state) == 2:\n                state, slotstate = state\n            if hasattr(instance, '__dict__'):\n                instance.__dict__.update(state)\n            elif state:\n                slotstate.update(state)\n            for key, value in slotstate.items():\n                setattr(instance, key, value)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _init_file(self):\n        \"\"\"Init the password file, set the password for it.\n        \"\"\"\n\n        password = None\n        while 1:\n            if not password:\n                password = self._getpass(\"Please set a password for your new keyring\")\n                password2 = self._getpass('Password (again): ')\n                if password != password2:\n                    sys.stderr.write(\"Error: Your passwords didn't match\\n\")\n                    password = None\n                    continue\n            if '' == password.strip():\n                # forbid the blank password\n                sys.stderr.write(\"Error: blank passwords aren't allowed.\\n\")\n                password = None\n                continue\n            if len(password) > _BLOCK_SIZE:\n                # block size of AES is less than 32\n                sys.stderr.write(\"Error: password can't be longer than 32.\\n\")\n                password = None\n                continue\n            break\n\n        # hash the password\n        import crypt\n        self.crypted_password = crypt.crypt(password, password)\n\n        # write down the initialization\n        config = ConfigParser.RawConfigParser()\n        config.add_section(_KEYRING_SETTING)\n        config.set(_KEYRING_SETTING, _CRYPTED_PASSWORD, self.crypted_password)\n\n        config_file = open(self.file_path,'w')\n        config.write(config_file)\n\n        if config_file:\n            config_file.close()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _download_artifact(artifact_path):\n    \"\"\"\n    A request handler for `GET /mlflow-artifacts/artifacts/<artifact_path>` to download an artifact\n    from `artifact_path` (a relative path from the root artifact directory).\n    \"\"\"\n    validate_path_is_safe(artifact_path)\n    tmp_dir = tempfile.TemporaryDirectory()\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    dst = artifact_repo.download_artifacts(artifact_path, tmp_dir.name)\n\n    # Ref: https://stackoverflow.com/a/24613980/6943581\n    file_handle = open(dst, \"rb\")  # noqa: SIM115\n\n    def stream_and_remove_file():\n        yield from file_handle\n        file_handle.close()\n        tmp_dir.cleanup()\n\n    file_sender_response = current_app.response_class(stream_and_remove_file())\n\n    return _response_with_file_attachment_headers(artifact_path, file_sender_response)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void DoCompute(const ComputeOptions& options, OpKernelContext* const context) {\n  ModelWeights model_weights;\n  OP_REQUIRES_OK(context, model_weights.Initialize(context));\n\n  Examples examples;\n  OP_REQUIRES_OK(\n      context,\n      examples.Initialize(context, model_weights, options.num_sparse_features,\n                          options.num_sparse_features_with_values,\n                          options.num_dense_features));\n\n  const Tensor* example_state_data_t;\n  OP_REQUIRES_OK(context,\n                 context->input(\"example_state_data\", &example_state_data_t));\n  TensorShape expected_example_state_shape({examples.num_examples(), 4});\n  OP_REQUIRES(context,\n              example_state_data_t->shape() == expected_example_state_shape,\n              errors::InvalidArgument(\n                  \"Expected shape \", expected_example_state_shape.DebugString(),\n                  \" for example_state_data, got \",\n                  example_state_data_t->shape().DebugString()));\n\n  Tensor mutable_example_state_data_t(*example_state_data_t);\n  auto example_state_data = mutable_example_state_data_t.matrix<float>();\n  OP_REQUIRES_OK(context, context->set_output(\"out_example_state_data\",\n                                              mutable_example_state_data_t));\n\n  if (options.adaptive) {\n    OP_REQUIRES_OK(context,\n                   examples.SampleAdaptiveProbabilities(\n                       options.num_loss_partitions, options.regularizations,\n                       model_weights, example_state_data, options.loss_updater,\n                       /*num_weight_vectors =*/1));\n  } else {\n    examples.RandomShuffle();\n  }\n  struct {\n    mutex mu;\n    Status value TF_GUARDED_BY(mu);\n  } train_step_status;\n  std::atomic<std::int64_t> atomic_index(-1);\n  auto train_step = [&](const int64_t begin, const int64_t end) {\n    // The static_cast here is safe since begin and end can be at most\n    // num_examples which is an int.\n    for (int id = static_cast<int>(begin); id < end; ++id) {\n      const int64_t example_index = examples.sampled_index(++atomic_index);\n      const Example& example = examples.example(example_index);\n      const float dual = example_state_data(example_index, 0);\n      const float example_weight = example.example_weight();\n      float example_label = example.example_label();\n      const Status conversion_status =\n          options.loss_updater->ConvertLabel(&example_label);\n      if (!conversion_status.ok()) {\n        mutex_lock l(train_step_status.mu);\n        train_step_status.value = conversion_status;\n        // Return from this worker thread - the calling thread is\n        // responsible for checking context status and returning on error.\n        return;\n      }\n\n      // Compute wx, example norm weighted by regularization, dual loss,\n      // primal loss.\n      // For binary SDCA, num_weight_vectors should be one.\n      const ExampleStatistics example_statistics =\n          example.ComputeWxAndWeightedExampleNorm(\n              options.num_loss_partitions, model_weights,\n              options.regularizations, 1 /* num_weight_vectors */);\n\n      const double new_dual = options.loss_updater->ComputeUpdatedDual(\n          options.num_loss_partitions, example_label, example_weight, dual,\n          example_statistics.wx[0], example_statistics.normalized_squared_norm);\n\n      // Compute new weights.\n      const double normalized_bounded_dual_delta =\n          (new_dual - dual) * example_weight /\n          options.regularizations.symmetric_l2();\n      model_weights.UpdateDeltaWeights(\n          context->eigen_cpu_device(), example,\n          std::vector<double>{normalized_bounded_dual_delta});\n\n      // Update example data.\n      example_state_data(example_index, 0) = new_dual;\n      example_state_data(example_index, 1) =\n          options.loss_updater->ComputePrimalLoss(\n              example_statistics.prev_wx[0], example_label, example_weight);\n      example_state_data(example_index, 2) =\n          options.loss_updater->ComputeDualLoss(dual, example_label,\n                                                example_weight);\n      example_state_data(example_index, 3) = example_weight;\n    }\n  };\n  // TODO(sibyl-Aix6ihai): Tune this properly based on sparsity of the data,\n  // number of cpus, and cost per example.\n  const int64_t kCostPerUnit = examples.num_features();\n  const DeviceBase::CpuWorkerThreads& worker_threads =\n      *context->device()->tensorflow_cpu_worker_threads();\n\n  Shard(worker_threads.num_threads, worker_threads.workers,\n        examples.num_examples(), kCostPerUnit, train_step);\n  mutex_lock l(train_step_status.mu);\n  OP_REQUIRES_OK(context, train_step_status.value);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def trim_punctuation(lead, middle, trail):\n        \"\"\"\n        Trim trailing and wrapping punctuation from `middle`. Return the items\n        of the new state.\n        \"\"\"\n        # Continue trimming until middle remains unchanged.\n        trimmed_something = True\n        while trimmed_something:\n            trimmed_something = False\n\n            # Trim trailing punctuation.\n            match = TRAILING_PUNCTUATION_RE.match(middle)\n            if match:\n                middle = match.group(1)\n                trail = match.group(2) + trail\n                trimmed_something = True\n\n            # Trim wrapping punctuation.\n            for opening, closing in WRAPPING_PUNCTUATION:\n                if middle.startswith(opening):\n                    middle = middle[len(opening):]\n                    lead += opening\n                    trimmed_something = True\n                # Keep parentheses at the end only if they're balanced.\n                if (middle.endswith(closing) and\n                        middle.count(closing) == middle.count(opening) + 1):\n                    middle = middle[:-len(closing)]\n                    trail = closing + trail\n                    trimmed_something = True\n        return lead, middle, trail\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _write_private_key_file(self, filename, key, format, password=None):\n        \"\"\"\n        Write an SSH2-format private key file in a form that can be read by\n        paramiko or openssh.  If no password is given, the key is written in\n        a trivially-encoded format (base64) which is completely insecure.  If\n        a password is given, DES-EDE3-CBC is used.\n\n        :param str tag:\n            ``\"RSA\"`` or ``\"DSA\"``, the tag used to mark the data block.\n        :param filename: name of the file to write.\n        :param str data: data blob that makes up the private key.\n        :param str password: an optional password to use to encrypt the file.\n\n        :raises: ``IOError`` -- if there was an error writing the file.\n        \"\"\"\n        with open(filename, \"w\") as f:\n            os.chmod(filename, o600)\n            self._write_private_key(f, key, format, password=password)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _property_resolver(arg):\n    \"\"\"\n    When arg is convertible to float, behave like operator.itemgetter(arg)\n    Otherwise, behave like Variable(arg).resolve\n\n    >>> _property_resolver(1)('abc')\n    'b'\n    >>> _property_resolver('1')('abc')\n    Traceback (most recent call last):\n    ...\n    TypeError: string indices must be integers\n    >>> class Foo:\n    ...     a = 42\n    ...     b = 3.14\n    ...     c = 'Hey!'\n    >>> _property_resolver('b')(Foo())\n    3.14\n    \"\"\"\n    try:\n        float(arg)\n    except ValueError:\n        return Variable(arg).resolve\n    else:\n        return itemgetter(arg)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def callback(_, pdu, redacted):\n            with logcontext.PreserveLoggingContext(ctx):\n                if not check_event_content_hash(pdu):\n                    logger.warn(\n                        \"Event content has been tampered, redacting %s: %s\",\n                        pdu.event_id, pdu.get_pdu_json()\n                    )\n                    return redacted\n\n                if self.spam_checker.check_event_for_spam(pdu):\n                    logger.warn(\n                        \"Event contains spam, redacting %s: %s\",\n                        pdu.event_id, pdu.get_pdu_json()\n                    )\n                    return redacted\n\n                return pdu\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    PartialTensorShape element_shape;\n    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(0), &element_shape));\n    int32 num_elements = c->input(1).scalar<int32>()();\n    TensorList output;\n    output.element_shape = element_shape;\n    output.element_dtype = element_dtype_;\n    output.tensors().resize(num_elements, Tensor(DT_INVALID));\n    Tensor* result;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape{}, &result, attr));\n    result->scalar<Variant>()() = std::move(output);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse(self):\n        \"\"\"\n        Parse the POST data and break it into a FILES MultiValueDict and a POST\n        MultiValueDict.\n\n        Return a tuple containing the POST and FILES dictionary, respectively.\n        \"\"\"\n        from django.http import QueryDict\n\n        encoding = self._encoding\n        handlers = self._upload_handlers\n\n        # HTTP spec says that Content-Length >= 0 is valid\n        # handling content-length == 0 before continuing\n        if self._content_length == 0:\n            return QueryDict(encoding=self._encoding), MultiValueDict()\n\n        # See if any of the handlers take care of the parsing.\n        # This allows overriding everything if need be.\n        for handler in handlers:\n            result = handler.handle_raw_input(\n                self._input_data,\n                self._meta,\n                self._content_length,\n                self._boundary,\n                encoding,\n            )\n            # Check to see if it was handled\n            if result is not None:\n                return result[0], result[1]\n\n        # Create the data structures to be used later.\n        self._post = QueryDict(mutable=True)\n        self._files = MultiValueDict()\n\n        # Instantiate the parser and stream:\n        stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n        # Whether or not to signal a file-completion at the beginning of the loop.\n        old_field_name = None\n        counters = [0] * len(handlers)\n\n        # Number of bytes that have been read.\n        num_bytes_read = 0\n        # To count the number of keys in the request.\n        num_post_keys = 0\n        # To limit the amount of data read from the request.\n        read_size = None\n\n        try:\n            for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                if old_field_name:\n                    # We run this at the beginning of the next loop\n                    # since we cannot be sure a file is complete until\n                    # we hit the next boundary/part of the multipart content.\n                    self.handle_file_complete(old_field_name, counters)\n                    old_field_name = None\n\n                try:\n                    disposition = meta_data['content-disposition'][1]\n                    field_name = disposition['name'].strip()\n                except (KeyError, IndexError, AttributeError):\n                    continue\n\n                transfer_encoding = meta_data.get('content-transfer-encoding')\n                if transfer_encoding is not None:\n                    transfer_encoding = transfer_encoding[0].strip()\n                field_name = force_text(field_name, encoding, errors='replace')\n\n                if item_type == FIELD:\n                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                    num_post_keys += 1\n                    if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and\n                            settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):\n                        raise TooManyFieldsSent(\n                            'The number of GET/POST parameters exceeded '\n                            'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'\n                        )\n\n                    # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                    if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                        read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n\n                    # This is a post field, we can just set it in the post\n                    if transfer_encoding == 'base64':\n                        raw_data = field_stream.read(size=read_size)\n                        num_bytes_read += len(raw_data)\n                        try:\n                            data = base64.b64decode(raw_data)\n                        except binascii.Error:\n                            data = raw_data\n                    else:\n                        data = field_stream.read(size=read_size)\n                        num_bytes_read += len(data)\n\n                    # Add two here to make the check consistent with the\n                    # x-www-form-urlencoded check that includes '&='.\n                    num_bytes_read += len(field_name) + 2\n                    if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and\n                            num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):\n                        raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')\n\n                    self._post.appendlist(field_name, force_text(data, encoding, errors='replace'))\n                elif item_type == FILE:\n                    # This is a file, use the handler...\n                    file_name = disposition.get('filename')\n                    if file_name:\n                        file_name = force_text(file_name, encoding, errors='replace')\n                        file_name = self.sanitize_file_name(file_name)\n                    if not file_name:\n                        continue\n\n                    content_type, content_type_extra = meta_data.get('content-type', ('', {}))\n                    content_type = content_type.strip()\n                    charset = content_type_extra.get('charset')\n\n                    try:\n                        content_length = int(meta_data.get('content-length')[0])\n                    except (IndexError, TypeError, ValueError):\n                        content_length = None\n\n                    counters = [0] * len(handlers)\n                    try:\n                        for handler in handlers:\n                            try:\n                                handler.new_file(\n                                    field_name, file_name, content_type,\n                                    content_length, charset, content_type_extra,\n                                )\n                            except StopFutureHandlers:\n                                break\n\n                        for chunk in field_stream:\n                            if transfer_encoding == 'base64':\n                                # We only special-case base64 transfer encoding\n                                # We should always decode base64 chunks by multiple of 4,\n                                # ignoring whitespace.\n\n                                stripped_chunk = b\"\".join(chunk.split())\n\n                                remaining = len(stripped_chunk) % 4\n                                while remaining != 0:\n                                    over_chunk = field_stream.read(4 - remaining)\n                                    stripped_chunk += b\"\".join(over_chunk.split())\n                                    remaining = len(stripped_chunk) % 4\n\n                                try:\n                                    chunk = base64.b64decode(stripped_chunk)\n                                except Exception as exc:\n                                    # Since this is only a chunk, any error is an unfixable error.\n                                    raise MultiPartParserError(\"Could not decode base64 data.\") from exc\n\n                            for i, handler in enumerate(handlers):\n                                chunk_length = len(chunk)\n                                chunk = handler.receive_data_chunk(chunk, counters[i])\n                                counters[i] += chunk_length\n                                if chunk is None:\n                                    # Don't continue if the chunk received by\n                                    # the handler is None.\n                                    break\n\n                    except SkipFile:\n                        self._close_files()\n                        # Just use up the rest of this file...\n                        exhaust(field_stream)\n                    else:\n                        # Handle file upload completions on next iteration.\n                        old_field_name = field_name\n                else:\n                    # If this is neither a FIELD or a FILE, just exhaust the stream.\n                    exhaust(stream)\n        except StopUpload as e:\n            self._close_files()\n            if not e.connection_reset:\n                exhaust(self._input_data)\n        else:\n            # Make sure that the request data is all fed\n            exhaust(self._input_data)\n\n        # Signal that the upload has completed.\n        # any() shortcircuits if a handler's upload_complete() returns a value.\n        any(handler.upload_complete() for handler in handlers)\n        self._post._mutable = False\n        return self._post, self._files\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def getbbox(self, text, *args, **kwargs):\n        \"\"\"\n        Returns bounding box (in pixels) of given text.\n\n        .. versionadded:: 9.2.0\n\n        :param text: Text to render.\n        :param mode: Used by some graphics drivers to indicate what mode the\n                     driver prefers; if empty, the renderer may return either\n                     mode. Note that the mode is always a string, to simplify\n                     C-level implementations.\n\n        :return: ``(left, top, right, bottom)`` bounding box\n        \"\"\"\n        width, height = self.font.getsize(text)\n        return 0, 0, width, height\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def read_other_paths(request: Request) -> Response:\n    \"\"\"\n    Return related frontend files. Adapted from https://github.com/tiangolo/fastapi/issues/130\n    \"\"\"\n    # check first if requested file exists (for frontend assets)\n    path = request.path_params[\"catchall\"]\n\n    # search for matching route in package (i.e. /dataset)\n    ui_file = match_route(get_ui_file_map(), path)\n\n    # if not, check if the requested file is an asset (i.e. /_next/static/...)\n    if not ui_file:\n        ui_file = get_path_to_admin_ui_file(path)\n\n    # Serve up the file as long as it is within the UI directory\n    if ui_file and ui_file.is_file():\n        if not path_is_in_ui_directory(ui_file):\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND, detail=\"Item not found\"\n            )\n        logger.debug(\n            \"catchall request path '{}' matched static admin UI file: {}\",\n            path,\n            ui_file,\n        )\n        return FileResponse(ui_file)\n\n    # raise 404 for anything that should be backend endpoint but we can't find it\n    if path.startswith(API_PREFIX[1:]):\n        logger.debug(\n            \"catchall request path '{}' matched an invalid API route, return 404\",\n            path,\n        )\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Item not found\"\n        )\n\n    # otherwise return the index\n    logger.debug(\n        \"catchall request path '{}' did not match any admin UI routes, return generic admin UI index\",\n        path,\n    )\n    return get_admin_index_as_response()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _recv_packet(self) -> bool:\n        \"\"\"Receive the remainder of an SSH packet and process it\"\"\"\n\n        rem = 4 + self._pktlen + self._recv_macsize - self._recv_blocksize\n        if len(self._inpbuf) < rem:\n            return False\n\n        seq = self._recv_seq\n        rest = self._inpbuf[:rem-self._recv_macsize]\n        mac = self._inpbuf[rem-self._recv_macsize:rem]\n\n        if self._recv_encryption:\n            packet_data = self._recv_encryption.decrypt_packet(\n                seq, self._packet, rest, 4, mac)\n\n            if not packet_data:\n                raise MACError('MAC verification failed')\n        else:\n            packet_data = self._packet[4:] + rest\n\n        self._inpbuf = self._inpbuf[rem:]\n        self._packet = b''\n\n        orig_payload = packet_data[1:-packet_data[0]]\n\n        if self._decompressor and (self._auth_complete or\n                                   not self._decompress_after_auth):\n            payload = self._decompressor.decompress(orig_payload)\n\n            if payload is None:\n                raise CompressionError('Decompression failed')\n        else:\n            payload = orig_payload\n\n        packet = SSHPacket(payload)\n        pkttype = packet.get_byte()\n        handler: SSHPacketHandler = self\n        skip_reason = ''\n        exc_reason = ''\n\n        if self._kex and MSG_KEX_FIRST <= pkttype <= MSG_KEX_LAST:\n            if self._ignore_first_kex: # pragma: no cover\n                skip_reason = 'ignored first kex'\n                self._ignore_first_kex = False\n            else:\n                handler = self._kex\n        elif (self._auth and\n              MSG_USERAUTH_FIRST <= pkttype <= MSG_USERAUTH_LAST):\n            handler = self._auth\n        elif pkttype > MSG_USERAUTH_LAST and not self._auth_complete:\n            skip_reason = 'invalid request before auth complete'\n            exc_reason = 'Invalid request before authentication was complete'\n        elif MSG_CHANNEL_FIRST <= pkttype <= MSG_CHANNEL_LAST:\n            try:\n                recv_chan = packet.get_uint32()\n            except PacketDecodeError:\n                skip_reason = 'incomplete channel request'\n                exc_reason = 'Incomplete channel request received'\n            else:\n                try:\n                    handler = self._channels[recv_chan]\n                except KeyError:\n                    skip_reason = 'invalid channel number'\n                    exc_reason = 'Invalid channel number %d ' \\\n                                 'received' % recv_chan\n\n        handler.log_received_packet(pkttype, seq, packet, skip_reason)\n\n        if not skip_reason:\n            try:\n                processed = handler.process_packet(pkttype, seq, packet)\n            except PacketDecodeError as exc:\n                raise ProtocolError(str(exc)) from None\n\n            if not processed:\n                self.logger.debug1('Unknown packet type %d received', pkttype)\n                self.send_packet(MSG_UNIMPLEMENTED, UInt32(seq))\n\n        if exc_reason:\n            raise ProtocolError(exc_reason)\n\n        if self._transport:\n            self._recv_seq = (seq + 1) & 0xffffffff\n            self._recv_handler = self._recv_pkthdr\n\n        return True\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def visit_Attribute(self, node):\n        \"\"\"Checks and mutates attribute access/assignment.\n\n        'a.b' becomes '_getattr_(a, \"b\")'\n        'a.b = c' becomes '_write_(a).b = c'\n        'del a.b' becomes 'del _write_(a).b'\n\n        The _write_ function should return a security proxy.\n        \"\"\"\n        if node.attr.startswith('_') and node.attr != '_':\n            self.error(\n                node,\n                '\"{name}\" is an invalid attribute name because it starts '\n                'with \"_\".'.format(name=node.attr))\n\n        if node.attr.endswith('__roles__'):\n            self.error(\n                node,\n                '\"{name}\" is an invalid attribute name because it ends '\n                'with \"__roles__\".'.format(name=node.attr))\n\n        if isinstance(node.ctx, ast.Load):\n            node = self.node_contents_visit(node)\n            new_node = ast.Call(\n                func=ast.Name('_getattr_', ast.Load()),\n                args=[node.value, ast.Str(node.attr)],\n                keywords=[])\n\n            copy_locations(new_node, node)\n            return new_node\n\n        elif isinstance(node.ctx, (ast.Store, ast.Del)):\n            node = self.node_contents_visit(node)\n            new_value = ast.Call(\n                func=ast.Name('_write_', ast.Load()),\n                args=[node.value],\n                keywords=[])\n\n            copy_locations(new_value, node.value)\n            node.value = new_value\n            return node\n\n        else:  # pragma: no cover\n            # Impossible Case only ctx Load, Store and Del are defined in ast.\n            raise NotImplementedError(\n                f\"Unknown ctx type: {type(node.ctx)}\")\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse_message(self, lines: List[bytes]) -> RawRequestMessage:\n        # request line\n        line = lines[0].decode(\"utf-8\", \"surrogateescape\")\n        try:\n            method, path, version = line.split(None, 2)\n        except ValueError:\n            raise BadStatusLine(line) from None\n\n        if len(path) > self.max_line_size:\n            raise LineTooLong(\n                \"Status line is too long\", str(self.max_line_size), str(len(path))\n            )\n\n        # method\n        if not METHRE.match(method):\n            raise BadStatusLine(method)\n\n        # version\n        try:\n            if version.startswith(\"HTTP/\"):\n                n1, n2 = version[5:].split(\".\", 1)\n                version_o = HttpVersion(int(n1), int(n2))\n            else:\n                raise BadStatusLine(version)\n        except Exception:\n            raise BadStatusLine(version)\n\n        if method == \"CONNECT\":\n            # authority-form,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.3\n            url = URL.build(authority=path, encoded=True)\n        elif path.startswith(\"/\"):\n            # origin-form,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.1\n            path_part, _hash_separator, url_fragment = path.partition(\"#\")\n            path_part, _question_mark_separator, qs_part = path_part.partition(\"?\")\n\n            # NOTE: `yarl.URL.build()` is used to mimic what the Cython-based\n            # NOTE: parser does, otherwise it results into the same\n            # NOTE: HTTP Request-Line input producing different\n            # NOTE: `yarl.URL()` objects\n            url = URL.build(\n                path=path_part,\n                query_string=qs_part,\n                fragment=url_fragment,\n                encoded=True,\n            )\n        else:\n            # absolute-form for proxy maybe,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.2\n            url = URL(path, encoded=True)\n\n        # read headers\n        (\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        ) = self.parse_headers(lines)\n\n        if close is None:  # then the headers weren't set in the request\n            if version_o <= HttpVersion10:  # HTTP 1.0 must asks to not close\n                close = True\n            else:  # HTTP 1.1 must ask to close.\n                close = False\n\n        return RawRequestMessage(\n            method,\n            path,\n            version_o,\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n            url,\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def has_access(permissions: Sequence[tuple[str, str]] | None = None) -> Callable[[T], T]:\n    \"\"\"Factory for decorator that checks current user's permissions against required permissions.\"\"\"\n\n    def requires_access_decorator(func: T):\n        @wraps(func)\n        def decorated(*args, **kwargs):\n            __tracebackhide__ = True  # Hide from pytest traceback.\n\n            appbuilder = current_app.appbuilder\n\n            dag_id = (\n                kwargs.get(\"dag_id\")\n                or request.args.get(\"dag_id\")\n                or request.form.get(\"dag_id\")\n                or (request.is_json and request.json.get(\"dag_id\"))\n                or None\n            )\n            if appbuilder.sm.check_authorization(permissions, dag_id):\n                return func(*args, **kwargs)\n            elif get_auth_manager().is_logged_in() and not g.user.perms:\n                return (\n                    render_template(\n                        \"airflow/no_roles_permissions.html\",\n                        hostname=get_hostname()\n                        if conf.getboolean(\"webserver\", \"EXPOSE_HOSTNAME\")\n                        else \"redact\",\n                        logout_url=get_auth_manager().get_url_logout(),\n                    ),\n                    403,\n                )\n            else:\n                access_denied = get_access_denied_message()\n                flash(access_denied, \"danger\")\n            return redirect(get_auth_manager().get_url_login(next=request.url))\n\n        return cast(T, decorated)\n\n    return requires_access_decorator\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_part_begin(self) -> None:\n        message = (MultiPartMessage.PART_BEGIN, b\"\")\n        self.messages.append(message)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void pointZZ_pDouble(PointZZ_p * rop, const PointZZ_p * op, const CurveZZ_p * curve) {\n    mpz_t numer, denom, lambda;\n    mpz_inits(numer, denom, lambda, NULL);\n\n    // calculate lambda\n    mpz_mul(numer, op->x, op->x);\n    mpz_mul_ui(numer, numer, 3);\n    mpz_add(numer, numer, curve->a);\n    mpz_mul_ui(denom, op->y, 2);\n    mpz_invert(denom, denom, curve->p);  // TODO check status\n    mpz_mul(lambda, numer, denom);\n    mpz_mod(lambda, lambda, curve->p);\n\n    // calculate resulting x coord\n    mpz_mul(rop->x, lambda, lambda);\n    mpz_sub(rop->x, rop->x, op->x);\n    mpz_sub(rop->x, rop->x, op->x);\n    mpz_mod(rop->x, rop->x, curve->p);\n\n    //calculate resulting y coord\n    mpz_sub(rop->y, op->x, rop->x);\n    mpz_mul(rop->y, lambda, rop->y);\n    mpz_sub(rop->y, rop->y, op->y);\n    mpz_mod(rop->y, rop->y, curve->p);\n\n    mpz_clears(numer, denom, lambda, NULL);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def group_detail(group_id):\n    group = Group.query.get_or_404(group_id)\n    submit = map(lambda p: p.division,\n            filter(lambda p: p.permission == PermissionType.submit,\n                group.permissions))\n    review = map(lambda p: p.division,\n            filter(lambda p: p.permission == PermissionType.review,\n                group.permissions))\n    pay = map(lambda p: p.division,\n            filter(lambda p: p.permission == PermissionType.pay,\n                group.permissions))\n    resp = {\n        u'name': group.name,\n        u'users': list(group.users),\n        u'divisions': {\n            u'submit': list(set(submit)),\n            u'review': list(set(review)),\n            u'pay': list(set(pay)),\n        },\n    }\n    return jsonify(**resp)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def perform_parsing(self):\n        try:\n            return yaml.load(self.content)\n        except yaml.error.YAMLError:\n            raise ResponseParseError(self._error_message, self.content)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _save(self, name, content):\n        full_path = self.path(name)\n\n        # Create any intermediate directories that do not exist.\n        directory = os.path.dirname(full_path)\n        try:\n            if self.directory_permissions_mode is not None:\n                # Set the umask because os.makedirs() doesn't apply the \"mode\"\n                # argument to intermediate-level directories.\n                old_umask = os.umask(0o777 & ~self.directory_permissions_mode)\n                try:\n                    os.makedirs(directory, self.directory_permissions_mode, exist_ok=True)\n                finally:\n                    os.umask(old_umask)\n            else:\n                os.makedirs(directory, exist_ok=True)\n        except FileExistsError:\n            raise FileExistsError('%s exists and is not a directory.' % directory)\n\n        # There's a potential race condition between get_available_name and\n        # saving the file; it's possible that two threads might return the\n        # same name, at which point all sorts of fun happens. So we need to\n        # try to create the file, but if it already exists we have to go back\n        # to get_available_name() and try again.\n\n        while True:\n            try:\n                # This file has a file path that we can move.\n                if hasattr(content, 'temporary_file_path'):\n                    file_move_safe(content.temporary_file_path(), full_path)\n\n                # This is a normal uploadedfile that we can stream.\n                else:\n                    # The current umask value is masked out by os.open!\n                    fd = os.open(full_path, self.OS_OPEN_FLAGS, 0o666)\n                    _file = None\n                    try:\n                        locks.lock(fd, locks.LOCK_EX)\n                        for chunk in content.chunks():\n                            if _file is None:\n                                mode = 'wb' if isinstance(chunk, bytes) else 'wt'\n                                _file = os.fdopen(fd, mode)\n                            _file.write(chunk)\n                    finally:\n                        locks.unlock(fd)\n                        if _file is not None:\n                            _file.close()\n                        else:\n                            os.close(fd)\n            except FileExistsError:\n                # A new name is needed if the file exists.\n                name = self.get_available_name(name)\n                full_path = self.path(name)\n            else:\n                # OK, the file save worked. Break out of the loop.\n                break\n\n        if self.file_permissions_mode is not None:\n            os.chmod(full_path, self.file_permissions_mode)\n\n        # Store filenames with forward slashes, even on Windows.\n        return str(name).replace('\\\\', '/')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_params(self, context, cluster_template, cluster, **kwargs):\n        osc = self.get_osc(context)\n\n        extra_params = kwargs.pop('extra_params', {})\n        extra_params['trustee_domain_id'] = osc.keystone().trustee_domain_id\n        extra_params['trustee_user_id'] = cluster.trustee_user_id\n        extra_params['trustee_username'] = cluster.trustee_username\n        extra_params['trustee_password'] = cluster.trustee_password\n        extra_params['trust_id'] = cluster.trust_id\n        extra_params['auth_url'] = context.auth_url\n\n        return super(BaseTemplateDefinition,\n                     self).get_params(context, cluster_template, cluster,\n                                      extra_params=extra_params,\n                                      **kwargs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def send(self, anyway=False, prefetch=False):\n        \"\"\"Sends the request. Returns True of successful, False if not.\n        If there was an HTTPError during transmission,\n        self.response.status_code will contain the HTTPError code.\n\n        Once a request is successfully sent, `sent` will equal True.\n\n        :param anyway: If True, request will be sent, even if it has\n        already been sent.\n        \"\"\"\n\n        # Build the URL\n        url = self.full_url\n\n        # Logging\n        if self.config.get('verbose'):\n            self.config.get('verbose').write('%s   %s   %s\\n' % (\n                datetime.now().isoformat(), self.method, url\n            ))\n\n        # Nottin' on you.\n        body = None\n        content_type = None\n\n        # Multi-part file uploads.\n        if self.files:\n            if not isinstance(self.data, str):\n\n                try:\n                    fields = self.data.copy()\n                except AttributeError:\n                    fields = dict(self.data)\n\n                for (k, v) in list(self.files.items()):\n                    # support for explicit filename\n                    if isinstance(v, (tuple, list)):\n                        fn, fp = v\n                    else:\n                        fn = guess_filename(v) or k\n                        fp = v\n                    fields.update({k: (fn, fp.read())})\n\n                (body, content_type) = encode_multipart_formdata(fields)\n            else:\n                pass\n                # TODO: Conflict?\n        else:\n            if self.data:\n\n                body = self._enc_data\n                if isinstance(self.data, str):\n                    content_type = None\n                else:\n                    content_type = 'application/x-www-form-urlencoded'\n\n        # Add content-type if it wasn't explicitly provided.\n        if (content_type) and (not 'content-type' in self.headers):\n            self.headers['Content-Type'] = content_type\n\n        # Use .netrc auth if none was provided.\n        if not self.auth and self.config.get('trust_env'):\n            self.auth = get_netrc_auth(url)\n\n        if self.auth:\n            if isinstance(self.auth, tuple) and len(self.auth) == 2:\n                # special-case basic HTTP auth\n                self.auth = HTTPBasicAuth(*self.auth)\n\n            # Allow auth to make its changes.\n            r = self.auth(self)\n\n            # Update self to reflect the auth changes.\n            self.__dict__.update(r.__dict__)\n\n        _p = urlparse(url)\n        proxy = self.proxies.get(_p.scheme)\n\n        if proxy:\n            conn = poolmanager.proxy_from_url(proxy)\n            _proxy = urlparse(proxy)\n            if '@' in _proxy.netloc:\n                auth, url = _proxy.netloc.split('@', 1)\n                self.proxy_auth = HTTPProxyAuth(*auth.split(':', 1))\n                r = self.proxy_auth(self)\n                self.__dict__.update(r.__dict__)\n        else:\n            # Check to see if keep_alive is allowed.\n            if self.config.get('keep_alive'):\n                conn = self._poolmanager.connection_from_url(url)\n            else:\n                conn = connectionpool.connection_from_url(url)\n\n        if url.startswith('https') and self.verify:\n\n            cert_loc = None\n\n            # Allow self-specified cert location.\n            if self.verify is not True:\n                cert_loc = self.verify\n\n            # Look for configuration.\n            if not cert_loc and self.config.get('trust_env'):\n                cert_loc = os.environ.get('REQUESTS_CA_BUNDLE')\n\n            # Curl compatiblity.\n            if not cert_loc and self.config.get('trust_env'):\n                cert_loc = os.environ.get('CURL_CA_BUNDLE')\n\n            # Use the awesome certifi list.\n            if not cert_loc:\n                cert_loc = __import__('certifi').where()\n\n            conn.cert_reqs = 'CERT_REQUIRED'\n            conn.ca_certs = cert_loc\n        else:\n            conn.cert_reqs = 'CERT_NONE'\n            conn.ca_certs = None\n\n        if self.cert and self.verify:\n            if len(self.cert) == 2:\n                conn.cert_file = self.cert[0]\n                conn.key_file = self.cert[1]\n            else:\n                conn.cert_file = self.cert\n\n        if not self.sent or anyway:\n\n            if self.cookies:\n\n                # Skip if 'cookie' header is explicitly set.\n                if 'cookie' not in self.headers:\n\n                    # Simple cookie with our dict.\n                    c = SimpleCookie()\n                    for (k, v) in list(self.cookies.items()):\n                        c[k] = v\n\n                    # Turn it into a header.\n                    cookie_header = c.output(header='', sep='; ').strip()\n\n                    # Attach Cookie header to request.\n                    self.headers['Cookie'] = cookie_header\n\n            # Pre-request hook.\n            r = dispatch_hook('pre_request', self.hooks, self)\n            self.__dict__.update(r.__dict__)\n\n            try:\n                # The inner try .. except re-raises certain exceptions as\n                # internal exception types; the outer suppresses exceptions\n                # when safe mode is set.\n                try:\n                    # Send the request.\n                    r = conn.urlopen(\n                        method=self.method,\n                        url=self.path_url,\n                        body=body,\n                        headers=self.headers,\n                        redirect=False,\n                        assert_same_host=False,\n                        preload_content=False,\n                        decode_content=False,\n                        retries=self.config.get('max_retries', 0),\n                        timeout=self.timeout,\n                    )\n                    self.sent = True\n\n                except MaxRetryError as e:\n                    raise ConnectionError(e)\n\n                except (_SSLError, _HTTPError) as e:\n                    if self.verify and isinstance(e, _SSLError):\n                        raise SSLError(e)\n\n                    raise Timeout('Request timed out.')\n\n            except RequestException as e:\n                if self.config.get('safe_mode', False):\n                    # In safe mode, catch the exception and attach it to\n                    # a blank urllib3.HTTPResponse object.\n                    r = HTTPResponse()\n                    r.error = e\n                else:\n                    raise\n\n            self._build_response(r)\n\n            # Response manipulation hook.\n            self.response = dispatch_hook('response', self.hooks, self.response)\n\n            # Post-request hook.\n            r = dispatch_hook('post_request', self.hooks, self)\n            self.__dict__.update(r.__dict__)\n\n            # If prefetch is True, mark content as consumed.\n            if prefetch:\n                # Save the response.\n                self.response.content\n\n            if self.config.get('danger_mode'):\n                self.response.raise_for_status()\n\n            return self.sent\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_send_leave_request(self, origin, content, room_id):\n        logger.debug(\"on_send_leave_request: content: %s\", content)\n\n        room_version = yield self.store.get_room_version(room_id)\n        format_ver = room_version_to_event_format(room_version)\n        pdu = event_from_pdu_json(content, format_ver)\n\n        origin_host, _ = parse_server_name(origin)\n        yield self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_leave_request: pdu sigs: %s\", pdu.signatures)\n        yield self.handler.on_send_leave_request(origin, pdu)\n        return 200, {}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_safe_url(url, host=None):\n    \"\"\"\n    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to\n    a different host and uses a safe scheme).\n\n    Always returns ``False`` on an empty url.\n    \"\"\"\n    if not url:\n        return False\n    # Chrome treats \\ completely as /\n    url = url.replace('\\\\', '/')\n    # Chrome considers any URL with more than two slashes to be absolute, but\n    # urlaprse is not so flexible. Treat any url with three slashes as unsafe.\n    if url.startswith('///'):\n        return False\n    url_info = urlparse(url)\n    # Forbid URLs like http:///example.com - with a scheme, but without a hostname.\n    # In that URL, example.com is not the hostname but, a path component. However,\n    # Chrome will still consider example.com to be the hostname, so we must not\n    # allow this syntax.\n    if not url_info.netloc and url_info.scheme:\n        return False\n    return (not url_info.netloc or url_info.netloc == host) and \\\n        (not url_info.scheme or url_info.scheme in ['http', 'https'])\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def dagrun_success(self):\n        dag_id = request.form.get('dag_id')\n        execution_date = request.form.get('execution_date')\n        confirmed = request.form.get('confirmed') == 'true'\n        origin = request.form.get('origin')\n        return self._mark_dagrun_state_as_success(dag_id, execution_date,\n                                                  confirmed, origin)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _load_pkcs7_certificates(self, p7) -> typing.List[x509.Certificate]:\n        nid = self._lib.OBJ_obj2nid(p7.type)\n        self.openssl_assert(nid != self._lib.NID_undef)\n        if nid != self._lib.NID_pkcs7_signed:\n            raise UnsupportedAlgorithm(\n                \"Only basic signed structures are currently supported. NID\"\n                \" for this data was {}\".format(nid),\n                _Reasons.UNSUPPORTED_SERIALIZATION,\n            )\n\n        sk_x509 = p7.d.sign.cert\n        num = self._lib.sk_X509_num(sk_x509)\n        certs = []\n        for i in range(num):\n            x509 = self._lib.sk_X509_value(sk_x509, i)\n            self.openssl_assert(x509 != self._ffi.NULL)\n            cert = self._ossl2cert(x509)\n            certs.append(cert)\n\n        return certs\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def as_sql(self, compiler, connection):\n        key_transforms = [self.key_name]\n        previous = self.lhs\n        while isinstance(previous, KeyTransform):\n            key_transforms.insert(0, previous.key_name)\n            previous = previous.lhs\n        lhs, params = compiler.compile(previous)\n        if len(key_transforms) > 1:\n            return \"(%s %s %%s)\" % (lhs, self.nested_operator), [key_transforms] + params\n        try:\n            int(self.key_name)\n        except ValueError:\n            lookup = \"'%s'\" % self.key_name\n        else:\n            lookup = \"%s\" % self.key_name\n        return \"(%s %s %s)\" % (lhs, self.operator, lookup), params\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def main():\n    parser = get_argparser()\n    argv = sys.argv[1:]\n    args = parser.parse_args(argv)\n    if args.json_args is not None:\n        args = _unpack_json_args(args)\n    earlyinit.early_init(args)\n    # We do this imports late as earlyinit needs to be run first (because of\n    # version checking and other early initialization)\n    from qutebrowser import app\n    return app.run(args)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def json_errors(method):\n    \"\"\"Decorate methods with this to return GitHub style JSON errors.\n    \n    This should be used on any JSON API on any handler method that can raise HTTPErrors.\n    \n    This will grab the latest HTTPError exception using sys.exc_info\n    and then:\n    \n    1. Set the HTTP status code based on the HTTPError\n    2. Create and return a JSON body with a message field describing\n       the error in a human readable form.\n    \"\"\"\n    @functools.wraps(method)\n    def wrapper(self, *args, **kwargs):\n        try:\n            result = method(self, *args, **kwargs)\n        except web.HTTPError as e:\n            status = e.status_code\n            message = e.log_message\n            self.log.warn(message)\n            self.set_status(e.status_code)\n            self.finish(json.dumps(dict(message=message)))\n        except Exception:\n            self.log.error(\"Unhandled error in API request\", exc_info=True)\n            status = 500\n            message = \"Unknown server error\"\n            t, value, tb = sys.exc_info()\n            self.set_status(status)\n            tb_text = ''.join(traceback.format_exception(t, value, tb))\n            reply = dict(message=message, traceback=tb_text)\n            self.finish(json.dumps(reply))\n        else:\n            return result\n    return wrapper\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "int ossl_dh_compute_key(unsigned char *key, const BIGNUM *pub_key, DH *dh)\n{\n    BN_CTX *ctx = NULL;\n    BN_MONT_CTX *mont = NULL;\n    BIGNUM *z = NULL, *pminus1;\n    int ret = -1;\n\n    if (BN_num_bits(dh->params.p) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n        goto err;\n    }\n\n    if (BN_num_bits(dh->params.p) < DH_MIN_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_SMALL);\n        return 0;\n    }\n\n    ctx = BN_CTX_new_ex(dh->libctx);\n    if (ctx == NULL)\n        goto err;\n    BN_CTX_start(ctx);\n    pminus1 = BN_CTX_get(ctx);\n    z = BN_CTX_get(ctx);\n    if (z == NULL)\n        goto err;\n\n    if (dh->priv_key == NULL) {\n        ERR_raise(ERR_LIB_DH, DH_R_NO_PRIVATE_VALUE);\n        goto err;\n    }\n\n    if (dh->flags & DH_FLAG_CACHE_MONT_P) {\n        mont = BN_MONT_CTX_set_locked(&dh->method_mont_p,\n                                      dh->lock, dh->params.p, ctx);\n        BN_set_flags(dh->priv_key, BN_FLG_CONSTTIME);\n        if (!mont)\n            goto err;\n    }\n\n    /* (Step 1) Z = pub_key^priv_key mod p */\n    if (!dh->meth->bn_mod_exp(dh, z, pub_key, dh->priv_key, dh->params.p, ctx,\n                              mont)) {\n        ERR_raise(ERR_LIB_DH, ERR_R_BN_LIB);\n        goto err;\n    }\n\n    /* (Step 2) Error if z <= 1 or z = p - 1 */\n    if (BN_copy(pminus1, dh->params.p) == NULL\n        || !BN_sub_word(pminus1, 1)\n        || BN_cmp(z, BN_value_one()) <= 0\n        || BN_cmp(z, pminus1) == 0) {\n        ERR_raise(ERR_LIB_DH, DH_R_INVALID_SECRET);\n        goto err;\n    }\n\n    /* return the padded key, i.e. same number of bytes as the modulus */\n    ret = BN_bn2binpad(z, key, BN_num_bytes(dh->params.p));\n err:\n    BN_clear(z); /* (Step 2) destroy intermediate values */\n    BN_CTX_end(ctx);\n    BN_CTX_free(ctx);\n    return ret;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def sendfile(request, filename, **kwargs):\n    response = HttpResponse()\n    url = _convert_file_to_url(filename)\n    response['X-Accel-Redirect'] = url.encode('utf-8')\n\n    return response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def predict(self, model_uri, input_path, output_path, content_type):\n        \"\"\"\n        Generate predictions using generic python model saved with MLflow. The expected format of\n        the input JSON is the Mlflow scoring format.\n        Return the prediction results as a JSON.\n        \"\"\"\n        local_path = _download_artifact_from_uri(model_uri)\n        # NB: Absolute windows paths do not work with mlflow apis, use file uri to ensure\n        # platform compatibility.\n        local_uri = path_to_local_file_uri(local_path)\n\n        if self._env_manager != _EnvManager.LOCAL:\n            command = (\n                'python -c \"from mlflow.pyfunc.scoring_server import _predict; _predict('\n                \"model_uri={model_uri}, \"\n                \"input_path={input_path}, \"\n                \"output_path={output_path}, \"\n                \"content_type={content_type})\"\n                '\"'\n            ).format(\n                model_uri=repr(local_uri),\n                input_path=repr(input_path),\n                output_path=repr(output_path),\n                content_type=repr(content_type),\n            )\n            return self.prepare_env(local_path).execute(command)\n        else:\n            scoring_server._predict(local_uri, input_path, output_path, content_type)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def lookup_path(\n        self, path: str\n    ) -> typing.Tuple[str, typing.Optional[os.stat_result]]:\n        for directory in self.all_directories:\n            joined_path = os.path.join(directory, path)\n            if self.follow_symlink:\n                full_path = os.path.abspath(joined_path)\n            else:\n                full_path = os.path.realpath(joined_path)\n            directory = os.path.realpath(directory)\n            if os.path.commonprefix([full_path, directory]) != directory:\n                # Don't allow misbehaving clients to break out of the static files\n                # directory.\n                continue\n            try:\n                return full_path, os.stat(full_path)\n            except (FileNotFoundError, NotADirectoryError):\n                continue\n        return \"\", None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render_POST(self, request):\n        \"\"\"\n        Register with the Identity Server\n        \"\"\"\n        send_cors(request)\n\n        args = get_args(request, ('matrix_server_name', 'access_token'))\n\n        matrix_server = args['matrix_server_name'].lower()\n\n        if not is_valid_hostname(matrix_server):\n            request.setResponseCode(400)\n            return {\n                'errcode': 'M_INVALID_PARAM',\n                'error': 'matrix_server_name must be a valid hostname'\n            }\n\n        result = yield self.client.get_json(\n            \"matrix://%s/_matrix/federation/v1/openid/userinfo?access_token=%s\"\n            % (\n                matrix_server,\n                urllib.parse.quote(args['access_token']),\n            ),\n            1024 * 5,\n        )\n\n        if 'sub' not in result:\n            raise Exception(\"Invalid response from homeserver\")\n\n        user_id = result['sub']\n\n        if not isinstance(user_id, str):\n            request.setResponseCode(500)\n            return {\n                'errcode': 'M_UNKNOWN',\n                'error': 'The Matrix homeserver returned a malformed reply'\n            }\n\n        user_id_components = user_id.split(':', 1)\n\n        # Ensure there's a localpart and domain in the returned user ID.\n        if len(user_id_components) != 2:\n            request.setResponseCode(500)\n            return {\n                'errcode': 'M_UNKNOWN',\n                'error': 'The Matrix homeserver returned an invalid MXID'\n            }\n\n        user_id_server = user_id_components[1]\n\n        if not is_valid_hostname(user_id_server):\n            request.setResponseCode(500)\n            return {\n                'errcode': 'M_UNKNOWN',\n                'error': 'The Matrix homeserver returned an invalid MXID'\n            }\n\n        if user_id_server != matrix_server:\n            request.setResponseCode(500)\n            return {\n                'errcode': 'M_UNKNOWN',\n                'error': 'The Matrix homeserver returned a MXID belonging to another homeserver'\n            }\n\n        tok = yield issueToken(self.sydent, user_id)\n\n        # XXX: `token` is correct for the spec, but we released with `access_token`\n        # for a substantial amount of time. Serve both to make spec-compliant clients\n        # happy.\n        defer.returnValue({\n            \"access_token\": tok,\n            \"token\": tok,\n        })\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def failure(cls, request):\n        '''\n        Creates an RPCMessage instance containing exception information,\n        when the exception is from user-defined handlers or upper adaptation layers.\n\n        It must be called in an exception handler context, where ``sys.exc_info()``\n        returns a non-null tuple.\n        '''\n        exc_info = sys.exc_info()\n        return cls(\n            request.peer_id,\n            RPCMessageTypes.FAILURE,\n            request.method, request.order_key, request.seq_id,\n            ErrorMetadata(exc_info[0].__name__, traceback.format_exc()),\n            None,\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    PartialTensorShape element_shape_except_first_dim;\n    if (!element_shape_.unknown_rank()) {\n      element_shape_except_first_dim = PartialTensorShape(\n          gtl::ArraySlice<int64_t>(element_shape_.dim_sizes()).subspan(1));\n    }\n    // Check that the input Variant tensor is indeed a TensorList and has the\n    // correct element type.\n    const TensorList* tensor_list = nullptr;\n    OP_REQUIRES_OK(c, GetInputList(c, 0, &tensor_list));\n    OP_REQUIRES(\n        c, element_dtype_ == tensor_list->element_dtype,\n        errors::InvalidArgument(\n            \"Invalid data types; op elements \", DataTypeString(element_dtype_),\n            \" but list elements \", DataTypeString(tensor_list->element_dtype)));\n    // The leading dimension of all list elements if they are all the same.\n    // This is used as the leading dim of uninitialized tensors in the list\n    // if leading_dims is not provided.\n    int64_t first_dim = -1;\n    if (c->num_inputs() > 1) {\n      // TensorListConcatV2\n      PartialTensorShape element_shape;\n      OP_REQUIRES_OK(\n          c, GetElementShapeFromInput(c, *tensor_list, 1, &element_shape));\n      OP_REQUIRES(c, element_shape.unknown_rank() || element_shape.dims() >= 1,\n                  errors::InvalidArgument(\n                      \"Concat requires elements to be at least vectors, \",\n                      \"found scalars instead.\"));\n      // Split `element_shape` into `first_dim` and\n      // `element_shape_except_first_dim`.\n      first_dim = element_shape.dim_size(0);\n      element_shape_except_first_dim = element_shape;\n      element_shape_except_first_dim.RemoveDim(0);\n    }\n    // If the TensorList is empty, element_shape_except_first_dim must be fully\n    // defined.\n    OP_REQUIRES(c,\n                !tensor_list->tensors().empty() ||\n                    element_shape_except_first_dim.IsFullyDefined(),\n                errors::InvalidArgument(\n                    \"All except the first dimension must be fully defined \",\n                    \"when concating an empty tensor list. element_shape: \",\n                    element_shape_except_first_dim.DebugString()));\n    // 1. Check that `element_shape_except_first_dim` input tensor is\n    //    compatible with the shapes of element tensors.\n    // 2. Check that the elements have the same shape except the first dim.\n    // 3. If `first_dim` is known, check that it is compatible with the leading\n    //    dims of all elements.\n    // 4. If `first_dim` is unknown (-1), check whether all initialized\n    //    elements have the same leading dim and if so set `first_dim` to that\n    //    value.\n    if (!tensor_list->element_shape.IsFullyDefined()) {\n      bool check_dim = (first_dim == -1);\n      int64_t inferred_first_dim = first_dim;\n      for (int i = 0; i < tensor_list->tensors().size(); ++i) {\n        const Tensor& t = tensor_list->tensors()[i];\n        if (t.dtype() != DT_INVALID) {\n          PartialTensorShape tmp = element_shape_except_first_dim;\n          OP_REQUIRES(\n              c, TensorShapeUtils::IsVectorOrHigher(t.shape()),\n              errors::InvalidArgument(\"Concat saw a scalar shape at index \", i,\n                                      \" but requires at least vectors.\"));\n          TensorShape shape_except_first_dim = TensorShape(\n              gtl::ArraySlice<int64_t>(t.shape().dim_sizes()).subspan(1));\n          OP_REQUIRES_OK(c, tmp.MergeWith(shape_except_first_dim,\n                                          &element_shape_except_first_dim));\n          OP_REQUIRES(c, first_dim == -1 || first_dim == t.shape().dim_size(0),\n                      errors::InvalidArgument(\n                          \"First entry of element_shape input does not match \",\n                          \"the first dim of list element at index: \", i,\n                          \" Expected: \", first_dim,\n                          \" Actual: \", t.shape().dim_size(0)));\n          if (check_dim) {\n            if (inferred_first_dim == -1) {\n              inferred_first_dim = t.shape().dim_size(0);\n            } else if (inferred_first_dim != t.shape().dim_size(0)) {\n              inferred_first_dim = -1;\n              check_dim = false;\n            }\n          }\n        }\n      }\n      first_dim = inferred_first_dim;\n    }\n    TensorShape output_shape;\n    OP_REQUIRES(c, element_shape_except_first_dim.AsTensorShape(&output_shape),\n                errors::InvalidArgument(\n                    \"Trying to concat list with only uninitialized tensors \",\n                    \"but element_shape_except_first_dim is not fully defined: \",\n                    element_shape_except_first_dim.DebugString()));\n    // Build the lengths_tensor and leading dim of the output tensor by\n    // iterating over all element tensors.\n    Tensor* lengths_tensor = nullptr;\n    OP_REQUIRES_OK(c, c->allocate_output(1,\n                                         TensorShape({static_cast<int64_t>(\n                                             tensor_list->tensors().size())}),\n                                         &lengths_tensor));\n    auto lengths_tensor_vec = lengths_tensor->vec<int64_t>();\n    int64_t leading_dim = 0;\n    for (size_t i = 0; i < tensor_list->tensors().size(); i++) {\n      int64_t dim;\n      if (tensor_list->tensors()[i].dtype() != DT_INVALID) {\n        dim = tensor_list->tensors()[i].shape().dim_size(0);\n      } else {\n        // If leading_dims is not provided or does not contain an entry for\n        // index i use the inferred `first_dim` if set.\n        if ((c->num_inputs() <= 2 || i >= c->input(2).NumElements()) &&\n            first_dim != -1) {\n          dim = first_dim;\n        } else {\n          OP_REQUIRES(c, c->num_inputs() > 2,\n                      errors::InvalidArgument(\n                          \"Concating lists with uninitialized tensors is not \",\n                          \"supported in this version of TensorListConcat. \",\n                          \"Consider updating your GraphDef to run the newer \",\n                          \"version.\"));\n          OP_REQUIRES(c, i < c->input(2).NumElements(),\n                      errors::InvalidArgument(\n                          \"List contains uninitialized tensor at index \", i,\n                          \" but leading_dims has only \",\n                          c->input(2).NumElements(), \" elements.\"));\n          dim = c->input(2).vec<int64_t>()(i);\n        }\n      }\n      leading_dim += dim;\n      lengths_tensor_vec(i) = dim;\n    }\n    output_shape.InsertDim(0, leading_dim);\n    Tensor* output;\n    // Allocate the output tensor and fill it up with the concated element\n    // tensors.\n    OP_REQUIRES_OK(c, c->allocate_output(0, output_shape, &output));\n    if (output->NumElements() == 0) {\n      return;\n    }\n\n    ConstMatrixVector inputs_flat;\n    inputs_flat.reserve(tensor_list->tensors().size());\n    // Store the zeros tensors in a vector to prevent them from being GC'ed till\n    // concat is complete.\n    std::vector<Tensor> zeros_vec;\n    for (int i = 0; i < tensor_list->tensors().size(); i++) {\n      const Tensor& element_tensor = tensor_list->tensors()[i];\n      if (element_tensor.dtype() != DT_INVALID) {\n        if (element_tensor.NumElements() > 0) {\n          inputs_flat.emplace_back(new typename TTypes<T, 2>::ConstMatrix(\n              element_tensor.shaped<T, 2>({1, element_tensor.NumElements()})));\n        }\n      } else {\n        AllocatorAttributes attr;\n        if (element_dtype_ == DT_VARIANT) {\n          attr.set_on_host(true);\n        }\n        TensorShape element_shape = output_shape;\n        element_shape.set_dim(0, lengths_tensor_vec(i));\n        zeros_vec.emplace_back();\n        Tensor& zeros = zeros_vec.back();\n        OP_REQUIRES_OK(\n            c, c->allocate_temp(element_dtype_, element_shape, &zeros, attr));\n        SetZero<Device, T>(c, zeros);\n        inputs_flat.emplace_back(new typename TTypes<T, 2>::ConstMatrix(\n            const_cast<const Tensor&>(zeros).shaped<T, 2>(\n                {1, zeros.NumElements()})));\n      }\n    }\n    auto output_flat = output->shaped<T, 2>({1, output->NumElements()});\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n    if (std::is_same<Device, Eigen::GpuDevice>::value) {\n      ConcatGPU<T>(c, inputs_flat, output, &output_flat);\n      return;\n    }\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n    if (IsPluggableDevice(c)) {\n      ConcatPluggableDevice<T>(c, inputs_flat, &output_flat);\n    } else {\n      ConcatCPU<T>(c->device(), inputs_flat, &output_flat);\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_index_absolute_path(index):\n    return os.path.join(INDEXDIR, index)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def start(self):\n        \"\"\"\n        Prepare the server to start serving connections.\n\n        Configure the server socket handler and establish a TLS wrapping\n        socket from which all client connections descend. Bind this TLS\n        socket to the specified network address for the server.\n\n        Raises:\n            NetworkingError: Raised if the TLS socket cannot be bound to the\n                network address.\n        \"\"\"\n        self.manager = multiprocessing.Manager()\n        self.policies = self.manager.dict()\n        policies = copy.deepcopy(operation_policy.policies)\n        for policy_name, policy_set in six.iteritems(policies):\n            self.policies[policy_name] = policy_set\n\n        self.policy_monitor = monitor.PolicyDirectoryMonitor(\n            self.config.settings.get('policy_path'),\n            self.policies,\n            self.live_policies\n        )\n\n        def interrupt_handler(trigger, frame):\n            self.policy_monitor.stop()\n        signal.signal(signal.SIGINT, interrupt_handler)\n        signal.signal(signal.SIGTERM, interrupt_handler)\n\n        self.policy_monitor.start()\n\n        self._engine = engine.KmipEngine(\n            policies=self.policies,\n            database_path=self.config.settings.get('database_path')\n        )\n\n        self._logger.info(\"Starting server socket handler.\")\n\n        # Create a TCP stream socket and configure it for immediate reuse.\n        self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n\n        self._logger.debug(\n            \"Configured cipher suites: {0}\".format(\n                len(self.config.settings.get('tls_cipher_suites'))\n            )\n        )\n        for cipher in self.config.settings.get('tls_cipher_suites'):\n            self._logger.debug(cipher)\n        auth_suite_ciphers = self.auth_suite.ciphers.split(':')\n        self._logger.debug(\n            \"Authentication suite ciphers to use: {0}\".format(\n                len(auth_suite_ciphers)\n            )\n        )\n        for cipher in auth_suite_ciphers:\n            self._logger.debug(cipher)\n\n        self._socket = ssl.wrap_socket(\n            self._socket,\n            keyfile=self.config.settings.get('key_path'),\n            certfile=self.config.settings.get('certificate_path'),\n            server_side=True,\n            cert_reqs=ssl.CERT_REQUIRED,\n            ssl_version=self.auth_suite.protocol,\n            ca_certs=self.config.settings.get('ca_path'),\n            do_handshake_on_connect=True,\n            suppress_ragged_eofs=True,\n            ciphers=self.auth_suite.ciphers\n        )\n\n        try:\n            self._socket.bind(\n                (\n                    self.config.settings.get('hostname'),\n                    int(self.config.settings.get('port'))\n                )\n            )\n        except Exception as e:\n            self._logger.exception(e)\n            raise exceptions.NetworkingError(\n                \"Server failed to bind socket handler to {0}:{1}\".format(\n                    self.config.settings.get('hostname'),\n                    self.config.settings.get('port')\n                )\n            )\n        else:\n            self._logger.info(\n                \"Server successfully bound socket handler to {0}:{1}\".format(\n                    self.config.settings.get('hostname'),\n                    self.config.settings.get('port')\n                )\n            )\n            self._is_serving = True\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def delete_department(department_id=False):\n    if department_id:\n\n        # check user is authorised to delete departments. Only admin or super_user can do this.\n        if not any([g.user.is_admin, g.user.is_super_user]):\n            flash(gettext('You are not authorised to delete departments.'), category='warning')\n            return redirect('flicket_bp.departments')\n\n        form = ConfirmPassword()\n\n        #\n        departments = FlicketCategory.query.filter_by(department_id=department_id)\n        department = FlicketDepartment.query.filter_by(id=department_id).first()\n\n        # we can't delete any departments associated with categories.\n        if departments.count() > 0:\n            flash(gettext(\n                ('Department has categories linked to it. Department can not be deleted unless all categories are '\n                 'first removed.')),\n                category=\"danger\")\n            return redirect(url_for('flicket_bp.departments'))\n\n        if form.validate_on_submit():\n            # delete category from database\n            department = FlicketDepartment.query.filter_by(id=department_id).first()\n\n            db.session.delete(department)\n            # commit changes\n            db.session.commit()\n            flash('Department \"{}\" deleted.'.format(department.department), category='success')\n            return redirect(url_for('flicket_bp.departments'))\n\n        notification = gettext(\n            \"You are trying to delete department <span class=\\\"label label-default\\\">%(value)s</span>.\",\n            value=department.department)\n\n        title = gettext('Delete Department')\n\n        return render_template('flicket_delete.html',\n                               form=form,\n                               notification=notification,\n                               title=title)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def release(self):\n        await self.transaction.connection.execute(\n            f\"RELEASE SAVEPOINT {self.name}\"\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def application(environ, start_response):\n\n    my_uri = urllib.unquote(environ['REQUEST_URI'])\n    \n    form = {}\n\n    if my_uri.find(\"?\") == -1:\n       # support fake query strings\n       # something log /cobbler/web/op/ks/server/foo\n       # which is needed because of xend parser errors\n       # not tolerating \";\" and also libvirt on 5.1 not\n       # tolerating \"&amp;\" (nor \"&\").\n\n       # canonicalizes uri, mod_python does this, mod_wsgi does not\n       my_uri = os.path.realpath(my_uri)\n\n       tokens = my_uri.split(\"/\")\n       tokens = tokens[3:]\n       label = True\n       field = \"\"\n       for t in tokens:\n          if label:\n             field = t\n          else:\n             form[field] = t\n          label = not label\n\n    # This MAC header is set by anaconda during a kickstart booted with the \n    # kssendmac kernel option. The field will appear here as something \n    # like: eth0 XX:XX:XX:XX:XX:XX\n    form[\"REMOTE_MAC\"]  = form.get(\"HTTP_X_RHN_PROVISIONING_MAC_0\", None)\n\n    # Read config for the XMLRPC port to connect to:\n    fd = open(\"/etc/cobbler/settings\")\n    data = fd.read()\n    fd.close()\n    ydata = yaml.load(data)\n    remote_port = ydata.get(\"xmlrpc_port\",25151)\n\n    # instantiate a CobblerWeb object\n    cw = CobblerSvc(server = \"http://127.0.0.1:%s\" % remote_port)\n\n    # check for a valid path/mode\n    # handle invalid paths gracefully\n    mode = form.get('op','index')\n\n    # TODO: We could do proper exception handling here and return\n    # corresponding HTTP status codes:\n\n    # Execute corresponding operation on the CobblerSvc object:\n    func = getattr( cw, mode )\n    content = func( **form )\n\n    content = unicode(content).encode('utf-8')\n    status = '200 OK'\n    \n    if content.find(\"# *** ERROR ***\") != -1:\n        status = '500 SERVER ERROR'\n        print(\"possible cheetah template error\")\n\n    # TODO: Not sure these strings are the right ones to look for...\n    elif content.find(\"# profile not found\") != -1 or \\\n            content.find(\"# system not found\") != -1 or \\\n            content.find(\"# object not found\") != -1:\n        print(\"content not found: %s\" % my_uri)\n        status = \"404 NOT FOUND\"\n\n #   req.content_type = \"text/plain;charset=utf-8\"\n    response_headers = [('Content-type', 'text/plain;charset=utf-8'),\n                        ('Content-Length', str(len(content)))]\n    start_response(status, response_headers)\n\n    return [content]\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get(self, key, default=None, version=None):\n        key = self.make_key(key, version=version)\n        return self._cache.get(key, default)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _redirect_request_using_get(self, request, redirect_url):\n        redirected = request.replace(url=redirect_url, method='GET', body='')\n        redirected.headers.pop('Content-Type', None)\n        redirected.headers.pop('Content-Length', None)\n        return redirected\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def bad_url(prefix):\n    p = request.fullpath\n    if p.endswith(\"/\"):\n        p = p[:-1]\n    p = p.rsplit('/', 1)[0]\n    p += \"/simple/%s/\" % prefix\n\n    return redirect(p)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render_entries(self, entries: Entries) -> Generator[str, None, None]:\n        \"\"\"Return entries in Beancount format.\n\n        Only renders :class:`.Balance` and :class:`.Transaction`.\n\n        Args:\n            entries: A list of entries.\n\n        Yields:\n            The entries rendered in Beancount format.\n        \"\"\"\n        indent = self.ledger.fava_options.indent\n        for entry in entries:\n            if isinstance(entry, (Balance, Transaction)):\n                if isinstance(entry, Transaction) and entry.flag in EXCL_FLAGS:\n                    continue\n                try:\n                    yield get_entry_slice(entry)[0] + \"\\n\"\n                except (KeyError, FileNotFoundError):\n                    yield _format_entry(\n                        entry,\n                        self.ledger.fava_options.currency_column,\n                        indent,\n                    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "int VP8LBuildHuffmanTable(HuffmanCode* const root_table, int root_bits,\n                          const int code_lengths[], int code_lengths_size) {\n  int total_size;\n  assert(code_lengths_size <= MAX_CODE_LENGTHS_SIZE);\n  if (root_table == NULL) {\n    total_size = BuildHuffmanTable(NULL, root_bits,\n                                   code_lengths, code_lengths_size, NULL);\n  } else if (code_lengths_size <= SORTED_SIZE_CUTOFF) {\n    // use local stack-allocated array.\n    uint16_t sorted[SORTED_SIZE_CUTOFF];\n    total_size = BuildHuffmanTable(root_table, root_bits,\n                                   code_lengths, code_lengths_size, sorted);\n  } else {   // rare case. Use heap allocation.\n    uint16_t* const sorted =\n        (uint16_t*)WebPSafeMalloc(code_lengths_size, sizeof(*sorted));\n    if (sorted == NULL) return 0;\n    total_size = BuildHuffmanTable(root_table, root_bits,\n                                   code_lengths, code_lengths_size, sorted);\n    WebPSafeFree(sorted);\n  }\n  return total_size;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    StagingMap<Ordered>* map = nullptr;\n    OP_REQUIRES_OK(ctx, GetStagingMap(ctx, def(), &map));\n    core::ScopedUnref scope(map);\n    typename StagingMap<Ordered>::OptionalTuple tuple;\n\n    const Tensor* key_tensor;\n    const Tensor* indices_tensor;\n    OpInputList values_tensor;\n\n    OP_REQUIRES_OK(ctx, ctx->input(\"key\", &key_tensor));\n    OP_REQUIRES_OK(ctx, ctx->input(\"indices\", &indices_tensor));\n    OP_REQUIRES_OK(ctx, ctx->input_list(\"values\", &values_tensor));\n    OP_REQUIRES(ctx, key_tensor->NumElements() > 0,\n                errors::InvalidArgument(\"key must not be empty\"));\n\n    // Create copy for insertion into Staging Area\n    Tensor key(*key_tensor);\n\n    // Create the tuple to store\n    for (std::size_t i = 0; i < values_tensor.size(); ++i) {\n      tuple.push_back(values_tensor[i]);\n    }\n\n    // Store the tuple in the map\n    OP_REQUIRES_OK(ctx, map->put(&key, indices_tensor, &tuple));\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def handle_response(self, response, **kwargs):\n        \"\"\"Takes the given response and tries kerberos-auth, as needed.\"\"\"\n\n        if self.pos is not None:\n            # Rewind the file position indicator of the body to where\n            # it was to resend the request.\n            response.request.body.seek(self.pos)\n\n        if response.status_code == 401:\n            _r = self.handle_401(response, **kwargs)\n            log.debug(\"handle_response(): returning {0}\".format(_r))\n            return _r\n        else:\n            _r = self.handle_other(response)\n            log.debug(\"handle_response(): returning {0}\".format(_r))\n            return _r\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _build_pretty_email_regex() -> re.Pattern:\n    name_chars = r'[\\w!#$%&\\'*+\\-/=?^_`{|}~]'\n    unquoted_name_group = fr'((?:{name_chars}+\\s+)*{name_chars}+)'\n    quoted_name_group = r'\"((?:[^\"]|\\\")+)\"'\n    email_group = r'<\\s*(.+)\\s*>'\n    return re.compile(rf'\\s*(?:{unquoted_name_group}|{quoted_name_group})?\\s*{email_group}\\s*')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def complete_reg(request):\n    \"\"\"Completes the registeration, called by API\"\"\"\n    try:\n        data = cbor.decode(request.body)\n\n        client_data = CollectedClientData(data['clientDataJSON'])\n        att_obj = AttestationObject((data['attestationObject']))\n        server = getServer()\n        auth_data = server.register_complete(\n            request.session['fido_state'],\n            client_data,\n            att_obj\n        )\n        encoded = websafe_encode(auth_data.credential_data)\n        uk = User_Keys()\n        uk.username = request.user.username\n        uk.properties = {\"device\": encoded, \"type\": att_obj.fmt, }\n        uk.owned_by_enterprise = getattr(settings, \"MFA_OWNED_BY_ENTERPRISE\", False)\n        uk.key_type = \"FIDO2\"\n        uk.save()\n        return HttpResponse(simplejson.dumps({'status': 'OK'}))\n    except Exception as exp:\n        import traceback\n        print(traceback.format_exc())\n        try:\n            from raven.contrib.django.raven_compat.models import client\n            client.captureException()\n        except:\n            pass\n        return HttpResponse(simplejson.dumps({'status': 'ERR', \"message\": \"Error on server, please try again later\"}))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_current_phase(self, requested_phase_identifier):\n        found = False\n        for phase in self.phases:\n            if phase.is_valid():\n                phase.process()\n            if found or not requested_phase_identifier or requested_phase_identifier == phase.identifier:\n                found = True  # We're at or past the requested phase\n                if not phase.should_skip():\n                    return phase\n            if not phase.should_skip() and not phase.is_valid():  # A past phase is not valid, that's the current one\n                return phase\n        raise Http404(\"Error! Phase with identifier `%s` not found.\" % requested_phase_identifier)  # pragma: no cover\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def put_table_description() -> Response:\n\n    @action_logging\n    def _log_put_table_description(*, table_key: str, description: str, source: str) -> None:\n        pass  # pragma: no cover\n\n    try:\n        args = request.get_json()\n        table_endpoint = _get_table_endpoint()\n\n        table_key = get_query_param(args, 'key')\n\n        description = get_query_param(args, 'description')\n        src = get_query_param(args, 'source')\n\n        url = '{0}/{1}/description'.format(table_endpoint, table_key)\n        _log_put_table_description(table_key=table_key, description=description, source=src)\n\n        response = request_metadata(url=url, method='PUT', data=json.dumps({'description': description}))\n        status_code = response.status_code\n\n        if status_code == HTTPStatus.OK:\n            message = 'Success'\n        else:\n            message = 'Update table description failed'\n\n        payload = jsonify({'msg': message})\n        return make_response(payload, status_code)\n    except Exception as e:\n        payload = jsonify({'msg': 'Encountered exception: ' + str(e)})\n        return make_response(payload, HTTPStatus.INTERNAL_SERVER_ERROR)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  QInt32() {}\n  QInt32(const int8_t v) : value(v) {}"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init_on_load__(self):\n        RdiffRepo.__init__(\n            self, self.user.user_root, self.repopath, encoding=self.encoding or RepoObject.DEFAULT_REPO_ENCODING\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_initial(self):\n        \"\"\"Override of CreateView method, queries for which T&C to accept and catches returnTo from URL\"\"\"\n        LOGGER.debug(\"termsandconditions.views.AcceptTermsView.get_initial\")\n\n        terms = self.get_terms(self.kwargs)\n        return_to = self.request.GET.get(\"returnTo\", \"/\")\n\n        return {\"terms\": terms, \"returnTo\": return_to}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, instance, filename):\n        \"\"\"\n        Apply (if callable) or prepend (if a string) upload_to to the filename,\n        then delegate further processing of the name to the storage backend.\n        Until the storage layer, all file paths are expected to be Unix style\n        (with forward slashes).\n        \"\"\"\n        if callable(self.upload_to):\n            filename = self.upload_to(instance, filename)\n        else:\n            dirname = datetime.datetime.now().strftime(self.upload_to)\n            filename = posixpath.join(dirname, filename)\n        return self.storage.generate_filename(filename)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def from_crawler(cls, crawler):\n        splash_base_url = crawler.settings.get('SPLASH_URL',\n                                               cls.default_splash_url)\n        log_400 = crawler.settings.getbool('SPLASH_LOG_400', True)\n        slot_policy = crawler.settings.get('SPLASH_SLOT_POLICY',\n                                           cls.default_policy)\n        if slot_policy not in SlotPolicy._known:\n            raise NotConfigured(\"Incorrect slot policy: %r\" % slot_policy)\n\n        return cls(crawler, splash_base_url, slot_policy, log_400)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void RepeatsTensor2IndexTensor(const Context& ctx,\n                               const DenseTensor& repeats,\n                               DenseTensor* index) {\n  DenseTensor repeats_cpu_copy;\n  if (repeats.place().GetType() != phi::AllocationType::CPU) {\n    phi::Copy(ctx, repeats, phi::CPUPlace(), true, &repeats_cpu_copy);\n  }\n  const RepeatsT* repeats_data =\n      repeats.place().GetType() == phi::AllocationType::CPU\n          ? repeats.data<RepeatsT>()\n          : repeats_cpu_copy.data<RepeatsT>();\n\n  int64_t index_size = 0;\n  for (int i = 0; i < repeats.dims()[0]; i++) {\n    index_size += repeats_data[i];\n  }\n  std::vector<RepeatsT> index_vec(index_size);\n  int offset = 0;\n  for (int i = 0; i < repeats.dims()[0]; i++) {\n    std::fill_n(index_vec.begin() + offset, repeats_data[i], i);\n    offset += repeats_data[i];\n  }\n  index->Resize(phi::make_ddim({index_size}));\n\n  phi::TensorFromVector<RepeatsT>(index_vec, ctx, index);\n}\n}  // namespace funcs"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    // Get inputs\n    const Tensor& input_tensor = context->input(0);\n    const auto input_tensor_flat = input_tensor.flat<int32>();\n    const Tensor& input_splits = context->input(1);\n    const auto input_splits_flat = input_splits.flat<SPLITS_TYPE>();\n\n    // Operation will treat first argument in input_splits as if it were zero\n    // regardless of its actual value since splits should begin with zero and\n    // end with the length of the input values vector.\n    OP_REQUIRES(\n        context, input_splits_flat(0) == 0,\n        errors::InvalidArgument(\"First value in input_splits must be zero.\"));\n    OP_REQUIRES(context,\n                input_splits_flat(input_splits_flat.size() - 1) ==\n                    input_tensor_flat.size(),\n                errors::InvalidArgument(\"Last value in input_splits must be \"\n                                        \"equal to length of input_tensor.\"));\n    // Since we limit to a 2-D input (flat_values of rank 1 and a single splits\n    // tensor), our output dimension will be 1 with it's size equal to the\n    // number of splits (outer dimension or ragged tensor).\n    TensorShape output_shape({input_splits.dim_size(0) - 1});\n    Tensor* output_tensor;\n    OP_REQUIRES_OK(context, context->allocate_output(\"output\", output_shape,\n                                                     &output_tensor));\n    auto output_tensor_flat = output_tensor->flat<tstring>();\n\n    // Use a single index over the flattened input values tensor.\n    int idx = 0;\n    // Loop through our split dimension to create a new string at each split.\n    for (int i = 1; i < input_splits_flat.size(); ++i) {\n      icu::UnicodeString unicode_string;\n      icu::UnicodeStringAppendable appendable_unicode_string(unicode_string);\n      OP_REQUIRES(\n          context, input_splits_flat(i - 1) <= input_splits_flat(i),\n          errors::InvalidArgument(\n              \"Values in input_splits must be equal or in ascending order.\"));\n      OP_REQUIRES(\n          context, input_splits_flat(i) <= input_tensor_flat.size(),\n          errors::InvalidArgument(\"Values in input_splits must be less than or \"\n                                  \"equal to input_tensor length.\"));\n      for (; idx < input_splits_flat(i); ++idx) {\n        int32_t code_point = input_tensor_flat(idx);\n        // Check for invalid code point\n        if (!U_IS_UNICODE_CHAR(code_point)) {\n          if (error_options_.error_on_malformatting) {\n            context->CtxFailure(errors::InvalidArgument(\n                \"Code point is out of range for Unicode, or a noncharacter.\"));\n            return;\n          } else if (!error_options_.elide_replacement) {\n            code_point = error_options_.subst;\n          }\n        }\n        appendable_unicode_string.appendCodePoint(code_point);\n      }\n      // Encode our string and save in the output.\n      tstring result;\n      Encode(encoding_, unicode_string, &result);\n      output_tensor_flat(i - 1) = std::move(result);\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void operator()(OpKernelContext* context, const Tensor& x_input,\n                  const Tensor& scale_input, const Tensor& offset_input,\n                  const Tensor& estimated_mean_input,\n                  const Tensor& estimated_variance_input,\n                  const Tensor* side_input, U epsilon, U exponential_avg_factor,\n                  FusedBatchNormActivationMode activation_mode,\n                  Tensor* y_output, Tensor* batch_mean_output,\n                  Tensor* batch_var_output, Tensor* saved_mean_output,\n                  Tensor* saved_var_output, TensorFormat tensor_format,\n                  bool use_reserved_space) {\n    OP_REQUIRES(context, side_input == nullptr,\n                errors::Internal(\n                    \"The CPU implementation of FusedBatchNorm does not support \"\n                    \"side input.\"));\n    OP_REQUIRES(context,\n                activation_mode == FusedBatchNormActivationMode::kIdentity,\n                errors::Internal(\"The CPU implementation of FusedBatchNorm \"\n                                 \"does not support activations.\"));\n\n    if (use_reserved_space) {\n      Tensor* dummy_reserve_space = nullptr;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(5, {}, &dummy_reserve_space));\n      // Initialize the memory, to avoid sanitizer alerts.\n      dummy_reserve_space->flat<U>()(0) = U();\n    }\n    Tensor transformed_x;\n    Tensor transformed_y;\n    if (tensor_format == FORMAT_NCHW) {\n      const int64 in_batch = GetTensorDim(x_input, tensor_format, 'N');\n      const int64 in_rows = GetTensorDim(x_input, tensor_format, 'H');\n      const int64 in_cols = GetTensorDim(x_input, tensor_format, 'W');\n      const int64 in_depths = GetTensorDim(x_input, tensor_format, 'C');\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_y));\n      // Perform NCHW to NHWC\n      std::vector<int32> perm = {0, 2, 3, 1};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             x_input, perm, &transformed_x));\n    } else {\n      transformed_x = x_input;\n      transformed_y = *y_output;\n    }\n    typename TTypes<T, 4>::Tensor x(transformed_x.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec offset(offset_input.vec<U>());\n    typename TTypes<U>::ConstVec estimated_mean(estimated_mean_input.vec<U>());\n    typename TTypes<U>::ConstVec estimated_variance(\n        estimated_variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor y(transformed_y.tensor<T, 4>());\n    typename TTypes<U>::Vec batch_mean(batch_mean_output->vec<U>());\n    typename TTypes<U>::Vec batch_variance(batch_var_output->vec<U>());\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    const int depth = x.dimension(3);\n    const int size = x.size();\n    const int rest_size = size / depth;\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 1> reduce_dims({0});\n    Eigen::array<int, 2> bcast_spec({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> bcast_spec;\n    bcast_spec.set(0, rest_size);\n#endif\n\n    auto x_rest_by_depth = x.reshape(rest_by_depth).template cast<U>();\n    auto x_centered =\n        x_rest_by_depth -\n        estimated_mean.reshape(one_by_depth).broadcast(bcast_spec);\n    auto scaling_factor = ((estimated_variance + epsilon).rsqrt() * scale)\n                              .eval()\n                              .reshape(one_by_depth)\n                              .broadcast(bcast_spec);\n    auto x_scaled = x_centered * scaling_factor;\n    auto x_shifted =\n        (x_scaled + offset.reshape(one_by_depth).broadcast(bcast_spec))\n            .template cast<T>();\n\n    y.reshape(rest_by_depth).device(d) = x_shifted;\n    batch_mean.device(d) = estimated_mean;\n    batch_variance.device(d) = estimated_variance;\n\n    if (tensor_format == FORMAT_NCHW) {\n      // Perform NHWC to NCHW\n      const std::vector<int32> perm = {0, 3, 1, 2};\n      const Status s = ::tensorflow::DoTranspose(\n          context->eigen_device<CPUDevice>(), transformed_y, perm, y_output);\n      if (!s.ok()) {\n        context->SetStatus(errors::InvalidArgument(\"Transpose failed: \", s));\n      }\n    }\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _cli_deploy_image(args):\n\n    docker_repository = _resolve_property(args, 'docker_repository')\n    vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)\n    vault_endpoint = _resolve_vault_endpoint(args)\n    image_tar_path = args.image_tar_path if args.image_tar_path else find_image_file()\n\n    bigflow.deploy.deploy_docker_image(\n        image_tar_path=image_tar_path,\n        auth_method=args.auth_method,\n        docker_repository=docker_repository,\n        vault_endpoint=vault_endpoint,\n        vault_secret=vault_secret,\n    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def index_html(self, REQUEST, RESPONSE):\n        \"\"\"\n        The default view of the contents of a File or Image.\n\n        Returns the contents of the file or image.  Also, sets the\n        Content-Type HTTP header to the objects content type.\n        \"\"\"\n\n        if self._if_modified_since_request_handler(REQUEST, RESPONSE):\n            # we were able to handle this by returning a 304\n            # unfortunately, because the HTTP cache manager uses the cache\n            # API, and because 304 responses are required to carry the Expires\n            # header for HTTP/1.1, we need to call ZCacheable_set here.\n            # This is nonsensical for caches other than the HTTP cache manager\n            # unfortunately.\n            self.ZCacheable_set(None)\n            return b''\n\n        if self.precondition and hasattr(self, str(self.precondition)):\n            # Grab whatever precondition was defined and then\n            # execute it.  The precondition will raise an exception\n            # if something violates its terms.\n            c = getattr(self, str(self.precondition))\n            if hasattr(c, 'isDocTemp') and c.isDocTemp:\n                c(REQUEST['PARENTS'][1], REQUEST)\n            else:\n                c()\n\n        if self._range_request_handler(REQUEST, RESPONSE):\n            # we served a chunk of content in response to a range request.\n            return b''\n\n        RESPONSE.setHeader('Last-Modified', rfc1123_date(self._p_mtime))\n        RESPONSE.setHeader('Content-Type', self.content_type)\n        RESPONSE.setHeader('Content-Length', self.size)\n        RESPONSE.setHeader('Accept-Ranges', 'bytes')\n\n        if self.ZCacheable_isCachingEnabled():\n            result = self.ZCacheable_get(default=None)\n            if result is not None:\n                # We will always get None from RAMCacheManager and HTTP\n                # Accelerated Cache Manager but we will get\n                # something implementing the IStreamIterator interface\n                # from a \"FileCacheManager\"\n                return result\n\n        self.ZCacheable_set(None)\n\n        data = self.data\n        if isinstance(data, bytes):\n            RESPONSE.setBase(None)\n            return data\n\n        while data is not None:\n            RESPONSE.write(data.data)\n            data = data.next\n\n        return b''\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  static port::StatusOr<CudnnRnnSequenceTensorDescriptor> Create(\n      GpuExecutor* parent, int max_seq_length, int batch_size, int data_size,\n      cudnnDataType_t data_type) {\n    CHECK_GT(max_seq_length, 0);\n    int dims[] = {batch_size, data_size, 1};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    TensorDescriptor tensor_desc = CreateTensorDescriptor();\n    RETURN_IF_CUDNN_ERROR(cudnnSetTensorNdDescriptor(\n        /*tensorDesc=*/tensor_desc.get(), /*dataType=*/data_type,\n        /*nbDims=*/sizeof(dims) / sizeof(dims[0]), /*dimA=*/dims,\n        /*strideA=*/strides));\n    return CudnnRnnSequenceTensorDescriptor(parent, max_seq_length, batch_size,\n                                            data_size, data_type,\n                                            nullptr,\n                                            std::move(tensor_desc));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def real_download(self, filename, info_dict):\n        self.report_destination(filename)\n        tmpfilename = self.temp_name(filename)\n\n        try:\n            started = time.time()\n            retval = self._call_downloader(tmpfilename, info_dict)\n        except KeyboardInterrupt:\n            if not info_dict.get('is_live'):\n                raise\n            # Live stream downloading cancellation should be considered as\n            # correct and expected termination thus all postprocessing\n            # should take place\n            retval = 0\n            self.to_screen('[%s] Interrupted by user' % self.get_basename())\n\n        if retval == 0:\n            status = {\n                'filename': filename,\n                'status': 'finished',\n                'elapsed': time.time() - started,\n            }\n            if filename != '-':\n                fsize = os.path.getsize(encodeFilename(tmpfilename))\n                self.try_rename(tmpfilename, filename)\n                status.update({\n                    'downloaded_bytes': fsize,\n                    'total_bytes': fsize,\n                })\n            self._hook_progress(status, info_dict)\n            return True\n        else:\n            self.to_stderr('\\n')\n            self.report_error('%s exited with code %d' % (\n                self.get_basename(), retval))\n            return False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _runtime_ir(runtime_functions, all_sigs, global_ctx):\n    # categorize the runtime functions because we will organize the runtime\n    # code into the following sections:\n    # payable functions, nonpayable functions, fallback function, internal_functions\n    internal_functions = [f for f in runtime_functions if _is_internal(f)]\n\n    external_functions = [f for f in runtime_functions if not _is_internal(f)]\n    default_function = next((f for f in external_functions if _is_default_func(f)), None)\n\n    # functions that need to go exposed in the selector section\n    regular_functions = [f for f in external_functions if not _is_default_func(f)]\n    payables = [f for f in regular_functions if _is_payable(f)]\n    nonpayables = [f for f in regular_functions if not _is_payable(f)]\n\n    # create a map of the IR functions since they might live in both\n    # runtime and deploy code (if init function calls them)\n    internal_functions_map: Dict[str, IRnode] = {}\n\n    for func_ast in internal_functions:\n        func_ir = generate_ir_for_function(func_ast, all_sigs, global_ctx, False)\n        internal_functions_map[func_ast.name] = func_ir\n\n    # for some reason, somebody may want to deploy a contract with no\n    # external functions, or more likely, a \"pure data\" contract which\n    # contains immutables\n    if len(external_functions) == 0:\n        # TODO: prune internal functions in this case?\n        runtime = [\"seq\"] + list(internal_functions_map.values())\n        return runtime, internal_functions_map\n\n    # note: if the user does not provide one, the default fallback function\n    # reverts anyway. so it does not hurt to batch the payable check.\n    default_is_nonpayable = default_function is None or not _is_payable(default_function)\n\n    # when a contract has a nonpayable default function,\n    # we can do a single check for all nonpayable functions\n    batch_payable_check = len(nonpayables) > 0 and default_is_nonpayable\n    skip_nonpayable_check = batch_payable_check\n\n    selector_section = [\"seq\"]\n\n    for func_ast in payables:\n        func_ir = generate_ir_for_function(func_ast, all_sigs, global_ctx, False)\n        selector_section.append(func_ir)\n\n    if batch_payable_check:\n        selector_section.append([\"assert\", [\"iszero\", \"callvalue\"]])\n\n    for func_ast in nonpayables:\n        func_ir = generate_ir_for_function(func_ast, all_sigs, global_ctx, skip_nonpayable_check)\n        selector_section.append(func_ir)\n\n    if default_function:\n        fallback_ir = generate_ir_for_function(\n            default_function, all_sigs, global_ctx, skip_nonpayable_check\n        )\n    else:\n        fallback_ir = IRnode.from_list(\n            [\"revert\", 0, 0], annotation=\"Default function\", error_msg=\"fallback function\"\n        )\n\n    # ensure the external jumptable section gets closed out\n    # (for basic block hygiene and also for zksync interpreter)\n    # NOTE: this jump gets optimized out in assembly since the\n    # fallback label is the immediate next instruction,\n    close_selector_section = [\"goto\", \"fallback\"]\n\n    runtime = [\n        \"seq\",\n        # check that calldatasize is at least 4, otherwise\n        # calldataload will load zeros (cf. yellow paper).\n        [\"if\", [\"lt\", \"calldatasize\", 4], [\"goto\", \"fallback\"]],\n        [\"with\", \"_calldata_method_id\", shr(224, [\"calldataload\", 0]), selector_section],\n        close_selector_section,\n        [\"label\", \"fallback\", [\"var_list\"], fallback_ir],\n    ]\n\n    # TODO: prune unreachable functions?\n    runtime.extend(internal_functions_map.values())\n\n    return runtime, internal_functions_map\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n\n  int batches = input->dims->data[0];\n  int height = input->dims->data[1];\n  int width = input->dims->data[2];\n  int channels_out = input->dims->data[3];\n\n  // Matching GetWindowedOutputSize in TensorFlow.\n  auto padding = params->padding;\n  int out_width, out_height;\n\n  data->padding = ComputePaddingHeightWidth(\n      params->stride_height, params->stride_width, 1, 1, height, width,\n      params->filter_height, params->filter_width, padding, &out_height,\n      &out_width);\n\n  if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8) {\n    if (pool_type == kAverage || pool_type == kMax) {\n      TFLITE_DCHECK_LE(std::abs(input->params.scale - output->params.scale),\n                       1.0e-6);\n      TFLITE_DCHECK_EQ(input->params.zero_point, output->params.zero_point);\n    }\n    if (pool_type == kL2) {\n      // We currently don't have a quantized implementation of L2Pool\n      TF_LITE_ENSURE_TYPES_EQ(context, input->type, kTfLiteFloat32);\n    }\n  }\n\n  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);\n  output_size->data[0] = batches;\n  output_size->data[1] = out_height;\n  output_size->data[2] = out_width;\n  output_size->data[3] = channels_out;\n  return context->ResizeTensor(context, output, output_size);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create_service(self, context, OS_KSADM_service):\n        service_id = uuid.uuid4().hex\n        service_ref = OS_KSADM_service.copy()\n        service_ref['id'] = service_id\n        new_service_ref = self.catalog_api.create_service(\n                context, service_id, service_ref)\n        return {'OS-KSADM:service': new_service_ref}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs(\n    OpKernelContext* context, TensorInputs* inputs,\n    TensorShapes* input_matrix_shapes, TensorShape* batch_shape) {\n  int input_rank = -1;\n  for (int i = 0; i < NumMatrixInputs(context); ++i) {\n    const Tensor& in = context->input(i);\n    if (i == 0) {\n      input_rank = in.dims();\n      OP_REQUIRES(\n          context, input_rank >= 2,\n          errors::InvalidArgument(\"Input tensor \", i,\n                                  \" must have rank >= 2, got \", input_rank));\n      // If the tensor rank is greater than 2, we consider the inner-most\n      // dimensions as matrices, and loop over all the other outer (\"batch\")\n      // dimensions to compute the results.\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        batch_shape->AddDim(in.dim_size(dim));\n      }\n    } else {\n      // Make sure that all inputs have the same rank and outer dimensions.\n      OP_REQUIRES(context, input_rank == in.dims(),\n                  errors::InvalidArgument(\n                      \"All input tensors must have the same rank.\"));\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        OP_REQUIRES(\n            context, in.dim_size(dim) == batch_shape->dim_size(dim),\n            errors::InvalidArgument(\n                \"All input tensors must have the same outer dimensions.\"));\n      }\n    }\n\n    const int row_dimension = input_rank - 2;\n    const int col_dimension = input_rank - 1;\n    const int64_t num_rows = in.dim_size(row_dimension);\n    const int64_t num_cols = in.dim_size(col_dimension);\n    input_matrix_shapes->emplace_back(\n        std::initializer_list<int64_t>({num_rows, num_cols}));\n    inputs->emplace_back(&in);\n  }\n  // Have the derived class validate that the inputs are as expected.\n  ValidateInputMatrixShapes(context, *input_matrix_shapes);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def yum_sync(self, repo):\n\n        \"\"\"\n        Handle copying of http:// and ftp:// yum repos.\n        \"\"\"\n\n        # create the config file the hosts will use to access the repository.\n        repo_mirror = repo.mirror.strip()\n        dest_path = os.path.join(self.settings.webdir + \"/repo_mirror\", repo.name.strip())\n        self.create_local_file(dest_path, repo)\n\n        if not repo.mirror_locally:\n            return\n\n        cmd = self.reposync_cmd()  # command to run\n        has_rpm_list = False       # flag indicating not to pull the whole repo\n\n        # detect cases that require special handling\n\n        if repo.rpm_list != \"\" and repo.rpm_list != []:\n            has_rpm_list = True\n\n        # create yum config file for use by reposync\n        temp_path = os.path.join(dest_path, \".origin\")\n\n        if not os.path.isdir(temp_path):\n            # FIXME: there's a chance this might break the RHN D/L case\n            os.makedirs(temp_path)\n\n        temp_file = self.create_local_file(temp_path, repo, output=False)\n\n        if not has_rpm_list:\n            # if we have not requested only certain RPMs, use reposync\n            cmd = \"%s %s --config=%s --repoid=%s --download_path=%s\" % (cmd, self.rflags, temp_file, repo.name, self.settings.webdir + \"/repo_mirror\")\n            if repo.arch != \"\":\n                if repo.arch == \"x86\":\n                    repo.arch = \"i386\"      # FIX potential arch errors\n                if repo.arch == \"i386\":\n                    # counter-intuitive, but we want the newish kernels too\n                    cmd = \"%s -a i686\" % (cmd)\n                else:\n                    cmd = \"%s -a %s\" % (cmd, repo.arch)\n\n        else:\n\n            # create the output directory if it doesn't exist\n            if not os.path.exists(dest_path):\n                os.makedirs(dest_path)\n\n            use_source = \"\"\n            if repo.arch == \"src\":\n                use_source = \"--source\"\n\n            # older yumdownloader sometimes explodes on --resolvedeps\n            # if this happens to you, upgrade yum & yum-utils\n            extra_flags = self.settings.yumdownloader_flags\n            cmd = \"\"\n            if os.path.exists(\"/usr/bin/dnf\"):\n                cmd = \"/usr/bin/dnf download\"\n            else:\n                cmd = \"/usr/bin/yumdownloader\"\n            cmd = \"%s %s %s --disablerepo=* --enablerepo=%s -c %s --destdir=%s %s\" % (cmd, extra_flags, use_source, repo.name, temp_file, dest_path, \" \".join(repo.rpm_list))\n\n        # now regardless of whether we're doing yumdownloader or reposync\n        # or whether the repo was http://, ftp://, or rhn://, execute all queued\n        # commands here.  Any failure at any point stops the operation.\n\n        rc = utils.subprocess_call(self.logger, cmd)\n        if rc != 0:\n            utils.die(self.logger, \"cobbler reposync failed\")\n\n        repodata_path = os.path.join(dest_path, \"repodata\")\n\n        # grab repomd.xml and use it to download any metadata we can use\n        proxies = None\n        if repo.proxy == '<<inherit>>':\n            proxies = {'http': self.settings.proxy_url_ext}\n        elif repo.proxy != '<<None>>' and repo.proxy != '':\n            proxies = {'http': repo.proxy, 'https': repo.proxy}\n        src = repo_mirror + \"/repodata/repomd.xml\"\n        dst = temp_path + \"/repomd.xml\"\n        urlgrab_ssl_opts = self.gen_urlgrab_ssl_opts(repo.yumopts)\n        try:\n            urlgrabber.grabber.urlgrab(src, filename=dst, proxies=proxies, **urlgrab_ssl_opts)\n        except Exception as e:\n            utils.die(self.logger, \"failed to fetch \" + src + \" \" + e.args)\n\n        # create our repodata directory now, as any extra metadata we're\n        # about to download probably lives there\n        if not os.path.isdir(repodata_path):\n            os.makedirs(repodata_path)\n        rmd = yum.repoMDObject.RepoMD('', \"%s/repomd.xml\" % (temp_path))\n        for mdtype in rmd.repoData.keys():\n            # don't download metadata files that are created by default\n            if mdtype not in [\"primary\", \"primary_db\", \"filelists\", \"filelists_db\", \"other\", \"other_db\"]:\n                mdfile = rmd.getData(mdtype).location[1]\n                src = repo_mirror + \"/\" + mdfile\n                dst = dest_path + \"/\" + mdfile\n                try:\n                    urlgrabber.grabber.urlgrab(src, filename=dst, proxies=proxies, **urlgrab_ssl_opts)\n                except Exception as e:\n                    utils.die(self.logger, \"failed to fetch \" + src + \" \" + e.args)\n\n        # now run createrepo to rebuild the index\n        if repo.mirror_locally:\n            os.path.walk(dest_path, self.createrepo_walker, repo)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, *, tokens: List[str], max_cache_size: int = config.DEFAULT_CACHE_SIZE, vanitize: bool = False):\n        self._tokens = tokens\n        self._max_cache_size = max_cache_size\n        self._vanitize = vanitize\n        self._check_args()\n        self._tokens = sorted(self._tokens)  # Sorted for subsequent reproducible randomization.\n\n        self._shorten_url = lru_cache(maxsize=self._max_cache_size)(self._shorten_url)  # type: ignore  # Instance level cache\n        self._init_executor()\n        if config.TEST_API_ON_INIT:\n            self._test()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _set_image_file_metadata(self):\n        self.file.open()\n\n        # Set new image file size\n        self.file_size = self.file.size\n\n        # Set new image file hash\n        self._set_file_hash(self.file.read())\n        self.file.seek(0)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def publish(self, id_, identity, uow=None):\n        \"\"\"Publish a draft.\n\n        Idea:\n            - Get the draft from the data layer (draft is not passed in)\n            - Validate it more strictly than when it was originally saved\n              (drafts can be incomplete but only complete drafts can be turned\n              into records)\n            - Create or update associated (published) record with data\n        \"\"\"\n        self.require_permission(identity, \"publish\")\n\n        # Get the draft\n        draft = self.draft_cls.pid.resolve(id_, registered_only=False)\n\n        # Validate the draft strictly - since a draft can be saved with errors\n        # we do a strict validation here to make sure only valid drafts can be\n        # published.\n        self._validate_draft(identity, draft)\n\n        # Create the record from the draft\n        latest_id = draft.versions.latest_id\n        record = self.record_cls.publish(draft)\n\n        # Run components\n        self.run_components(\n            'publish', identity, draft=draft, record=record, uow=uow)\n\n        # Commit and index\n        uow.register(RecordCommitOp(record, indexer=self.indexer))\n        uow.register(RecordDeleteOp(draft, force=False, indexer=self.indexer))\n\n        if latest_id:\n            self._reindex_latest(latest_id, uow=uow)\n\n        return self.result_item(\n            self, identity, record, links_tpl=self.links_item_tpl)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def shutdown():\n    task = int(request.args.get(\"parameter\").strip())\n    showtext = {}\n    if task in (0, 1):  # valid commandos received\n        # close all database connections\n        calibre_db.dispose()\n        ub.dispose()\n\n        if task == 0:\n            showtext['text'] = _(u'Server restarted, please reload page')\n        else:\n            showtext['text'] = _(u'Performing shutdown of server, please close window')\n        # stop gevent/tornado server\n        web_server.stop(task == 0)\n        return json.dumps(showtext)\n\n    if task == 2:\n        log.warning(\"reconnecting to calibre database\")\n        calibre_db.reconnect_db(config, ub.app_DB_path)\n        showtext['text'] = _(u'Reconnect successful')\n        return json.dumps(showtext)\n\n    showtext['text'] = _(u'Unknown command')\n    return json.dumps(showtext), 400\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status FractionalPoolShapeFn(InferenceContext* c) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n  std::vector<float> pooling_ratio;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"pooling_ratio\", &pooling_ratio));\n  if (pooling_ratio.size() != 4) {\n    return errors::InvalidArgument(\n        \"pooling_ratio field must specify 4 dimensions\");\n  }\n  std::vector<DimensionHandle> output_dims;\n  for (int i = 0; i < 4; ++i) {\n    DimensionHandle d = c->Dim(input, i);\n    if (c->ValueKnown(d)) {\n      // This must match the same logic in the kernel function in\n      // core/kernels/fractional_max_pool_op.cc.\n      auto val =\n          static_cast<int64_t>(std::floor(c->Value(d) / pooling_ratio[i]));\n      if (val < 0) {\n        return errors::InvalidArgument(\"Size computed for dim \", i,\n                                       \" is negative: \", val);\n      }\n      output_dims.push_back(c->MakeDim(val));\n    } else {\n      output_dims.push_back(c->UnknownDim());\n    }\n  }\n\n  c->set_output(0, c->MakeShape(output_dims));\n  c->set_output(1, c->Vector(output_dims[1]));\n  c->set_output(2, c->Vector(output_dims[2]));\n  return OkStatus();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _handle_carbon_received(self, msg):\n        self.xmpp.event('carbon_received', msg)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& gradient = ctx->input(0);\n    const Tensor& input = ctx->input(1);\n    Tensor* input_backprop = nullptr;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, input.shape(), &input_backprop));\n    OP_REQUIRES(\n        ctx, axis_ >= -1,\n        errors::InvalidArgument(\"Axis must be at least -1. Found \", axis_));\n    OP_REQUIRES(ctx, (axis_ == -1 || axis_ < input.shape().dims()),\n                errors::InvalidArgument(\n                    \"Axis should be -1 or 0 or a positive value less than \",\n                    input.shape().dims(), \"but given axis value was \", axis_));\n\n    OP_REQUIRES(\n        ctx, input.IsSameSize(gradient),\n        errors::InvalidArgument(\"gradient and input must be the same size\"));\n    const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n    const Tensor& input_min_tensor = ctx->input(2);\n    OP_REQUIRES(ctx,\n                input_min_tensor.dims() == 0 || input_min_tensor.dims() == 1,\n                errors::InvalidArgument(\n                    \"Input min tensor must have dimension 1. Recieved \",\n                    input_min_tensor.dims(), \".\"));\n    const Tensor& input_max_tensor = ctx->input(3);\n    OP_REQUIRES(ctx,\n                input_max_tensor.dims() == 0 || input_max_tensor.dims() == 1,\n                errors::InvalidArgument(\n                    \"Input max tensor must have dimension 1. Recieved \",\n                    input_max_tensor.dims(), \".\"));\n    if (axis_ != -1) {\n      OP_REQUIRES(\n          ctx, input_min_tensor.dim_size(0) == depth,\n          errors::InvalidArgument(\"min has incorrect size, expected \", depth,\n                                  \" was \", input_min_tensor.dim_size(0)));\n      OP_REQUIRES(\n          ctx, input_max_tensor.dim_size(0) == depth,\n          errors::InvalidArgument(\"max has incorrect size, expected \", depth,\n                                  \" was \", input_max_tensor.dim_size(0)));\n    }\n\n    TensorShape min_max_shape(input_min_tensor.shape());\n    Tensor* input_min_backprop;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(1, min_max_shape, &input_min_backprop));\n\n    Tensor* input_max_backprop;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(2, min_max_shape, &input_max_backprop));\n\n    if (axis_ == -1) {\n      functor::QuantizeAndDequantizeOneScaleGradientFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(), gradient.template flat<T>(),\n        input.template flat<T>(), input_min_tensor.scalar<T>(),\n        input_max_tensor.scalar<T>(), input_backprop->template flat<T>(),\n        input_min_backprop->template scalar<T>(),\n        input_max_backprop->template scalar<T>());\n    } else {\n      functor::QuantizeAndDequantizePerChannelGradientFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(),\n        gradient.template flat_inner_outer_dims<T, 3>(axis_ - 1),\n        input.template flat_inner_outer_dims<T, 3>(axis_ - 1),\n        &input_min_tensor, &input_max_tensor,\n        input_backprop->template flat_inner_outer_dims<T, 3>(axis_ - 1),\n        input_min_backprop->template flat<T>(),\n        input_max_backprop->template flat<T>());\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def autolink(self, link, is_email=False):\n        \"\"\"Rendering a given link or email address.\n\n        :param link: link content or email address.\n        :param is_email: whether this is an email or not.\n        \"\"\"\n        text = link = escape(link)\n        if is_email:\n            link = 'mailto:%s' % link\n        return '<a href=\"%s\">%s</a>' % (link, text)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def received(self, s):\n        # Returns the number of bytes consumed.\n        if self.completed:\n            return 0\n        orig_size = len(s)\n        while s:\n            rm = self.chunk_remainder\n            if rm > 0:\n                # Receive the remainder of a chunk.\n                to_write = s[:rm]\n                self.buf.append(to_write)\n                written = len(to_write)\n                s = s[written:]\n                self.chunk_remainder -= written\n            elif not self.all_chunks_received:\n                # Receive a control line.\n                s = self.control_line + s\n                pos = s.find(b\"\\n\")\n                if pos < 0:\n                    # Control line not finished.\n                    self.control_line = s\n                    s = \"\"\n                else:\n                    # Control line finished.\n                    line = s[:pos]\n                    s = s[pos + 1 :]\n                    self.control_line = b\"\"\n                    line = line.strip()\n                    if line:\n                        # Begin a new chunk.\n                        semi = line.find(b\";\")\n                        if semi >= 0:\n                            # discard extension info.\n                            line = line[:semi]\n                        try:\n                            sz = int(line.strip(), 16)  # hexadecimal\n                        except ValueError:  # garbage in input\n                            self.error = BadRequest(\"garbage in chunked encoding input\")\n                            sz = 0\n                        if sz > 0:\n                            # Start a new chunk.\n                            self.chunk_remainder = sz\n                        else:\n                            # Finished chunks.\n                            self.all_chunks_received = True\n                    # else expect a control line.\n            else:\n                # Receive the trailer.\n                trailer = self.trailer + s\n                if trailer.startswith(b\"\\r\\n\"):\n                    # No trailer.\n                    self.completed = True\n                    return orig_size - (len(trailer) - 2)\n                elif trailer.startswith(b\"\\n\"):\n                    # No trailer.\n                    self.completed = True\n                    return orig_size - (len(trailer) - 1)\n                pos = find_double_newline(trailer)\n                if pos < 0:\n                    # Trailer not finished.\n                    self.trailer = trailer\n                    s = b\"\"\n                else:\n                    # Finished the trailer.\n                    self.completed = True\n                    self.trailer = trailer[:pos]\n                    return orig_size - (len(trailer) - pos)\n        return orig_size\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _set_headers(self):\n        response = cherrypy.serving.response\n        # Define X-Frame-Options to avoid Clickjacking\n        response.headers['X-Frame-Options'] = 'DENY'\n        # Awaiting bug fix in cherrypy\n        # https://github.com/cherrypy/cherrypy/issues/1767\n        # Force SameSite to Lax\n        cookie = response.cookie.get('session_id', None)\n        if cookie:\n            cookie['samesite'] = 'Lax'\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "encode_unicode(PyObject *unicode)\n{\n    PyObject *repr;\n    Py_UNICODE *s;\n    Py_ssize_t size;\n    char *p;\n\n    static const char *hexdigit = \"0123456789abcdef\";\n\n    s = PyUnicode_AS_UNICODE(unicode);\n    size = PyUnicode_GET_SIZE(unicode);\n\n    if (size > (PY_SSIZE_T_MAX-2-1)/6) {\n        PyErr_SetString(PyExc_OverflowError,\n                        \"unicode object is too large to make repr\");\n        return NULL;\n    }\n\n    repr = PyString_FromStringAndSize(NULL, 2 + 6*size + 1);\n    if (repr == NULL)\n        return NULL;\n\n    p = PyString_AS_STRING(repr);\n\n    *p++ = '\"';\n\n    while (size-- > 0) {\n        Py_UNICODE ch = *s++;\n\n        /* Escape quotes */\n        if ((ch == (Py_UNICODE) PyString_AS_STRING(repr)[0] || ch == '\\\\')) {\n            *p++ = '\\\\';\n            *p++ = (char) ch;\n            continue;\n        }\n\n#ifdef Py_UNICODE_WIDE\n        /* Map 21-bit characters to '\\U00xxxxxx' */\n        else if (ch >= 0x10000) {\n            int offset = p - PyString_AS_STRING(repr);\n\n            /* Resize the string if necessary */\n            if (offset + 12 > PyString_GET_SIZE(repr)) {\n                if (_PyString_Resize(&repr, PyString_GET_SIZE(repr) + 100))\n                    return NULL;\n                p = PyString_AS_STRING(repr) + offset;\n            }\n\n            *p++ = '\\\\';\n            *p++ = 'U';\n            *p++ = hexdigit[(ch >> 28) & 0x0000000F];\n            *p++ = hexdigit[(ch >> 24) & 0x0000000F];\n            *p++ = hexdigit[(ch >> 20) & 0x0000000F];\n            *p++ = hexdigit[(ch >> 16) & 0x0000000F];\n            *p++ = hexdigit[(ch >> 12) & 0x0000000F];\n            *p++ = hexdigit[(ch >> 8) & 0x0000000F];\n            *p++ = hexdigit[(ch >> 4) & 0x0000000F];\n            *p++ = hexdigit[ch & 0x0000000F];\n            continue;\n        }\n#endif\n        /* Map UTF-16 surrogate pairs to Unicode \\UXXXXXXXX escapes */\n        else if (ch >= 0xD800 && ch < 0xDC00) {\n            Py_UNICODE ch2;\n            Py_UCS4 ucs;\n\n            ch2 = *s++;\n            size--;\n            if (ch2 >= 0xDC00 && ch2 <= 0xDFFF) {\n                ucs = (((ch & 0x03FF) << 10) | (ch2 & 0x03FF)) + 0x00010000;\n                *p++ = '\\\\';\n                *p++ = 'U';\n                *p++ = hexdigit[(ucs >> 28) & 0x0000000F];\n                *p++ = hexdigit[(ucs >> 24) & 0x0000000F];\n                *p++ = hexdigit[(ucs >> 20) & 0x0000000F];\n                *p++ = hexdigit[(ucs >> 16) & 0x0000000F];\n                *p++ = hexdigit[(ucs >> 12) & 0x0000000F];\n                *p++ = hexdigit[(ucs >> 8) & 0x0000000F];\n                *p++ = hexdigit[(ucs >> 4) & 0x0000000F];\n                *p++ = hexdigit[ucs & 0x0000000F];\n                continue;\n            }\n            /* Fall through: isolated surrogates are copied as-is */\n            s--;\n            size++;\n        }\n\n        /* Map 16-bit characters to '\\uxxxx' */\n        if (ch >= 256) {\n            *p++ = '\\\\';\n            *p++ = 'u';\n            *p++ = hexdigit[(ch >> 12) & 0x000F];\n            *p++ = hexdigit[(ch >> 8) & 0x000F];\n            *p++ = hexdigit[(ch >> 4) & 0x000F];\n            *p++ = hexdigit[ch & 0x000F];\n        }\n\n        /* Map special whitespace to '\\t', \\n', '\\r', '\\f', '\\b' */\n        else if (ch == '\\t') {\n            *p++ = '\\\\';\n            *p++ = 't';\n        }\n        else if (ch == '\\n') {\n            *p++ = '\\\\';\n            *p++ = 'n';\n        }\n        else if (ch == '\\r') {\n            *p++ = '\\\\';\n            *p++ = 'r';\n        }\n        else if (ch == '\\f') {\n            *p++ = '\\\\';\n            *p++ = 'f';\n        }\n        else if (ch == '\\b') {\n            *p++ = '\\\\';\n            *p++ = 'b';\n        }\n\n        /* Map non-printable US ASCII to '\\u00hh' */\n        else if (ch < ' ' || ch >= 0x7F) {\n            *p++ = '\\\\';\n            *p++ = 'u';\n            *p++ = '0';\n            *p++ = '0';\n            *p++ = hexdigit[(ch >> 4) & 0x000F];\n            *p++ = hexdigit[ch & 0x000F];\n        }\n\n        /* Copy everything else as-is */\n        else\n            *p++ = (char) ch;\n    }\n\n    *p++ = PyString_AS_STRING(repr)[0];\n\n    *p = '\\0';\n    _PyString_Resize(&repr, p - PyString_AS_STRING(repr));\n    return repr;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    // Get the input Tensors.\n    OpInputList params_nested_splits_in;\n    OP_REQUIRES_OK(context, context->input_list(\"params_nested_splits\",\n                                                &params_nested_splits_in));\n    const Tensor& params_dense_values_in =\n        context->input(params_nested_splits_in.size());\n    const Tensor& indices_in =\n        context->input(params_nested_splits_in.size() + 1);\n\n    DCHECK_GT(params_nested_splits_in.size(), 0);  // Enforced by REGISTER_OP.\n    SPLITS_TYPE num_params = params_nested_splits_in[0].dim_size(0) - 1;\n    OP_REQUIRES_OK(context, ValidateIndices(indices_in, num_params));\n\n    OP_REQUIRES(context, params_dense_values_in.dims() > 0,\n                errors::InvalidArgument(\"params.rank must be nonzero\"));\n    SPLITS_TYPE num_params_dense_values = params_dense_values_in.dim_size(0);\n\n    // Calculate the `splits`, and store the value slices that we need to\n    // copy in `value_slices`.\n    std::vector<std::pair<SPLITS_TYPE, SPLITS_TYPE>> value_slices;\n    SPLITS_TYPE num_values = 0;\n    std::vector<std::vector<SPLITS_TYPE>> out_splits;\n    OP_REQUIRES_OK(context, MakeSplits(indices_in, params_nested_splits_in,\n                                       num_params_dense_values, &out_splits,\n                                       &value_slices, &num_values));\n\n    // Write the output tensors.\n    OP_REQUIRES_OK(context, WriteSplits(out_splits, context));\n    OP_REQUIRES_OK(context,\n                   WriteValues(params_dense_values_in, value_slices,\n                               out_splits.size(), num_values, context));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def default(self, path=b\"\", action=None, **kwargs):\n        repo_obj = self.app.store.get_repo(path)\n        if kwargs.get('keepdays'):\n            return self._remove_older(repo_obj, **kwargs)\n        elif kwargs.get('new_encoding'):\n            return self._set_encoding(repo_obj, **kwargs)\n        elif kwargs.get('maxage'):\n            return self._set_maxage(repo_obj, **kwargs)\n        # Get page data.\n        params = {\n            'repo': repo_obj,\n            'keepdays': repo_obj.keepdays,\n        }\n        # Generate page.\n        return self._compile_template(\"settings.html\", **params)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def put_file(self, in_path, out_path):\n        ''' transfer a file from local to chroot '''\n\n        out_path = self._normalize_path(out_path, self.get_jail_path())\n        vvv(\"PUT %s TO %s\" % (in_path, out_path), host=self.jail)\n\n        self._copy_file(in_path, out_path)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def validate_value(self, value):\n        if not is_list_like(value):\n            self.invalidate_value(\n                value,\n                msg=\"must be list-like object such as array or tuple\",\n            )\n\n        if len(value) != len(self.encoders):\n            self.invalidate_value(\n                value,\n                exc=ValueOutOfBounds,\n                msg=\"value has {} items when {} were expected\".format(\n                    len(value),\n                    len(self.encoders),\n                ),\n            )\n\n        for item, encoder in zip(value, self.encoders):\n            try:\n                encoder.validate_value(item)\n            except AttributeError:\n                encoder(item)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "m3ApiRawFunction(m3_wasi_generic_fd_read)\n{\n    m3ApiReturnType  (uint32_t)\n    m3ApiGetArg      (uvwasi_fd_t          , fd)\n    m3ApiGetArgMem   (wasi_iovec_t *       , wasi_iovs)\n    m3ApiGetArg      (uvwasi_size_t        , iovs_len)\n    m3ApiGetArgMem   (uvwasi_size_t *      , nread)\n\n    m3ApiCheckMem(wasi_iovs,    iovs_len * sizeof(wasi_iovec_t));\n    m3ApiCheckMem(nread,        sizeof(uvwasi_size_t));\n\n#if defined(M3_COMPILER_MSVC)\n    if (iovs_len > 32) m3ApiReturn(UVWASI_EINVAL);\n    uvwasi_ciovec_t  iovs[32];\n#else\n    if (iovs_len > 128) m3ApiReturn(UVWASI_EINVAL);\n    uvwasi_ciovec_t  iovs[iovs_len];\n#endif\n    uvwasi_size_t num_read;\n    uvwasi_errno_t ret;\n\n    for (uvwasi_size_t i = 0; i < iovs_len; ++i) {\n        iovs[i].buf = m3ApiOffsetToPtr(m3ApiReadMem32(&wasi_iovs[i].buf));\n        iovs[i].buf_len = m3ApiReadMem32(&wasi_iovs[i].buf_len);\n\n        //fprintf(stderr, \"> fd_read fd:%d iov%d.len:%d\\n\", fd, i, iovs[i].buf_len);\n    }\n\n    ret = uvwasi_fd_read(&uvwasi, fd, (const uvwasi_iovec_t *) iovs, iovs_len, &num_read);\n\n    WASI_TRACE(\"fd:%d | nread:%d\", fd, num_read);\n\n    m3ApiWriteMem32(nread, num_read);\n    m3ApiReturn(ret);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _list_artifacts_mlflow_artifacts():\n    \"\"\"\n    A request handler for `GET /mlflow-artifacts/artifacts?path=<value>` to list artifacts in `path`\n    (a relative path from the root artifact directory).\n    \"\"\"\n    request_message = _get_request_message(ListArtifactsMlflowArtifacts())\n    if request_message.HasField(\"path\"):\n        validate_path_is_safe(request_message.path)\n        path = request_message.path\n    else:\n        path = None\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    files = []\n    for file_info in artifact_repo.list_artifacts(path):\n        basename = posixpath.basename(file_info.path)\n        new_file_info = FileInfo(basename, file_info.is_dir, file_info.file_size)\n        files.append(new_file_info.to_proto())\n    response_message = ListArtifacts.Response()\n    response_message.files.extend(files)\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def login(cls, username: str, password: str) -> t.Optional[int]:\n        \"\"\"\n        Make sure the user exists and the password is valid. If so, the\n        ``last_login`` value is updated in the database.\n\n        :returns:\n            The id of the user if a match is found, otherwise ``None``.\n\n        \"\"\"\n        if len(username) > cls.username.length:\n            logger.warning(\"Excessively long username provided.\")\n            return None\n\n        if len(password) > cls._max_password_length:\n            logger.warning(\"Excessively long password provided.\")\n            return None\n\n        response = (\n            await cls.select(cls._meta.primary_key, cls.password)\n            .where(cls.username == username)\n            .first()\n            .run()\n        )\n        if not response:\n            # No match found\n            return None\n\n        stored_password = response[\"password\"]\n\n        algorithm, iterations_, salt, hashed = cls.split_stored_password(\n            stored_password\n        )\n        iterations = int(iterations_)\n\n        if cls.hash_password(password, salt, iterations) == stored_password:\n            # If the password was hashed in an earlier Piccolo version, update\n            # it so it's hashed with the currently recommended number of\n            # iterations:\n            if iterations != cls._pbkdf2_iteration_count:\n                await cls.update_password(username, password)\n\n            await cls.update({cls.last_login: datetime.datetime.now()}).where(\n                cls.username == username\n            )\n            return response[\"id\"]\n        else:\n            return None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def cors_after_request(resp):\n        # If CORS headers are set in a view decorator, pass\n        if resp.headers is not None and resp.headers.get(ACL_ORIGIN):\n            LOG.debug('CORS have been already evaluated, skipping')\n            return resp\n\n        for res_regex, res_options in resources:\n            if try_match(request.path, res_regex):\n                LOG.debug(\"Request to '%s' matches CORS resource '%s'. Using options: %s\",\n                      request.path, get_regexp_pattern(res_regex), res_options)\n                set_cors_headers(resp, res_options)\n                break\n        else:\n            LOG.debug('No CORS rule matches')\n        return resp\n    return cors_after_request"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def boboAwareZopeTraverse(object, path_items, econtext):\n    \"\"\"Traverses a sequence of names, first trying attributes then items.\n\n    This uses zope.traversing path traversal where possible and interacts\n    correctly with objects providing OFS.interface.ITraversable when\n    necessary (bobo-awareness).\n    \"\"\"\n    request = getattr(econtext, 'request', None)\n    path_items = list(path_items)\n    path_items.reverse()\n\n    while path_items:\n        name = path_items.pop()\n        if OFS.interfaces.ITraversable.providedBy(object):\n            object = object.restrictedTraverse(name)\n        else:\n            object = traversePathElement(object, name, path_items,\n                                         request=request)\n    return object\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self) -> Model:\n        self.validate()\n        try:\n            dashboard = DashboardDAO.create(self._properties, commit=False)\n            dashboard = DashboardDAO.update_charts_owners(dashboard, commit=True)\n        except DAOCreateFailedError as ex:\n            logger.exception(ex.exception)\n            raise DashboardCreateFailedError() from ex\n        return dashboard\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _get_most_recent_version(self, allow_local):\n        # type: (bool) -> CryptographicMaterialsProvider\n        \"\"\"Get the most recent version of the provider.\n\n        If allowing local and we cannot obtain the lock, just return the most recent local\n        version. Otherwise, wait for the lock and ask the provider store for the most recent\n        version of the provider.\n\n        :param bool allow_local: Should we allow returning the local version if we cannot obtain the lock?\n        :returns: version and corresponding cryptographic materials provider\n        :rtype: CryptographicMaterialsProvider\n        \"\"\"\n        acquired = self._lock.acquire(not allow_local)\n\n        if not acquired:\n            # We failed to acquire the lock.\n            # If blocking, we will never reach this point.\n            # If not blocking, we want whatever the latest local version is.\n            _LOGGER.debug(\"Failed to acquire lock. Returning the last cached version.\")\n            version = self._version\n            return self._cache.get(version)\n\n        try:\n            max_version = self._get_max_version()\n            try:\n                provider = self._cache.get(max_version)\n            except KeyError:\n                provider = self._get_provider(max_version)\n            received_version = self._provider_store.version_from_material_description(\n                provider._material_description  # pylint: disable=protected-access\n            )\n\n            _LOGGER.debug(\"Caching materials provider version %d\", received_version)\n            self._version = received_version  # pylint: disable=attribute-defined-outside-init\n            self._last_updated = time.time()  # pylint: disable=attribute-defined-outside-init\n            self._cache.put(received_version, provider)\n        finally:\n            self._lock.release()\n\n        _LOGGER.debug(\"New latest version is %d\", self._version)\n\n        return provider\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def authorisation_window_loaded(self):\n        for window in webview.windows[:]:  # iterate over a copy; remove (in destroy()) from original\n            if not hasattr(window, 'get_title'):\n                continue  # skip dummy window\n\n            url = window.get_current_url()\n            account_name = window.get_title(window).split(' ')[-1]  # see note above: title *must* match this format\n            if not url or not account_name:\n                continue  # skip any invalid windows\n\n            # respond to both the original request and any duplicates in the list\n            completed_request = None\n            for request in self.authorisation_requests[:]:  # iterate over a copy; remove from original\n                if url.startswith(request['redirect_uri']) and account_name == request['username']:\n                    Log.info('Successfully authorised request for', request['username'])\n                    RESPONSE_QUEUE.put({'connection': request['connection'], 'response_url': url})\n                    self.authorisation_requests.remove(request)\n                    completed_request = request\n                else:\n                    Log.debug('Waiting for URL matching `redirect_uri`; following browser redirection to',\n                              '%s/[...]' % urllib.parse.urlparse(url).hostname)\n\n            if completed_request is None:\n                continue  # no requests processed for this window - nothing to do yet\n\n            window.destroy()\n            self.icon.update_menu()\n\n            # note that in this part of the interaction we don't actually check the *use* of the authorisation code,\n            # but just whether it was successfully acquired - if there is an error in the subsequent access/refresh\n            # token request then we still send an 'authentication completed' notification here, but in the background\n            # we close the connection with a failure message and re-request authorisation next time the client\n            # interacts, which may potentially lead to repeated and conflicting (and confusing) notifications - improve?\n            if len(self.authorisation_requests) > 0:\n                self.notify(APP_NAME,\n                            'Authentication completed for %s. Please authorise an additional account %s from the '\n                            'menu' % (completed_request['username'], self.authorisation_requests[0]['username']))\n            else:\n                self.notify(APP_NAME, 'Authentication completed for %s' % completed_request['username'])\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _async_render_POST(self, request):\n        try:\n            try:\n                # json.loads doesn't allow bytes in Python 3.5\n                body = json.loads(request.content.read().decode(\"UTF-8\"))\n            except ValueError:\n                request.setResponseCode(400)\n                request.write(dict_to_json_bytes({'errcode': 'M_BAD_JSON', 'error': 'Malformed JSON'}))\n                request.finish()\n                return\n\n            missing = [k for k in (\"threepid\", \"mxid\") if k not in body]\n            if len(missing) > 0:\n                request.setResponseCode(400)\n                msg = \"Missing parameters: \"+(\",\".join(missing))\n                request.write(dict_to_json_bytes({'errcode': 'M_MISSING_PARAMS', 'error': msg}))\n                request.finish()\n                return\n\n            threepid = body['threepid']\n            mxid = body['mxid']\n\n            if 'medium' not in threepid or 'address' not in threepid:\n                request.setResponseCode(400)\n                request.write(dict_to_json_bytes({'errcode': 'M_MISSING_PARAMS', 'error': 'Threepid lacks medium / address'}))\n                request.finish()\n                return\n\n            # We now check for authentication in two different ways, depending\n            # on the contents of the request. If the user has supplied \"sid\"\n            # (the Session ID returned by Sydent during the original binding)\n            # and \"client_secret\" fields, they are trying to prove that they\n            # were the original author of the bind. We then check that what\n            # they supply matches and if it does, allow the unbind.\n            # \n            # However if these fields are not supplied, we instead check\n            # whether the request originated from a homeserver, and if so the\n            # same homeserver that originally created the bind. We do this by\n            # checking the signature of the request. If it all matches up, we\n            # allow the unbind.\n            #\n            # Only one method of authentication is required.\n            if 'sid' in body and 'client_secret' in body:\n                sid = body['sid']\n                client_secret = body['client_secret']\n\n                if not is_valid_client_secret(client_secret):\n                    request.setResponseCode(400)\n                    request.write(dict_to_json_bytes({\n                        'errcode': 'M_INVALID_PARAM',\n                        'error': 'Invalid client_secret provided'\n                    }))\n                    request.finish()\n                    return\n\n                valSessionStore = ThreePidValSessionStore(self.sydent)\n\n                try:\n                    s = valSessionStore.getValidatedSession(sid, client_secret)\n                except (IncorrectClientSecretException, InvalidSessionIdException):\n                    request.setResponseCode(401)\n                    request.write(dict_to_json_bytes({\n                        'errcode': 'M_NO_VALID_SESSION',\n                        'error': \"No valid session was found matching that sid and client secret\"\n                    }))\n                    request.finish()\n                    return\n                except SessionNotValidatedException:\n                    request.setResponseCode(403)\n                    request.write(dict_to_json_bytes({\n                        'errcode': 'M_SESSION_NOT_VALIDATED',\n                        'error': \"This validation session has not yet been completed\"\n                    }))\n                    return\n                \n                if s.medium != threepid['medium'] or s.address != threepid['address']:\n                    request.setResponseCode(403)\n                    request.write(dict_to_json_bytes({\n                        'errcode': 'M_FORBIDDEN',\n                        'error': 'Provided session information does not match medium/address combo',\n                    }))\n                    request.finish()\n                    return\n            else:\n                try:\n                    origin_server_name = yield self.sydent.sig_verifier.authenticate_request(request, body)\n                except SignatureVerifyException as ex:\n                    request.setResponseCode(401)\n                    request.write(dict_to_json_bytes({'errcode': 'M_FORBIDDEN', 'error': str(ex)}))\n                    request.finish()\n                    return\n                except NoAuthenticationError as ex:\n                    request.setResponseCode(401)\n                    request.write(dict_to_json_bytes({'errcode': 'M_FORBIDDEN', 'error': str(ex)}))\n                    request.finish()\n                    return\n                except:\n                    logger.exception(\"Exception whilst authenticating unbind request\")\n                    request.setResponseCode(500)\n                    request.write(dict_to_json_bytes({'errcode': 'M_UNKNOWN', 'error': 'Internal Server Error'}))\n                    request.finish()\n                    return\n\n                if not mxid.endswith(':' + origin_server_name):\n                    request.setResponseCode(403)\n                    request.write(dict_to_json_bytes({'errcode': 'M_FORBIDDEN', 'error': 'Origin server name does not match mxid'}))\n                    request.finish()\n                    return\n\n            self.sydent.threepidBinder.removeBinding(threepid, mxid)\n\n            request.write(dict_to_json_bytes({}))\n            request.finish()\n        except Exception as ex:\n            logger.exception(\"Exception whilst handling unbind\")\n            request.setResponseCode(500)\n            request.write(dict_to_json_bytes({'errcode': 'M_UNKNOWN', 'error': str(ex)}))\n            request.finish()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_python_instance_state(self, instance, state):\n        if hasattr(instance, '__setstate__'):\n            instance.__setstate__(state)\n        else:\n            slotstate = {}\n            if isinstance(state, tuple) and len(state) == 2:\n                state, slotstate = state\n            if hasattr(instance, '__dict__'):\n                instance.__dict__.update(state)\n            elif state:\n                slotstate.update(state)\n            for key, value in slotstate.items():\n                setattr(instance, key, value)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def handler_for(calldata_kwargs, default_kwargs):\n        calldata_args = sig.base_args + calldata_kwargs\n        # create a fake type so that get_element_ptr works\n        calldata_args_t = TupleT(list(arg.typ for arg in calldata_args))\n\n        abi_sig = sig.abi_signature_for_kwargs(calldata_kwargs)\n        method_id = _annotated_method_id(abi_sig)\n\n        calldata_kwargs_ofst = IRnode(\n            4, location=CALLDATA, typ=calldata_args_t, encoding=Encoding.ABI\n        )\n\n        # a sequence of statements to strictify kwargs into memory\n        ret = [\"seq\"]\n\n        # ensure calldata is at least of minimum length\n        args_abi_t = calldata_args_t.abi_type\n        calldata_min_size = args_abi_t.min_size() + 4\n        ret.append([\"assert\", [\"ge\", \"calldatasize\", calldata_min_size]])\n\n        # TODO optimize make_setter by using\n        # TupleT(list(arg.typ for arg in calldata_kwargs + default_kwargs))\n        # (must ensure memory area is contiguous)\n\n        n_base_args = len(sig.base_args)\n\n        for i, arg_meta in enumerate(calldata_kwargs):\n            k = n_base_args + i\n\n            dst = context.lookup_var(arg_meta.name).pos\n\n            lhs = IRnode(dst, location=MEMORY, typ=arg_meta.typ)\n\n            rhs = get_element_ptr(calldata_kwargs_ofst, k, array_bounds_check=False)\n\n            copy_arg = make_setter(lhs, rhs)\n            copy_arg.source_pos = getpos(arg_meta.ast_source)\n            ret.append(copy_arg)\n\n        for x in default_kwargs:\n            dst = context.lookup_var(x.name).pos\n            lhs = IRnode(dst, location=MEMORY, typ=x.typ)\n            lhs.source_pos = getpos(x.ast_source)\n            kw_ast_val = sig.default_values[x.name]  # e.g. `3` in x: int = 3\n            rhs = Expr(kw_ast_val, context).ir_node\n\n            copy_arg = make_setter(lhs, rhs)\n            copy_arg.source_pos = getpos(x.ast_source)\n            ret.append(copy_arg)\n\n        ret.append([\"goto\", sig.external_function_base_entry_label])\n\n        ret = [\"if\", [\"eq\", \"_calldata_method_id\", method_id], ret]\n        return ret\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_oauth2_credentials(username, password, connection_info, recurse_retries=True):\n        \"\"\"Using the given username (i.e., email address) and password, reads account details from AppConfig and\n        handles OAuth 2.0 token request and renewal, saving the updated details back to AppConfig (or removing them\n        if invalid). Returns either (True, '[OAuth2 string for authentication]') or (False, '[Error message]')\"\"\"\n        if username not in AppConfig.accounts():\n            Log.error('Proxy config file entry missing for account', username, '- aborting login')\n            return (False, '%s: No config file entry found for account %s - please add a new section with values '\n                           'for permission_url, token_url, oauth2_scope, redirect_uri, client_id and '\n                           'client_secret' % (APP_NAME, username))\n\n        config = AppConfig.get()\n        current_time = int(time.time())\n\n        permission_url = config.get(username, 'permission_url', fallback=None)\n        token_url = config.get(username, 'token_url', fallback=None)\n        oauth2_scope = config.get(username, 'oauth2_scope', fallback=None)\n        redirect_uri = config.get(username, 'redirect_uri', fallback=None)\n        redirect_listen_address = config.get(username, 'redirect_listen_address', fallback=None)\n        client_id = config.get(username, 'client_id', fallback=None)\n        client_secret = config.get(username, 'client_secret', fallback=None)\n\n        # note that we don't require client_secret here because it can be optional for Office 365 configurations\n        if not (permission_url and token_url and oauth2_scope and redirect_uri and client_id):\n            Log.error('Proxy config file entry incomplete for account', username, '- aborting login')\n            return (False, '%s: Incomplete config file entry found for account %s - please make sure all required '\n                           'fields are added (permission_url, token_url, oauth2_scope, redirect_uri, client_id '\n                           'and client_secret)' % (APP_NAME, username))\n\n        # while not technically forbidden (RFC 6749, A.1 and A.2), it is highly unlikely the example value is valid\n        example_client_value = '*** your client'\n        example_client_status = [example_client_value in i for i in [client_id, client_secret] if i]\n        if any(example_client_status):\n            if all(example_client_status) or example_client_value in client_id:\n                Log.info('Warning: client configuration for account', username, 'seems to contain example values -',\n                         'if authentication fails, please double-check these values are correct')\n            elif example_client_value in client_secret:\n                Log.info('Warning: client secret for account', username, 'seems to contain the example value - if you',\n                         'are using an Office 365 setup that does not need a secret, please delete this line entirely;',\n                         'otherwise, if authentication fails, please double-check this value is correct')\n\n        token_salt = config.get(username, 'token_salt', fallback=None)\n        access_token = config.get(username, 'access_token', fallback=None)\n        access_token_expiry = config.getint(username, 'access_token_expiry', fallback=current_time)\n        refresh_token = config.get(username, 'refresh_token', fallback=None)\n\n        # we hash locally-stored tokens with the given password\n        if not token_salt:\n            token_salt = base64.b64encode(os.urandom(16)).decode('utf-8')\n\n        # generate encrypter/decrypter based on password and random salt\n        key_derivation_function = PBKDF2HMAC(algorithm=hashes.SHA256(), length=32,\n                                             salt=base64.b64decode(token_salt.encode('utf-8')), iterations=100000,\n                                             backend=default_backend())\n        key = base64.urlsafe_b64encode(key_derivation_function.derive(password.encode('utf-8')))\n        cryptographer = Fernet(key)\n\n        try:\n            if not refresh_token:\n                permission_url = OAuth2Helper.construct_oauth2_permission_url(permission_url, redirect_uri, client_id,\n                                                                              oauth2_scope, username)\n                # note: get_oauth2_authorisation_code is a blocking call\n                success, authorisation_code = OAuth2Helper.get_oauth2_authorisation_code(permission_url, redirect_uri,\n                                                                                         redirect_listen_address,\n                                                                                         username, connection_info)\n                if not success:\n                    Log.info('Authentication request failed or expired for account', username, '- aborting login')\n                    return False, '%s: Login failed - the authentication request expired or was cancelled for ' \\\n                                  'account %s' % (APP_NAME, username)\n\n                response = OAuth2Helper.get_oauth2_authorisation_tokens(token_url, redirect_uri, client_id,\n                                                                        client_secret, authorisation_code)\n\n                access_token = response['access_token']\n                config.set(username, 'token_salt', token_salt)\n                config.set(username, 'access_token', OAuth2Helper.encrypt(cryptographer, access_token))\n                config.set(username, 'access_token_expiry', str(current_time + response['expires_in']))\n                config.set(username, 'refresh_token', OAuth2Helper.encrypt(cryptographer, response['refresh_token']))\n                AppConfig.save()\n\n            else:\n                if access_token_expiry - current_time < TOKEN_EXPIRY_MARGIN:  # if expiring soon, refresh token\n                    response = OAuth2Helper.refresh_oauth2_access_token(token_url, client_id, client_secret,\n                                                                        OAuth2Helper.decrypt(cryptographer,\n                                                                                             refresh_token))\n\n                    access_token = response['access_token']\n                    config.set(username, 'access_token', OAuth2Helper.encrypt(cryptographer, access_token))\n                    config.set(username, 'access_token_expiry', str(current_time + response['expires_in']))\n                    if 'refresh_token' in response:\n                        config.set(username, 'refresh_token',\n                                   OAuth2Helper.encrypt(cryptographer, response['refresh_token']))\n                    AppConfig.save()\n                else:\n                    access_token = OAuth2Helper.decrypt(cryptographer, access_token)\n\n            # send authentication command to server (response checked in ServerConnection) - note: we only support\n            # single-trip authentication (SASL) without actually checking the server's capabilities - improve?\n            oauth2_string = OAuth2Helper.construct_oauth2_string(username, access_token)\n            return True, oauth2_string\n\n        except InvalidToken as e:\n            # if invalid details are the reason for failure we remove our cached version and re-authenticate - this can\n            # be disabled by a configuration setting, but note that we always remove credentials on 400 Bad Request\n            if e.args == (400, APP_PACKAGE) or AppConfig.globals().getboolean('delete_account_token_on_password_error',\n                                                                              fallback=True):\n                config.remove_option(username, 'token_salt')\n                config.remove_option(username, 'access_token')\n                config.remove_option(username, 'access_token_expiry')\n                config.remove_option(username, 'refresh_token')\n                AppConfig.save()\n            else:\n                recurse_retries = False  # no need to recurse if we are just trying the same credentials again\n\n            if recurse_retries:\n                Log.info('Retrying login due to exception while requesting OAuth 2.0 credentials:', Log.error_string(e))\n                return OAuth2Helper.get_oauth2_credentials(username, password, connection_info, recurse_retries=False)\n            else:\n                Log.error('Invalid password to decrypt', username, 'credentials - aborting login:', Log.error_string(e))\n                return False, '%s: Login failed - the password for account %s is incorrect' % (APP_NAME, username)\n\n        except Exception as e:\n            # note that we don't currently remove cached credentials here, as failures on the initial request are\n            # before caching happens, and the assumption is that refresh token request exceptions are temporal (e.g.,\n            # network errors: URLError(OSError(50, 'Network is down'))) rather than e.g., bad requests\n            Log.info('Caught exception while requesting OAuth 2.0 credentials:', Log.error_string(e))\n            return False, '%s: Login failed for account %s - please check your internet connection and retry' % (\n                APP_NAME, username)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _configure_session(cls, app):\n        tempdir = app.config[\"PYLOAD_API\"].get_cachedir()\n        cache_path = os.path.join(tempdir, \"flask\")\n        os.makedirs(cache_path, exist_ok=True)\n\n        app.config[\"SESSION_FILE_DIR\"] = cache_path\n        app.config[\"SESSION_TYPE\"] = \"filesystem\"\n        app.config[\"SESSION_COOKIE_NAME\"] = \"pyload_session\"\n        app.config[\"SESSION_COOKIE_SAMESITE\"] = \"None\"\n        app.config[\"SESSION_COOKIE_SECURE\"] = app.config[\"PYLOAD_API\"].get_config_value(\"webui\", \"use_ssl\")\n        app.config[\"SESSION_PERMANENT\"] = False\n\n        session_lifetime = max(app.config[\"PYLOAD_API\"].get_config_value(\"webui\", \"session_lifetime\"), 1) * 60\n        app.config[\"PERMANENT_SESSION_LIFETIME\"] = session_lifetime\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __post_init__(self) -> None:\n        super().__post_init__()\n        if self.default is not None:\n            self.default = f'\"{self.default}\"'\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def build_IR(self, expr, args, kwargs, context):\n        unwrap_tuple = kwargs[\"unwrap_tuple\"]\n\n        data = args[0]\n        output_typ = args[1]\n        wrapped_typ = output_typ\n\n        if unwrap_tuple is True:\n            wrapped_typ = calculate_type_for_external_return(output_typ)\n\n        abi_size_bound = wrapped_typ.abi_type.size_bound()\n        abi_min_size = wrapped_typ.abi_type.min_size()\n\n        # Get the size of data\n        input_max_len = data.typ.maxlen\n\n        assert abi_min_size <= abi_size_bound, \"bad abi type\"\n        if input_max_len < abi_size_bound:\n            raise StructureException(\n                (\n                    \"Mismatch between size of input and size of decoded types. \"\n                    f\"length of ABI-encoded {wrapped_typ} must be equal to or greater \"\n                    f\"than {abi_size_bound}\"\n                ),\n                expr.args[0],\n            )\n\n        data = ensure_in_memory(data, context)\n        with data.cache_when_complex(\"to_decode\") as (b1, data):\n            data_ptr = bytes_data_ptr(data)\n            data_len = get_bytearray_length(data)\n\n            # Normally, ABI-encoded data assumes the argument is a tuple\n            # (See comments for `wrap_value_for_external_return`)\n            # However, we do not want to use `wrap_value_for_external_return`\n            # technique as used in external call codegen because in order to be\n            # type-safe we would need an extra memory copy. To avoid a copy,\n            # we manually add the ABI-dynamic offset so that it is\n            # re-interpreted in-place.\n            if (\n                unwrap_tuple is True\n                and needs_external_call_wrap(output_typ)\n                and output_typ.abi_type.is_dynamic()\n            ):\n                data_ptr = add_ofst(data_ptr, 32)\n\n            ret = [\"seq\"]\n\n            if abi_min_size == abi_size_bound:\n                ret.append([\"assert\", [\"eq\", abi_min_size, data_len]])\n            else:\n                # runtime assert: abi_min_size <= data_len <= abi_size_bound\n                ret.append(clamp2(abi_min_size, data_len, abi_size_bound, signed=False))\n\n            # return pointer to the buffer\n            ret.append(data_ptr)\n\n            return b1.resolve(\n                IRnode.from_list(\n                    ret,\n                    typ=output_typ,\n                    location=data.location,\n                    encoding=Encoding.ABI,\n                    annotation=f\"abi_decode({output_typ})\",\n                )\n            )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get(self, path: str) -> None:\n        parts = path.split(\"/\")\n        component_name = parts[0]\n        component_root = self._registry.get_component_path(component_name)\n        if component_root is None:\n            self.write(\"not found\")\n            self.set_status(404)\n            return\n\n        # follow symlinks to get an accurate normalized path\n        component_root = os.path.realpath(component_root)\n        filename = \"/\".join(parts[1:])\n        abspath = os.path.realpath(os.path.join(component_root, filename))\n\n        # Do NOT expose anything outside of the component root.\n        if os.path.commonprefix([component_root, abspath]) != component_root or (\n            not os.path.normpath(abspath).startswith(\n                component_root\n            )  # this is a recommendation from CodeQL, probably a bit redundant\n        ):\n            self.write(\"forbidden\")\n            self.set_status(403)\n            return\n        try:\n            with open(abspath, \"rb\") as file:\n                contents = file.read()\n        except (OSError) as e:\n            _LOGGER.error(\n                \"ComponentRequestHandler: GET %s read error\", abspath, exc_info=e\n            )\n            self.write(\"read error\")\n            self.set_status(404)\n            return\n\n        self.write(contents)\n        self.set_header(\"Content-Type\", self.get_content_type(abspath))\n\n        self.set_extra_headers(path)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def generate_nonce(length=8):\n    \"\"\"Generate pseudorandom number.\"\"\"\n    return ''.join([str(random.randint(0, 9)) for i in range(length)])\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, data):\n        if self.USE_TIMESTAMP and self.TIMESTAMP_FIELD not in data:\n            data = data.copy()\n            data[self.TIMESTAMP_FIELD] = get_current_timestamp()\n        self._data = data\n        self._salt = self._calculate_salt(data)\n        self._signer = Signer(self._salt)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def handle(request, message=None, redirect=None, ignore=False, escalate=False):\n    \"\"\" Centralized error handling for Horizon.\n\n    Because Horizon consumes so many different APIs with completely\n    different ``Exception`` types, it's necessary to have a centralized\n    place for handling exceptions which may be raised.\n\n    Exceptions are roughly divided into 3 types:\n\n    #. ``UNAUTHORIZED``: Errors resulting from authentication or authorization\n       problems. These result in being logged out and sent to the login screen.\n    #. ``NOT_FOUND``: Errors resulting from objects which could not be\n       located via the API. These generally result in a user-facing error\n       message, but are otherwise returned to the normal code flow. Optionally\n       a redirect value may be passed to the error handler so users are\n       returned to a different view than the one requested in addition to the\n       error message.\n    #. RECOVERABLE: Generic API errors which generate a user-facing message\n       but drop directly back to the regular code flow.\n\n    All other exceptions bubble the stack as normal unless the ``ignore``\n    argument is passed in as ``True``, in which case only unrecognized\n    errors are bubbled.\n\n    If the exception is not re-raised, an appropriate wrapper exception\n    class indicating the type of exception that was encountered will be\n    returned.\n    \"\"\"\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n\n    # Because the same exception may travel through this method more than\n    # once (if it's re-raised) we may want to treat it differently\n    # the second time (e.g. no user messages/logging).\n    handled = issubclass(exc_type, HandledException)\n    wrap = False\n\n    # Restore our original exception information, but re-wrap it at the end\n    if handled:\n        exc_type, exc_value, exc_traceback = exc_value.wrapped\n        wrap = True\n\n    # We trust messages from our own exceptions\n    if issubclass(exc_type, HorizonException):\n        message = exc_value\n    # If the message has a placeholder for the exception, fill it in\n    elif message and \"%(exc)s\" in message:\n        message = message % {\"exc\": exc_value}\n\n    if issubclass(exc_type, UNAUTHORIZED):\n        if ignore:\n            return NotAuthorized\n        request.session.clear()\n        if not handled:\n            LOG.debug(\"Unauthorized: %s\" % exc_value)\n            # We get some pretty useless error messages back from\n            # some clients, so let's define our own fallback.\n            fallback = _(\"Unauthorized. Please try logging in again.\")\n            messages.error(request, message or fallback, extra_tags=\"login\")\n        raise NotAuthorized  # Redirect handled in middleware\n\n    if issubclass(exc_type, NOT_FOUND):\n        wrap = True\n        if not ignore and not handled:\n            LOG.debug(\"Not Found: %s\" % exc_value)\n            messages.error(request, message or exc_value)\n        if redirect:\n            raise Http302(redirect)\n        if not escalate:\n            return NotFound  # return to normal code flow\n\n    if issubclass(exc_type, RECOVERABLE):\n        wrap = True\n        if not ignore and not handled:\n            LOG.debug(\"Recoverable error: %s\" % exc_value)\n            messages.error(request, message or exc_value)\n        if redirect:\n            raise Http302(redirect)\n        if not escalate:\n            return RecoverableError  # return to normal code flow\n\n    # If we've gotten here, time to wrap and/or raise our exception.\n    if wrap:\n        raise HandledException([exc_type, exc_value, exc_traceback])\n    raise exc_type, exc_value, exc_traceback\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _handle_incoming_transaction(self, transaction, request_time):\n        \"\"\" Process an incoming transaction and return the HTTP response\n\n        Args:\n            transaction (Transaction): incoming transaction\n            request_time (int): timestamp that the HTTP request arrived at\n\n        Returns:\n            Deferred[(int, object)]: http response code and body\n        \"\"\"\n        response = yield self.transaction_actions.have_responded(transaction)\n\n        if response:\n            logger.debug(\n                \"[%s] We've already responded to this request\",\n                transaction.transaction_id\n            )\n            defer.returnValue(response)\n            return\n\n        logger.debug(\"[%s] Transaction is new\", transaction.transaction_id)\n\n        received_pdus_counter.inc(len(transaction.pdus))\n\n        origin_host, _ = parse_server_name(transaction.origin)\n\n        pdus_by_room = {}\n\n        for p in transaction.pdus:\n            if \"unsigned\" in p:\n                unsigned = p[\"unsigned\"]\n                if \"age\" in unsigned:\n                    p[\"age\"] = unsigned[\"age\"]\n            if \"age\" in p:\n                p[\"age_ts\"] = request_time - int(p[\"age\"])\n                del p[\"age\"]\n\n            event = event_from_pdu_json(p)\n            room_id = event.room_id\n            pdus_by_room.setdefault(room_id, []).append(event)\n\n        pdu_results = {}\n\n        # we can process different rooms in parallel (which is useful if they\n        # require callouts to other servers to fetch missing events), but\n        # impose a limit to avoid going too crazy with ram/cpu.\n\n        @defer.inlineCallbacks\n        def process_pdus_for_room(room_id):\n            logger.debug(\"Processing PDUs for %s\", room_id)\n            try:\n                yield self.check_server_matches_acl(origin_host, room_id)\n            except AuthError as e:\n                logger.warn(\n                    \"Ignoring PDUs for room %s from banned server\", room_id,\n                )\n                for pdu in pdus_by_room[room_id]:\n                    event_id = pdu.event_id\n                    pdu_results[event_id] = e.error_dict()\n                return\n\n            for pdu in pdus_by_room[room_id]:\n                event_id = pdu.event_id\n                try:\n                    yield self._handle_received_pdu(\n                        transaction.origin, pdu\n                    )\n                    pdu_results[event_id] = {}\n                except FederationError as e:\n                    logger.warn(\"Error handling PDU %s: %s\", event_id, e)\n                    pdu_results[event_id] = {\"error\": str(e)}\n                except Exception as e:\n                    f = failure.Failure()\n                    pdu_results[event_id] = {\"error\": str(e)}\n                    logger.error(\n                        \"Failed to handle PDU %s: %s\",\n                        event_id, f.getTraceback().rstrip(),\n                    )\n\n        yield async.concurrently_execute(\n            process_pdus_for_room, pdus_by_room.keys(),\n            TRANSACTION_CONCURRENCY_LIMIT,\n        )\n\n        if hasattr(transaction, \"edus\"):\n            for edu in (Edu(**x) for x in transaction.edus):\n                yield self.received_edu(\n                    transaction.origin,\n                    edu.edu_type,\n                    edu.content\n                )\n\n        response = {\n            \"pdus\": pdu_results,\n        }\n\n        logger.debug(\"Returning: %s\", str(response))\n\n        yield self.transaction_actions.set_response(\n            transaction,\n            200, response\n        )\n        defer.returnValue((200, response))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _not_here_yet(request, *args, **kwargs):\n    return HttpResponse(\"Not here yet: %s (%r, %r)\" % (request.path, args, kwargs), status=410)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_comparison_class_for_block(block):\n    if hasattr(block, 'get_comparison_class'):\n        return block.get_comparison_class()\n    elif isinstance(block, blocks.CharBlock):\n        return CharBlockComparison\n    elif isinstance(block, blocks.RichTextBlock):\n        return RichTextBlockComparison\n    elif isinstance(block, blocks.StructBlock):\n        return StructBlockComparison\n    else:\n        # As all stream field blocks have a HTML representation, fall back to diffing that.\n        return RichTextBlockComparison\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_task_instances_batch(session: Session = NEW_SESSION) -> APIResponse:\n    \"\"\"Get list of task instances.\"\"\"\n    body = get_json_request_dict()\n    try:\n        data = task_instance_batch_form.load(body)\n    except ValidationError as err:\n        raise BadRequest(detail=str(err.messages))\n    states = _convert_ti_states(data[\"state\"])\n    base_query = select(TI).join(TI.dag_run)\n\n    base_query = _apply_array_filter(base_query, key=TI.dag_id, values=data[\"dag_ids\"])\n    base_query = _apply_array_filter(base_query, key=TI.run_id, values=data[\"dag_run_ids\"])\n    base_query = _apply_array_filter(base_query, key=TI.task_id, values=data[\"task_ids\"])\n    base_query = _apply_range_filter(\n        base_query,\n        key=DR.execution_date,\n        value_range=(data[\"execution_date_gte\"], data[\"execution_date_lte\"]),\n    )\n    base_query = _apply_range_filter(\n        base_query,\n        key=TI.start_date,\n        value_range=(data[\"start_date_gte\"], data[\"start_date_lte\"]),\n    )\n    base_query = _apply_range_filter(\n        base_query, key=TI.end_date, value_range=(data[\"end_date_gte\"], data[\"end_date_lte\"])\n    )\n    base_query = _apply_range_filter(\n        base_query, key=TI.duration, value_range=(data[\"duration_gte\"], data[\"duration_lte\"])\n    )\n    base_query = _apply_array_filter(base_query, key=TI.state, values=states)\n    base_query = _apply_array_filter(base_query, key=TI.pool, values=data[\"pool\"])\n    base_query = _apply_array_filter(base_query, key=TI.queue, values=data[\"queue\"])\n\n    # Count elements before joining extra columns\n    total_entries = get_query_count(base_query, session=session)\n    # Add join\n    base_query = base_query.join(\n        SlaMiss,\n        and_(\n            SlaMiss.dag_id == TI.dag_id,\n            SlaMiss.task_id == TI.task_id,\n            SlaMiss.execution_date == DR.execution_date,\n        ),\n        isouter=True,\n    ).add_columns(SlaMiss)\n    ti_query = base_query.options(joinedload(TI.rendered_task_instance_fields))\n    # using execute because we want the SlaMiss entity. Scalars don't return None for missing entities\n    task_instances = session.execute(ti_query).all()\n\n    return task_instance_collection_schema.dump(\n        TaskInstanceCollection(task_instances=task_instances, total_entries=total_entries)\n    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& diagonal = context->input(0);\n\n    // MatrixDiag and MatrixDiagV2 both use this OpKernel. MatrixDiag only has\n    // one input, so we have to check the number of inputs before reading\n    // additional parameters in MatrixDiagV2.\n    int32_t lower_diag_index = 0;\n    int32_t upper_diag_index = 0;\n    int32_t num_rows = -1;\n    int32_t num_cols = -1;\n    T padding_value(0);\n\n    // MatrixDiagOpV2-specific.\n    if (context->num_inputs() > kNumV1Inputs) {\n      auto& diag_index = context->input(1);\n      OP_REQUIRES(context,\n                  TensorShapeUtils::IsScalar(diag_index.shape()) ||\n                      TensorShapeUtils::IsVector(diag_index.shape()),\n                  errors::InvalidArgument(\n                      \"diag_index must be a scalar or vector, received shape: \",\n                      diag_index.shape().DebugString()));\n      lower_diag_index = diag_index.flat<int32>()(0);\n      upper_diag_index = lower_diag_index;\n      if (TensorShapeUtils::IsVector(diag_index.shape())) {\n        auto diag_index_size = diag_index.dim_size(0);\n        OP_REQUIRES(\n            context, 0 < diag_index_size && diag_index_size <= 2,\n            errors::InvalidArgument(\n                \"diag_index must have only one or two elements, received \",\n                diag_index_size, \" elements.\"));\n        if (diag_index_size > 1) {\n          upper_diag_index = diag_index.flat<int32>()(1);\n        }\n      }\n\n      auto& num_rows_tensor = context->input(2);\n      OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_rows_tensor.shape()),\n                  errors::InvalidArgument(\"num_rows must be a scalar\"));\n      num_rows = num_rows_tensor.flat<int32>()(0);\n\n      auto& num_cols_tensor = context->input(3);\n      OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_cols_tensor.shape()),\n                  errors::InvalidArgument(\"num_cols must be a scalar\"));\n      num_cols = num_cols_tensor.flat<int32>()(0);\n\n      auto& padding_value_tensor = context->input(4);\n      OP_REQUIRES(context,\n                  TensorShapeUtils::IsScalar(padding_value_tensor.shape()),\n                  errors::InvalidArgument(\"padding_value must be a scalar\"));\n      padding_value = padding_value_tensor.flat<T>()(0);\n    }\n\n    // Size validations.\n    const TensorShape& diagonal_shape = diagonal.shape();\n    const int diag_rank = diagonal_shape.dims();\n    const Eigen::Index num_diags = upper_diag_index - lower_diag_index + 1;\n    OP_REQUIRES(context, TensorShapeUtils::IsVectorOrHigher(diagonal_shape),\n                errors::InvalidArgument(\n                    \"diagonal must be at least 1-dim, received shape: \",\n                    diagonal.shape().DebugString()));\n    OP_REQUIRES(\n        context, lower_diag_index <= upper_diag_index,\n        errors::InvalidArgument(\n            \"lower_diag_index must not be larger than upper_diag_index: \",\n            lower_diag_index, \" > \", upper_diag_index));\n    OP_REQUIRES(context,\n                lower_diag_index == upper_diag_index ||\n                    diagonal_shape.dim_size(diag_rank - 2) == num_diags,\n                errors::InvalidArgument(\n                    \"The number of diagonals provided in the input does not \"\n                    \"match the lower_diag_index and upper_diag_index range.\"));\n\n    const Eigen::Index max_diag_len = diagonal_shape.dim_size(diag_rank - 1);\n    const int32_t min_num_rows = max_diag_len - std::min(upper_diag_index, 0);\n    const int32_t min_num_cols = max_diag_len + std::max(lower_diag_index, 0);\n    OP_REQUIRES(context, num_rows == -1 || num_rows >= min_num_rows,\n                errors::InvalidArgument(\"The number of rows is too small.\"));\n    OP_REQUIRES(context, num_cols == -1 || num_cols >= min_num_cols,\n                errors::InvalidArgument(\"The number of columns is too small.\"));\n\n    // If both num_rows and num_cols are unknown, assume that output is square.\n    // Otherwise, use smallest possible values.\n    if (num_rows == -1 && num_cols == -1) {\n      num_rows = std::max(min_num_rows, min_num_cols);\n      num_cols = num_rows;\n    } else if (num_rows == -1) {\n      num_rows = min_num_rows;\n    } else if (num_cols == -1) {\n      num_cols = min_num_cols;\n    }\n    OP_REQUIRES(context, num_rows == min_num_rows || num_cols == min_num_cols,\n                errors::InvalidArgument(\n                    \"The number of rows or columns is not consistent with \"\n                    \"the specified d_lower, d_upper, and diagonal.\"));\n\n    TensorShape output_shape = diagonal_shape;\n    if (num_diags == 1) {  // Output has rank `rank+1`.\n      output_shape.set_dim(diag_rank - 1, num_rows);\n      output_shape.AddDim(num_cols);\n    } else {  // Output has rank `rank`.\n      output_shape.set_dim(diag_rank - 2, num_rows);\n      output_shape.set_dim(diag_rank - 1, num_cols);\n    }\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &output));\n    auto output_reshaped = output->flat_inner_dims<T, 3>();\n    auto diag_reshaped = diagonal.flat<T>();\n    functor::MatrixDiag<Device, T>::Compute(\n        context, context->eigen_device<Device>(), diag_reshaped,\n        output_reshaped, lower_diag_index, upper_diag_index, max_diag_len,\n        padding_value, left_align_superdiagonal_, left_align_subdiagonal_);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_path(self, scope):\n        return scope.get(\"root_path\", \"\") + scope[\"path\"]\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_email(value: str) -> tuple[str, str]:\n    \"\"\"Email address validation using [email-validator](https://pypi.org/project/email-validator/).\n\n    Note:\n        Note that:\n\n        * Raw IP address (literal) domain parts are not allowed.\n        * `\"John Doe <local_part@domain.com>\"` style \"pretty\" email addresses are processed.\n        * Spaces are striped from the beginning and end of addresses, but no error is raised.\n    \"\"\"\n    if email_validator is None:\n        import_email_validator()\n\n    m = pretty_email_regex.fullmatch(value)\n    name: str | None = None\n    if m:\n        unquoted_name, quoted_name, value = m.groups()\n        name = unquoted_name or quoted_name\n\n    email = value.strip()\n\n    try:\n        parts = email_validator.validate_email(email, check_deliverability=False)\n    except email_validator.EmailNotValidError as e:\n        raise PydanticCustomError(\n            'value_error', 'value is not a valid email address: {reason}', {'reason': str(e.args[0])}\n        ) from e\n\n    email = parts.normalized\n    assert email is not None\n    name = name or parts.local_part\n    return name, email\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def settings(request):\n   \"\"\"\n   This page presents a list of all the settings to the user.  They are not editable.\n   \"\"\"\n   if not test_user_authenticated(request): return login(request, next=\"/cobbler_web/settings\")\n   settings = remote.get_settings()\n   skeys = settings.keys()\n   skeys.sort()\n\n   results = []\n   for k in skeys:\n      results.append([k,settings[k]])\n\n   t = get_template('settings.tmpl')\n   html = t.render(Context({\n        'settings' : results,\n        'version'  : remote.version(request.session['token']),\n        'username' : username,\n   }))\n   return HttpResponse(html)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "                        def get_content_length() -> Optional[int]:\n                            # payload length\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\n                            if length_hdr is None:\n                                return None\n\n                            try:\n                                length = int(length_hdr)\n                            except ValueError:\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            if length < 0:\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            return length\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_local_uri(uri):\n    \"\"\"Returns true if this is a local file path (/foo or file:/foo).\"\"\"\n    if uri == \"databricks\":\n        return False\n\n    if is_windows() and uri.startswith(\"\\\\\\\\\"):\n        # windows network drive path looks like: \"\\\\<server name>\\path\\...\"\n        return False\n\n    parsed_uri = urllib.parse.urlparse(uri)\n    if parsed_uri.hostname:\n        return False\n\n    scheme = parsed_uri.scheme\n    if scheme == \"\" or scheme == \"file\":\n        return True\n\n    if is_windows() and len(scheme) == 1 and scheme.lower() == pathlib.Path(uri).drive.lower()[0]:\n        return True\n\n    return False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "inline TfLiteTensor* GetTemporary(TfLiteContext* context,\n                                  const TfLiteNode* node, int index) {\n  return &context->tensors[node->temporaries->data[index]];\n}\ninline const TfLiteTensor* GetIntermediates(TfLiteContext* context,"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static int ReadHuffmanCodes(VP8LDecoder* const dec, int xsize, int ysize,\n                            int color_cache_bits, int allow_recursion) {\n  int i, j;\n  VP8LBitReader* const br = &dec->br_;\n  VP8LMetadata* const hdr = &dec->hdr_;\n  uint32_t* huffman_image = NULL;\n  HTreeGroup* htree_groups = NULL;\n  HuffmanCode* huffman_tables = NULL;\n  HuffmanCode* huffman_table = NULL;\n  int num_htree_groups = 1;\n  int num_htree_groups_max = 1;\n  const int max_alphabet_size =\n      kAlphabetSize[0] + ((color_cache_bits > 0) ? 1 << color_cache_bits : 0);\n  int* code_lengths = NULL;\n  const int table_size = kTableSize[color_cache_bits];\n  int* mapping = NULL;\n  int ok = 0;\n\n  if (allow_recursion && VP8LReadBits(br, 1)) {\n    // use meta Huffman codes.\n    const int huffman_precision = VP8LReadBits(br, 3) + 2;\n    const int huffman_xsize = VP8LSubSampleSize(xsize, huffman_precision);\n    const int huffman_ysize = VP8LSubSampleSize(ysize, huffman_precision);\n    const int huffman_pixs = huffman_xsize * huffman_ysize;\n    if (!DecodeImageStream(huffman_xsize, huffman_ysize, /*is_level0=*/0, dec,\n                           &huffman_image)) {\n      goto Error;\n    }\n    hdr->huffman_subsample_bits_ = huffman_precision;\n    for (i = 0; i < huffman_pixs; ++i) {\n      // The huffman data is stored in red and green bytes.\n      const int group = (huffman_image[i] >> 8) & 0xffff;\n      huffman_image[i] = group;\n      if (group >= num_htree_groups_max) {\n        num_htree_groups_max = group + 1;\n      }\n    }\n    // Check the validity of num_htree_groups_max. If it seems too big, use a\n    // smaller value for later. This will prevent big memory allocations to end\n    // up with a bad bitstream anyway.\n    // The value of 1000 is totally arbitrary. We know that num_htree_groups_max\n    // is smaller than (1 << 16) and should be smaller than the number of pixels\n    // (though the format allows it to be bigger).\n    if (num_htree_groups_max > 1000 || num_htree_groups_max > xsize * ysize) {\n      // Create a mapping from the used indices to the minimal set of used\n      // values [0, num_htree_groups)\n      mapping = (int*)WebPSafeMalloc(num_htree_groups_max, sizeof(*mapping));\n      if (mapping == NULL) {\n        VP8LSetError(dec, VP8_STATUS_OUT_OF_MEMORY);\n        goto Error;\n      }\n      // -1 means a value is unmapped, and therefore unused in the Huffman\n      // image.\n      memset(mapping, 0xff, num_htree_groups_max * sizeof(*mapping));\n      for (num_htree_groups = 0, i = 0; i < huffman_pixs; ++i) {\n        // Get the current mapping for the group and remap the Huffman image.\n        int* const mapped_group = &mapping[huffman_image[i]];\n        if (*mapped_group == -1) *mapped_group = num_htree_groups++;\n        huffman_image[i] = *mapped_group;\n      }\n    } else {\n      num_htree_groups = num_htree_groups_max;\n    }\n  }\n\n  if (br->eos_) goto Error;\n\n  code_lengths = (int*)WebPSafeCalloc((uint64_t)max_alphabet_size,\n                                      sizeof(*code_lengths));\n  huffman_tables = (HuffmanCode*)WebPSafeMalloc(num_htree_groups * table_size,\n                                                sizeof(*huffman_tables));\n  htree_groups = VP8LHtreeGroupsNew(num_htree_groups);\n\n  if (htree_groups == NULL || code_lengths == NULL || huffman_tables == NULL) {\n    VP8LSetError(dec, VP8_STATUS_OUT_OF_MEMORY);\n    goto Error;\n  }\n\n  huffman_table = huffman_tables;\n  for (i = 0; i < num_htree_groups_max; ++i) {\n    // If the index \"i\" is unused in the Huffman image, just make sure the\n    // coefficients are valid but do not store them.\n    if (mapping != NULL && mapping[i] == -1) {\n      for (j = 0; j < HUFFMAN_CODES_PER_META_CODE; ++j) {\n        int alphabet_size = kAlphabetSize[j];\n        if (j == 0 && color_cache_bits > 0) {\n          alphabet_size += (1 << color_cache_bits);\n        }\n        // Passing in NULL so that nothing gets filled.\n        if (!ReadHuffmanCode(alphabet_size, dec, code_lengths, NULL)) {\n          goto Error;\n        }\n      }\n    } else {\n      HTreeGroup* const htree_group =\n          &htree_groups[(mapping == NULL) ? i : mapping[i]];\n      HuffmanCode** const htrees = htree_group->htrees;\n      int size;\n      int total_size = 0;\n      int is_trivial_literal = 1;\n      int max_bits = 0;\n      for (j = 0; j < HUFFMAN_CODES_PER_META_CODE; ++j) {\n        int alphabet_size = kAlphabetSize[j];\n        htrees[j] = huffman_table;\n        if (j == 0 && color_cache_bits > 0) {\n          alphabet_size += (1 << color_cache_bits);\n        }\n        size = ReadHuffmanCode(alphabet_size, dec, code_lengths, huffman_table);\n        if (size == 0) {\n          goto Error;\n        }\n        if (is_trivial_literal && kLiteralMap[j] == 1) {\n          is_trivial_literal = (huffman_table->bits == 0);\n        }\n        total_size += huffman_table->bits;\n        huffman_table += size;\n        if (j <= ALPHA) {\n          int local_max_bits = code_lengths[0];\n          int k;\n          for (k = 1; k < alphabet_size; ++k) {\n            if (code_lengths[k] > local_max_bits) {\n              local_max_bits = code_lengths[k];\n            }\n          }\n          max_bits += local_max_bits;\n        }\n      }\n      htree_group->is_trivial_literal = is_trivial_literal;\n      htree_group->is_trivial_code = 0;\n      if (is_trivial_literal) {\n        const int red = htrees[RED][0].value;\n        const int blue = htrees[BLUE][0].value;\n        const int alpha = htrees[ALPHA][0].value;\n        htree_group->literal_arb = ((uint32_t)alpha << 24) | (red << 16) | blue;\n        if (total_size == 0 && htrees[GREEN][0].value < NUM_LITERAL_CODES) {\n          htree_group->is_trivial_code = 1;\n          htree_group->literal_arb |= htrees[GREEN][0].value << 8;\n        }\n      }\n      htree_group->use_packed_table =\n          !htree_group->is_trivial_code && (max_bits < HUFFMAN_PACKED_BITS);\n      if (htree_group->use_packed_table) BuildPackedTable(htree_group);\n    }\n  }\n  ok = 1;\n\n  // All OK. Finalize pointers.\n  hdr->huffman_image_ = huffman_image;\n  hdr->num_htree_groups_ = num_htree_groups;\n  hdr->htree_groups_ = htree_groups;\n  hdr->huffman_tables_ = huffman_tables;\n\n Error:\n  WebPSafeFree(code_lengths);\n  WebPSafeFree(mapping);\n  if (!ok) {\n    WebPSafeFree(huffman_image);\n    WebPSafeFree(huffman_tables);\n    VP8LHtreeGroupsFree(htree_groups);\n  }\n  return ok;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def run(argv: List[str]) -> None:\n    args = parse_command_line(argv)\n    remarshal(\n        args.input,\n        args.output,\n        args.input_format,\n        args.output_format,\n        json_indent=args.json_indent,\n        ordered=args.ordered,\n        stringify=args.stringify,\n        unwrap=args.unwrap,\n        wrap=args.wrap,\n        yaml_options=args.yaml_options,\n    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  explicit FractionalAvgPoolOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pooling_ratio\", &pooling_ratio_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"pseudo_random\", &pseudo_random_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n    OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                errors::InvalidArgument(\n                    \"pooling_ratio field must specify 4 dimensions\"));\n    OP_REQUIRES(\n        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n        errors::Unimplemented(\"Fractional average pooling is not yet \"\n                              \"supported on the batch nor channel dimension.\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed2\", &seed2_));\n    if (deterministic_) {\n      // If both seeds are not set when deterministic_ is true, force set seeds.\n      if ((seed_ == 0) && (seed2_ == 0)) {\n        seed_ = random::New64();\n        seed2_ = random::New64();\n      }\n    } else {\n      OP_REQUIRES(\n          context, (seed_ == 0) && (seed2_ == 0),\n          errors::InvalidArgument(\n              \"Both seed and seed2 should be 0 if deterministic is false.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def generic_edit(request, what=None, obj_name=None, editmode=\"new\"):\n\n   \"\"\"\n   Presents an editor page for any type of object.\n   While this is generally standardized, systems are a little bit special.\n   \"\"\"\n   target = \"\"\n   if obj_name != None:\n       target = \"/%s\" % obj_name\n   if not test_user_authenticated(request): return login(request, next=\"/cobbler_web/%s/edit%s\" % (what,target))\n\n   obj = None\n\n   settings = remote.get_settings()\n\n   child = False\n   if what == \"subprofile\":\n      what = \"profile\"\n      child = True\n   \n\n   if not obj_name is None:\n      editable = remote.check_access_no_fail(request.session['token'], \"modify_%s\" % what, obj_name)\n      obj = remote.get_item(what, obj_name, True)\n   #\n   #   if obj.has_key('ctime'):\n   #      obj['ctime'] = time.ctime(obj['ctime'])\n   #   if obj.has_key('mtime'):\n   #      obj['mtime'] = time.ctime(obj['mtime'])\n\n   else:\n       editable = remote.check_access_no_fail(request.session['token'], \"new_%s\" % what, None)\n       obj = None\n\n\n   interfaces = {}\n   if what == \"system\":\n       if obj:\n           interfaces = obj.get(\"interfaces\",{})\n       else:\n           interfaces = {}\n\n   fields = get_fields(what, child, obj)\n\n   # populate some select boxes\n   # FIXME: we really want to just populate with the names, right?\n   if what == \"profile\":\n      if (obj and obj[\"parent\"] not in (None,\"\")) or child:\n         __tweak_field(fields, \"parent\", \"choices\", __names_from_dicts(remote.get_profiles()))\n      else:\n         __tweak_field(fields, \"distro\", \"choices\", __names_from_dicts(remote.get_distros()))\n      __tweak_field(fields, \"repos\", \"choices\",     __names_from_dicts(remote.get_repos()))\n   elif what == \"system\":\n      __tweak_field(fields, \"profile\", \"choices\",      __names_from_dicts(remote.get_profiles()))\n      __tweak_field(fields, \"image\", \"choices\",        __names_from_dicts(remote.get_images(),optional=True))\n   elif what == \"mgmtclass\":\n        __tweak_field(fields, \"packages\", \"choices\", __names_from_dicts(remote.get_packages()))\n        __tweak_field(fields, \"files\", \"choices\",    __names_from_dicts(remote.get_files()))\n\n   if what in (\"distro\",\"profile\",\"system\"):\n       __tweak_field(fields, \"mgmt_classes\", \"choices\", __names_from_dicts(remote.get_mgmtclasses(),optional=False))\n\n   # save the fields in the session for comparison later\n   request.session['%s_%s' % (what,obj_name)] = fields\n\n   t = get_template('generic_edit.tmpl')\n   inames = interfaces.keys()\n   inames.sort()\n   html = t.render(Context({\n       'what'            : what, \n       'fields'          : fields, \n       'subobject'       : child,\n       'editmode'        : editmode, \n       'editable'        : editable,\n       'interfaces'      : interfaces,\n       'interface_names' : inames,\n       'interface_length': len(inames),\n       'version'         : remote.version(request.session['token']),\n       'username'        : username,\n       'name'            : obj_name\n   }))\n\n   return HttpResponse(html)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse_headers(\n        self, lines: List[bytes]\n    ) -> Tuple[\"CIMultiDictProxy[str]\", RawHeaders]:\n        headers: CIMultiDict[str] = CIMultiDict()\n        raw_headers = []\n\n        lines_idx = 1\n        line = lines[1]\n        line_count = len(lines)\n\n        while line:\n            # Parse initial header name : value pair.\n            try:\n                bname, bvalue = line.split(b\":\", 1)\n            except ValueError:\n                raise InvalidHeader(line) from None\n\n            bname = bname.strip(b\" \\t\")\n            bvalue = bvalue.lstrip()\n            if HDRRE.search(bname):\n                raise InvalidHeader(bname)\n            if len(bname) > self.max_field_size:\n                raise LineTooLong(\n                    \"request header name {}\".format(\n                        bname.decode(\"utf8\", \"xmlcharrefreplace\")\n                    ),\n                    str(self.max_field_size),\n                    str(len(bname)),\n                )\n\n            header_length = len(bvalue)\n\n            # next line\n            lines_idx += 1\n            line = lines[lines_idx]\n\n            # consume continuation lines\n            continuation = line and line[0] in (32, 9)  # (' ', '\\t')\n\n            if continuation:\n                bvalue_lst = [bvalue]\n                while continuation:\n                    header_length += len(line)\n                    if header_length > self.max_field_size:\n                        raise LineTooLong(\n                            \"request header field {}\".format(\n                                bname.decode(\"utf8\", \"xmlcharrefreplace\")\n                            ),\n                            str(self.max_field_size),\n                            str(header_length),\n                        )\n                    bvalue_lst.append(line)\n\n                    # next line\n                    lines_idx += 1\n                    if lines_idx < line_count:\n                        line = lines[lines_idx]\n                        if line:\n                            continuation = line[0] in (32, 9)  # (' ', '\\t')\n                    else:\n                        line = b\"\"\n                        break\n                bvalue = b\"\".join(bvalue_lst)\n            else:\n                if header_length > self.max_field_size:\n                    raise LineTooLong(\n                        \"request header field {}\".format(\n                            bname.decode(\"utf8\", \"xmlcharrefreplace\")\n                        ),\n                        str(self.max_field_size),\n                        str(header_length),\n                    )\n\n            bvalue = bvalue.strip()\n            name = bname.decode(\"utf-8\", \"surrogateescape\")\n            value = bvalue.decode(\"utf-8\", \"surrogateescape\")\n\n            headers.add(name, value)\n            raw_headers.append((bname, bvalue))\n\n        return (CIMultiDictProxy(headers), tuple(raw_headers))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _build_docker_image(\n    project_spec: BigflowProjectSpec,\n    tag: str,\n    cache_params: BuildImageCacheParams | None,\n):\n\n    logger.debug(\"Run docker build...\")\n    cmd = [\"docker\", \"build\", project_spec.project_dir, \"--tag\", tag]\n\n    if cache_params:\n\n        logger.debug(\"Authenticate to docker registry\")\n        bigflow.deploy.authenticate_to_registry(\n            auth_method=cache_params.auth_method or bigflow.deploy.AuthorizationType.LOCAL_ACCOUNT,\n            vault_endpoint=cache_params.vault_endpoint,\n            vault_secret=cache_params.vault_secret,\n        )\n\n        for image in (cache_params.cache_from_image or []):\n            logger.debug(\"Add --cache-from=%s to `docker build`\", image)\n            cmd.extend([\"--cache-from\", image])\n\n        for version in (cache_params.cache_from_version or []):\n            image = f\"{project_spec.docker_repository}:{version}\"\n            logger.debug(\"Add --cache-from=%s to `docker build`\", image)\n            cmd.extend([\"--cache-from\", image])\n\n        # noop when building backend is not a buildkit\n        logger.debug(\"Enable buildkit inline cache\")\n        cmd.extend([\"--build-arg\", \"BUILDKIT_INLINE_CACHE=1\"])\n\n    return bf_commons.run_process(cmd)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, hs):\n        super().__init__(hs)\n        self.clock = hs.get_clock()\n        self.client = hs.get_http_client()\n        self.key_servers = self.config.key_servers\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def redirect_request(self, req, fp, code, msg, headers, newurl):\n        if code not in (301, 302, 303, 307, 308):\n            raise urllib.error.HTTPError(req.full_url, code, msg, headers, fp)\n\n        new_method = req.get_method()\n        new_data = req.data\n        remove_headers = []\n        # A 303 must either use GET or HEAD for subsequent request\n        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.4\n        if code == 303 and req.get_method() != 'HEAD':\n            new_method = 'GET'\n        # 301 and 302 redirects are commonly turned into a GET from a POST\n        # for subsequent requests by browsers, so we'll do the same.\n        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.2\n        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.3\n        elif code in (301, 302) and req.get_method() == 'POST':\n            new_method = 'GET'\n\n        # only remove payload if method changed (e.g. POST to GET)\n        if new_method != req.get_method():\n            new_data = None\n            remove_headers.extend(['Content-Length', 'Content-Type'])\n\n        new_headers = {k: v for k, v in req.headers.items() if k.lower() not in remove_headers}\n\n        return urllib.request.Request(\n            newurl, headers=new_headers, origin_req_host=req.origin_req_host,\n            unverifiable=True, method=new_method, data=new_data)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def export_with_optimum(args):\n    if is_optimum_available():\n        from optimum.version import __version__ as optimum_version\n\n        parsed_optimum_version = version.parse(optimum_version)\n        if parsed_optimum_version < version.parse(MIN_OPTIMUM_VERSION):\n            raise RuntimeError(\n                f\"transformers.onnx requires optimum >= {MIN_OPTIMUM_VERSION} but {optimum_version} is installed. You \"\n                \"can upgrade optimum by running: pip install -U optimum[exporters]\"\n            )\n    else:\n        raise RuntimeError(\n            \"transformers.onnx requires optimum to run, you can install the library by running: pip install \"\n            \"optimum[exporters]\"\n        )\n    cmd_line = [\n        sys.executable,\n        \"-m\",\n        \"optimum.exporters.onnx\",\n        f\"--model {args.model}\",\n        f\"--task {args.feature}\",\n        f\"--framework {args.framework}\" if args.framework is not None else \"\",\n        f\"{args.output}\",\n    ]\n    proc = subprocess.Popen(\" \".join(cmd_line), stdout=subprocess.PIPE, shell=True)\n    proc.wait()\n\n    logger.info(\n        \"The export was done by optimum.exporters.onnx. We recommend using to use this package directly in future, as \"\n        \"transformers.onnx is deprecated, and will be removed in v5. You can find more information here: \"\n        \"https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model.\"\n    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_file_hash(self):\n        if self.file_hash == \"\":\n            with self.open_file() as f:\n                self._set_file_hash(f.read())\n\n            self.save(update_fields=[\"file_hash\"])\n\n        return self.file_hash\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status InferenceContext::Multiply(DimensionHandle first,\n                                  DimensionOrConstant second,\n                                  DimensionHandle* out) {\n  const int64_t first_value = Value(first);\n  const int64_t second_value = Value(second);\n  // Special cases.\n  if (first_value == 0) {\n    *out = first;\n  } else if (second_value == 0) {\n    *out = MakeDim(second);\n  } else if (first_value == 1) {\n    *out = MakeDim(second);\n  } else if (second_value == 1) {\n    *out = first;\n  } else if (first_value == kUnknownDim || second_value == kUnknownDim) {\n    *out = UnknownDim();\n  } else {\n    // Invariant: Both values are known and greater than 1.\n    const int64_t product = first_value * second_value;\n    if (product < 0) {\n      return errors::InvalidArgument(\n          \"Negative dimension size caused by overflow when multiplying \",\n          first_value, \" and \", second_value);\n    }\n    *out = MakeDim(product);\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& true_classes = context->input(0);\n    OP_REQUIRES(context, true_classes.dims() == 2,\n                errors::InvalidArgument(\"true_classes must be a matrix\"));\n    const int32_t batch_size = true_classes.dim_size(0);\n    OP_REQUIRES(\n        context, true_classes.dim_size(1) == num_true_,\n        errors::InvalidArgument(\"true_classes must have \"\n                                \"num_true columns, expected: \",\n                                true_classes.dim_size(1), \" was: \", num_true_));\n    CHECK(sampler_) << \"CandidateSamplerOp did not set sampler_\";\n\n    if (unique_) {\n      OP_REQUIRES(context, num_sampled_ <= sampler_->range(),\n                  errors::InvalidArgument(\"Sampler's range is too small.\"));\n    }\n\n    // Output candidates and expected_count.\n    Tensor* out_sampled_candidates = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({num_sampled_}),\n                                            &out_sampled_candidates));\n\n    Tensor* out_true_expected_count = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                1, TensorShape({batch_size, num_true_}),\n                                &out_true_expected_count));\n    Tensor* out_sampled_expected_count = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(2, TensorShape({num_sampled_}),\n                                            &out_sampled_expected_count));\n\n    gtl::ArraySlice<int64_t> true_candidate(\n        true_classes.matrix<int64_t>().data(), batch_size * num_true_);\n    gtl::MutableArraySlice<int64_t> sampled_candidate(\n        out_sampled_candidates->vec<int64_t>().data(), num_sampled_);\n    gtl::MutableArraySlice<float> true_expected_count(\n        out_true_expected_count->matrix<float>().data(),\n        batch_size * num_true_);\n    gtl::MutableArraySlice<float> sampled_expected_count(\n        out_sampled_expected_count->vec<float>().data(), num_sampled_);\n\n    // Approximately conservatively estimate the number of samples required.\n    // In cases where rejection sampling is used we may occasionally use more\n    // samples than expected, which will result in reused random bits.\n    const int64_t samples32 = 2048 * num_sampled_;\n\n    // Pick sampled candidates.\n    auto local_gen = generator_.ReserveSamples32(samples32);\n    random::SimplePhilox random(&local_gen);\n    sampler_->SampleBatchGetExpectedCount(&random, unique_, sampled_candidate,\n                                          sampled_expected_count,\n                                          true_candidate, true_expected_count);\n\n    if (sampler_->NeedsUpdates()) {\n      sampler_->Update(true_candidate);\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def format_errormsg(message: str) -> str:\n    \"\"\"Match account names in error messages and insert HTML links for them.\"\"\"\n    match = re.search(ACCOUNT_RE, message)\n    if not match:\n        return message\n    account = match.group()\n    url = flask.url_for(\"account\", name=account)\n    return (\n        message.replace(account, f'<a href=\"{url}\">{account}</a>')\n        .replace(\"for '\", \"for \")\n        .replace(\"': \", \": \")\n    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __call__(self, value):\n        if not isinstance(value, str):\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n        if self.unsafe_chars.intersection(value):\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n        # Check if the scheme is valid.\n        scheme = value.split(\"://\")[0].lower()\n        if scheme not in self.schemes:\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n\n        # Then check full URL\n        try:\n            splitted_url = urlsplit(value)\n        except ValueError:\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n        try:\n            super().__call__(value)\n        except ValidationError as e:\n            # Trivial case failed. Try for possible IDN domain\n            if value:\n                scheme, netloc, path, query, fragment = splitted_url\n                try:\n                    netloc = punycode(netloc)  # IDN -> ACE\n                except UnicodeError:  # invalid domain part\n                    raise e\n                url = urlunsplit((scheme, netloc, path, query, fragment))\n                super().__call__(url)\n            else:\n                raise\n        else:\n            # Now verify IPv6 in the netloc part\n            host_match = re.search(r\"^\\[(.+)\\](?::[0-9]{1,5})?$\", splitted_url.netloc)\n            if host_match:\n                potential_ip = host_match[1]\n                try:\n                    validate_ipv6_address(potential_ip)\n                except ValidationError:\n                    raise ValidationError(\n                        self.message, code=self.code, params={\"value\": value}\n                    )\n\n        # The maximum length of a full host name is 253 characters per RFC 1034\n        # section 3.1. It's defined to be 255 bytes or less, but this includes\n        # one byte for the length of the name and one byte for the trailing dot\n        # that's used to indicate absolute names in DNS.\n        if splitted_url.hostname is None or len(splitted_url.hostname) > 253:\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def element():\n    element_url = src_url = request.args.get('url')\n    if element_url.startswith('gAAAAA'):\n        try:\n            cipher_suite = Fernet(g.session_key)\n            src_url = cipher_suite.decrypt(element_url.encode()).decode()\n        except (InvalidSignature, InvalidToken) as e:\n            return render_template(\n                'error.html',\n                error_message=str(e)), 401\n\n    src_type = request.args.get('type')\n\n    try:\n        file_data = g.user_request.send(base_url=src_url).content\n        tmp_mem = io.BytesIO()\n        tmp_mem.write(file_data)\n        tmp_mem.seek(0)\n\n        return send_file(tmp_mem, mimetype=src_type)\n    except exceptions.RequestException:\n        pass\n\n    empty_gif = base64.b64decode(\n        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n    return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def explain_query_prefix(self, format=None, **options):\n        prefix = super().explain_query_prefix(format)\n        extra = {}\n        if format:\n            extra[\"FORMAT\"] = format\n        if options:\n            extra.update(\n                {\n                    name.upper(): \"true\" if value else \"false\"\n                    for name, value in options.items()\n                }\n            )\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status BuildInputArgIndex(const OpDef::ArgDef& arg_def, AttrSlice attr_values,\n                            const FunctionDef::ArgAttrs* arg_attrs,\n                            bool ints_on_device,\n                            int64_t resource_arg_unique_id) {\n    bool is_type_list;\n    DataTypeVector dtypes;\n    TF_RETURN_IF_ERROR(\n        ArgNumType(attr_values, arg_def, &is_type_list, &dtypes));\n    if (dtypes.size() < size_t{1}) {\n      return errors::Internal(\"Expected a list of at least one dtype\");\n    }\n    int arg_index = result_.nodes.size();\n    TF_RETURN_IF_ERROR(\n        AddItem(arg_def.name(), {true, arg_index, 0, is_type_list, dtypes}));\n    // Creates dtypes.size() nodes in the graph.\n    for (size_t i = 0; i < dtypes.size(); ++i) {\n      TF_RETURN_IF_ERROR(AddItem(strings::StrCat(arg_def.name(), \":\", i),\n                                 {true, arg_index, 0, false, {dtypes[i]}}));\n      DCHECK_EQ(arg_index, result_.nodes.size());\n      string name = arg_def.name();\n      if (dtypes.size() > 1) {\n        strings::StrAppend(&name, \"_\", i);\n      }\n      NodeDef* gnode = AddNode(name);\n      if (ints_on_device && dtypes[i] == DataType::DT_INT32) {\n        gnode->set_op(FunctionLibraryDefinition::kDeviceArgOp);\n      } else {\n        gnode->set_op(FunctionLibraryDefinition::kArgOp);\n      }\n      DataType dtype = arg_def.is_ref() ? MakeRefType(dtypes[i]) : dtypes[i];\n      AddAttr(\"T\", dtype, gnode);\n      AddAttr(\"index\", arg_index, gnode);\n      if (resource_arg_unique_id >= 0) {\n        AddAttr(\"_resource_arg_unique_id\", resource_arg_unique_id, gnode);\n      }\n      if (arg_attrs) {\n        for (const auto& arg_attr : arg_attrs->attr()) {\n          AddAttr(arg_attr.first, arg_attr.second, gnode->mutable_attr());\n        }\n      }\n      result_.arg_types.push_back(dtypes[i]);\n      ++arg_index;\n    }\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool MfccMelFilterbank::Initialize(int input_length, double input_sample_rate,\n                                   int output_channel_count,\n                                   double lower_frequency_limit,\n                                   double upper_frequency_limit) {\n  num_channels_ = output_channel_count;\n  sample_rate_ = input_sample_rate;\n  input_length_ = input_length;\n\n  if (num_channels_ < 1) {\n    LOG(ERROR) << \"Number of filterbank channels must be positive.\";\n    return false;\n  }\n\n  if (sample_rate_ <= 0) {\n    LOG(ERROR) << \"Sample rate must be positive.\";\n    return false;\n  }\n\n  if (input_length < 2) {\n    LOG(ERROR) << \"Input length must greater than 1.\";\n    return false;\n  }\n\n  if (lower_frequency_limit < 0) {\n    LOG(ERROR) << \"Lower frequency limit must be nonnegative.\";\n    return false;\n  }\n\n  if (upper_frequency_limit <= lower_frequency_limit) {\n    LOG(ERROR) << \"Upper frequency limit must be greater than \"\n               << \"lower frequency limit.\";\n    return false;\n  }\n\n  // An extra center frequency is computed at the top to get the upper\n  // limit on the high side of the final triangular filter.\n  center_frequencies_.resize(num_channels_ + 1);\n  const double mel_low = FreqToMel(lower_frequency_limit);\n  const double mel_hi = FreqToMel(upper_frequency_limit);\n  const double mel_span = mel_hi - mel_low;\n  const double mel_spacing = mel_span / static_cast<double>(num_channels_ + 1);\n  for (int i = 0; i < num_channels_ + 1; ++i) {\n    center_frequencies_[i] = mel_low + (mel_spacing * (i + 1));\n  }\n\n  // Always exclude DC; emulate HTK.\n  const double hz_per_sbin =\n      0.5 * sample_rate_ / static_cast<double>(input_length_ - 1);\n  start_index_ = static_cast<int>(1.5 + (lower_frequency_limit / hz_per_sbin));\n  end_index_ = static_cast<int>(upper_frequency_limit / hz_per_sbin);\n\n  // Maps the input spectrum bin indices to filter bank channels/indices. For\n  // each FFT bin, band_mapper tells us which channel this bin contributes to\n  // on the right side of the triangle.  Thus this bin also contributes to the\n  // left side of the next channel's triangle response.\n  band_mapper_.resize(input_length_);\n  int channel = 0;\n  for (int i = 0; i < input_length_; ++i) {\n    double melf = FreqToMel(i * hz_per_sbin);\n    if ((i < start_index_) || (i > end_index_)) {\n      band_mapper_[i] = -2;  // Indicate an unused Fourier coefficient.\n    } else {\n      while ((channel < num_channels_) &&\n             (center_frequencies_[channel] < melf)) {\n        ++channel;\n      }\n      band_mapper_[i] = channel - 1;  // Can be == -1\n    }\n  }\n\n  // Create the weighting functions to taper the band edges.  The contribution\n  // of any one FFT bin is based on its distance along the continuum between two\n  // mel-channel center frequencies.  This bin contributes weights_[i] to the\n  // current channel and 1-weights_[i] to the next channel.\n  weights_.resize(input_length_);\n  for (int i = 0; i < input_length_; ++i) {\n    channel = band_mapper_[i];\n    if ((i < start_index_) || (i > end_index_)) {\n      weights_[i] = 0.0;\n    } else {\n      if (channel >= 0) {\n        weights_[i] =\n            (center_frequencies_[channel + 1] - FreqToMel(i * hz_per_sbin)) /\n            (center_frequencies_[channel + 1] - center_frequencies_[channel]);\n      } else {\n        weights_[i] = (center_frequencies_[0] - FreqToMel(i * hz_per_sbin)) /\n                      (center_frequencies_[0] - mel_low);\n      }\n    }\n  }\n  // Check the sum of FFT bin weights for every mel band to identify\n  // situations where the mel bands are so narrow that they don't get\n  // significant weight on enough (or any) FFT bins -- i.e., too many\n  // mel bands have been requested for the given FFT size.\n  std::vector<int> bad_channels;\n  for (int c = 0; c < num_channels_; ++c) {\n    float band_weights_sum = 0.0;\n    for (int i = 0; i < input_length_; ++i) {\n      if (band_mapper_[i] == c - 1) {\n        band_weights_sum += (1.0 - weights_[i]);\n      } else if (band_mapper_[i] == c) {\n        band_weights_sum += weights_[i];\n      }\n    }\n    // The lowest mel channels have the fewest FFT bins and the lowest\n    // weights sum.  But given that the target gain at the center frequency\n    // is 1.0, if the total sum of weights is 0.5, we're in bad shape.\n    if (band_weights_sum < 0.5) {\n      bad_channels.push_back(c);\n    }\n  }\n  if (!bad_channels.empty()) {\n    LOG(ERROR) << \"Missing \" << bad_channels.size() << \" bands \"\n               << \" starting at \" << bad_channels[0]\n               << \" in mel-frequency design. \"\n               << \"Perhaps too many channels or \"\n               << \"not enough frequency resolution in spectrum. (\"\n               << \"input_length: \" << input_length\n               << \" input_sample_rate: \" << input_sample_rate\n               << \" output_channel_count: \" << output_channel_count\n               << \" lower_frequency_limit: \" << lower_frequency_limit\n               << \" upper_frequency_limit: \" << upper_frequency_limit;\n  }\n  initialized_ = true;\n  return true;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(tensorflow::OpKernelContext* context) override {\n    const tensorflow::Tensor* data;\n    OP_REQUIRES_OK(context, context->input(\"data\", &data));\n    const auto& input_data = data->flat<tstring>().data();\n\n    const tensorflow::Tensor* splits;\n    OP_REQUIRES_OK(context, context->input(\"data_splits\", &splits));\n    const auto& splits_vec = splits->flat<SPLITS_TYPE>();\n\n    // Validate that the splits are valid indices into data, only if there are\n    // splits specified.\n    const int input_data_size = data->flat<tstring>().size();\n    const int splits_vec_size = splits_vec.size();\n    if (splits_vec_size > 0) {\n      int prev_split = splits_vec(0);\n      OP_REQUIRES(context, prev_split == 0,\n                  errors::InvalidArgument(\"First split value must be 0, got \",\n                                          prev_split));\n      for (int i = 1; i < splits_vec_size; ++i) {\n        bool valid_splits = splits_vec(i) >= prev_split;\n        valid_splits = valid_splits && (splits_vec(i) <= input_data_size);\n        OP_REQUIRES(context, valid_splits,\n                    errors::InvalidArgument(\n                        \"Invalid split value \", splits_vec(i), \", must be in [\",\n                        prev_split, \", \", input_data_size, \"]\"));\n        prev_split = splits_vec(i);\n      }\n      OP_REQUIRES(context, prev_split == input_data_size,\n                  errors::InvalidArgument(\n                      \"Last split value must be data size. Expected \",\n                      input_data_size, \", got \", prev_split));\n    }\n\n    int num_batch_items = splits_vec.size() - 1;\n    tensorflow::Tensor* ngrams_splits;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(1, splits->shape(), &ngrams_splits));\n    auto ngrams_splits_data = ngrams_splits->flat<SPLITS_TYPE>().data();\n\n    // If there is no data or size, return an empty RT.\n    if (data->flat<tstring>().size() == 0 || splits_vec.size() == 0) {\n      tensorflow::Tensor* empty;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(0, data->shape(), &empty));\n      for (int i = 0; i <= num_batch_items; ++i) {\n        ngrams_splits_data[i] = 0;\n      }\n      return;\n    }\n\n    ngrams_splits_data[0] = 0;\n    for (int i = 1; i <= num_batch_items; ++i) {\n      int length = splits_vec(i) - splits_vec(i - 1);\n      int num_ngrams = 0;\n      for (int ngram_width : ngram_widths_)\n        num_ngrams += get_num_ngrams(length, ngram_width);\n      if (preserve_short_ && length > 0 && num_ngrams == 0) {\n        num_ngrams = 1;\n      }\n      ngrams_splits_data[i] = ngrams_splits_data[i - 1] + num_ngrams;\n    }\n\n    tensorflow::Tensor* ngrams;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            0, TensorShape({ngrams_splits_data[num_batch_items]}), &ngrams));\n    auto ngrams_data = ngrams->flat<tstring>().data();\n\n    for (int i = 0; i < num_batch_items; ++i) {\n      auto data_start = &input_data[splits_vec(i)];\n      int output_start_idx = ngrams_splits_data[i];\n      for (int ngram_width : ngram_widths_) {\n        auto output_start = &ngrams_data[output_start_idx];\n        int length = splits_vec(i + 1) - splits_vec(i);\n        int num_ngrams = get_num_ngrams(length, ngram_width);\n        CreateNgrams(data_start, output_start, num_ngrams, ngram_width);\n        output_start_idx += num_ngrams;\n      }\n      // If we're preserving short sequences, check to see if no sequence was\n      // generated by comparing the current output start idx to the original\n      // one (ngram_splits_data). If no ngrams were generated, then they will\n      // be equal (since we increment output_start_idx by num_ngrams every\n      // time we create a set of ngrams.)\n      if (preserve_short_ && output_start_idx == ngrams_splits_data[i]) {\n        int data_length = splits_vec(i + 1) - splits_vec(i);\n        // One legitimate reason to not have any ngrams when preserve_short_\n        // is true is if the sequence itself is empty. In that case, move on.\n        if (data_length == 0) {\n          continue;\n        }\n        // We don't have to worry about dynamic padding sizes here: if padding\n        // was dynamic, every sequence would have had sufficient padding to\n        // generate at least one ngram.\n        int ngram_width = data_length + 2 * pad_width_;\n        auto output_start = &ngrams_data[output_start_idx];\n        int num_ngrams = 1;\n        CreateNgrams(data_start, output_start, num_ngrams, ngram_width);\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor x = context->input(0);\n    const Tensor& scale = context->input(1);\n    const Tensor& offset = context->input(2);\n    const Tensor& estimated_mean = context->input(3);\n    const Tensor& estimated_variance = context->input(4);\n    const Tensor* side_input = has_side_input_ ? &context->input(5) : nullptr;\n\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(context, offset.dims() == 1,\n                errors::InvalidArgument(\"offset must be 1-dimensional\",\n                                        offset.shape().DebugString()));\n    OP_REQUIRES(context, estimated_mean.dims() == 1,\n                errors::InvalidArgument(\"estimated_mean must be 1-dimensional\",\n                                        estimated_mean.shape().DebugString()));\n    OP_REQUIRES(\n        context, estimated_variance.dims() == 1,\n        errors::InvalidArgument(\"estimated_variance must be 1-dimensional\",\n                                estimated_variance.shape().DebugString()));\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64 in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64 in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64 in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64 in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64 in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    if (has_side_input_) {\n      OP_REQUIRES(context, side_input->shape() == x.shape(),\n                  errors::InvalidArgument(\n                      \"side_input shape must be equal to input shape: \",\n                      side_input->shape().DebugString(),\n                      \" != \", x.shape().DebugString()));\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity) {\n      // NOTE(ezhulenev): This requirement is coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx.\n      OP_REQUIRES(\n          context, !is_training_ || x.dim_size(3) % 4 == 0,\n          errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                  \"channel dimension to be a multiple of 4.\"));\n    }\n\n    Tensor* y = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, alloc_shape, &y));\n\n    Tensor* batch_mean = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {3}, 1, scale.shape(), &batch_mean));\n    Tensor* batch_var = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {4}, 2, scale.shape(), &batch_var));\n    Tensor* saved_mean = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(3, scale.shape(), &saved_mean));\n    Tensor* saved_maybe_inv_var = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(4, scale.shape(),\n                                                     &saved_maybe_inv_var));\n\n    if (is_training_) {\n      functor::FusedBatchNorm<Device, T, U, true>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    } else {\n      functor::FusedBatchNorm<Device, T, U, false>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, y->CopyFrom(*y, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _w(self, content):\n        self.output.append(format_data(content, format_money_values=True))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        self.save = function () {\n            if (!CONFIG_ACCESS_CONTROL) return;\n\n            self.userSettingsDialog.trigger(\"beforeSave\");\n\n            if (self.access_password() && !self.passwordMismatch()) {\n                self.users.updatePassword(\n                    self.currentUser().name,\n                    self.access_password(),\n                    function () {}\n                );\n            }\n\n            var settings = {\n                interface: {\n                    language: self.interface_language()\n                }\n            };\n            self.updateSettings(self.currentUser().name, settings).done(function () {\n                // close dialog\n                self.currentUser(undefined);\n                self.userSettingsDialog.modal(\"hide\");\n                self.loginState.reloadUser();\n            });\n        };\n\n        self.copyApikey = function () {\n            copyToClipboard(self.access_apikey());\n        };\n\n        self.generateApikey = function () {\n            if (!CONFIG_ACCESS_CONTROL) return;\n\n            var generate = function () {\n                self.users\n                    .generateApikey(self.currentUser().name)\n                    .done(function (response) {\n                        self.access_apikey(response.apikey);\n                    });\n            };\n\n            if (self.access_apikey()) {\n                showConfirmationDialog(\n                    gettext(\n                        \"This will generate a new API Key. The old API Key will cease to function immediately.\"\n                    ),\n                    generate\n                );\n            } else {\n                generate();\n            }\n        };\n\n        self.deleteApikey = function () {\n            if (!CONFIG_ACCESS_CONTROL) return;\n            if (!self.access_apikey()) return;\n\n            showConfirmationDialog(\n                gettext(\n                    \"This will delete the API Key. It will cease to to function immediately.\"\n                ),\n                function () {\n                    self.users.deleteApikey(self.currentUser().name).done(function () {\n                        self.access_apikey(undefined);\n                    });\n                }\n            );\n        };\n\n        self.updateSettings = function (username, settings) {\n            return OctoPrint.access.users.saveSettings(username, settings);\n        };\n\n        self.saveEnabled = function () {\n            return !self.passwordMismatch();\n        };\n\n        self.onStartup = function () {\n            self.userSettingsDialog = $(\"#usersettings_dialog\");\n        };\n\n        self.onAllBound = function (allViewModels) {\n            self.userSettingsDialog.on(\"show\", function () {\n                callViewModels(allViewModels, \"onUserSettingsShown\");\n            });\n            self.userSettingsDialog.on(\"hidden\", function () {\n                callViewModels(allViewModels, \"onUserSettingsHidden\");\n            });\n            self.userSettingsDialog.on(\"beforeSave\", function () {\n                callViewModels(allViewModels, \"onUserSettingsBeforeSave\");\n            });\n        };\n    }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ConstantFolding::EvaluateOneFoldable(const NodeDef& node,\n                                            std::vector<NodeDef>* outputs,\n                                            bool* result_too_large) {\n  TensorVector inputs;\n  TensorVector output_tensors;\n  auto inputs_cleanup = gtl::MakeCleanup([&inputs, &output_tensors] {\n    for (const auto& input : inputs) {\n      delete input.tensor;\n    }\n    for (const auto& output : output_tensors) {\n      if (output.tensor) {\n        delete output.tensor;\n      }\n    }\n  });\n\n  size_t total_inputs_size = 0;\n  for (const auto& input : node.input()) {\n    const TensorId input_tensor = ParseTensorName(input);\n    if (input_tensor.index() < 0) {\n      // Control dependency\n      break;\n    }\n    const NodeDef* input_node = node_map_->GetNode(input);\n    if (!IsReallyConstant(*input_node)) {\n      return Status(error::INVALID_ARGUMENT,\n                    strings::StrCat(\"Can't fold \", node.name(), \", its \", input,\n                                    \" isn't constant\"));\n    }\n    TF_RETURN_IF_ERROR(CheckAttrExists(*input_node, \"value\"));\n    const TensorProto& raw_val = input_node->attr().at(\"value\").tensor();\n    if (raw_val.dtype() == DT_INVALID) {\n      return Status(\n          error::INVALID_ARGUMENT,\n          strings::StrCat(\"A tensor in the input node, with TensorId of \",\n                          input_tensor.ToString(),\n                          \" has a dtype of DT_INVALID.\"));\n    }\n    Tensor* value = new Tensor(raw_val.dtype(), raw_val.tensor_shape());\n    if (!value->FromProto(raw_val)) {\n      delete (value);\n      return errors::InvalidArgument(\"Unable to make Tensor from proto for \",\n                                     node.name(), \" with shape \",\n                                     raw_val.tensor_shape().DebugString());\n    }\n    inputs.emplace_back(value);\n    total_inputs_size += value->TotalBytes();\n  }\n\n  TF_RETURN_IF_ERROR(EvaluateNode(node, inputs, &output_tensors));\n  if (output_tensors.empty()) {\n    return Status(error::INVALID_ARGUMENT, \"Expected at least one output.\");\n  }\n\n  outputs->resize(output_tensors.size());\n  for (size_t i = 0; i < output_tensors.size(); i++) {\n    string node_name = OptimizedNodeName(node, \"-folded\");\n    if (output_tensors.size() > 1) {\n      node_name = strings::StrCat(node_name, \"-\", i);\n    }\n    if (output_tensors[i].tensor) {\n      Status s = CreateNodeDef(node_name, output_tensors[i], &outputs->at(i),\n                               total_inputs_size);\n      if (!s.ok()) {\n        *result_too_large = true;\n        return s;\n      }\n    } else {\n      // Create an empty NodeDef to identify dead outputs (e.g. the output of a\n      // switch that's not selected by the switch predicate).\n      outputs->at(i) = NodeDef();\n    }\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def to_python(self, value):\n        return unicodedata.normalize(\"NFKC\", super().to_python(value))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def urlopen(self, method, url, redirect=True, **kw):\n        \"\"\"\n        Same as :meth:`urllib3.connectionpool.HTTPConnectionPool.urlopen`\n        with custom cross-host redirect logic and only sends the request-uri\n        portion of the ``url``.\n\n        The given ``url`` parameter must be absolute, such that an appropriate\n        :class:`urllib3.connectionpool.ConnectionPool` can be chosen for it.\n        \"\"\"\n        u = parse_url(url)\n        conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)\n\n        kw['assert_same_host'] = False\n        kw['redirect'] = False\n\n        if 'headers' not in kw:\n            kw['headers'] = self.headers.copy()\n\n        if self.proxy is not None and u.scheme == \"http\":\n            response = conn.urlopen(method, url, **kw)\n        else:\n            response = conn.urlopen(method, u.request_uri, **kw)\n\n        redirect_location = redirect and response.get_redirect_location()\n        if not redirect_location:\n            return response\n\n        # Support relative URLs for redirecting.\n        redirect_location = urljoin(url, redirect_location)\n\n        # RFC 7231, Section 6.4.4\n        if response.status == 303:\n            method = 'GET'\n\n        retries = kw.get('retries')\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect)\n\n        # Strip headers marked as unsafe to forward to the redirected location.\n        # Check remove_headers_on_redirect to avoid a potential network call within\n        # conn.is_same_host() which may use socket.gethostbyname() in the future.\n        if (retries.remove_headers_on_redirect\n                and not conn.is_same_host(redirect_location)):\n            for header in retries.remove_headers_on_redirect:\n                kw['headers'].pop(header, None)\n\n        try:\n            retries = retries.increment(method, url, response=response, _pool=conn)\n        except MaxRetryError:\n            if retries.raise_on_redirect:\n                raise\n            return response\n\n        kw['retries'] = retries\n        kw['redirect'] = redirect\n\n        log.info(\"Redirecting %s -> %s\", url, redirect_location)\n        return self.urlopen(method, redirect_location, **kw)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Subgraph::Invoke() {\n  if (!consistent_) {\n    ReportError(\"Invoke called on model that is not consistent.\");\n    return kTfLiteError;\n  }\n\n  TfLiteStatus status = kTfLiteOk;\n  if (state_ == kStateUninvokable) {\n    ReportError(\"Invoke called on model that is not ready.\");\n    return kTfLiteError;\n  } else if (memory_planner_ && !memory_planner_->HasNonPersistentMemory()) {\n    ReportError(\"Non-persistent memory is not available.\");\n    return kTfLiteError;\n  }\n  TFLITE_SCOPED_TAGGED_DEFAULT_PROFILE(profiler_.get(), \"Invoke\");\n\n  // Invocations are always done in node order.\n  // Note that calling Invoke repeatedly will cause the original memory plan to\n  // be reused, unless either ResizeInputTensor() or AllocateTensors() has been\n  // called.\n  for (int execution_plan_index = 0;\n       execution_plan_index < execution_plan_.size(); execution_plan_index++) {\n    if (execution_plan_index == next_execution_plan_index_to_prepare_) {\n      TF_LITE_ENSURE_STATUS(PrepareOpsAndTensors());\n      TF_LITE_ENSURE(&context_, next_execution_plan_index_to_prepare_ >=\n                                    execution_plan_index);\n    }\n    int node_index = execution_plan_[execution_plan_index];\n    TfLiteNode& node = nodes_and_registration_[node_index].first;\n    const TfLiteRegistration& registration =\n        nodes_and_registration_[node_index].second;\n\n    const char* op_name = nullptr;\n    if (profiler_) op_name = GetTFLiteOpName(registration);\n    TFLITE_SCOPED_TAGGED_OPERATOR_PROFILE(profiler_.get(), op_name, node_index);\n\n    for (int i = 0; i < node.inputs->size; ++i) {\n      int tensor_index = node.inputs->data[i];\n      if (tensor_index == kTfLiteOptionalTensor) {\n        continue;\n      }\n      TfLiteTensor* tensor = &tensors_[tensor_index];\n      if (tensor->delegate && tensor->delegate != node.delegate &&\n          tensor->data_is_stale) {\n        TF_LITE_ENSURE_STATUS(EnsureTensorDataIsReadable(tensor_index));\n      }\n      if (tensor->data.raw == nullptr && tensor->bytes > 0) {\n        if (registration.builtin_code == kTfLiteBuiltinReshape && i == 1) {\n          // In general, having a tensor here with no buffer will be an error.\n          // However, for the reshape operator, the second input tensor is only\n          // used for the shape, not for the data. Thus, null buffer is ok.\n          continue;\n        } else {\n          // In all other cases, we need to return an error as otherwise we will\n          // trigger a null pointer dereference (likely).\n          ReportError(\"Input tensor %d lacks data\", tensor_index);\n          return kTfLiteError;\n        }\n      }\n    }\n\n    if (check_cancelled_func_ != nullptr &&\n        check_cancelled_func_(cancellation_data_)) {\n      ReportError(\"Client requested cancel during Invoke()\");\n      return kTfLiteError;\n    }\n\n    EnsureTensorsVectorCapacity();\n    tensor_resized_since_op_invoke_ = false;\n    if (OpInvoke(registration, &node) != kTfLiteOk) {\n      return ReportOpError(&context_, node, registration, node_index,\n                           \"failed to invoke\");\n    }\n\n    // Force execution prep for downstream ops if the latest op triggered the\n    // resize of a dynamic tensor.\n    if (tensor_resized_since_op_invoke_ &&\n        HasDynamicTensor(context_, node.outputs)) {\n      next_execution_plan_index_to_prepare_ = execution_plan_index + 1;\n\n      // This happens when an intermediate dynamic tensor is resized.\n      // We don't have to prepare all the ops, but we need to recompute\n      // the allocation plan.\n      if (next_execution_plan_index_to_plan_allocation_ >\n          next_execution_plan_index_to_prepare_) {\n        next_execution_plan_index_to_plan_allocation_ =\n            next_execution_plan_index_to_prepare_;\n        if (memory_planner_) {\n          TF_LITE_ENSURE_STATUS(memory_planner_->ResetAllocationsAfter(\n              next_execution_plan_index_to_plan_allocation_ - 1));\n        }\n      }\n    }\n  }\n\n  return status;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def edit_file(self):\n\n        if not HAS_AES or not HAS_COUNTER or not HAS_PBKDF2 or not HAS_HASH:\n            raise errors.AnsibleError(CRYPTO_UPGRADE)\n\n        # decrypt to tmpfile\n        tmpdata = self.read_data(self.filename)\n        this_vault = VaultLib(self.password)\n        dec_data = this_vault.decrypt(tmpdata)\n        _, tmp_path = tempfile.mkstemp()\n        self.write_data(dec_data, tmp_path)\n\n        # drop the user into vim on the tmp file\n        EDITOR = os.environ.get('EDITOR','vim')\n        call([EDITOR, tmp_path])\n        new_data = self.read_data(tmp_path)\n\n        # create new vault\n        new_vault = VaultLib(self.password)\n\n        # we want the cipher to default to AES256\n        #new_vault.cipher_name = this_vault.cipher_name\n\n        # encrypt new data a write out to tmp\n        enc_data = new_vault.encrypt(new_data)\n        self.write_data(enc_data, tmp_path)\n\n        # shuffle tmp file into place\n        self.shuffle_files(tmp_path, self.filename)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def execute(self, image_id, file_path):\n\n        # NOTE(flaper87): A format must be explicitly\n        # specified. There's no \"sane\" default for this\n        # because the dest format may work differently depending\n        # on the environment OpenStack is running in.\n        conversion_format = CONF.taskflow_executor.conversion_format\n        if conversion_format is None:\n            if not _Convert.conversion_missing_warned:\n                msg = _LW('The conversion format is None, please add a value '\n                          'for it in the config file for this task to '\n                          'work: %s')\n                LOG.warn(msg, self.task_id)\n                _Convert.conversion_missing_warned = True\n            return\n\n        # TODO(flaper87): Check whether the image is in the desired\n        # format already. Probably using `qemu-img` just like the\n        # `Introspection` task.\n        dest_path = os.path.join(CONF.task.work_dir, \"%s.converted\" % image_id)\n        stdout, stderr = putils.trycmd('qemu-img', 'convert', '-O',\n                                       conversion_format, file_path, dest_path,\n                                       log_errors=putils.LOG_ALL_ERRORS)\n\n        if stderr:\n            raise RuntimeError(stderr)\n\n        os.rename(dest_path, file_path.split(\"file://\")[-1])\n        return file_path\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _read(self, ti, try_number, metadata=None):\n        \"\"\"\n        Template method that contains custom logic of reading\n        logs given the try_number.\n\n        :param ti: task instance record\n        :param try_number: current try_number to read log from\n        :param metadata: log metadata,\n                         can be used for steaming log reading and auto-tailing.\n        :return: log message as a string and metadata.\n        \"\"\"\n        from airflow.utils.jwt_signer import JWTSigner\n\n        # Task instance here might be different from task instance when\n        # initializing the handler. Thus explicitly getting log location\n        # is needed to get correct log path.\n        log_relative_path = self._render_filename(ti, try_number)\n        location = os.path.join(self.local_base, log_relative_path)\n\n        log = \"\"\n\n        if os.path.exists(location):\n            try:\n                with open(location, encoding=\"utf-8\", errors=\"surrogateescape\") as file:\n                    log += f\"*** Reading local file: {location}\\n\"\n                    log += \"\".join(file.readlines())\n            except Exception as e:\n                log = f\"*** Failed to load local log file: {location}\\n\"\n                log += f\"*** {str(e)}\\n\"\n        elif conf.get('core', 'executor') == 'KubernetesExecutor':\n            try:\n                from airflow.kubernetes.kube_client import get_kube_client\n\n                kube_client = get_kube_client()\n\n                if len(ti.hostname) >= 63:\n                    # Kubernetes takes the pod name and truncates it for the hostname. This truncated hostname\n                    # is returned for the fqdn to comply with the 63 character limit imposed by DNS standards\n                    # on any label of a FQDN.\n                    pod_list = kube_client.list_namespaced_pod(conf.get('kubernetes', 'namespace'))\n                    matches = [\n                        pod.metadata.name\n                        for pod in pod_list.items\n                        if pod.metadata.name.startswith(ti.hostname)\n                    ]\n                    if len(matches) == 1:\n                        if len(matches[0]) > len(ti.hostname):\n                            ti.hostname = matches[0]\n\n                log += f'*** Trying to get logs (last 100 lines) from worker pod {ti.hostname} ***\\n\\n'\n\n                res = kube_client.read_namespaced_pod_log(\n                    name=ti.hostname,\n                    namespace=conf.get('kubernetes', 'namespace'),\n                    container='base',\n                    follow=False,\n                    tail_lines=100,\n                    _preload_content=False,\n                )\n\n                for line in res:\n                    log += line.decode()\n\n            except Exception as f:\n                log += f'*** Unable to fetch logs from worker pod {ti.hostname} ***\\n{str(f)}\\n\\n'\n        else:\n            import httpx\n\n            url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(\n                ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')\n            )\n            log += f\"*** Log file does not exist: {location}\\n\"\n            log += f\"*** Fetching from: {url}\\n\"\n            try:\n                timeout = None  # No timeout\n                try:\n                    timeout = conf.getint('webserver', 'log_fetch_timeout_sec')\n                except (AirflowConfigException, ValueError):\n                    pass\n\n                signer = JWTSigner(\n                    secret_key=conf.get('webserver', 'secret_key'),\n                    expiration_time_in_seconds=conf.getint(\n                        'webserver', 'log_request_clock_grace', fallback=30\n                    ),\n                    audience=\"task-instance-logs\",\n                )\n                response = httpx.get(\n                    url,\n                    timeout=timeout,\n                    headers={b'Authorization': signer.generate_signed_token({\"filename\": log_relative_path})},\n                )\n                response.encoding = \"utf-8\"\n\n                if response.status_code == 403:\n                    log += (\n                        \"*** !!!! Please make sure that all your Airflow components (e.g. \"\n                        \"schedulers, webservers and workers) have \"\n                        \"the same 'secret_key' configured in 'webserver' section and \"\n                        \"time is synchronized on all your machines (for example with ntpd) !!!!!\\n***\"\n                    )\n                    log += (\n                        \"*** See more at https://airflow.apache.org/docs/apache-airflow/\"\n                        \"stable/configurations-ref.html#secret-key\\n***\"\n                    )\n                # Check if the resource was properly fetched\n                response.raise_for_status()\n\n                log += '\\n' + response.text\n            except Exception as e:\n                log += f\"*** Failed to fetch log file from worker. {str(e)}\\n\"\n\n        return log, {'end_of_log': True}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def login_record(success=True):\n    denied_hosts = read_hosts_deny()\n    val = (0, 0)\n    if success and request.client in denied_hosts:\n        del denied_hosts[request.client]\n    elif not success and not request.is_local:\n        val = denied_hosts.get(request.client, (0, 0))\n        if time.time() - val[1] < expiration_failed_logins \\\n            and val[0] >= allowed_number_of_attempts:\n            return val[0]  # locked out\n        time.sleep(2 ** val[0])\n        val = (val[0] + 1, int(time.time()))\n        denied_hosts[request.client] = val\n    write_hosts_deny(denied_hosts)\n    return val[0]\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def xsrf_token(self):\n        \"\"\"The XSRF-prevention token for the current user/session.\n\n        To prevent cross-site request forgery, we set an '_xsrf' cookie\n        and include the same '_xsrf' value as an argument with all POST\n        requests. If the two do not match, we reject the form submission\n        as a potential forgery.\n\n        See http://en.wikipedia.org/wiki/Cross-site_request_forgery\n        \"\"\"\n        if not hasattr(self, \"_xsrf_token\"):\n            token = self.get_cookie(\"_xsrf\")\n            if not token:\n                token = binascii.b2a_hex(os.urandom(16))\n                expires_days = 30 if self.current_user else None\n                self.set_cookie(\"_xsrf\", token, expires_days=expires_days)\n            self._xsrf_token = token\n        return self._xsrf_token\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse_native_url(url):\n        \"\"\"\n        Support https://maker.ifttt.com/use/WEBHOOK_ID/EVENT_ID\n        \"\"\"\n\n        result = re.match(\n            r'^https?://maker\\.ifttt\\.com/use/'\n            r'(?P<webhook_id>[A-Z0-9_-]+)'\n            r'/?(?P<events>([A-Z0-9_-]+/?)+)?'\n            r'/?(?P<params>\\?.+)?$', url, re.I)\n\n        if result:\n            return NotifyIFTTT.parse_url(\n                '{schema}://{webhook_id}{events}{params}'.format(\n                    schema=NotifyIFTTT.secure_protocol,\n                    webhook_id=result.group('webhook_id'),\n                    events='' if not result.group('events')\n                    else '@{}'.format(result.group('events')),\n                    params='' if not result.group('params')\n                    else result.group('params')))\n\n        return None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _require_verified_user(self, request):\n        user = request.user\n\n        if not settings.WAGTAIL_2FA_REQUIRED:\n            # If two factor authentication is disabled in the settings\n            return False\n\n        if not user.is_authenticated:\n            return False\n\n        # If the user has no access to the admin anyway then don't require a\n        # verified user here\n        if not (\n            user.is_staff\n            or user.is_superuser\n            or user.has_perms([\"wagtailadmin.access_admin\"])\n        ):\n            return False\n\n        # Allow the user to a fixed number of paths when not verified\n        if request.path in self._allowed_paths:\n            return False\n\n        # For all other cases require that the user is verfied via otp\n        return True\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _glob_matches(glob: str, value: str, word_boundary: bool = False) -> bool:\n    \"\"\"Tests if value matches glob.\n\n    Args:\n        glob\n        value: String to test against glob.\n        word_boundary: Whether to match against word boundaries or entire\n            string. Defaults to False.\n    \"\"\"\n\n    try:\n        r = regex_cache.get((glob, True, word_boundary), None)\n        if not r:\n            r = _glob_to_re(glob, word_boundary)\n            regex_cache[(glob, True, word_boundary)] = r\n        return bool(r.search(value))\n    except re.error:\n        logger.warning(\"Failed to parse glob to regex: %r\", glob)\n        return False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "                repeatedPassword: ko.observable(undefined),\n                passwordMismatch: ko.pureComputed(function () {\n                    return self.editor.password() !== self.editor.repeatedPassword();\n                }),\n                apikey: ko.observable(undefined),\n                active: ko.observable(undefined),\n                permissionSelectable: function (permission) {\n                    return true;\n                },\n                permissionSelected: function (permission) {\n                    var index = self.editor.permissions().indexOf(permission);\n                    return index >= 0;\n                },\n                togglePermission: function (permission) {\n                    var permissions = self.editor.permissions();\n                    var index = permissions.indexOf(permission);\n                    if (index < 0) {\n                        permissions.push(permission);\n                    } else {\n                        permissions.splice(index, 1);\n                    }\n                    self.editor.permissions(permissions);\n                },\n                groupSelected: function (group) {\n                    var index = self.editor.groups().indexOf(group);\n                    return index >= 0;\n                },\n                toggleGroup: function (group) {\n                    var groups = self.editor.groups();\n                    var index = groups.indexOf(group);\n                    if (index < 0) {\n                        groups.push(group);\n                    } else {\n                        groups.splice(index, 1);\n                    }\n                    self.editor.groups(groups);\n                },\n                joinedGroupPermissions: function (group) {\n                    return access.permissionList(group);\n                },\n                header: ko.observable(undefined),\n                new: ko.observable(true),\n                confirm: undefined,\n                valid: ko.pureComputed(function () {\n                    return (\n                        self.editor.name() &&\n                        self.editor.name().trim() &&\n                        (!self.editor.new() ||\n                            (self.editor.password() &&\n                                self.editor.password().trim() &&\n                                !self.editor.passwordMismatch()))\n                    );\n                }),\n                dangerRestricted: function () {\n                    return false;\n                },\n                dangerRestrictedText: gettext(\n                    \"This user may not have dangerous permissions.\"\n                )\n            };\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def read_config(self, config, **kwargs):\n        consent_config = config.get(\"user_consent\")\n        self.terms_template = self.read_templates([\"terms.html\"], autoescape=True)[0]\n\n        if consent_config is None:\n            return\n        self.user_consent_version = str(consent_config[\"version\"])\n        self.user_consent_template_dir = self.abspath(consent_config[\"template_dir\"])\n        if not path.isdir(self.user_consent_template_dir):\n            raise ConfigError(\n                \"Could not find template directory '%s'\"\n                % (self.user_consent_template_dir,)\n            )\n        self.user_consent_server_notice_content = consent_config.get(\n            \"server_notice_content\"\n        )\n        self.block_events_without_consent_error = consent_config.get(\n            \"block_events_error\"\n        )\n        self.user_consent_server_notice_to_guests = bool(\n            consent_config.get(\"send_server_notice_to_guests\", False)\n        )\n        self.user_consent_at_registration = bool(\n            consent_config.get(\"require_at_registration\", False)\n        )\n        self.user_consent_policy_name = consent_config.get(\n            \"policy_name\", \"Privacy Policy\"\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    INDEX_TYPE first_dimension;\n    OP_REQUIRES_OK(context, GetFirstDimensionSize(context, &first_dimension));\n    vector<INDEX_TYPE> output_size;\n    OP_REQUIRES_OK(context,\n                   CalculateOutputSize(first_dimension, context, &output_size));\n    vector<INDEX_TYPE> multiplier;\n    multiplier.resize(ragged_rank_ + 1);\n\n    multiplier[multiplier.size() - 1] = 1;\n    for (int i = multiplier.size() - 2; i >= 0; --i) {\n      multiplier[i] = multiplier[i + 1] * output_size[i + 1];\n    }\n    // Full size of the tensor.\n    TensorShape output_shape;\n    OP_REQUIRES_OK(context,\n                   TensorShapeUtils::MakeShape(output_size, &output_shape));\n    Tensor* output_tensor = nullptr;\n\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, output_shape, &output_tensor));\n    const INDEX_TYPE full_size = multiplier[0] * output_size[0];\n    if (full_size > 0) {\n      vector<INDEX_TYPE> output_index, new_output_index;\n      int nvals = context->input(kValueInputIndex).shape().dim_size(0);\n      output_index.reserve(nvals);\n      new_output_index.reserve(nvals);\n\n      CalculateFirstParentOutputIndex(first_dimension, multiplier[0],\n                                      output_size[0], &output_index);\n      for (int i = 1; i <= ragged_rank_; ++i) {\n        OP_REQUIRES_OK(context, CalculateOutputIndex(\n                                    context, i - 1, output_index, multiplier[i],\n                                    output_size[i], &new_output_index));\n        output_index.swap(new_output_index);\n        new_output_index.clear();\n      }\n\n      SetOutput(context, ragged_rank_, output_index, output_tensor);\n    }\n  }\n  virtual void SetOutput(OpKernelContext* context, int ragged_rank,"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_users(self, email):\n        \"\"\"Given an email, return matching user(s) who should receive a reset.\n\n        This allows subclasses to more easily customize the default policies\n        that prevent inactive users and users with unusable passwords from\n        resetting their password.\n        \"\"\"\n        active_users = UserModel._default_manager.filter(**{\n            '%s__iexact' % UserModel.get_email_field_name(): email,\n            'is_active': True,\n        })\n        return (u for u in active_users if u.has_usable_password())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _read_lockfile(self, lockfile: Mapping[str, Any]) -> None:\n        root = self.environment.project.root\n        with cd(root):\n            for package in lockfile.get(\"package\", []):\n                version = package.get(\"version\")\n                if version:\n                    package[\"version\"] = f\"=={version}\"\n                package_name = package.pop(\"name\")\n                req_dict = {\n                    k: v for k, v in package.items() if k not in (\"dependencies\", \"requires_python\", \"summary\", \"files\")\n                }\n                req = Requirement.from_req_dict(package_name, req_dict)\n                if req.is_file_or_url and req.path and not req.url:  # type: ignore[attr-defined]\n                    req.url = path_to_url(posixpath.join(root, req.path))  # type: ignore[attr-defined]\n                can = make_candidate(req, name=package_name, version=version)\n                can.hashes = package.get(\"files\", [])\n                can_id = self._identify_candidate(can)\n                self.packages[can_id] = can\n                candidate_info: CandidateInfo = (\n                    package.get(\"dependencies\", []),\n                    package.get(\"requires_python\", \"\"),\n                    package.get(\"summary\", \"\"),\n                )\n                self.candidate_info[can_id] = candidate_info\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def deserialize_raw(collection_type):\n    __connect()\n    contents = simplejson.loads(couchdb.listDoc(collection_type))\n    items = []\n    if contents.has_key(\"error\") and contents.get(\"reason\",\"\").find(\"Missing\") != -1:\n        # no items in the DB yet\n        return []\n    for x in contents[\"rows\"]:\n       items.append(x[\"key\"])\n\n    if collection_type == \"settings\":\n         fd = open(\"/etc/cobbler/settings\")\n         datastruct = yaml.load(fd.read())\n         fd.close()\n         return datastruct\n    else:\n         results = []\n         for f in items:\n             data = couchdb.openDoc(collection_type, f)\n             datastruct = simplejson.loads(data, encoding='utf-8')\n             results.append(datastruct)\n         return results    \n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _open_index(self, index=1):\n        #\n        # get the Image Contents Property Set\n\n        prop = self.ole.getproperties(\n            [\"Data Object Store %06d\" % index, \"\\005Image Contents\"]\n        )\n\n        # size (highest resolution)\n\n        self._size = prop[0x1000002], prop[0x1000003]\n\n        size = max(self.size)\n        i = 1\n        while size > 64:\n            size = size / 2\n            i += 1\n        self.maxid = i - 1\n\n        # mode.  instead of using a single field for this, flashpix\n        # requires you to specify the mode for each channel in each\n        # resolution subimage, and leaves it to the decoder to make\n        # sure that they all match.  for now, we'll cheat and assume\n        # that this is always the case.\n\n        id = self.maxid << 16\n\n        s = prop[0x2000002 | id]\n\n        colors = []\n        for i in range(i32(s, 4)):\n            # note: for now, we ignore the \"uncalibrated\" flag\n            colors.append(i32(s, 8 + i * 4) & 0x7FFFFFFF)\n\n        self.mode, self.rawmode = MODES[tuple(colors)]\n\n        # load JPEG tables, if any\n        self.jpeg = {}\n        for i in range(256):\n            id = 0x3000001 | (i << 16)\n            if id in prop:\n                self.jpeg[i] = prop[id]\n\n        self._open_subimage(1, self.maxid)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def remove_auth_hashes(input: Optional[str]):\n    if not input:\n        return input\n    # If there are no hashes, skip the RE for performance.\n    if not any([pw_hash in input for pw_hash in PASSWORD_HASHERS_ALL.keys()]):\n        return input\n    return re_remove_passwords.sub(r'\\1 %s  # Filtered for security' % PASSWORD_HASH_DUMMY_VALUE, input)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "export const redirectAnyRequest = response => {\n  const {headers: {location}} = Response.message (response);\n  const original = Response.request (response);\n  const oldUrl = Request.url (original);\n  const newUrl = mergeUrls (oldUrl, location);\n  const request = Request (Request.options (original))\n                          (newUrl)\n                          (Request.body (original));\n\n  return sameHost (oldUrl, newUrl) ? request : overHeaders (request, xs => (\n    xs.filter (([name]) => !confidentialHeaders.includes (name.toLowerCase ()))\n  ));\n};\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def load(self):\n        if self.saving_method == \"mongo\":\n            data = self._load_mongo()\n        elif self.saving_method == \"local\":\n            data = self._load_local()\n        data = data[\"members\"][self.sid]\n        self.member.data = GroupMemberData(**data)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def remove_file_ref(filename):\n        logger.debug('removing ref on %s', filename)\n        # TODO: fix the race condition below\n        HTMLAsset.objects.filter(filename=filename).update(ref_count=models.F('ref_count') - 1)\n        refs_count = HTMLAsset.objects.filter(filename=filename, ref_count__gt=0).aggregate(models.Sum('ref_count'))\n        refs_count = refs_count.get('ref_count__sum')\n\n        # refs_count is None when HTMLAsset.objects.filter(...) returns an empty set\n        if refs_count is None or refs_count <= 0:\n            logger.debug('removing file %s', filename)\n            remove_html_asset_file(settings.SOSSE_HTML_SNAPSHOT_DIR + filename)\n\n        HTMLAsset.objects.filter(filename=filename, ref_count__lte=0).delete()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& indices_tensor = ctx->input(0);\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(indices_tensor.shape()) ||\n                    TensorShapeUtils::IsScalar(indices_tensor.shape()),\n                errors::InvalidArgument(\n                    \"The indices can only be scalar or vector, got \\\"\",\n                    indices_tensor.shape().DebugString(), \"\\\"\"));\n\n    const Tensor& dims_tensor = ctx->input(1);\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(dims_tensor.shape()),\n        errors::InvalidArgument(\"The indices can only be 1-D, got \\\"\",\n                                dims_tensor.shape().DebugString(), \"\\\"\"));\n\n    auto dims = dims_tensor.vec<Tidx>();\n    // Make sure dims does not contain a zero\n    for (int i = 0; i < dims.size(); i++) {\n      OP_REQUIRES(\n          ctx, dims(i) != 0,\n          errors::InvalidArgument(\"Input dims cannot contain a dim of zero, \"\n                                  \"but dims contains zero at index \",\n                                  i));\n    }\n\n    // Check to make sure indices is not out of boundary\n    Eigen::Tensor<Tidx, 0, Eigen::RowMajor> dims_prod_eigen = dims.prod();\n    Tidx dims_prod = dims_prod_eigen();\n    const Tidx* indices = indices_tensor.flat<Tidx>().data();\n    int64_t size = indices_tensor.NumElements();\n    bool check = std::all_of(indices, indices + size,\n                             [&](Tidx index) { return index < dims_prod; });\n    OP_REQUIRES(ctx, check,\n                errors::InvalidArgument(\"index is out of bound as with dims\"));\n\n    Eigen::array<bool, 1> reverse({true});\n\n    Tensor strides_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<Tidx>::value,\n                                      TensorShape({dims_tensor.NumElements()}),\n                                      &strides_tensor));\n\n    auto strides = strides_tensor.vec<Tidx>();\n    strides = dims.reverse(reverse)\n                  .scan(0, Eigen::internal::ProdReducer<Tidx>(), false)\n                  .reverse(reverse);\n\n    Tensor strides_shifted_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<Tidx>::value,\n                                      TensorShape({dims_tensor.NumElements()}),\n                                      &strides_shifted_tensor));\n\n    auto strides_shifted = strides_shifted_tensor.vec<Tidx>();\n    strides_shifted = dims.reverse(reverse)\n                          .scan(0, Eigen::internal::ProdReducer<Tidx>(), true)\n                          .reverse(reverse);\n\n    Tensor* output_tensor = nullptr;\n    if (TensorShapeUtils::IsScalar(indices_tensor.shape())) {\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({dims_tensor.NumElements()}),\n                                    &output_tensor));\n\n      auto output = output_tensor->vec<Tidx>();\n\n      output = output.constant(indices_tensor.scalar<Tidx>()());\n      output = output.binaryExpr(strides, mod_op<Tidx>()) / strides_shifted;\n    } else {\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0,\n                                    TensorShape({dims_tensor.NumElements(),\n                                                 indices_tensor.NumElements()}),\n                                    &output_tensor));\n\n      auto output = output_tensor->matrix<Tidx>();\n\n      Eigen::array<Eigen::Index, 2> reshape{\n          {static_cast<Eigen::Index>(dims_tensor.NumElements()), 1}};\n      Eigen::array<Eigen::Index, 2> bcast(\n          {1, static_cast<Eigen::Index>(indices_tensor.NumElements())});\n      Eigen::array<Eigen::Index, 2> indices_reshape{\n          {1, static_cast<Eigen::Index>(indices_tensor.NumElements())}};\n      Eigen::array<Eigen::Index, 2> indices_bcast(\n          {static_cast<Eigen::Index>(dims_tensor.NumElements()), 1});\n\n      output = indices_tensor.vec<Tidx>()\n                   .reshape(indices_reshape)\n                   .broadcast(indices_bcast);\n      output = output.binaryExpr(strides.reshape(reshape).broadcast(bcast),\n                                 mod_op<Tidx>()) /\n               strides_shifted.reshape(reshape).broadcast(bcast);\n    }\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def decrypt(self, password_encrypted):\n        \"\"\"Decrypt the given password using the pycryto.\n        \"\"\"\n        crypter = self._init_crypter()\n        return crypter.decrypt(password_encrypted)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def upload_template(filename, destination, context=None, use_jinja=False,\n    template_dir=None, use_sudo=False, backup=True, mirror_local_mode=False,\n    mode=None):\n    \"\"\"\n    Render and upload a template text file to a remote host.\n\n    ``filename`` should be the path to a text file, which may contain `Python\n    string interpolation formatting\n    <http://docs.python.org/release/2.5.4/lib/typesseq-strings.html>`_ and will\n    be rendered with the given context dictionary ``context`` (if given.)\n\n    Alternately, if ``use_jinja`` is set to True and you have the Jinja2\n    templating library available, Jinja will be used to render the template\n    instead. Templates will be loaded from the invoking user's current working\n    directory by default, or from ``template_dir`` if given.\n\n    The resulting rendered file will be uploaded to the remote file path\n    ``destination`` (which should include the desired remote filename.) If the\n    destination file already exists, it will be renamed with a ``.bak``\n    extension unless ``backup=False`` is specified.\n\n    By default, the file will be copied to ``destination`` as the logged-in\n    user; specify ``use_sudo=True`` to use `sudo` instead.\n\n    In some use cases, it is desirable to force a newly uploaded file to match\n    the mode of its local counterpart (such as when uploading executable\n    scripts). To do this, specify ``mirror_local_mode=True``.\n\n    Alternately, you may use the ``mode`` kwarg to specify an exact mode, in\n    the same vein as ``os.chmod`` or the Unix ``chmod`` command.\n    \"\"\"\n    basename = os.path.basename(filename)\n\n    # This temporary file should not be automatically deleted on close, as we\n    # need it there to upload it (Windows locks the file for reading while\n    # open).\n    tempdir = tempfile.mkdtemp()\n    tempfile_name = os.path.join(tempdir, basename)\n    output = open(tempfile_name, \"w+b\")\n    # Init\n    text = None\n    if use_jinja:\n        try:\n            from jinja2 import Environment, FileSystemLoader\n            jenv = Environment(loader=FileSystemLoader(template_dir or '.'))\n            text = jenv.get_template(filename).render(**context or {})\n        except ImportError, e:\n            abort(\"tried to use Jinja2 but was unable to import: %s\" % e)\n    else:\n        with open(filename) as inputfile:\n            text = inputfile.read()\n        if context:\n            text = text % context\n    output.write(text)\n    output.close()\n\n    # Back up any original file \n    func = use_sudo and sudo or run\n    # Back up any original file (need to do figure out ultimate destination)\n    if backup:\n        to_backup = destination\n        with settings(hide('everything'), warn_only=True):\n            # Is destination a directory?\n            if func('test -f %s' % to_backup).failed:\n                # If so, tack on the filename to get \"real\" destination\n                to_backup = destination + '/' + basename\n        if exists(to_backup):\n            func(\"cp %s %s.bak\" % (to_backup, to_backup))\n\n    # Upload the file.\n    put(\n        local_path=tempfile_name,\n        remote_path=destination,\n        use_sudo=use_sudo,\n        mirror_local_mode=mirror_local_mode,\n        mode=mode\n    )\n    output.close()\n    os.remove(tempfile_name)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        dir_name, file_name = os.path.split(name)\n        if '..' in pathlib.PurePath(dir_name).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n        validate_file_name(file_name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, generate an alternative filename\n        # until it doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, self.get_alternative_name(file_root, file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, self.get_alternative_name(file_root, file_ext))\n        return name\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def write_exception_sync(exception, before=None, after=None, file=None, *, filter=None, highlighter=None):\n    \"\"\"\n    Writes the given exception's traceback.\n    \n    Parameters\n    ----------\n    exception : ``BaseException``\n        The exception to render.\n    \n    before : `str`, `list` of `str` = `None`, Optional\n        Any content, what should go before the exception's traceback.\n        \n        If given as `str`, or if `list`, then the last element of it should end with linebreak.\n    \n    after : `str`, `list` of `str` = `None`, Optional\n        Any content, what should go after the exception's traceback.\n        \n        If given as `str`, or if `list`, then the last element of it should end with linebreak.\n    \n    file : `None`, `I/O stream` = `None`, Optional\n        The file to print the stack to. Defaults to `sys.stderr`.\n    \n    filter : `None`, `callable` = `None`, Optional (Keyword only)\n        Additional filter to check whether a frame should be shown.\n        \n        Called with 4 parameters:\n        \n        +---------------+-----------+---------------------------------------------------------------+\n        | Name          | Type      | Description                                                   |\n        +===============+===========+===============================================================+\n        | file_name     | `str`     | The frame's file's name.                                      |\n        +---------------+-----------+---------------------------------------------------------------+\n        | name          | `str`     | The name of the function.                                     |\n        +---------------+-----------+---------------------------------------------------------------+\n        | line_number   | `int`     | The line's index of the file where the exception occurred.    |\n        +---------------+-----------+---------------------------------------------------------------+\n        | line          | `str`     | The line of the file.                                         |\n        +---------------+-----------+---------------------------------------------------------------+\n    \n    highlighter : `None`, ``HighlightFormatterContext`` = `None`, Optional (Keyword only)\n        Formatter storing highlighting details.\n    \"\"\"\n    extracted = []\n    \n    if before is None:\n        pass\n    elif isinstance(before, str):\n        extracted.append(before)\n    elif isinstance(before, list):\n        for element in before:\n            if isinstance(element, str):\n                extracted.append(element)\n            else:\n                extracted.append(repr(element))\n                extracted.append('\\n')\n    else:\n        # ignore exception cases\n        extracted.append(repr(before))\n        extracted.append('\\n')\n    \n    if (file is None) and (highlighter is None):\n        highlighter = get_default_trace_writer_highlighter()\n    \n    render_exception_into(exception, extracted, filter=filter, highlighter=highlighter)\n    \n    if after is None:\n        pass\n    elif isinstance(after, str):\n        extracted.append(after)\n    elif isinstance(after, list):\n        for element in after:\n            if isinstance(element, str):\n                extracted.append(element)\n            else:\n                extracted.append(repr(element))\n                extracted.append('\\n')\n    else:\n        extracted.append(repr(after))\n        extracted.append('\\n')\n    \n    if file is None:\n        # ignore exception cases\n        file = sys.stderr\n    \n    file.write(''.join(extracted))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def dataReceived(self, data: bytes) -> None:\n        self.stream.write(data)\n        self.length += len(data)\n        if self.max_size is not None and self.length >= self.max_size:\n            self.deferred.errback(\n                SynapseError(\n                    502,\n                    \"Requested file is too large > %r bytes\" % (self.max_size,),\n                    Codes.TOO_LARGE,\n                )\n            )\n            self.deferred = defer.Deferred()\n            self.transport.loseConnection()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def get(self, request: web.Request) -> web.Response:\n        \"\"\"Get available auth providers.\"\"\"\n        hass: HomeAssistant = request.app[\"hass\"]\n        if not onboarding.async_is_user_onboarded(hass):\n            return self.json_message(\n                message=\"Onboarding not finished\",\n                status_code=HTTPStatus.BAD_REQUEST,\n                message_code=\"onboarding_required\",\n            )\n\n        try:\n            remote_address = ip_address(request.remote)  # type: ignore[arg-type]\n        except ValueError:\n            return self.json_message(\n                message=\"Invalid remote IP\",\n                status_code=HTTPStatus.BAD_REQUEST,\n                message_code=\"invalid_remote_ip\",\n            )\n\n        cloud_connection = is_cloud_connection(hass)\n\n        providers = []\n        for provider in hass.auth.auth_providers:\n            additional_data = {}\n\n            if provider.type == \"trusted_networks\":\n                if cloud_connection:\n                    # Skip quickly as trusted networks are not available on cloud\n                    continue\n\n                try:\n                    cast(\"TrustedNetworksAuthProvider\", provider).async_validate_access(\n                        remote_address\n                    )\n                except InvalidAuthError:\n                    # Not a trusted network, so we don't expose that trusted_network authenticator is setup\n                    continue\n            elif (\n                provider.type == \"homeassistant\"\n                and not cloud_connection\n                and is_local(remote_address)\n                and \"person\" in hass.config.components\n            ):\n                # We are local, return user id and username\n                users = await provider.store.async_get_users()\n                additional_data[\"users\"] = {\n                    user.id: credentials.data[\"username\"]\n                    for user in users\n                    for credentials in user.credentials\n                    if (\n                        credentials.auth_provider_type == provider.type\n                        and credentials.auth_provider_id == provider.id\n                    )\n                }\n\n            providers.append(\n                {\n                    \"name\": provider.name,\n                    \"id\": provider.id,\n                    \"type\": provider.type,\n                    **additional_data,\n                }\n            )\n\n        return self.json(providers)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __call__(self, request):\n        file_fields = request.POST.getlist(\"s3file\")\n        for field_name in file_fields:\n            paths = request.POST.getlist(field_name)\n            request.FILES.setlist(field_name, list(self.get_files_from_storage(paths)))\n\n        if local_dev and request.path == \"/__s3_mock__/\":\n            return views.S3MockView.as_view()(request)\n\n        return self.get_response(request)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& indices = ctx->input(0);\n    const auto values = ctx->input(1).flat<Tidx>();\n    const Tensor& dense_shape = ctx->input(2);\n    const Tensor& size_t = ctx->input(3);\n    const auto weights = ctx->input(4).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    OP_REQUIRES(ctx, size_t.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_t.dims()));\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    bool is_1d = dense_shape.NumElements() == 1;\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (is_1d) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(ctx,\n                       functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                           ctx, values, weights, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, values, weights, out, size));\n      }\n    } else {\n      const auto shape = dense_shape.flat<int64_t>();\n      const int64_t num_rows = shape(0);\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      const auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      const auto indices_mat = indices.matrix<int64_t>();\n      for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {\n        const int64_t batch = indices_mat(i, 0);\n        const Tidx bin = values(i);\n        OP_REQUIRES(\n            ctx, batch < out.dimension(0),\n            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,\n                                    \") must be less than the dimension size (\",\n                                    out.dimension(0), \").\"));\n        OP_REQUIRES(\n            ctx, bin < out.dimension(1),\n            errors::InvalidArgument(\"Index out ouf bound. `bin` (\", bin,\n                                    \") must be less then the dimension size (\",\n                                    out.dimension(1), \").\"));\n        if (bin < size) {\n          if (binary_output_) {\n            out(batch, bin) = T(1);\n          } else {\n            if (weights_size) {\n              out(batch, bin) += weights(i);\n            } else {\n              out(batch, bin) += T(1);\n            }\n          }\n        }\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "_tiffReadProc(thandle_t hdata, tdata_t buf, tsize_t size) {\n    TIFFSTATE *state = (TIFFSTATE *)hdata;\n    tsize_t to_read;\n\n    TRACE((\"_tiffReadProc: %d \\n\", (int)size));\n    dump_state(state);\n\n    to_read = min(size, min(state->size, (tsize_t)state->eof) - (tsize_t)state->loc);\n    TRACE((\"to_read: %d\\n\", (int)to_read));\n\n    _TIFFmemcpy(buf, (UINT8 *)state->data + state->loc, to_read);\n    state->loc += (toff_t)to_read;\n\n    TRACE((\"location: %u\\n\", (uint)state->loc));\n    return to_read;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def unarchive(request, bookmark_id: int):\n    bookmark = Bookmark.objects.get(pk=bookmark_id)\n    unarchive_bookmark(bookmark)\n    return_url = request.GET.get('return_url')\n    return_url = return_url if return_url else reverse('bookmarks:archived')\n    return HttpResponseRedirect(return_url)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def execute(self, raise_on_error: bool = True):\n        \"\"\"Execute all the commands in the current pipeline\"\"\"\n        stack = self.command_stack\n        if not stack and not self.watching:\n            return []\n        if self.scripts:\n            await self.load_scripts()\n        if self.is_transaction or self.explicit_transaction:\n            execute = self._execute_transaction\n        else:\n            execute = self._execute_pipeline\n\n        conn = self.connection\n        if not conn:\n            conn = await self.connection_pool.get_connection(\"MULTI\", self.shard_hint)\n            # assign to self.connection so reset() releases the connection\n            # back to the pool after we're done\n            self.connection = conn\n        conn = cast(Connection, conn)\n\n        try:\n            return await conn.retry.call_with_retry(\n                lambda: execute(conn, stack, raise_on_error),\n                lambda error: self._disconnect_raise_reset(conn, error),\n            )\n        finally:\n            await self.reset()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _on_ssl_errors(self, error):\n        self._has_ssl_errors = True\n\n        url = error.url()\n        log.webview.debug(\"Certificate error: {}\".format(error))\n\n        if error.is_overridable():\n            error.ignore = shared.ignore_certificate_errors(\n                url, [error], abort_on=[self.abort_questions])\n        else:\n            log.webview.error(\"Non-overridable certificate error: \"\n                              \"{}\".format(error))\n\n        log.webview.debug(\"ignore {}, URL {}, requested {}\".format(\n            error.ignore, url, self.url(requested=True)))\n\n        # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-56207\n        # We can't really know when to show an error page, as the error might\n        # have happened when loading some resource.\n        # However, self.url() is not available yet and the requested URL\n        # might not match the URL we get from the error - so we just apply a\n        # heuristic here.\n        if (not qtutils.version_check('5.9') and\n                not error.ignore and\n                url.matches(self.url(requested=True), QUrl.RemoveScheme)):\n            self._show_error_page(url, str(error))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def to_yaml(self, **kwargs):\n    \"\"\"Returns a yaml string containing the network configuration.\n\n    To load a network from a yaml save file, use\n    `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n\n    `custom_objects` should be a dictionary mapping\n    the names of custom losses / layers / etc to the corresponding\n    functions / classes.\n\n    Args:\n        **kwargs: Additional keyword arguments\n            to be passed to `yaml.dump()`.\n\n    Returns:\n        A YAML string.\n\n    Raises:\n        ImportError: if yaml module is not found.\n    \"\"\"\n    if yaml is None:\n      raise ImportError(\n          'Requires yaml module installed (`pip install pyyaml`).')\n    return yaml.dump(self._updated_config(), **kwargs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool SetOpAttrList(TFE_Context* ctx, TFE_Op* op, const char* key,\n                   PyObject* py_list, TF_AttrType type,\n                   tensorflow::gtl::FlatMap<string, int64_t>* attr_list_sizes,\n                   TF_Status* status) {\n  if (!PySequence_Check(py_list)) {\n    TF_SetStatus(\n        status, TF_INVALID_ARGUMENT,\n        tensorflow::strings::StrCat(\"Expecting sequence value for attr \", key,\n                                    \", got \", py_list->ob_type->tp_name)\n            .c_str());\n    return false;\n  }\n  const int num_values = PySequence_Size(py_list);\n  if (attr_list_sizes != nullptr) (*attr_list_sizes)[key] = num_values;\n\n#define PARSE_LIST(c_type, parse_fn)                                      \\\n  std::unique_ptr<c_type[]> values(new c_type[num_values]);               \\\n  for (int i = 0; i < num_values; ++i) {                                  \\\n    tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));   \\\n    if (!parse_fn(key, py_value.get(), status, &values[i])) return false; \\\n  }\n\n  if (type == TF_ATTR_STRING) {\n    std::unique_ptr<const void*[]> values(new const void*[num_values]);\n    std::unique_ptr<size_t[]> lengths(new size_t[num_values]);\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::StringPiece value;\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      if (!ParseStringValue(key, py_value.get(), status, &value)) return false;\n      values[i] = value.data();\n      lengths[i] = value.size();\n    }\n    TFE_OpSetAttrStringList(op, key, values.get(), lengths.get(), num_values);\n  } else if (type == TF_ATTR_INT) {\n    PARSE_LIST(int64_t, ParseInt64Value);\n    TFE_OpSetAttrIntList(op, key, values.get(), num_values);\n  } else if (type == TF_ATTR_FLOAT) {\n    PARSE_LIST(float, ParseFloatValue);\n    TFE_OpSetAttrFloatList(op, key, values.get(), num_values);\n  } else if (type == TF_ATTR_BOOL) {\n    PARSE_LIST(unsigned char, ParseBoolValue);\n    TFE_OpSetAttrBoolList(op, key, values.get(), num_values);\n  } else if (type == TF_ATTR_TYPE) {\n    PARSE_LIST(int, ParseTypeValue);\n    TFE_OpSetAttrTypeList(op, key,\n                          reinterpret_cast<const TF_DataType*>(values.get()),\n                          num_values);\n  } else if (type == TF_ATTR_SHAPE) {\n    // Make one pass through the input counting the total number of\n    // dims across all the input lists.\n    int total_dims = 0;\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      if (py_value.get() != Py_None) {\n        if (!PySequence_Check(py_value.get())) {\n          TF_SetStatus(\n              status, TF_INVALID_ARGUMENT,\n              tensorflow::strings::StrCat(\n                  \"Expecting None or sequence value for element\", i,\n                  \" of attr \", key, \", got \", py_value->ob_type->tp_name)\n                  .c_str());\n          return false;\n        }\n        const auto size = TensorShapeNumDims(py_value.get());\n        if (size >= 0) {\n          total_dims += size;\n        }\n      }\n    }\n    // Allocate a buffer that can fit all of the dims together.\n    std::unique_ptr<int64_t[]> buffer(new int64_t[total_dims]);\n    // Copy the input dims into the buffer and set dims to point to\n    // the start of each list's dims.\n    std::unique_ptr<const int64_t*[]> dims(new const int64_t*[num_values]);\n    std::unique_ptr<int[]> num_dims(new int[num_values]);\n    int64_t* offset = buffer.get();\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      if (py_value.get() == Py_None) {\n        dims[i] = nullptr;\n        num_dims[i] = -1;\n      } else {\n        const auto size = TensorShapeNumDims(py_value.get());\n        if (size == -1) {\n          dims[i] = nullptr;\n          num_dims[i] = -1;\n          continue;\n        }\n        dims[i] = offset;\n        num_dims[i] = size;\n        for (int j = 0; j < size; ++j) {\n          tensorflow::Safe_PyObjectPtr inner_py_value(\n              PySequence_ITEM(py_value.get(), j));\n          if (inner_py_value.get() == Py_None) {\n            *offset = -1;\n          } else if (!ParseDimensionValue(key, inner_py_value.get(), status,\n                                          offset)) {\n            return false;\n          }\n          ++offset;\n        }\n      }\n    }\n    TFE_OpSetAttrShapeList(op, key, dims.get(), num_dims.get(), num_values,\n                           status);\n    if (!status->status.ok()) return false;\n  } else if (type == TF_ATTR_FUNC) {\n    std::unique_ptr<const TFE_Op*[]> funcs(new const TFE_Op*[num_values]);\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      // Allow:\n      // (1) String function name, OR\n      // (2) A Python object with a .name attribute\n      //     (A crude test for being a\n      //     tensorflow.python.framework.function._DefinedFunction)\n      //     (which is what the various \"defun\" or \"Defun\" decorators do).\n      // And in the future also allow an object that can encapsulate\n      // the function name and its attribute values.\n      tensorflow::StringPiece func_name;\n      if (!ParseStringValue(key, py_value.get(), status, &func_name)) {\n        PyObject* name_attr = PyObject_GetAttrString(py_value.get(), \"name\");\n        if (name_attr == nullptr ||\n            !ParseStringValue(key, name_attr, status, &func_name)) {\n          TF_SetStatus(\n              status, TF_INVALID_ARGUMENT,\n              tensorflow::strings::StrCat(\n                  \"unable to set function value attribute from a \",\n                  py_value.get()->ob_type->tp_name,\n                  \" object. If you think this is an error, please file an \"\n                  \"issue at \"\n                  \"https://github.com/tensorflow/tensorflow/issues/new\")\n                  .c_str());\n          return false;\n        }\n      }\n      funcs[i] = TFE_NewOp(ctx, func_name.data(), status);\n      if (!status->status.ok()) return false;\n    }\n    TFE_OpSetAttrFunctionList(op, key, funcs.get(), num_values);\n    if (!status->status.ok()) return false;\n  } else {\n    TF_SetStatus(status, TF_UNIMPLEMENTED,\n                 tensorflow::strings::StrCat(\"Attr \", key,\n                                             \" has unhandled list type \", type)\n                     .c_str());\n    return false;\n  }\n#undef PARSE_LIST\n  return true;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static int BuildHuffmanTable(HuffmanCode* const root_table, int root_bits,\n                             const int code_lengths[], int code_lengths_size,\n                             uint16_t sorted[]) {\n  HuffmanCode* table = root_table;  // next available space in table\n  int total_size = 1 << root_bits;  // total size root table + 2nd level table\n  int len;                          // current code length\n  int symbol;                       // symbol index in original or sorted table\n  // number of codes of each length:\n  int count[MAX_ALLOWED_CODE_LENGTH + 1] = { 0 };\n  // offsets in sorted table for each length:\n  int offset[MAX_ALLOWED_CODE_LENGTH + 1];\n\n  assert(code_lengths_size != 0);\n  assert(code_lengths != NULL);\n  assert((root_table != NULL && sorted != NULL) ||\n         (root_table == NULL && sorted == NULL));\n  assert(root_bits > 0);\n\n  // Build histogram of code lengths.\n  for (symbol = 0; symbol < code_lengths_size; ++symbol) {\n    if (code_lengths[symbol] > MAX_ALLOWED_CODE_LENGTH) {\n      return 0;\n    }\n    ++count[code_lengths[symbol]];\n  }\n\n  // Error, all code lengths are zeros.\n  if (count[0] == code_lengths_size) {\n    return 0;\n  }\n\n  // Generate offsets into sorted symbol table by code length.\n  offset[1] = 0;\n  for (len = 1; len < MAX_ALLOWED_CODE_LENGTH; ++len) {\n    if (count[len] > (1 << len)) {\n      return 0;\n    }\n    offset[len + 1] = offset[len] + count[len];\n  }\n\n  // Sort symbols by length, by symbol order within each length.\n  for (symbol = 0; symbol < code_lengths_size; ++symbol) {\n    const int symbol_code_length = code_lengths[symbol];\n    if (code_lengths[symbol] > 0) {\n      if (sorted != NULL) {\n        sorted[offset[symbol_code_length]++] = symbol;\n      } else {\n        offset[symbol_code_length]++;\n      }\n    }\n  }\n\n  // Special case code with only one value.\n  if (offset[MAX_ALLOWED_CODE_LENGTH] == 1) {\n    if (sorted != NULL) {\n      HuffmanCode code;\n      code.bits = 0;\n      code.value = (uint16_t)sorted[0];\n      ReplicateValue(table, 1, total_size, code);\n    }\n    return total_size;\n  }\n\n  {\n    int step;              // step size to replicate values in current table\n    uint32_t low = 0xffffffffu;        // low bits for current root entry\n    uint32_t mask = total_size - 1;    // mask for low bits\n    uint32_t key = 0;      // reversed prefix code\n    int num_nodes = 1;     // number of Huffman tree nodes\n    int num_open = 1;      // number of open branches in current tree level\n    int table_bits = root_bits;        // key length of current table\n    int table_size = 1 << table_bits;  // size of current table\n    symbol = 0;\n    // Fill in root table.\n    for (len = 1, step = 2; len <= root_bits; ++len, step <<= 1) {\n      num_open <<= 1;\n      num_nodes += num_open;\n      num_open -= count[len];\n      if (num_open < 0) {\n        return 0;\n      }\n      if (root_table == NULL) continue;\n      for (; count[len] > 0; --count[len]) {\n        HuffmanCode code;\n        code.bits = (uint8_t)len;\n        code.value = (uint16_t)sorted[symbol++];\n        ReplicateValue(&table[key], step, table_size, code);\n        key = GetNextKey(key, len);\n      }\n    }\n\n    // Fill in 2nd level tables and add pointers to root table.\n    for (len = root_bits + 1, step = 2; len <= MAX_ALLOWED_CODE_LENGTH;\n         ++len, step <<= 1) {\n      num_open <<= 1;\n      num_nodes += num_open;\n      num_open -= count[len];\n      if (num_open < 0) {\n        return 0;\n      }\n      if (root_table == NULL) continue;\n      for (; count[len] > 0; --count[len]) {\n        HuffmanCode code;\n        if ((key & mask) != low) {\n          table += table_size;\n          table_bits = NextTableBitSize(count, len, root_bits);\n          table_size = 1 << table_bits;\n          total_size += table_size;\n          low = key & mask;\n          root_table[low].bits = (uint8_t)(table_bits + root_bits);\n          root_table[low].value = (uint16_t)((table - root_table) - low);\n        }\n        code.bits = (uint8_t)(len - root_bits);\n        code.value = (uint16_t)sorted[symbol++];\n        ReplicateValue(&table[key >> root_bits], step, table_size, code);\n        key = GetNextKey(key, len);\n      }\n    }\n\n    // Check if tree is full.\n    if (num_nodes != 2 * offset[MAX_ALLOWED_CODE_LENGTH] - 1) {\n      return 0;\n    }\n  }\n\n  return total_size;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get(self, key, default=None, version=None):\n        key = self.make_key(key, version=version)\n        val = self._cache.get(key)\n        if val is None:\n            return default\n        return val\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def load(self):\n\n        if len(self.tile) != 1 or self.tile[0][0] != \"iptc\":\n            return ImageFile.ImageFile.load(self)\n\n        type, tile, box = self.tile[0]\n\n        encoding, offset = tile\n\n        self.fp.seek(offset)\n\n        # Copy image data to temporary file\n        outfile = tempfile.mktemp()\n        o = open(outfile, \"wb\")\n        if encoding == \"raw\":\n            # To simplify access to the extracted file,\n            # prepend a PPM header\n            o.write(\"P5\\n%d %d\\n255\\n\" % self.size)\n        while True:\n            type, size = self.field()\n            if type != (8, 10):\n                break\n            while size > 0:\n                s = self.fp.read(min(size, 8192))\n                if not s:\n                    break\n                o.write(s)\n                size = size - len(s)\n        o.close()\n\n        try:\n            try:\n                # fast\n                self.im = Image.core.open_ppm(outfile)\n            except:\n                # slightly slower\n                im = Image.open(outfile)\n                im.load()\n                self.im = im.im\n        finally:\n            try: os.unlink(outfile)\n            except: pass\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& indices = context->input(0);\n    const Tensor& values = context->input(1);\n    const Tensor& shape = context->input(2);\n    const Tensor& weights = context->input(3);\n    bool use_weights = weights.NumElements() > 0;\n\n    bool is_1d = shape.NumElements() == 1;\n    int num_batches = is_1d ? 1 : shape.flat<int64>()(0);\n    int num_values = values.NumElements();\n\n    const auto indices_values = indices.matrix<int64>();\n    const auto values_values = values.flat<T>();\n    const auto weight_values = weights.flat<W>();\n\n    auto per_batch_counts = BatchedMap<W>(num_batches);\n\n    T max_value = 0;\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      int batch = is_1d ? 0 : indices_values(idx, 0);\n      const auto& value = values_values(idx);\n      if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {\n        if (binary_output_) {\n          per_batch_counts[batch][value] = 1;\n        } else if (use_weights) {\n          per_batch_counts[batch][value] += weight_values(idx);\n        } else {\n          per_batch_counts[batch][value]++;\n        }\n        if (value > max_value) {\n          max_value = value;\n        }\n      }\n    }\n\n    int num_output_values = GetOutputSize(max_value, maxlength_, minlength_);\n    OP_REQUIRES_OK(context, OutputSparse<W>(per_batch_counts, num_output_values,\n                                            is_1d, context));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "pyi_get_temp_path(char *buffer, char *runtime_tmpdir)\n{\n    int i;\n    wchar_t *wchar_ret;\n    wchar_t prefix[16];\n    wchar_t wchar_buffer[PATH_MAX];\n    char *original_tmpdir;\n    char runtime_tmpdir_abspath[PATH_MAX + 1];\n\n    if (runtime_tmpdir != NULL) {\n      /*\n       * Get original TMP environment variable so it can be restored\n       * after this is done.\n       */\n      original_tmpdir = pyi_getenv(\"TMP\");\n      /*\n       * Set TMP to runtime_tmpdir for _wtempnam() later\n       */\n      pyi_path_fullpath(runtime_tmpdir_abspath, PATH_MAX, runtime_tmpdir);\n      pyi_setenv(\"TMP\", runtime_tmpdir_abspath);\n    }\n\n    GetTempPathW(PATH_MAX, wchar_buffer);\n\n    swprintf(prefix, 16, L\"_MEI%d\", getpid());\n\n    /*\n     * Windows does not have a race-free function to create a temporary\n     * directory. Thus, we rely on _tempnam, and simply try several times\n     * to avoid stupid race conditions.\n     */\n    for (i = 0; i < 5; i++) {\n        /* TODO use race-free fuction - if any exists? */\n        wchar_ret = _wtempnam(wchar_buffer, prefix);\n\n        if (_wmkdir(wchar_ret) == 0) {\n            pyi_win32_utils_to_utf8(buffer, wchar_ret, PATH_MAX);\n            free(wchar_ret);\n            if (runtime_tmpdir != NULL) {\n              /*\n               * Restore TMP to what it was\n               */\n              if (original_tmpdir != NULL) {\n                pyi_setenv(\"TMP\", original_tmpdir);\n                free(original_tmpdir);\n              } else {\n                pyi_unsetenv(\"TMP\");\n              }\n            }\n            return 1;\n        }\n        free(wchar_ret);\n    }\n    if (runtime_tmpdir != NULL) {\n      /*\n       * Restore TMP to what it was\n       */\n      if (original_tmpdir != NULL) {\n        pyi_setenv(\"TMP\", original_tmpdir);\n        free(original_tmpdir);\n      } else {\n        pyi_unsetenv(\"TMP\");\n      }\n    }\n    return 0;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n  int num_inputs = node->inputs->size;\n  // The number of outputs should be the same as number of inputs.\n  TF_LITE_ENSURE_EQ(context, node->outputs->size, num_inputs);\n\n  // Check subgraph indices and get subgraphs.\n  Subgraph* this_subgraph = reinterpret_cast<Subgraph*>(context->impl_);\n  auto* subgraphs = this_subgraph->GetSubgraphs();\n  TF_LITE_ENSURE(context, op_data->cond_subgraph_index < subgraphs->size());\n  TF_LITE_ENSURE(context, op_data->body_subgraph_index < subgraphs->size());\n\n  Subgraph* cond_subgraph = (*subgraphs)[op_data->cond_subgraph_index].get();\n  Subgraph* body_subgraph = (*subgraphs)[op_data->body_subgraph_index].get();\n\n  // Check input & output count of the condition subgraph.\n  TF_LITE_ENSURE_EQ(context, cond_subgraph->inputs().size(), num_inputs);\n  TF_LITE_ENSURE_EQ(context, cond_subgraph->outputs().size(), 1);\n\n  // Check input & output count of the body subgraph.\n  TF_LITE_ENSURE_EQ(context, body_subgraph->inputs().size(), num_inputs);\n  TF_LITE_ENSURE_EQ(context, body_subgraph->outputs().size(), num_inputs);\n\n  // Prepare and check the condition subgraph.\n  TF_LITE_ENSURE_OK(\n      context, CopyTensorsShapeAndType(\n                   context, this_subgraph, TfLiteIntArrayView(node->inputs),\n                   cond_subgraph, cond_subgraph->inputs(), true));\n  TF_LITE_ENSURE_OK(context, cond_subgraph->AllocateTensors());\n  TfLiteTensor* cond_output =\n      cond_subgraph->tensor(cond_subgraph->outputs()[0]);\n  // This should rarely happens. In most cases the output is static with shape\n  // [1]. However theoretically intermediate tensors in the cond subgraph\n  // can be dynamic.\n  if (IsDynamicTensor(cond_output)) {\n    op_data->cond_has_dynamic_output_tensors = true;\n  } else {\n    TF_LITE_ENSURE_STATUS(CheckCondOutput(context, cond_output));\n  }\n\n  // Prepare and check the body subgraph.\n  TF_LITE_ENSURE_OK(\n      context, CopyTensorsShapeAndType(\n                   context, this_subgraph, TfLiteIntArrayView(node->inputs),\n                   body_subgraph, body_subgraph->inputs(), true));\n  TF_LITE_ENSURE_OK(context, body_subgraph->AllocateTensors());\n  if (body_subgraph->HasDynamicTensors()) {\n    op_data->body_has_dynamic_output_tensors = true;\n  } else {\n    for (int i = 0; i < num_inputs; ++i) {\n      TfLiteTensor* body_input =\n          body_subgraph->tensor(body_subgraph->inputs()[i]);\n      TfLiteTensor* body_output =\n          body_subgraph->tensor(body_subgraph->outputs()[i]);\n      TF_LITE_ENSURE_TYPES_EQ(context, body_input->type, body_output->type);\n\n      TF_LITE_ENSURE(context, !IsDynamicTensor(body_output));\n      if (!TfLiteIntArrayEqual(body_input->dims, body_output->dims)) {\n        // If the output shape of the body subgraph is static w.r.t. a fixed\n        // input size, but it's different from input size, it's still considered\n        // dynamic. For example: If a subgraph keeps padding its input with a\n        // fixed padding, the output shape is static w.r.t the input shape and\n        // padding, but running it in a loop will keep bloating the tensor.\n        op_data->body_has_dynamic_output_tensors = true;\n        break;\n      }\n    }\n  }\n  for (int i = 0; i < num_inputs; ++i) {\n    TfLiteTensor* output;\n    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, i, &output));\n    if (op_data->body_has_dynamic_output_tensors) {\n      SetTensorToDynamic(output);\n    } else {\n      TfLiteTensor* body_output =\n          body_subgraph->tensor(body_subgraph->outputs()[i]);\n      TfLiteIntArray* output_size = TfLiteIntArrayCopy(body_output->dims);\n      TF_LITE_ENSURE_OK(context,\n                        context->ResizeTensor(context, output, output_size));\n    }\n  }\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def save(self) -> FlaskResponse:\n        data = request.form.get(\"data\")\n        if not isinstance(data, str):\n            return json_error_response(_(\"Request missing data field.\"), status=500)\n\n        datasource_dict = json.loads(data)\n        datasource_id = datasource_dict.get(\"id\")\n        datasource_type = datasource_dict.get(\"type\")\n        database_id = datasource_dict[\"database\"].get(\"id\")\n        orm_datasource = ConnectorRegistry.get_datasource(\n            datasource_type, datasource_id, db.session\n        )\n        orm_datasource.database_id = database_id\n\n        if \"owners\" in datasource_dict and orm_datasource.owner_class is not None:\n            # Check ownership\n            if app.config.get(\"OLD_API_CHECK_DATASET_OWNERSHIP\"):\n                # mimic the behavior of the new dataset command that\n                # checks ownership and ensures that non-admins aren't locked out\n                # of the object\n                try:\n                    check_ownership(orm_datasource)\n                except SupersetSecurityException as ex:\n                    raise DatasetForbiddenError() from ex\n                user = security_manager.get_user_by_id(g.user.id)\n                datasource_dict[\"owners\"] = populate_owners(\n                    user, datasource_dict[\"owners\"], default_to_user=False\n                )\n            else:\n                # legacy behavior\n                datasource_dict[\"owners\"] = (\n                    db.session.query(orm_datasource.owner_class)\n                    .filter(\n                        orm_datasource.owner_class.id.in_(\n                            datasource_dict[\"owners\"] or []\n                        )\n                    )\n                    .all()\n                )\n\n        duplicates = [\n            name\n            for name, count in Counter(\n                [col[\"column_name\"] for col in datasource_dict[\"columns\"]]\n            ).items()\n            if count > 1\n        ]\n        if duplicates:\n            return json_error_response(\n                _(\n                    \"Duplicate column name(s): %(columns)s\",\n                    columns=\",\".join(duplicates),\n                ),\n                status=409,\n            )\n        orm_datasource.update_from_object(datasource_dict)\n        data = orm_datasource.data\n        db.session.commit()\n\n        return self.json_response(sanitize_datasource_data(data))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& a = context->input(0);\n    const Tensor& b = context->input(1);\n    const float min_a = context->input(2).flat<float>()(0);\n    const float max_a = context->input(3).flat<float>()(0);\n    const float min_b = context->input(4).flat<float>()(0);\n    const float max_b = context->input(5).flat<float>()(0);\n\n    // Make sure that we have valid quantization ranges for the input buffers.\n    // If the difference between the min and max is negative or zero, it makes\n    // it hard to do meaningful intermediate operations on the values.\n    OP_REQUIRES(context, (max_a > min_a),\n                errors::InvalidArgument(\"max_a must be larger than min_a.\"));\n    OP_REQUIRES(context, (max_b > min_b),\n                errors::InvalidArgument(\"max_b must be larger than min_b.\"));\n    const int32_t offset_a = FloatToQuantizedUnclamped<T1>(0.0f, min_a, max_a);\n    const int32_t offset_b = FloatToQuantizedUnclamped<T2>(0.0f, min_b, max_b);\n    const int32_t offset_c = 0;\n    const int32_t mult_c = 1;\n    const int32_t shift_c = 0;\n\n    // Check that the dimensions of the two matrices are valid.\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(a.shape()),\n                errors::InvalidArgument(\"In[0] is not a matrix\"));\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(b.shape()),\n                errors::InvalidArgument(\"In[1] is not a matrix\"));\n    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n    dim_pair[0].first = transpose_a_ ? 0 : 1;\n    dim_pair[0].second = transpose_b_ ? 1 : 0;\n\n    OP_REQUIRES(context,\n                a.dim_size(dim_pair[0].first) == b.dim_size(dim_pair[0].second),\n                errors::InvalidArgument(\"Matrix size-incompatible: In[0]: \",\n                                        a.shape().DebugString(),\n                                        \", In[1]: \", b.shape().DebugString()));\n\n    OP_REQUIRES(context, ((shift_c >= 0) && (shift_c <= 31)),\n                errors::InvalidArgument(\"shift_c must be between 0 and 31, \"\n                                        \"inclusive.\"));\n\n    int a_dim_remaining = 1 - dim_pair[0].first;\n    int b_dim_remaining = 1 - dim_pair[0].second;\n    TensorShape out_shape(\n        {a.dim_size(a_dim_remaining), b.dim_size(b_dim_remaining)});\n    Tensor* c = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &c));\n    CHECK(c);\n\n    const T1* a_data = a.flat<T1>().data();\n    const T2* b_data = b.flat<T2>().data();\n    Toutput* c_data = c->flat<Toutput>().data();\n\n    const bool transpose_c = false;\n    const size_t m = a.dim_size(a_dim_remaining);\n    const size_t n = b.dim_size(b_dim_remaining);\n    const size_t k = a.dim_size(dim_pair[0].first);\n    const size_t lda = a.dim_size(1);\n    const size_t ldb = b.dim_size(1);\n    const size_t ldc = n;\n\n    if (meta::IsSupportedAndEnabled() && std::is_same<T1, quint8>() &&\n        std::is_same<T2, quint8>() && std::is_same<Toutput, qint32>() &&\n        (offset_c == 0) && (mult_c == 1) && (shift_c == 0) &&\n        (transpose_c == false) && (k <= 2048)) {\n      // Gemmlowp/meta code path works on 32 & 64 bit Arm with NEON Simd and\n      // allows optimized quantized 8bit to 32bit gemm.\n      meta::QuantizedGemm(context, transpose_a_, transpose_b_, a_data, b_data,\n                          c_data, m, n, k, -offset_a, -offset_b, lda, ldb, ldc);\n    } else if (std::is_same<T1, quint8>() && std::is_same<T2, quint8>() &&\n               std::is_same<Toutput, qint32>() && (offset_c == 0) &&\n               (mult_c == 1) && (shift_c == 0) && (transpose_c == false)) {\n      // The gemmlowp optimized library only works for a particular set of data\n      // types, so check if we meet those requirements and fall back to a slower\n      // reference implementation if not.\n      if (transpose_a_) {\n        if (transpose_b_) {\n          GemmlowpMultiply<true, true, false>(context, a_data, b_data, c_data,\n                                              m, n, k, offset_a, offset_b, lda,\n                                              ldb, ldc);\n        } else {\n          GemmlowpMultiply<true, false, false>(context, a_data, b_data, c_data,\n                                               m, n, k, offset_a, offset_b, lda,\n                                               ldb, ldc);\n        }\n      } else {\n        if (transpose_b_) {\n          GemmlowpMultiply<false, true, false>(context, a_data, b_data, c_data,\n                                               m, n, k, offset_a, offset_b, lda,\n                                               ldb, ldc);\n        } else {\n          GemmlowpMultiply<false, false, false>(context, a_data, b_data, c_data,\n                                                m, n, k, offset_a, offset_b,\n                                                lda, ldb, ldc);\n        }\n      }\n    } else {\n      ReferenceGemm<T1, T2, Toutput>(\n          transpose_a_, transpose_b_, transpose_c, m, n, k, a_data, offset_a,\n          lda, b_data, offset_b, ldb, c_data, shift_c, offset_c, mult_c, ldc);\n    }\n\n    float min_c_value;\n    float max_c_value;\n    QuantizationRangeForMultiplication<T1, T2, Toutput>(\n        min_a, max_a, min_b, max_b, &min_c_value, &max_c_value);\n    Tensor* c_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &c_min));\n    c_min->flat<float>()(0) = min_c_value;\n\n    Tensor* c_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &c_max));\n    c_max->flat<float>()(0) = max_c_value;\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def run_command(cmd, log_output=False):\n    \"\"\"\n    Return (exitcode, output) of executing the provided `cmd` in a shell.\n    `cmd` can be provided as a string or as a list of arguments.\n\n    If `log_output` is True, the stdout and stderr of the process will be captured\n    and streamed to the `logger`.\n    \"\"\"\n    if isinstance(cmd, list):\n        cmd = \" \".join(cmd)\n\n    if not log_output:\n        exitcode, output = subprocess.getstatusoutput(cmd)\n        return exitcode, output\n\n    process = subprocess.Popen(\n        cmd,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        universal_newlines=True,\n    )\n\n    while _stream_process(process):\n        sleep(1)\n\n    exitcode = process.poll()\n    return exitcode, \"\"\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, sydent):\n        self.sydent = sydent\n\n        root = Resource()\n        matrix = Resource()\n        identity = Resource()\n        api = Resource()\n        v1 = self.sydent.servlets.v1\n        v2 = self.sydent.servlets.v2\n\n        validate = Resource()\n        validate_v2 = Resource()\n        email = Resource()\n        email_v2 = Resource()\n        msisdn = Resource()\n        msisdn_v2 = Resource()\n\n        threepid_v1 = Resource()\n        threepid_v2 = Resource()\n        unbind = self.sydent.servlets.threepidUnbind\n\n        pubkey = Resource()\n        ephemeralPubkey = Resource()\n\n        root.putChild(b'_matrix', matrix)\n        matrix.putChild(b'identity', identity)\n        identity.putChild(b'api', api)\n        identity.putChild(b'v2', v2)\n        api.putChild(b'v1', v1)\n\n        validate.putChild(b'email', email)\n        validate.putChild(b'msisdn', msisdn)\n\n        validate_v2.putChild(b'email', email_v2)\n        validate_v2.putChild(b'msisdn', msisdn_v2)\n\n        v1.putChild(b'validate', validate)\n\n        v1.putChild(b'lookup', self.sydent.servlets.lookup)\n        v1.putChild(b'bulk_lookup', self.sydent.servlets.bulk_lookup)\n\n        v1.putChild(b'pubkey', pubkey)\n        pubkey.putChild(b'isvalid', self.sydent.servlets.pubkeyIsValid)\n        pubkey.putChild(b'ed25519:0', self.sydent.servlets.pubkey_ed25519)\n        pubkey.putChild(b'ephemeral', ephemeralPubkey)\n        ephemeralPubkey.putChild(b'isvalid', self.sydent.servlets.ephemeralPubkeyIsValid)\n\n        threepid_v2.putChild(b'getValidated3pid', self.sydent.servlets.getValidated3pidV2)\n        threepid_v2.putChild(b'bind', self.sydent.servlets.threepidBindV2)\n        threepid_v2.putChild(b'unbind', unbind)\n\n        threepid_v1.putChild(b'getValidated3pid', self.sydent.servlets.getValidated3pid)\n        threepid_v1.putChild(b'unbind', unbind)\n        if self.sydent.enable_v1_associations:\n            threepid_v1.putChild(b'bind', self.sydent.servlets.threepidBind)\n\n        v1.putChild(b'3pid', threepid_v1)\n\n        email.putChild(b'requestToken', self.sydent.servlets.emailRequestCode)\n        email.putChild(b'submitToken', self.sydent.servlets.emailValidate)\n\n        email_v2.putChild(b'requestToken', self.sydent.servlets.emailRequestCodeV2)\n        email_v2.putChild(b'submitToken', self.sydent.servlets.emailValidateV2)\n\n        msisdn.putChild(b'requestToken', self.sydent.servlets.msisdnRequestCode)\n        msisdn.putChild(b'submitToken', self.sydent.servlets.msisdnValidate)\n\n        msisdn_v2.putChild(b'requestToken', self.sydent.servlets.msisdnRequestCodeV2)\n        msisdn_v2.putChild(b'submitToken', self.sydent.servlets.msisdnValidateV2)\n\n        v1.putChild(b'store-invite', self.sydent.servlets.storeInviteServlet)\n\n        v1.putChild(b'sign-ed25519', self.sydent.servlets.blindlySignStuffServlet)\n\n        # v2\n        # note v2 loses the /api so goes on 'identity' not 'api'\n        identity.putChild(b'v2', v2)\n\n        # v2 exclusive APIs\n        v2.putChild(b'terms', self.sydent.servlets.termsServlet)\n        account = self.sydent.servlets.accountServlet\n        v2.putChild(b'account', account)\n        account.putChild(b'register', self.sydent.servlets.registerServlet)\n        account.putChild(b'logout', self.sydent.servlets.logoutServlet)\n\n        # v2 versions of existing APIs\n        v2.putChild(b'validate', validate_v2)\n        v2.putChild(b'pubkey', pubkey)\n        v2.putChild(b'3pid', threepid_v2)\n        v2.putChild(b'store-invite', self.sydent.servlets.storeInviteServletV2)\n        v2.putChild(b'sign-ed25519', self.sydent.servlets.blindlySignStuffServletV2)\n        v2.putChild(b'lookup', self.sydent.servlets.lookup_v2)\n        v2.putChild(b'hash_details', self.sydent.servlets.hash_details)\n\n        self.factory = Site(root)\n        self.factory.displayTracebacks = False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def logout(self, message, relaystate=None, samlresponse=None):\n        \"\"\"\n        Handle HTTP logout. The supported logout methods are stored\n        in each session. First all the SOAP sessions are logged out\n        then the HTTP Redirect method is used for any remaining\n        sessions.\n\n        The basic process is this:\n         1. A logout request is received. It is processed and the response\n            cached.\n         2. If any other SP's have also logged in as this user then the\n            first such session is popped off and a logout request is\n            generated and forwarded to the SP.\n         3. If a logout response is received then the user is marked\n            as logged out from that SP.\n         Repeat steps 2-3 until only the initial logout request is\n         left unhandled, at which time the pre-generated response is sent\n         back to the SP that originated the logout request.\n\n        The final logout response is always a redirect.\n        \"\"\"\n        logout = self.cfg.idp.get_logout_handler()\n\n        us = UserSession()\n\n        saml_sessions = self.cfg.idp.sessionfactory\n\n        logout_type = None\n        try:\n            if lasso.SAML2_FIELD_REQUEST in message:\n                logout_type = \"request\"\n                self._handle_logout_request(us, logout, saml_sessions,\n                                            message)\n            elif samlresponse:\n                logout_type = \"response\"\n                self._handle_logout_response(us, logout, saml_sessions,\n                                             message, samlresponse)\n            else:\n                raise cherrypy.HTTPError(400, 'Bad Request. Not a ' +\n                                         'logout request or response.')\n        except InvalidRequest as e:\n            raise cherrypy.HTTPError(400, 'Bad Request. %s' % e)\n        except UnknownProvider as e:\n            raise cherrypy.HTTPError(\n                400, 'Invalid logout %s: %s' % (logout_type, e)\n            )\n\n        # Fall through to handle any remaining sessions.\n\n        # Find the next SP to logout and send a LogoutRequest\n        logout_order = [\n            lasso.SAML2_METADATA_BINDING_SOAP,\n            lasso.SAML2_METADATA_BINDING_REDIRECT,\n        ]\n        (logout_mech, session) = saml_sessions.get_next_logout(\n            logout_mechs=logout_order)\n        while session:\n            self.debug('Going to log out %s' % session.provider_id)\n\n            try:\n                logout.setSessionFromDump(session.login_session)\n            except lasso.ProfileBadSessionDumpError as e:\n                self.error('Failed to load session: %s' % e)\n                raise cherrypy.HTTPRedirect(400, 'Failed to log out user: %s '\n                                            % e)\n            try:\n                if logout_mech == lasso.SAML2_METADATA_BINDING_REDIRECT:\n                    logout.initRequest(session.provider_id,\n                                       lasso.HTTP_METHOD_REDIRECT)\n                else:\n                    logout.initRequest(session.provider_id,\n                                       lasso.HTTP_METHOD_SOAP)\n            except lasso.ServerProviderNotFoundError:\n                self.error(\n                    'Service Provider %s not found. Trying next session' %\n                    session.provider_id\n                )\n                saml_sessions.remove_session(session)\n                (logout_mech, session) = saml_sessions.get_next_logout(\n                    logout_mechs=logout_order)\n                continue\n\n            try:\n                logout.buildRequestMsg()\n            except lasso.Error, e:\n                self.error('failure to build logout request msg: %s' % e)\n                raise cherrypy.HTTPRedirect(400, 'Failed to log out user: %s '\n                                            % e)\n\n            # Set the full list of session indexes for this provider to\n            # log out\n            self.debug('logging out provider id %s' % session.provider_id)\n            indexes = saml_sessions.get_session_id_by_provider_id(\n                session.provider_id\n            )\n            self.debug('Requesting logout for sessions %s' % (indexes,))\n            req = logout.get_request()\n            req.setSessionIndexes(indexes)\n\n            session.set_logoutstate(relaystate=logout.msgUrl,\n                                    request_id=logout.request.id)\n            saml_sessions.start_logout(session, initial=False)\n\n            self.debug('Request logout ID %s for session ID %s' %\n                       (logout.request.id, session.session_id))\n\n            if logout_mech == lasso.SAML2_METADATA_BINDING_REDIRECT:\n                self.debug('Redirecting to another SP to logout on %s at %s' %\n                           (logout.remoteProviderId, logout.msgUrl))\n                raise cherrypy.HTTPRedirect(logout.msgUrl)\n            else:\n                self.debug('SOAP request to another SP to logout on %s at %s' %\n                           (logout.remoteProviderId, logout.msgUrl))\n                if logout.msgBody:\n                    message = self._soap_logout(logout)\n                    try:\n                        self._handle_logout_response(us,\n                                                     logout,\n                                                     saml_sessions,\n                                                     message,\n                                                     samlresponse)\n                    except Exception as e:  # pylint: disable=broad-except\n                        self.error('SOAP SLO failed %s' % e)\n                else:\n                    self.error('Provider does not support SOAP')\n\n            (logout_mech, session) = saml_sessions.get_next_logout(\n                logout_mechs=logout_order)\n\n        # done while\n\n        # All sessions should be logged out now. Respond to the\n        # original request using the response we cached earlier.\n\n        try:\n            session = saml_sessions.get_initial_logout()\n        except ValueError:\n            self.debug('SLO get_last_session() unable to find last session')\n            raise cherrypy.HTTPError(400, 'Unable to determine logout state')\n\n        redirect = session.relaystate\n        if not redirect:\n            redirect = self.basepath\n\n        saml_sessions.remove_session(session)\n\n        # Log out of cherrypy session\n        user = us.get_user()\n        self._audit('Logged out user: %s [%s] from %s' %\n                    (user.name, user.fullname,\n                     session.provider_id))\n        us.logout(user)\n\n        self.debug('SLO redirect to %s' % redirect)\n\n        raise cherrypy.HTTPRedirect(redirect)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def create_class_from_xml_string(target_class, xml_string):\n    \"\"\"Creates an instance of the target class from a string.\n\n    :param target_class: The class which will be instantiated and populated\n        with the contents of the XML. This class must have a c_tag and a\n        c_namespace class variable.\n    :param xml_string: A string which contains valid XML. The root element\n        of the XML string should match the tag and namespace of the desired\n        class.\n\n    :return: An instance of the target class with members assigned according to\n        the contents of the XML - or None if the root XML tag and namespace did\n        not match those of the target class.\n    \"\"\"\n    if not isinstance(xml_string, six.binary_type):\n        xml_string = xml_string.encode('utf-8')\n    tree = ElementTree.fromstring(xml_string)\n    return create_class_from_element_tree(target_class, tree)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  public static CloseableHttpClient createClient(int maxConnections, int timeoutMilliseconds) {\n    return createClient(maxConnections, timeoutMilliseconds, NO_PROXY, NO_STORE, true);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def create_trustee_and_trust(osc, cluster):\n    try:\n        password = utils.generate_password(length=18)\n        trustee = osc.keystone().create_trustee(\n            cluster.uuid,\n            password,\n        )\n        cluster.trustee_username = trustee.name\n        cluster.trustee_user_id = trustee.id\n        cluster.trustee_password = password\n        trust = osc.keystone().create_trust(trustee.id)\n        cluster.trust_id = trust.id\n    except Exception:\n        LOG.exception(\n            _LE('Failed to create trustee and trust for Cluster: %s'),\n            cluster.uuid)\n        raise exception.TrusteeOrTrustToClusterFailed(\n            cluster_uuid=cluster.uuid)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def check_table_permission(cls, user, table, permission):\n        \"\"\"Check if the provided user has the specified permission against the table.\"\"\"\n        # If the table does *not* require permissions\n        if table in cls.RULESET_IGNORE:\n            return True\n\n        # Work out which roles touch the given table\n        for role in cls.RULESET_NAMES:\n            if table in cls.RULESET_MODELS[role]:\n\n                if check_user_role(user, role, permission):\n                    return True\n\n        # Check for children models which inherits from parent role\n        for (parent, child) in cls.RULESET_CHANGE_INHERIT:\n            # Get child model name\n            parent_child_string = f'{parent}_{child}'\n\n            if parent_child_string == table:\n                # Check if parent role has change permission\n                if check_user_role(user, parent, 'change'):\n                    return True\n\n        # Print message instead of throwing an error\n        name = getattr(user, 'name', user.pk)\n\n        logger.info(f\"User '{name}' failed permission check for {table}.{permission}\")\n\n        return False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def on_send_join_request(\n        self, origin: str, content: JsonDict, room_id: str\n    ) -> Dict[str, Any]:\n        logger.debug(\"on_send_join_request: content: %s\", content)\n\n        room_version = await self.store.get_room_version(room_id)\n        pdu = event_from_pdu_json(content, room_version)\n\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_join_request: pdu sigs: %s\", pdu.signatures)\n\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n\n        res_pdus = await self.handler.on_send_join_request(origin, pdu)\n        time_now = self._clock.time_msec()\n        return {\n            \"state\": [p.get_pdu_json(time_now) for p in res_pdus[\"state\"]],\n            \"auth_chain\": [p.get_pdu_json(time_now) for p in res_pdus[\"auth_chain\"]],\n        }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_model_version_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    name = request_dict.get(\"name\")\n    version = request_dict.get(\"version\")\n    path = request_dict[\"path\"]\n    validate_path_is_safe(path)\n    artifact_uri = _get_model_registry_store().get_model_version_download_uri(name, version)\n    if _is_servable_proxied_run_artifact_root(artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_repo = get_artifact_repository(artifact_uri)\n        artifact_path = path\n\n    return _send_artifact(artifact_repo, artifact_path)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) final {\n    const Tensor& superdiag = context->input(0);\n    const Tensor& maindiag = context->input(1);\n    const Tensor& subdiag = context->input(2);\n    const Tensor& rhs = context->input(3);\n\n    const int ndims = rhs.dims();\n    int64 batch_size = 1;\n    for (int i = 0; i < ndims - 2; i++) {\n      batch_size *= rhs.dim_size(i);\n    }\n    const int m = rhs.dim_size(ndims - 2);\n    const int n = rhs.dim_size(ndims - 1);\n\n    // Allocate output.\n    Tensor* output;\n    OP_REQUIRES_OK(context, context->allocate_output(0, rhs.shape(), &output));\n\n    const Eigen::GpuDevice& device = context->eigen_device<Eigen::GpuDevice>();\n    GpuLaunchConfig cfg = GetGpuLaunchConfig(1, device);\n    TF_CHECK_OK(GpuLaunchKernel(\n        TridiagonalMatMulKernel<Scalar>, cfg.block_count, cfg.thread_per_block,\n        0, device.stream(), batch_size, m, n, superdiag.flat<Scalar>().data(),\n        maindiag.flat<Scalar>().data(), subdiag.flat<Scalar>().data(),\n        rhs.flat<Scalar>().data(), output->flat<Scalar>().data()));\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create_service(self, context, OS_KSADM_service):\n        service_id = uuid.uuid4().hex\n        service_ref = OS_KSADM_service.copy()\n        service_ref['id'] = service_id\n        new_service_ref = self.catalog_api.create_service(\n                context, service_id, service_ref)\n        return {'OS-KSADM:service': new_service_ref}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void operator()(const CPUDevice& d, typename TTypes<T, 4>::ConstTensor input,\n                  typename TTypes<T, 3>::ConstTensor filter,\n                  typename TTypes<T, 4>::ConstTensor out_backprop,\n                  int stride_rows, int stride_cols, int rate_rows,\n                  int rate_cols, int pad_top, int pad_left,\n                  typename TTypes<T, 3>::Tensor filter_backprop) {\n    const int batch = input.dimension(0);\n    const int input_rows = input.dimension(1);\n    const int input_cols = input.dimension(2);\n    const int depth = input.dimension(3);\n\n    const int filter_rows = filter.dimension(0);\n    const int filter_cols = filter.dimension(1);\n\n    const int output_rows = out_backprop.dimension(1);\n    const int output_cols = out_backprop.dimension(2);\n\n    // Initialize gradient with all zeros.\n    filter_backprop.setZero();\n\n    // This is a reference implementation, likely to be slow.\n    // TODO(gpapan): Write multi-threaded implementation.\n    // In the case of multiple argmax branches, we only back-propagate along the\n    // last branch, i.e., the one with largest value of `h * filter_cols + w`,\n    // similarly to the max-pooling backward routines.\n    for (int b = 0; b < batch; ++b) {\n      for (int h_out = 0; h_out < output_rows; ++h_out) {\n        int h_beg = h_out * stride_rows - pad_top;\n        for (int w_out = 0; w_out < output_cols; ++w_out) {\n          int w_beg = w_out * stride_cols - pad_left;\n          for (int d = 0; d < depth; ++d) {\n            T cur_val = Eigen::NumTraits<T>::lowest();\n            int h_max = 0;\n            int w_max = 0;\n            for (int h = 0; h < filter_rows; ++h) {\n              const int h_in = h_beg + h * rate_rows;\n              if (h_in >= 0 && h_in < input_rows) {\n                for (int w = 0; w < filter_cols; ++w) {\n                  const int w_in = w_beg + w * rate_cols;\n                  if (w_in >= 0 && w_in < input_cols) {\n                    const T val = input(b, h_in, w_in, d) + filter(h, w, d);\n                    if (val > cur_val) {\n                      cur_val = val;\n                      h_max = h;\n                      w_max = w;\n                    }\n                  }\n                }\n              }\n            }\n            filter_backprop(h_max, w_max, d) +=\n                out_backprop(b, h_out, w_out, d);\n          }\n        }\n      }\n    }\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def on_PUT(self, origin, content, query, room_id, event_id):\n        content = await self.handler.on_send_leave_request(origin, content, room_id)\n        return 200, (200, content)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _abort_multipart_upload_artifact(artifact_path):\n    \"\"\"\n    A request handler for `POST /mlflow-artifacts/mpu/abort` to abort a multipart upload\n    to `artifact_path` (a relative path from the root artifact directory).\n    \"\"\"\n    validate_path_is_safe(artifact_path)\n\n    request_message = _get_request_message(\n        AbortMultipartUpload(),\n        schema={\n            \"path\": [_assert_required, _assert_string],\n            \"upload_id\": [_assert_string],\n        },\n    )\n    path = request_message.path\n    upload_id = request_message.upload_id\n\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    _validate_support_multipart_upload(artifact_repo)\n\n    artifact_repo.abort_multipart_upload(\n        path,\n        upload_id,\n        artifact_path,\n    )\n    return _wrap_response(AbortMultipartUpload.Response())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self):\r\n        field_infos = [\r\n            FieldInfo.factory(key)\r\n            for key in list(request.form.keys()) + list(request.files.keys())\r\n        ]\r\n        # important to sort them so they will be in expected order on command line\r\n        self.field_infos = list(sorted(field_infos))\r\n\r"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status CalculateOutputIndex(OpKernelContext* context, int dimension,\n                              const vector<INDEX_TYPE>& parent_output_index,\n                              INDEX_TYPE output_index_multiplier,\n                              INDEX_TYPE output_size,\n                              vector<INDEX_TYPE>* result) {\n    const RowPartitionTensor row_partition_tensor =\n        GetRowPartitionTensor(context, dimension);\n    auto partition_type = GetRowPartitionTypeByDimension(dimension);\n    switch (partition_type) {\n      case RowPartitionType::VALUE_ROWIDS:\n        CalculateOutputIndexValueRowID(\n            row_partition_tensor, parent_output_index, output_index_multiplier,\n            output_size, result);\n        return tensorflow::Status::OK();\n      case RowPartitionType::ROW_SPLITS:\n        CalculateOutputIndexRowSplit(row_partition_tensor, parent_output_index,\n                                     output_index_multiplier, output_size,\n                                     result);\n        return tensorflow::Status::OK();\n      default:\n        return errors::InvalidArgument(\n            \"Unsupported partition type:\",\n            RowPartitionTypeToString(partition_type));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def save_to_file(self, save_file=None, so_path=None):\n        '''\n        Write the configuration in json format to a file.\n        Save file defaults to '/etc/target/saveconfig.json'.\n        '''\n        if not save_file:\n            save_file = default_save_file\n\n        if so_path:\n            saveconf = self._get_saveconf(so_path, save_file)\n        else:\n            saveconf = self.dump()\n\n        tmp_file = save_file + \".temp\"\n\n        mode = stat.S_IRUSR | stat.S_IWUSR  # 0o600\n        umask = 0o777 ^ mode  # Prevents always downgrading umask to 0\n\n        # For security, remove file with potentially elevated mode\n        try:\n            os.remove(tmp_file)\n        except OSError:\n            pass\n\n        umask_original = os.umask(umask)\n        # Even though the old file is first deleted, a race condition is still\n        # possible. Including os.O_EXCL with os.O_CREAT in the flags will\n        # prevent the file from being created if it exists due to a race\n        try:\n            fdesc = os.open(tmp_file, os.O_WRONLY | os.O_CREAT | os.O_EXCL, mode)\n        finally:\n            os.umask(umask_original)\n\n        with os.fdopen(fdesc, 'w+') as f:\n            f.write(json.dumps(saveconf, sort_keys=True, indent=2))\n            f.write(\"\\n\")\n            f.flush()\n            os.fsync(f.fileno())\n            f.close()\n\n        shutil.copyfile(tmp_file, save_file)\n        os.remove(tmp_file)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def gather_tree(ids, parents):\n    r\"\"\"\n    To be used after beam search. After beam search, we get selected ids at\n    each time step and the corresponding parents in the search tree. Both ids\n    and parents have the layout :attr:`[max_time, batch_size, beam_size]`. Then\n    :attr:`gather_tree` is used to backtrace from the last time step and\n    generate the full sequences by collecting selected ids.\n\n    Here is an example:\n\n    .. code-block:: text\n\n            Given:\n                ids = [[[2 2]\n                        [6 1]]\n                       [[3 9]\n                        [6 1]]\n                       [[0 1]\n                        [9 0]]]\n                parents = [[[0 0]\n                            [1 1]]\n                           [[1 0]\n                            [1 0]]\n                           [[0 0]\n                            [0 1]]]\n\n            Then:\n                gather_tree(ids, parents)\n                         = [[[2 2]\n                             [1 6]]\n                            [[3 3]\n                             [6 1]]\n                            [[0 1]\n                             [9 0]]]\n\n    Args:\n        ids(Tensor): A Tensor with shape :attr:`[length, batch_size, beam_size]`\n            and data type :attr:`int32` or :attr:`int64`. It contains the selected\n            ids of all time steps.\n        parents(Tensor): A Tensor with the same shape and data type as :attr:`ids`,\n            It contains the parents corresponding to selected ids when searching\n            among beams.\n\n    Returns:\n            A Tensor with the same shape and data type as :attr:`ids`. \\\n            It contains the full sequences. The sequences are collected from \\\n            :attr:`ids` by backtracing according to :attr:`parents`.\n\n    Examples:\n        .. code-block:: python\n\n            import paddle\n\n            ids = paddle.to_tensor([[[2, 2], [6, 1]], [[3, 9], [6, 1]], [[0, 1], [9, 0]]])\n\n            parents = paddle.to_tensor([[[0, 0], [1, 1]], [[1, 0], [1, 0]], [[0, 0], [0, 1]]])\n\n            final_sequences = paddle.nn.functional.gather_tree(ids, parents)\n            # [[[2, 2], [1, 6]], [[3, 3], [6, 1]], [[0, 1], [9, 0]]]\n\n    \"\"\"\n    if in_dygraph_mode():\n        return _C_ops.gather_tree(ids, parents)\n    else:\n        if _in_legacy_dygraph():\n            return _legacy_C_ops.gather_tree(ids, parents)\n        else:\n            helper = LayerHelper('gather_tree', **locals())\n            check_variable_and_dtype(ids, 'ids', ['int32', 'int64'],\n                                     'gather_tree')\n            check_variable_and_dtype(parents, 'parents', ['int32', 'int64'],\n                                     'gather_tree')\n            out = helper.create_variable_for_type_inference(dtype=ids.dtype)\n\n            helper.append_op(type=\"gather_tree\",\n                             inputs={\n                                 \"Ids\": ids,\n                                 \"Parents\": parents\n                             },\n                             outputs={\"Out\": out})\n\n            return out\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def safe_eval(expr, locals=None, include_exceptions=False):\n    '''\n    This is intended for allowing things like:\n    with_items: a_list_variable\n\n    Where Jinja2 would return a string but we do not want to allow it to\n    call functions (outside of Jinja2, where the env is constrained).\n\n    Based on:\n    http://stackoverflow.com/questions/12523516/using-ast-and-whitelists-to-make-pythons-eval-safe\n    '''\n    locals = {} if locals is None else locals\n\n    # define certain JSON types\n    # eg. JSON booleans are unknown to python eval()\n    JSON_TYPES = {\n        'false': False,\n        'null': None,\n        'true': True,\n    }\n\n    # this is the whitelist of AST nodes we are going to\n    # allow in the evaluation. Any node type other than\n    # those listed here will raise an exception in our custom\n    # visitor class defined below.\n    SAFE_NODES = set(\n        (\n            ast.Add,\n            ast.BinOp,\n            # ast.Call,\n            ast.Compare,\n            ast.Dict,\n            ast.Div,\n            ast.Expression,\n            ast.List,\n            ast.Load,\n            ast.Mult,\n            ast.Num,\n            ast.Name,\n            ast.Str,\n            ast.Sub,\n            ast.USub,\n            ast.Tuple,\n            ast.UnaryOp,\n        )\n    )\n\n    # AST node types were expanded after 2.6\n    if sys.version_info[:2] >= (2, 7):\n        SAFE_NODES.update(\n            set(\n                (ast.Set,)\n            )\n        )\n\n    # And in Python 3.4 too\n    if sys.version_info[:2] >= (3, 4):\n        SAFE_NODES.update(\n            set(\n                (ast.NameConstant,)\n            )\n        )\n\n    filter_list = []\n    for filter_ in filter_loader.all():\n        filter_list.extend(filter_.filters().keys())\n\n    test_list = []\n    for test in test_loader.all():\n        test_list.extend(test.tests().keys())\n\n    CALL_WHITELIST = C.DEFAULT_CALLABLE_WHITELIST + filter_list + test_list\n\n    class CleansingNodeVisitor(ast.NodeVisitor):\n        def generic_visit(self, node, inside_call=False):\n            if type(node) not in SAFE_NODES:\n                raise Exception(\"invalid expression (%s)\" % expr)\n            elif isinstance(node, ast.Call):\n                inside_call = True\n            elif isinstance(node, ast.Name) and inside_call:\n                # Disallow calls to builtin functions that we have not vetted\n                # as safe.  Other functions are excluded by setting locals in\n                # the call to eval() later on\n                if hasattr(builtins, node.id) and node.id not in CALL_WHITELIST:\n                    raise Exception(\"invalid function: %s\" % node.id)\n            # iterate over all child nodes\n            for child_node in ast.iter_child_nodes(node):\n                self.generic_visit(child_node, inside_call)\n\n    if not isinstance(expr, string_types):\n        # already templated to a datastructure, perhaps?\n        if include_exceptions:\n            return (expr, None)\n        return expr\n\n    cnv = CleansingNodeVisitor()\n    try:\n        parsed_tree = ast.parse(expr, mode='eval')\n        cnv.visit(parsed_tree)\n        compiled = compile(parsed_tree, expr, 'eval')\n        # Note: passing our own globals and locals here constrains what\n        # callables (and other identifiers) are recognized.  this is in\n        # addition to the filtering of builtins done in CleansingNodeVisitor\n        result = eval(compiled, JSON_TYPES, dict(locals))\n\n        if include_exceptions:\n            return (result, None)\n        else:\n            return result\n    except SyntaxError as e:\n        # special handling for syntax errors, we just return\n        # the expression string back as-is to support late evaluation\n        if include_exceptions:\n            return (expr, None)\n        return expr\n    except Exception as e:\n        if include_exceptions:\n            return (expr, e)\n        return expr\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _handle_carbon_sent(self, msg):\n        self.xmpp.event('carbon_sent', msg)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static int generate_key(DH *dh)\n{\n    int ok = 0;\n    int generate_new_key = 0;\n#ifndef FIPS_MODULE\n    unsigned l;\n#endif\n    BN_CTX *ctx = NULL;\n    BIGNUM *pub_key = NULL, *priv_key = NULL;\n\n    if (BN_num_bits(dh->params.p) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n        return 0;\n    }\n\n    if (BN_num_bits(dh->params.p) < DH_MIN_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_SMALL);\n        return 0;\n    }\n\n    ctx = BN_CTX_new_ex(dh->libctx);\n    if (ctx == NULL)\n        goto err;\n\n    if (dh->priv_key == NULL) {\n        priv_key = BN_secure_new();\n        if (priv_key == NULL)\n            goto err;\n        generate_new_key = 1;\n    } else {\n        priv_key = dh->priv_key;\n    }\n\n    if (dh->pub_key == NULL) {\n        pub_key = BN_new();\n        if (pub_key == NULL)\n            goto err;\n    } else {\n        pub_key = dh->pub_key;\n    }\n    if (generate_new_key) {\n        /* Is it an approved safe prime ?*/\n        if (DH_get_nid(dh) != NID_undef) {\n            int max_strength =\n                    ossl_ifc_ffc_compute_security_bits(BN_num_bits(dh->params.p));\n\n            if (dh->params.q == NULL\n                || dh->length > BN_num_bits(dh->params.q))\n                goto err;\n            /* dh->length = maximum bit length of generated private key */\n            if (!ossl_ffc_generate_private_key(ctx, &dh->params, dh->length,\n                                               max_strength, priv_key))\n                goto err;\n        } else {\n#ifdef FIPS_MODULE\n            if (dh->params.q == NULL)\n                goto err;\n#else\n            if (dh->params.q == NULL) {\n                /* secret exponent length, must satisfy 2^(l-1) <= p */\n                if (dh->length != 0\n                    && dh->length >= BN_num_bits(dh->params.p))\n                    goto err;\n                l = dh->length ? dh->length : BN_num_bits(dh->params.p) - 1;\n                if (!BN_priv_rand_ex(priv_key, l, BN_RAND_TOP_ONE,\n                                     BN_RAND_BOTTOM_ANY, 0, ctx))\n                    goto err;\n                /*\n                 * We handle just one known case where g is a quadratic non-residue:\n                 * for g = 2: p % 8 == 3\n                 */\n                if (BN_is_word(dh->params.g, DH_GENERATOR_2)\n                    && !BN_is_bit_set(dh->params.p, 2)) {\n                    /* clear bit 0, since it won't be a secret anyway */\n                    if (!BN_clear_bit(priv_key, 0))\n                        goto err;\n                }\n            } else\n#endif\n            {\n                /* Do a partial check for invalid p, q, g */\n                if (!ossl_ffc_params_simple_validate(dh->libctx, &dh->params,\n                                                     FFC_PARAM_TYPE_DH, NULL))\n                    goto err;\n                /*\n                 * For FFC FIPS 186-4 keygen\n                 * security strength s = 112,\n                 * Max Private key size N = len(q)\n                 */\n                if (!ossl_ffc_generate_private_key(ctx, &dh->params,\n                                                   BN_num_bits(dh->params.q),\n                                                   MIN_STRENGTH,\n                                                   priv_key))\n                    goto err;\n            }\n        }\n    }\n\n    if (!ossl_dh_generate_public_key(ctx, dh, priv_key, pub_key))\n        goto err;\n\n    dh->pub_key = pub_key;\n    dh->priv_key = priv_key;\n    dh->dirty_cnt++;\n    ok = 1;\n err:\n    if (ok != 1)\n        ERR_raise(ERR_LIB_DH, ERR_R_BN_LIB);\n\n    if (pub_key != dh->pub_key)\n        BN_free(pub_key);\n    if (priv_key != dh->priv_key)\n        BN_free(priv_key);\n    BN_CTX_free(ctx);\n    return ok;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self):\n        dag_id = request.form.get('dag_id')\n        task_id = request.form.get('task_id')\n        origin = request.form.get('origin')\n        dag = current_app.dag_bag.get_dag(dag_id)\n        task = dag.get_task(task_id)\n\n        execution_date = request.form.get('execution_date')\n        execution_date = timezone.parse(execution_date)\n        ignore_all_deps = request.form.get('ignore_all_deps') == \"true\"\n        ignore_task_deps = request.form.get('ignore_task_deps') == \"true\"\n        ignore_ti_state = request.form.get('ignore_ti_state') == \"true\"\n\n        executor = ExecutorLoader.get_default_executor()\n        valid_celery_config = False\n        valid_kubernetes_config = False\n\n        try:\n            from airflow.executors.celery_executor import CeleryExecutor\n            valid_celery_config = isinstance(executor, CeleryExecutor)\n        except ImportError:\n            pass\n\n        try:\n            from airflow.executors.kubernetes_executor import KubernetesExecutor\n            valid_kubernetes_config = isinstance(executor, KubernetesExecutor)\n        except ImportError:\n            pass\n\n        if not valid_celery_config and not valid_kubernetes_config:\n            flash(\"Only works with the Celery or Kubernetes executors, sorry\", \"error\")\n            return redirect(origin)\n\n        ti = models.TaskInstance(task=task, execution_date=execution_date)\n        ti.refresh_from_db()\n\n        # Make sure the task instance can be run\n        dep_context = DepContext(\n            deps=RUNNING_DEPS,\n            ignore_all_deps=ignore_all_deps,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state)\n        failed_deps = list(ti.get_failed_dep_statuses(dep_context=dep_context))\n        if failed_deps:\n            failed_deps_str = \", \".join(\n                [\"{}: {}\".format(dep.dep_name, dep.reason) for dep in failed_deps])\n            flash(\"Could not queue task instance for execution, dependencies not met: \"\n                  \"{}\".format(failed_deps_str),\n                  \"error\")\n            return redirect(origin)\n\n        executor.start()\n        executor.queue_task_instance(\n            ti,\n            ignore_all_deps=ignore_all_deps,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state)\n        executor.heartbeat()\n        flash(\n            \"Sent {} to the message queue, \"\n            \"it should start any moment now.\".format(ti))\n        return redirect(origin)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _set_name(self, name):\n        # Sanitize the file name so that it can't be dangerous.\n        if name is not None:\n            # Just use the basename of the file -- anything else is dangerous.\n            name = os.path.basename(name)\n\n            # File names longer than 255 characters can cause problems on older OSes.\n            if len(name) > 255:\n                name, ext = os.path.splitext(name)\n                ext = ext[:255]\n                name = name[:255 - len(ext)] + ext\n\n        self._name = name\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& indices = ctx->input(0);\n    const auto values = ctx->input(1).flat<Tidx>();\n    const Tensor& dense_shape = ctx->input(2);\n    const Tensor& size_t = ctx->input(3);\n    const auto weights = ctx->input(4).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    bool is_1d = dense_shape.NumElements() == 1;\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (is_1d) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(ctx,\n                       functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                           ctx, values, weights, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, values, weights, out, size));\n      }\n    } else {\n      const auto shape = dense_shape.flat<int64_t>();\n      const int64_t num_rows = shape(0);\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      const auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      const auto indices_mat = indices.matrix<int64_t>();\n      for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {\n        const int64_t batch = indices_mat(i, 0);\n        const Tidx bin = values(i);\n        OP_REQUIRES(\n            ctx, batch < out.dimension(0),\n            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,\n                                    \") must be less than the dimension size (\",\n                                    out.dimension(0), \").\"));\n        OP_REQUIRES(\n            ctx, bin < out.dimension(1),\n            errors::InvalidArgument(\"Index out ouf bound. `bin` (\", bin,\n                                    \") must be less then the dimension size (\",\n                                    out.dimension(1), \").\"));\n        if (bin < size) {\n          if (binary_output_) {\n            out(batch, bin) = T(1);\n          } else {\n            if (weights_size) {\n              out(batch, bin) += weights(i);\n            } else {\n              out(batch, bin) += T(1);\n            }\n          }\n        }\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _clone(\n        self,\n        src: \"DictionaryObject\",\n        pdf_dest: PdfWriterProtocol,\n        force_duplicate: bool,\n        ignore_fields: Optional[Sequence[Union[str, int]]],\n    ) -> None:\n        \"\"\"\n        Update the object from src.\n\n        Args:\n            src: \"DictionaryObject\":\n            pdf_dest:\n            force_duplicate:\n            ignore_fields:\n        \"\"\"\n        # first we remove for the ignore_fields\n        # that are for a limited number of levels\n        x = 0\n        assert ignore_fields is not None\n        ignore_fields = list(ignore_fields)\n        while x < len(ignore_fields):\n            if isinstance(ignore_fields[x], int):\n                if cast(int, ignore_fields[x]) <= 0:\n                    del ignore_fields[x]\n                    del ignore_fields[x]\n                    continue\n                else:\n                    ignore_fields[x] -= 1  # type:ignore\n            x += 1\n        #  First check if this is a chain list, we need to loop to prevent recur\n        if any(\n            field not in ignore_fields\n            and field in src\n            and isinstance(src.raw_get(field), IndirectObject)\n            and isinstance(src[field], DictionaryObject)\n            and (\n                src.get(\"/Type\", None) is None\n                or cast(DictionaryObject, src[field]).get(\"/Type\", None) is None\n                or src.get(\"/Type\", None)\n                == cast(DictionaryObject, src[field]).get(\"/Type\", None)\n            )\n            for field in [\"/Next\", \"/Prev\", \"/N\", \"/V\"]\n        ):\n            ignore_fields = list(ignore_fields)\n            for lst in ((\"/Next\", \"/Prev\"), (\"/N\", \"/V\")):\n                for k in lst:\n                    objs = []\n                    if (\n                        k in src\n                        and k not in self\n                        and isinstance(src.raw_get(k), IndirectObject)\n                        and isinstance(src[k], DictionaryObject)\n                        # IF need to go further the idea is to check\n                        # that the types are the same:\n                        and (\n                            src.get(\"/Type\", None) is None\n                            or cast(DictionaryObject, src[k]).get(\"/Type\", None) is None\n                            or src.get(\"/Type\", None)\n                            == cast(DictionaryObject, src[k]).get(\"/Type\", None)\n                        )\n                    ):\n                        cur_obj: Optional[DictionaryObject] = cast(\n                            \"DictionaryObject\", src[k]\n                        )\n                        prev_obj: Optional[DictionaryObject] = self\n                        while cur_obj is not None:\n                            clon = cast(\n                                \"DictionaryObject\",\n                                cur_obj._reference_clone(\n                                    cur_obj.__class__(), pdf_dest, force_duplicate\n                                ),\n                            )\n                            objs.append((cur_obj, clon))\n                            assert prev_obj is not None\n                            prev_obj[NameObject(k)] = clon.indirect_reference\n                            prev_obj = clon\n                            try:\n                                if cur_obj == src:\n                                    cur_obj = None\n                                else:\n                                    cur_obj = cast(\"DictionaryObject\", cur_obj[k])\n                            except Exception:\n                                cur_obj = None\n                        for s, c in objs:\n                            c._clone(s, pdf_dest, force_duplicate, ignore_fields)\n\n        for k, v in src.items():\n            if k not in ignore_fields:\n                if isinstance(v, StreamObject):\n                    if not hasattr(v, \"indirect_reference\"):\n                        v.indirect_reference = None\n                    vv = v.clone(pdf_dest, force_duplicate, ignore_fields)\n                    assert vv.indirect_reference is not None\n                    self[k.clone(pdf_dest)] = vv.indirect_reference  # type: ignore[attr-defined]\n                elif k not in self:\n                    self[NameObject(k)] = (\n                        v.clone(pdf_dest, force_duplicate, ignore_fields)\n                        if hasattr(v, \"clone\")\n                        else v\n                    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def cli(configfile, observed_since, setup_only=False, troubleshoot=False):\n    '''\n    Tenable.io -> Jira Cloud Transformer & Ingester\n    '''\n    # Load the config, but ensure that any additional fields are additive to the\n    # basic field set.\n    config_from_file = yaml.load(configfile, Loader=yaml.Loader)\n    fields = config_from_file.pop('custom_fields', list())\n    config = dict_merge(base_config(), config_from_file)\n    config['fields'] = config['fields'] + fields\n\n    if config['tenable'].get('tio_transform_tags'):\n        attr_cache = config['tenable'].get('tio_asset_attr_cache', list())\n        tag_attrs = config['tenable'].get('tio_transform_tags', list())\n        config['tenable']['tio_asset_attr_cache'] = attr_cache + tag_attrs\n\n\n    # Get the logging definition and define any defaults as need be.\n    log = config.get('log', {})\n    log_lvls = {'debug': 10, 'info': 20, 'warn': 30, 'error': 40}\n    log['level'] = log_lvls[log.get('level', 'warn')]\n    log['format'] = log.get('format',\n        '%(asctime)-15s %(name)s %(levelname)s %(message)s')\n\n    # Configure the root logging facility\n    if troubleshoot:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=log['format'],\n            filename='tenable_debug.log'\n        )\n    else:\n        logging.basicConfig(**log)\n\n    # Output some basic information detailing the config file used and the\n    # python version & system arch.\n    logging.info('Tenable2JiraCloud Version {}'.format(__version__))\n    logging.info('Using configuration file {}'.format(configfile.name))\n    uname = platform.uname()\n    logging.info('Running on Python {} {}/{}'.format(\n        '.'.join([str(i) for i in sys.version_info][0:3]),\n        uname[0], uname[-2]))\n\n    # instantiate the Jira object\n    jira = Jira(\n        'https://{}/rest/api/3'.format(config['jira']['address']),\n        config['jira']['api_username'],\n        config['jira']['api_token']\n    )\n\n    # Initiate the Tenable.io API model, the Ingester model, and start the\n    # ingestion and data transformation.\n    if config['tenable'].get('platform') == 'tenable.io':\n        if not observed_since:\n            # if no since field is supplied, then look in the config file to see\n            # if an age was applied, if not, then use the default of 30 days.\n            observed_since = arrow.now()\\\n                .shift(days=-config['tenable'].get('tio_age', 30))\\\n                .floor('day').timestamp()\n\n        source = TenableIO(\n            access_key=config['tenable'].get('access_key'),\n            secret_key=config['tenable'].get('secret_key'),\n            vendor='Tenable',\n            product='JiraCloud',\n            build=__version__\n        )\n        if int(source.session.details().get('permissions')) < 64:\n            logging.error('API Keys tie to non-admin user.')\n    elif config['tenable'].get('platform') == 'tenable.sc':\n        source = TenableSC(\n            config['tenable'].get('address'),\n            port=int(config['tenable'].get('port', 443)),\n            username=config['tenable'].get('username'),\n            password=config['tenable'].get('password'),\n            access_key=config['tenable'].get('access_key'),\n            secret_key=config['tenable'].get('secret_key'),\n            vendor='Tenable',\n            product='JiraCloud',\n            build=__version__\n        )\n    else:\n        logging.error('No valid Tenable platform configuration defined.')\n        exit(1)\n    ingest = Tio2Jira(source, jira, config)\n\n    if troubleshoot:\n        # if the troubleshooting flag is set, then we will be collecting some\n        # basic information and outputting it to the screen in a format that\n        # Github issues would expect to format it all pretty.  This should help\n        # reduce the amount of time that is spent with back-and-forth debugging.\n        try:\n            ingest.ingest(int(observed_since))\n        except:\n            logging.exception('Caught the following Exception')\n\n        # Some basic redaction of sensitive data, such as API Keys, Usernames,\n        # Passwords, and hostnames.\n        addr = config_from_file['jira']['address']\n        sc_addr = 'NOTHING_TO_SEE_HERE_AT_ALL'\n        config_from_file['jira']['address'] = '<REDACTED>'\n        config_from_file['jira']['api_token'] = '<REDACTED>'\n        config_from_file['jira']['api_username'] = '<REDACTED>'\n        config_from_file['project']['leadAccountId'] = '<REDACTED>'\n        if config_from_file['tenable'].get('address'):\n            sc_addr = config_from_file['tenable']['address']\n            config_from_file['tenable']['address'] = '<REDACTED>'\n        if config_from_file['tenable'].get('access_key'):\n            config_from_file['tenable']['access_key'] = '<REDACTED>'\n        if config_from_file['tenable'].get('secret_key'):\n            config_from_file['tenable']['secret_key'] = '<REDACTED>'\n        if config_from_file['tenable'].get('username'):\n            config_from_file['tenable']['username'] = '<REDACTED>'\n        if config_from_file['tenable'].get('password'):\n            config_from_file['tenable']['password'] = '<REDACTED>'\n\n        output = troubleshooting.format(\n            configfile=yaml.dump(config_from_file, default_flow_style=False),\n            logging=open('tenable_debug.log').read() \\\n                .replace(addr, '<JIRA_CLOUD_HOST>') \\\n                .replace(sc_addr, '<TENABLE_SC_HOST>'),\n            issuetypes='\\n'.join(\n                [\n                    '{id}: {name}'.format(**a)\n                    for a in jira.issue_types.list()\n                    if a.get('name').lower() in ['task', 'subtask', 'sub-task']\n                ]\n            )\n        )\n        print(output)\n        print('\\n'.join([\n            '/-------------------------------NOTICE-----------------------------------\\\\',\n            '| The output above is helpful for us to troubleshoot exactly what is     |',\n            '| happening within the code and offer a diagnosis for how to correct.    |',\n            '| Please note that while some basic redaction has already been performed |',\n            '| that we ask you to review the information you\\'re about to send and     |',\n            '| ensure that nothing deemed sensitive is transmitted.                   |',\n            '| ---------------------------------------------------------------------- |',\n            '| -- Copy of output saved to \"issue_debug.md\"                            |',\n            '\\\\------------------------------------------------------------------------/'\n        ]))\n        with open('issue_debug.md', 'w') as reportfile:\n            print(output, file=reportfile)\n        os.remove('tenable_debug.log')\n    elif not setup_only:\n        ingest.ingest(observed_since)\n\n        # If we are expected to continually re-run the transformer, then we will\n        # need to track the passage of time and run every X hours, where X is\n        # defined by the user in the configuration.\n        if config.get('service', {}).get('interval', 0) > 0:\n            sleeper = int(config['service']['interval']) * 3600\n            while True:\n                last_run = int(time.time())\n                logging.info(\n                    'Sleeping for {}h'.format(sleeper/3600))\n                time.sleep(sleeper)\n                logging.info(\n                    'Initiating ingest with observed_since={}'.format(last_run))\n                ingest.ingest(last_run)\n    elif setup_only:\n        # In setup-only mode, the ingest will not run, and instead a config file\n        # will be generated that will have all of the JIRA identifiers baked in\n        # and will also inform the integration to ignore the screen builder.\n        # When using this config, if there are any changes to the code, then\n        # this config will need to be re-generated.\n        config['screen']['no_create'] = True\n        logging.info('Set to setup-only.  Will not run ingest.')\n        logging.info('The following is the updated config file from the setup.')\n        with open('generated_config.yaml', 'w') as outfile:\n            outfile.write(yaml.dump(config, Dumper=yaml.Dumper))\n        logging.info('Generated \"generated_config.yaml\" config file.')\n        logging.info('This config file should be updated for every new version of this integration.')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def generate_verifier(length=8):\n    \"\"\"Generate pseudorandom number.\"\"\"\n    return ''.join([str(random.randint(0, 9)) for i in range(length)])\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def load_hparams_from_yaml(config_yaml: str, use_omegaconf: bool = True) -> Dict[str, Any]:\n    \"\"\"Load hparams from a file.\n\n        Args:\n            config_yaml: Path to config yaml file\n            use_omegaconf: If omegaconf is available and ``use_omegaconf=True``,\n                the hparams will be converted to ``DictConfig`` if possible.\n\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')\n    >>> path_yaml = './testing-hparams.yaml'\n    >>> save_hparams_to_yaml(path_yaml, hparams)\n    >>> hparams_new = load_hparams_from_yaml(path_yaml)\n    >>> vars(hparams) == hparams_new\n    True\n    >>> os.remove(path_yaml)\n    \"\"\"\n    fs = get_filesystem(config_yaml)\n    if not fs.exists(config_yaml):\n        rank_zero_warn(f\"Missing Tags: {config_yaml}.\", category=RuntimeWarning)\n        return {}\n\n    with fs.open(config_yaml, \"r\") as fp:\n        hparams = yaml.load(fp, Loader=yaml.UnsafeLoader)\n\n    if _OMEGACONF_AVAILABLE:\n        if use_omegaconf:\n            try:\n                return OmegaConf.create(hparams)\n            except (UnsupportedValueType, ValidationError):\n                pass\n    return hparams\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const float input_min_float = ctx->input(1).flat<float>()(0);\n    const float input_max_float = ctx->input(2).flat<float>()(0);\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(2, TensorShape({}), &output_max));\n\n    // See QuantizationRangeOp as well, which has a copy of this logic.\n    auto input_array = input.flat<T1>();\n    const int32_t input_lowest_quantized =\n        static_cast<int32>(Eigen::NumTraits<T1>::lowest());\n    const int32_t input_highest_quantized =\n        static_cast<int32>(Eigen::NumTraits<T1>::highest());\n    T1 actual_min_quantized = input_highest_quantized;\n    T1 actual_max_quantized = input_lowest_quantized;\n    for (int i = 0; i < input_array.size(); ++i) {\n      const T1 value = input_array(i);\n      actual_min_quantized = std::min(actual_min_quantized, value);\n      actual_max_quantized = std::max(actual_max_quantized, value);\n    }\n    // We want to make sure that the minimum is no larger than zero, so that the\n    // convolution operation can run efficiently.\n    const float actual_min_float =\n        std::min(0.0f, QuantizedToFloat(actual_min_quantized, input_min_float,\n                                        input_max_float));\n    const float actual_max_float = QuantizedToFloat(\n        actual_max_quantized, input_min_float, input_max_float);\n\n#if 0\n    // This is the reference, non-eigen implementation:\n    auto output_array = output->flat<T2>();\n    RequantizeManyInNewRange<T1, T2>(input_array.data(), input_array.size(),\n                                     input_min_float, input_max_float,\n                                     actual_min_float, actual_max_float,\n                                     output_array.data());\n#endif\n\n    if (input_array.size() > 0) {\n      if (meta::IsSupportedAndEnabled() && std::is_same<T1, qint32>() &&\n          std::is_same<T2, quint8>()) {\n        auto input_i32_array = input.flat<qint32>();\n        meta::Requantize(ctx, input_i32_array.data(), input_i32_array.size(),\n                         input_min_float, input_max_float, actual_min_float,\n                         actual_max_float, output->flat<quint8>().data());\n      } else {\n        RequantizeManyInNewRangeUsingEigen<T1, T2>(\n            ctx->eigen_device<CPUDevice>(), input, input_min_float,\n            input_max_float, actual_min_float, actual_max_float, output);\n      }\n    }\n\n    output_min->flat<float>().setConstant(actual_min_float);\n    output_max->flat<float>().setConstant(actual_max_float);\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def set_metric_variables(self, metric_variable_values: List[Any]) -> None:\n    \"\"\"Set metric variable values to the given values.\"\"\"\n    self._session.run(\n        fetches=self._all_metric_variable_assign_ops,\n        feed_dict=self._create_feed_for_metric_variables(\n            metric_variable_values))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def to_python(self, value):\n        return unicodedata.normalize(\"NFKC\", super().to_python(value))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_index_absolute_path(index):\n    return os.path.join(INDEXDIR, index)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self) -> Model:\n        self.validate()\n        assert self._model\n\n        try:\n            dashboard = DashboardDAO.update(self._model, self._properties, commit=False)\n            if self._properties.get(\"json_metadata\"):\n                dashboard = DashboardDAO.set_dash_metadata(\n                    dashboard,\n                    data=json.loads(self._properties.get(\"json_metadata\", \"{}\")),\n                    commit=False,\n                )\n            dashboard = DashboardDAO.update_charts_owners(dashboard, commit=False)\n            db.session.commit()\n        except DAOUpdateFailedError as ex:\n            logger.exception(ex.exception)\n            raise DashboardUpdateFailedError() from ex\n        return dashboard\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def trace_dump(self, pid: int, native: bool = False) -> (bool, str):\n        cmd = f\"py-spy dump -p {pid}\"\n        # We\n        if sys.platform == \"linux\" and native:\n            cmd += \" --native\"\n        if await _can_passwordless_sudo():\n            cmd = \"sudo -n \" + cmd.replace(\"py-spy\", \"$(which py-spy)\")\n        process = await asyncio.create_subprocess_shell(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            shell=True,\n        )\n        stdout, stderr = await process.communicate()\n        if process.returncode != 0:\n            return False, _format_failed_pyspy_command(cmd, stdout, stderr)\n        else:\n            return True, stdout.decode(\"utf-8\")\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def process(self, header, body):\n        try:\n            msg = self._connection.receive_from_header_and_body(header, body)\n        except ua.uaerrors.BadUserAccessDenied:\n            _logger.warning(\"Unauthenticated user attempted to connect\")\n            return False\n        if isinstance(msg, ua.Message):\n            if header.MessageType == ua.MessageType.SecureOpen:\n                self.open_secure_channel(msg.SecurityHeader(), msg.SequenceHeader(), msg.body())\n            elif header.MessageType == ua.MessageType.SecureClose:\n                self._connection.close()\n                return False\n            elif header.MessageType == ua.MessageType.SecureMessage:\n                return await self.process_message(msg.SequenceHeader(), msg.body())\n        elif isinstance(msg, ua.Hello):\n            ack = ua.Acknowledge()\n            ack.ReceiveBufferSize = msg.ReceiveBufferSize\n            ack.SendBufferSize = msg.SendBufferSize\n            data = uatcp_to_binary(ua.MessageType.Acknowledge, ack)\n            self._transport.write(data)\n        elif isinstance(msg, ua.ErrorMessage):\n            _logger.warning(\"Received an error message type\")\n        elif msg is None:\n            pass  # msg is a ChunkType.Intermediate of an ua.MessageType.SecureMessage\n        else:\n            _logger.warning(\"Unsupported message type: %s\", header.MessageType)\n            raise ServiceError(ua.StatusCodes.BadTcpMessageTypeInvalid)\n        return True\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& logits_in = context->input(0);\n    const Tensor& labels_in = context->input(1);\n\n    TensorShape shape_in = logits_in.shape();\n\n    BCast bcast(BCast::FromShape(logits_in.shape()),\n                BCast::FromShape(labels_in.shape()));\n    if (!logits_in.IsSameSize(labels_in)) {\n      OP_REQUIRES(context, bcast.IsValid(),\n                  errors::InvalidArgument(\n                      \"logits and labels must be broadcastable: logits_size=\",\n                      logits_in.shape().DebugString(),\n                      \" labels_size=\", labels_in.shape().DebugString()));\n      shape_in = BCast::ToShape(bcast.output_shape());\n    }\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(shape_in),\n                errors::InvalidArgument(\"logits and labels must be either \"\n                                        \"2-dimensional, or broadcasted to be \"\n                                        \"2-dimensional\"));\n\n    if (std::is_same<Device, GPUDevice>::value) {\n      OP_REQUIRES(context, !OpDeterminismRequired(),\n                  errors::Unimplemented(\n                      \"The GPU implementation of SoftmaxCrossEntropyWithLogits\"\n                      \" that would have been executed is not deterministic.\"\n                      \" Note that the Python API uses an alternative,\"\n                      \" deterministic, GPU-accelerated path when determinism is\"\n                      \" enabled.\"));\n    }\n\n    // loss is 1-D (one per example), and size is batch_size.\n\n    Tensor scratch;\n    OP_REQUIRES_OK(\n        context, context->allocate_temp(DataTypeToEnum<T>::value,\n                                        TensorShape({shape_in.dim_size(0), 1}),\n                                        &scratch));\n\n    Tensor* loss_out = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\n                       0, TensorShape({shape_in.dim_size(0)}), &loss_out));\n    Tensor* back_out = nullptr;\n    // Try to reuse the logits_in buffer for the backprop output.\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 1, shape_in, &back_out));\n    if (shape_in.dim_size(0) > 0) {\n      functor::XentFunctor<Device, T> functor;\n      if (logits_in.IsSameSize(labels_in)) {\n        functor(context->eigen_device<Device>(), shape_in.AsEigenDSizes<2>(),\n                Eigen::array<Eigen::DenseIndex, 2>{1, 1},\n                Eigen::array<Eigen::DenseIndex, 2>{1, 1}, logits_in.matrix<T>(),\n                labels_in.matrix<T>(), scratch.matrix<T>(), loss_out->vec<T>(),\n                back_out->matrix<T>());\n      } else {\n        functor(context->eigen_device<Device>(), shape_in.AsEigenDSizes<2>(),\n                BCast::ToIndexArray<2>(bcast.x_bcast()),\n                BCast::ToIndexArray<2>(bcast.y_bcast()),\n                logits_in.template shaped<T, 2>(bcast.x_reshape()),\n                labels_in.template shaped<T, 2>(bcast.y_reshape()),\n                scratch.matrix<T>(), loss_out->vec<T>(), back_out->matrix<T>());\n      }\n    }\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_next_logout(self, peek=False):\n        \"\"\"\n        Get the next session in the logged-in state and move\n        it to the logging_out state.  Return the session that is\n        found.\n\n        :param peek: for IdP-initiated logout we can't remove the\n                     session otherwise when the request comes back\n                     in the user won't be seen as being logged-on.\n\n        Return None if no more sessions in LOGGED_IN state.\n        \"\"\"\n        candidates = self._ss.get_user_sessions(self.user)\n\n        for c in candidates:\n            key = c.keys()[0]\n            if int(c[key].get('logoutstate', 0)) == LOGGED_IN:\n                samlsession = self._data_to_samlsession(key, c[key])\n                self.start_logout(samlsession, initial=False)\n                return samlsession\n        return None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def received(self, data):\n        \"\"\"\n        Receives the HTTP stream for one request.  Returns the number of\n        bytes consumed.  Sets the completed flag once both the header and the\n        body have been received.\n        \"\"\"\n        if self.completed:\n            return 0  # Can't consume any more.\n        datalen = len(data)\n        br = self.body_rcv\n        if br is None:\n            # In header.\n            s = self.header_plus + data\n            index = find_double_newline(s)\n            if index >= 0:\n                # Header finished.\n                header_plus = s[:index]\n                consumed = len(data) - (len(s) - index)\n                # Remove preceeding blank lines.\n                header_plus = header_plus.lstrip()\n                if not header_plus:\n                    self.empty = True\n                    self.completed = True\n                else:\n                    try:\n                        self.parse_header(header_plus)\n                    except ParsingError as e:\n                        self.error = BadRequest(e.args[0])\n                        self.completed = True\n                    else:\n                        if self.body_rcv is None:\n                            # no content-length header and not a t-e: chunked\n                            # request\n                            self.completed = True\n                        if self.content_length > 0:\n                            max_body = self.adj.max_request_body_size\n                            # we won't accept this request if the content-length\n                            # is too large\n                            if self.content_length >= max_body:\n                                self.error = RequestEntityTooLarge(\n                                    \"exceeds max_body of %s\" % max_body\n                                )\n                                self.completed = True\n                self.headers_finished = True\n                return consumed\n            else:\n                # Header not finished yet.\n                self.header_bytes_received += datalen\n                max_header = self.adj.max_request_header_size\n                if self.header_bytes_received >= max_header:\n                    # malformed header, we need to construct some request\n                    # on our own. we disregard the incoming(?) requests HTTP\n                    # version and just use 1.0. IOW someone just sent garbage\n                    # over the wire\n                    self.parse_header(b\"GET / HTTP/1.0\\n\")\n                    self.error = RequestHeaderFieldsTooLarge(\n                        \"exceeds max_header of %s\" % max_header\n                    )\n                    self.completed = True\n                self.header_plus = s\n                return datalen\n        else:\n            # In body.\n            consumed = br.received(data)\n            self.body_bytes_received += consumed\n            max_body = self.adj.max_request_body_size\n            if self.body_bytes_received >= max_body:\n                # this will only be raised during t-e: chunked requests\n                self.error = RequestEntityTooLarge(\"exceeds max_body of %s\" % max_body)\n                self.completed = True\n            elif br.error:\n                # garbage in chunked encoding input probably\n                self.error = br.error\n                self.completed = True\n            elif br.completed:\n                # The request (with the body) is ready to use.\n                self.completed = True\n                if self.chunked:\n                    # We've converted the chunked transfer encoding request\n                    # body into a normal request body, so we know its content\n                    # length; set the header here.  We already popped the\n                    # TRANSFER_ENCODING header in parse_header, so this will\n                    # appear to the client to be an entirely non-chunked HTTP\n                    # request with a valid content-length.\n                    self.headers[\"CONTENT_LENGTH\"] = str(br.__len__())\n            return consumed\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_object(self, indirect_reference: IndirectObject) -> Optional[PdfObject]:\n        retval = self.cache_get_indirect_object(\n            indirect_reference.generation, indirect_reference.idnum\n        )\n        if retval is not None:\n            return retval\n        if (\n            indirect_reference.generation == 0\n            and indirect_reference.idnum in self.xref_objStm\n        ):\n            retval = self._get_object_from_stream(indirect_reference)  # type: ignore\n        elif (\n            indirect_reference.generation in self.xref\n            and indirect_reference.idnum in self.xref[indirect_reference.generation]\n        ):\n            if self.xref_free_entry.get(indirect_reference.generation, {}).get(\n                indirect_reference.idnum, False\n            ):\n                return NullObject()\n            start = self.xref[indirect_reference.generation][indirect_reference.idnum]\n            self.stream.seek(start, 0)\n            try:\n                idnum, generation = self.read_object_header(self.stream)\n            except Exception:\n                if hasattr(self.stream, \"getbuffer\"):\n                    buf = bytes(self.stream.getbuffer())  # type: ignore\n                else:\n                    p = self.stream.tell()\n                    buf = self.stream.read(-1)\n                    self.stream.seek(p, 0)\n                m = re.search(\n                    rf\"\\s{indirect_reference.idnum}\\s+{indirect_reference.generation}\\s+obj\".encode(),\n                    buf,\n                )\n                if m is not None:\n                    logger_warning(\n                        f\"Object ID {indirect_reference.idnum},{indirect_reference.generation} ref repaired\",\n                        __name__,\n                    )\n                    self.xref[indirect_reference.generation][\n                        indirect_reference.idnum\n                    ] = (m.start(0) + 1)\n                    self.stream.seek(m.start(0) + 1)\n                    idnum, generation = self.read_object_header(self.stream)\n                else:\n                    idnum = -1  # exception will be raised below\n            if idnum != indirect_reference.idnum and self.xref_index:\n                # Xref table probably had bad indexes due to not being zero-indexed\n                if self.strict:\n                    raise PdfReadError(\n                        f\"Expected object ID ({indirect_reference.idnum} {indirect_reference.generation}) \"\n                        f\"does not match actual ({idnum} {generation}); \"\n                        \"xref table not zero-indexed.\"\n                    )\n                # xref table is corrected in non-strict mode\n            elif idnum != indirect_reference.idnum and self.strict:\n                # some other problem\n                raise PdfReadError(\n                    f\"Expected object ID ({indirect_reference.idnum} \"\n                    f\"{indirect_reference.generation}) does not match actual \"\n                    f\"({idnum} {generation}).\"\n                )\n            if self.strict:\n                assert generation == indirect_reference.generation\n            retval = read_object(self.stream, self)  # type: ignore\n\n            # override encryption is used for the /Encrypt dictionary\n            if not self._override_encryption and self._encryption is not None:\n                # if we don't have the encryption key:\n                if not self._encryption.is_decrypted():\n                    raise FileNotDecryptedError(\"File has not been decrypted\")\n                # otherwise, decrypt here...\n                retval = cast(PdfObject, retval)\n                retval = self._encryption.decrypt_object(\n                    retval, indirect_reference.idnum, indirect_reference.generation\n                )\n        else:\n            if hasattr(self.stream, \"getbuffer\"):\n                buf = bytes(self.stream.getbuffer())  # type: ignore\n            else:\n                p = self.stream.tell()\n                buf = self.stream.read(-1)\n                self.stream.seek(p, 0)\n            m = re.search(\n                rf\"\\s{indirect_reference.idnum}\\s+{indirect_reference.generation}\\s+obj\".encode(),\n                buf,\n            )\n            if m is not None:\n                logger_warning(\n                    f\"Object {indirect_reference.idnum} {indirect_reference.generation} found\",\n                    __name__,\n                )\n                if indirect_reference.generation not in self.xref:\n                    self.xref[indirect_reference.generation] = {}\n                self.xref[indirect_reference.generation][indirect_reference.idnum] = (\n                    m.start(0) + 1\n                )\n                self.stream.seek(m.end(0) + 1)\n                skip_over_whitespace(self.stream)\n                self.stream.seek(-1, 1)\n                retval = read_object(self.stream, self)  # type: ignore\n\n                # override encryption is used for the /Encrypt dictionary\n                if not self._override_encryption and self._encryption is not None:\n                    # if we don't have the encryption key:\n                    if not self._encryption.is_decrypted():\n                        raise FileNotDecryptedError(\"File has not been decrypted\")\n                    # otherwise, decrypt here...\n                    retval = cast(PdfObject, retval)\n                    retval = self._encryption.decrypt_object(\n                        retval, indirect_reference.idnum, indirect_reference.generation\n                    )\n            else:\n                logger_warning(\n                    f\"Object {indirect_reference.idnum} {indirect_reference.generation} not defined.\",\n                    __name__,\n                )\n                if self.strict:\n                    raise PdfReadError(\"Could not find object.\")\n        self.cache_indirect_object(\n            indirect_reference.generation, indirect_reference.idnum, retval\n        )\n        return retval\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):\n        if not isinstance(arcname, str):\n            raise ValueError(\"Unsupported arcname\")\n        if isinstance(data, str):\n            self.writef(io.BytesIO(data.encode(\"UTF-8\")), arcname)\n        elif isinstance(data, bytes) or isinstance(data, bytearray) or isinstance(data, memoryview):\n            self.writef(io.BytesIO(data), arcname)\n        else:\n            raise ValueError(\"Unsupported data type.\")\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def main(self):\n        while True:\n            request = self.rpc_read()\n            self.log(request)\n            if request is None:\n                self.log('No more requests, stop handler')\n                return\n            cmd = self.conf('cmd')\n            if not cmd:\n                return\n            if request.method == 'notification':\n                icon_data = self.read() if request.params.get('packageIcon', False) else None\n                if request.params.get('event') == 'posted':\n                    text, package = map(request.params.get, ('text', 'package'))\n                    title = request.params.get('title', 'NULL')\n                    name, uin = self.device.name, self.device.uin\n                    if text is None:\n                        text = ''\n                    icon_path = os.path.join(self.conf('icon_dir'), f'{package}.{self.device.uin}.icon.png')\n                    if bool(icon_data):\n                        try:\n                            open(icon_path, 'wb').write(icon_data)\n                        except Exception as e:\n                            self.log(e, logging.WARNING)\n                    icon = icon_path if icon_data else ''\n                    command = cmd.format(uin=uin, name=name, icon=icon, text=text, title=title, package=package)\n                    self.log('Execute: \"{}\"'.format(command))\n                    subprocess.call(command, shell=True)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _save(self, name, content):\n        full_path = self.path(name)\n\n        # Create any intermediate directories that do not exist.\n        directory = os.path.dirname(full_path)\n        if not os.path.exists(directory):\n            try:\n                if self.directory_permissions_mode is not None:\n                    # Set the umask because os.makedirs() doesn't apply the \"mode\"\n                    # argument to intermediate-level directories.\n                    old_umask = os.umask(0o777 & ~self.directory_permissions_mode)\n                    try:\n                        os.makedirs(directory, self.directory_permissions_mode)\n                    finally:\n                        os.umask(old_umask)\n                else:\n                    os.makedirs(directory)\n            except FileExistsError:\n                # There's a race between os.path.exists() and os.makedirs().\n                # If os.makedirs() fails with FileExistsError, the directory\n                # was created concurrently.\n                pass\n        if not os.path.isdir(directory):\n            raise IOError(\"%s exists and is not a directory.\" % directory)\n\n        # There's a potential race condition between get_available_name and\n        # saving the file; it's possible that two threads might return the\n        # same name, at which point all sorts of fun happens. So we need to\n        # try to create the file, but if it already exists we have to go back\n        # to get_available_name() and try again.\n\n        while True:\n            try:\n                # This file has a file path that we can move.\n                if hasattr(content, 'temporary_file_path'):\n                    file_move_safe(content.temporary_file_path(), full_path)\n\n                # This is a normal uploadedfile that we can stream.\n                else:\n                    # The current umask value is masked out by os.open!\n                    fd = os.open(full_path, self.OS_OPEN_FLAGS, 0o666)\n                    _file = None\n                    try:\n                        locks.lock(fd, locks.LOCK_EX)\n                        for chunk in content.chunks():\n                            if _file is None:\n                                mode = 'wb' if isinstance(chunk, bytes) else 'wt'\n                                _file = os.fdopen(fd, mode)\n                            _file.write(chunk)\n                    finally:\n                        locks.unlock(fd)\n                        if _file is not None:\n                            _file.close()\n                        else:\n                            os.close(fd)\n            except FileExistsError:\n                # A new name is needed if the file exists.\n                name = self.get_available_name(name)\n                full_path = self.path(name)\n            else:\n                # OK, the file save worked. Break out of the loop.\n                break\n\n        if self.file_permissions_mode is not None:\n            os.chmod(full_path, self.file_permissions_mode)\n\n        # Store filenames with forward slashes, even on Windows.\n        return name.replace('\\\\', '/')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def login(self, login, password, url=None):\n        \"\"\"login page\n        \"\"\"\n        auth = self._auth(login, password)\n        cherrypy.session['isadmin'] = auth['isadmin']\n        cherrypy.session['connected'] = auth['connected']\n\n        if auth['connected']:\n            if auth['isadmin']:\n                message = \\\n                    \"login success for user '%(user)s' as administrator\" % {\n                        'user': login\n                    }\n            else:\n                message = \\\n                    \"login success for user '%(user)s' as normal user\" % {\n                        'user': login\n                    }\n            cherrypy.log.error(\n                msg=message,\n                severity=logging.INFO\n            )\n            cherrypy.session[SESSION_KEY] = cherrypy.request.login = login\n            if url is None:\n                redirect = \"/\"\n            else:\n                redirect = base64.b64decode(url)\n            raise cherrypy.HTTPRedirect(redirect)\n        else:\n            message = \"login failed for user '%(user)s'\" % {\n                'user': login\n            }\n            cherrypy.log.error(\n                msg=message,\n                severity=logging.WARNING\n            )\n            if url is None:\n                qs = ''\n            else:\n                qs = '?url=' + url\n            raise cherrypy.HTTPRedirect(\"/signin\" + qs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _prepare_cli_cmd(self) -> list[Any]:\n        \"\"\"This function creates the command list from available information.\"\"\"\n        conn = self.conn\n        hive_bin = \"hive\"\n        cmd_extra = []\n\n        if self.use_beeline:\n            hive_bin = \"beeline\"\n            self._validate_beeline_parameters(conn)\n            jdbc_url = f\"jdbc:hive2://{conn.host}:{conn.port}/{conn.schema}\"\n            if conf.get(\"core\", \"security\") == \"kerberos\":\n                template = conn.extra_dejson.get(\"principal\", \"hive/_HOST@EXAMPLE.COM\")\n                if \"_HOST\" in template:\n                    template = utils.replace_hostname_pattern(utils.get_components(template))\n\n                proxy_user = self._get_proxy_user()\n\n                jdbc_url += f\";principal={template};{proxy_user}\"\n            elif self.auth:\n                jdbc_url += \";auth=\" + self.auth\n\n            jdbc_url = f'\"{jdbc_url}\"'\n\n            cmd_extra += [\"-u\", jdbc_url]\n            if conn.login:\n                cmd_extra += [\"-n\", conn.login]\n            if conn.password:\n                cmd_extra += [\"-p\", conn.password]\n\n        hive_params_list = self.hive_cli_params.split()\n\n        return [hive_bin] + cmd_extra + hive_params_list\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def gravatar(context, user, size=None):\n    \"\"\"\n    Outputs the HTML for displaying a user's gravatar.\n\n    This can take an optional size of the image (defaults to 80 if not\n    specified).\n\n    This is also influenced by the following settings:\n\n        GRAVATAR_SIZE    - Default size for gravatars\n        GRAVATAR_RATING  - Maximum allowed rating (g, pg, r, x)\n        GRAVATAR_DEFAULT - Default image set to show if the user hasn't\n                           specified a gravatar (identicon, monsterid, wavatar)\n\n    See http://www.gravatar.com/ for more information.\n    \"\"\"\n    url = get_gravatar_url(context['request'], user, size)\n\n    if url:\n        return ('<img src=\"%s\" width=\"%s\" height=\"%s\" alt=\"%s\" '\n                '     class=\"gravatar\"/>' %\n                (url, size, size, user.get_full_name() or user.username))\n    else:\n        return ''\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _recv_frame(self, message=None):\n        recv = self._get_bytes\n\n        # Unpacking the frame described in Section 5.2 of RFC6455\n        # (https://tools.ietf.org/html/rfc6455#section-5.2)\n        header = recv(2)\n        a, b = struct.unpack('!BB', header)\n        finished = a >> 7 == 1\n        rsv123 = a >> 4 & 7\n        rsv1 = rsv123 & 4\n        if rsv123:\n            if rsv1 and \"permessage-deflate\" not in self.extensions:\n                # must be zero - unless it's compressed then rsv1 is true\n                raise FailedConnectionError(\n                    1002,\n                    \"RSV1, RSV2, RSV3: MUST be 0 unless an extension is\"\n                    \" negotiated that defines meanings for non-zero values.\")\n        opcode = a & 15\n        if opcode not in (0, 1, 2, 8, 9, 0xA):\n            raise FailedConnectionError(1002, \"Unknown opcode received.\")\n        masked = b & 128 == 128\n        if not masked and not self.client:\n            raise FailedConnectionError(1002, \"A client MUST mask all frames\"\n                                        \" that it sends to the server\")\n        length = b & 127\n        if opcode & 8:\n            if not finished:\n                raise FailedConnectionError(1002, \"Control frames must not\"\n                                            \" be fragmented.\")\n            if length > 125:\n                raise FailedConnectionError(\n                    1002,\n                    \"All control frames MUST have a payload length of 125\"\n                    \" bytes or less\")\n        elif opcode and message:\n            raise FailedConnectionError(\n                1002,\n                \"Received a non-continuation opcode within\"\n                \" fragmented message.\")\n        elif not opcode and not message:\n            raise FailedConnectionError(\n                1002,\n                \"Received continuation opcode with no previous\"\n                \" fragments received.\")\n        if length == 126:\n            length = struct.unpack('!H', recv(2))[0]\n        elif length == 127:\n            length = struct.unpack('!Q', recv(8))[0]\n        if masked:\n            mask = struct.unpack('!BBBB', recv(4))\n        received = 0\n        if not message or opcode & 8:\n            decoder = self.UTF8Decoder() if opcode == 1 else None\n            decompressor = self._get_permessage_deflate_dec(rsv1)\n            message = self.Message(opcode, decoder=decoder, decompressor=decompressor)\n        if not length:\n            message.push(b'', final=finished)\n        else:\n            while received < length:\n                d = self.socket.recv(length - received)\n                if not d:\n                    raise ConnectionClosedError()\n                dlen = len(d)\n                if masked:\n                    d = self._apply_mask(d, mask, length=dlen, offset=received)\n                received = received + dlen\n                try:\n                    message.push(d, final=finished)\n                except (UnicodeDecodeError, ValueError):\n                    raise FailedConnectionError(\n                        1007, \"Text data must be valid utf-8\")\n        return message\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        dir_name, file_name = os.path.split(name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, add an underscore and a random 7\n        # character alphanumeric string (before the file extension, if one\n        # exists) to the filename until the generated filename doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n        return name\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def on_file(file):\n            nonlocal file_object\n            data[\"file\"] = file.file_name.decode()\n            file_object = file.file_object\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    OpInputList indices_list_in;\n    OP_REQUIRES_OK(context, context->input_list(\"indices\", &indices_list_in));\n    OpInputList values_list_in;\n    OP_REQUIRES_OK(context, context->input_list(\"values\", &values_list_in));\n    OpInputList shapes_list_in;\n    OP_REQUIRES_OK(context, context->input_list(\"shapes\", &shapes_list_in));\n    OpInputList dense_list_in;\n    OP_REQUIRES_OK(context,\n                   context->input_list(\"dense_inputs\", &dense_list_in));\n\n    OP_REQUIRES_OK(context, ValidateInput(indices_list_in, values_list_in,\n                                          shapes_list_in, dense_list_in));\n\n    const Tensor* sep_t;\n    OP_REQUIRES_OK(context, context->input(\"sep\", &sep_t));\n    const tstring separator = sep_t->scalar<tstring>()();\n\n    std::vector<std::unique_ptr<ColumnInterface<tstring>>> columns =\n        GenerateColumnsFromInput<tstring>(indices_list_in, values_list_in,\n                                          shapes_list_in, dense_list_in);\n    Tensor* indices_out;\n    Tensor* values_out;\n    Tensor* shape_out;\n    const int64 batch_size = CalculateBatchSize(shapes_list_in, dense_list_in);\n    std::vector<int64> output_start_indices(batch_size);\n    OP_REQUIRES_OK(\n        context,\n        CreateOutputTensors(columns, batch_size, context, &indices_out,\n                            &values_out, &shape_out, &output_start_indices));\n    StringCrosser<tstring> crosser(columns, 0, 0, separator);\n    OutputUpdater<tstring> updater(output_start_indices, indices_out,\n                                   values_out);\n    auto do_work = [&columns, crosser, updater](int64 begin, int64 end) {\n      for (int b = begin; b < end; b++) {\n        ProductIterator<tstring> product_iterator(columns, b);\n        int64 cross_count = 0;\n        while (product_iterator.HasNext()) {\n          const auto permutation = product_iterator.Next();\n          updater.Update(b, cross_count,\n                         crosser.Generate(b, permutation, false));\n          cross_count++;\n        }\n      }\n    };\n\n    auto* worker_threads = context->device()->tensorflow_cpu_worker_threads();\n    // TODO(zakaria): optimize kCostPerUnit\n    const int kCostPerUnit = 5000 * indices_list_in.size();\n    Shard(worker_threads->num_threads, worker_threads->workers, batch_size,\n          kCostPerUnit, do_work);\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status CompressElement(const std::vector<Tensor>& element,\n                       CompressedElement* out) {\n  // Step 1: Determine the total uncompressed size. This requires serializing\n  // non-memcopyable tensors, which we save to use again later.\n  std::vector<TensorProto> non_memcpy_components;\n  int64 total_size = 0;\n  for (auto& component : element) {\n    if (DataTypeCanUseMemcpy(component.dtype())) {\n      // Some datatypes can be memcopied, allowing us to save two copies\n      // (AsProtoTensorContent and SerializeToArray).\n      total_size += DMAHelper::buffer(&component)->size();\n    } else {\n      non_memcpy_components.emplace_back();\n      component.AsProtoTensorContent(&non_memcpy_components.back());\n      total_size += non_memcpy_components.back().ByteSizeLong();\n    }\n  }\n\n  // Step 2: Write the tensor data to a buffer, and compress that buffer.\n  // We use tstring for access to resize_uninitialized.\n  tstring uncompressed;\n  uncompressed.resize_uninitialized(total_size);\n  // Position in `uncompressed` to write the next component.\n  char* position = uncompressed.mdata();\n  int non_memcpy_component_index = 0;\n  for (auto& component : element) {\n    CompressedComponentMetadata* metadata =\n        out->mutable_component_metadata()->Add();\n    metadata->set_dtype(component.dtype());\n    component.shape().AsProto(metadata->mutable_tensor_shape());\n    if (DataTypeCanUseMemcpy(component.dtype())) {\n      const TensorBuffer* buffer = DMAHelper::buffer(&component);\n      memcpy(position, buffer->data(), buffer->size());\n      metadata->set_tensor_size_bytes(buffer->size());\n    } else {\n      TensorProto& proto = non_memcpy_components[non_memcpy_component_index++];\n      proto.SerializeToArray(position, proto.ByteSizeLong());\n      metadata->set_tensor_size_bytes(proto.ByteSizeLong());\n    }\n    position += metadata->tensor_size_bytes();\n  }\n  DCHECK_EQ(position, uncompressed.mdata() + total_size);\n\n  if (!port::Snappy_Compress(uncompressed.mdata(), total_size,\n                             out->mutable_data())) {\n    return errors::Internal(\"Failed to compress using snappy.\");\n  }\n  VLOG(3) << \"Compressed element from \" << total_size << \" bytes to \"\n          << out->data().size() << \" bytes\";\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def get_metric_variables(self) -> List[Any]:\n    \"\"\"Returns a list containing the metric variable values.\"\"\"\n    result = self._session.run(fetches=self._metric_variable_nodes)\n    return result\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext *ctx) override {\n    const Tensor *indices_t, *values_t, *shape_t, *reduction_axes_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_indices\", &indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_values\", &values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_shape\", &shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"reduction_axes\", &reduction_axes_t));\n\n    OP_REQUIRES_OK(ctx, ValidateInputs(shape_t, reduction_axes_t));\n\n    // TODO(zongheng): we will call Reorder() below, which will modify\n    // in-place the underlying indices and values buffers.  To avoid\n    // surprises of this kernel being stateful, we work around the above by\n    // making deep copies here.  Remove this if/when we change Reorder()'s\n    // semantics.\n    const auto shape_vec = shape_t->vec<int64>();\n    SparseTensor sp;\n    OP_REQUIRES_OK(ctx, SparseTensor::Create(\n        tensor::DeepCopy(*indices_t), tensor::DeepCopy(*values_t),\n                    TensorShape(shape_vec), &sp));\n    ReduceDetails reduction = SparseTensorReduceHelper(\n        sp, reduction_axes_t->flat<int32>(), keep_dims_);\n\n    Tensor *out_values;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, reduction.reduced_shape, &out_values));\n    auto out_flat = out_values->flat<T>();\n    out_flat.setZero();\n\n    Tensor tmp_reduced_val;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                           TensorShape({}), &tmp_reduced_val));\n    auto reduced_val = tmp_reduced_val.scalar<T>();\n\n    // Compute strides, and use it to convert coords to flat index.  The\n    // coordinates returned by .group() have the same ndims as group_by_dims.\n    gtl::InlinedVector<int64, 8> output_strides(reduction.group_by_dims.size());\n    if (!output_strides.empty()) {  // Do this iff we don't reduce all.\n      output_strides.back() = 1;\n      for (int d = output_strides.size() - 2; d >= 0; --d) {\n        output_strides[d] =\n            output_strides[d + 1] * shape_vec(reduction.group_by_dims[d + 1]);\n      }\n    }\n\n    auto CoordinatesToFlatIndex = [](ArraySlice<int64> coords,\n                                     ArraySlice<int64> strides) -> int64 {\n      if (strides.empty()) {  // Reduce all.\n        return 0;\n      }\n      CHECK_EQ(coords.size(), strides.size());\n      int64_t idx = 0;\n      for (int i = 0; i < coords.size(); ++i) {\n        idx += coords[i] * strides[i];\n      }\n      return idx;\n    };\n\n    // Each group maps one-on-one onto a value in the reduced tensor.\n    // g.group() provides the coordinates of a particular reduced value.\n    sp.Reorder<T>(reduction.reorder_dims);\n    for (const auto &g : sp.group(reduction.group_by_dims)) {\n      Op::template Run<T>(ctx, reduced_val, g.template values<T>());\n      const int64_t idx = CoordinatesToFlatIndex(g.group(), output_strides);\n      out_flat(idx) = reduced_val();\n      VLOG(2) << \"coords: \" << absl::StrJoin(g.group(), \",\")\n              << \"; idx: \" << idx << \"; group \" << Op::Name() << \": \"\n              << reduced_val();\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_files_from_storage(paths):\n        \"\"\"Return S3 file where the name does not include the path.\"\"\"\n        for path in paths:\n            path = pathlib.PurePosixPath(path)\n            try:\n                location = storage.aws_location\n            except AttributeError:\n                location = storage.location\n            try:\n                f = storage.open(str(path.relative_to(location)))\n                f.name = path.name\n                yield f\n            except (OSError, ValueError):\n                logger.exception(\"File not found: %s\", path)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def configure_branch(self, branch):\n        \"\"\"Configure repository branch.\"\"\"\n        if not self.on_branch(branch):\n            self.execute([\"update\", branch])\n        self.branch = branch\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def import_prompt(request):\n   if not test_user_authenticated(request): return login(request, next=\"/cobbler_web/import/prompt\")\n   t = get_template('import.tmpl')\n   html = t.render(Context({\n       'version'  : remote.version(request.session['token']),\n       'username' : username,\n   }))\n   return HttpResponse(html)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self, *args, **data):\n        \"\"\"Process a single Onboarding Task instance.\"\"\"\n        self.logger.info(\"START: onboard device\")\n        credentials = self._parse_credentials(data[\"credentials\"])\n        platform = data[\"platform\"]\n        netdev = NetdevKeeper(\n            hostname=data[\"ip_address\"],\n            port=data[\"port\"],\n            timeout=data[\"timeout\"],\n            username=credentials[\"username\"],\n            password=credentials[\"password\"],\n            secret=credentials[\"secret\"],\n            napalm_driver=platform.napalm_driver if platform and platform.napalm_driver else None,\n            optional_args=platform.napalm_args if platform and platform.napalm_args else settings.NAPALM_ARGS,\n        )\n        netdev.get_onboarding_facts()\n        netdev_dict = netdev.get_netdev_dict()\n\n        onboarding_kwargs = {\n            # Kwargs extracted from OnboardingTask:\n            \"netdev_mgmt_ip_address\": data[\"ip_address\"],\n            \"netdev_nb_location_name\": data[\"location\"].name,\n            \"netdev_nb_device_type_name\": data[\"device_type\"],\n            \"netdev_nb_role_name\": data[\"role\"].name if data[\"role\"] else PLUGIN_SETTINGS[\"default_device_role\"],\n            \"netdev_nb_role_color\": PLUGIN_SETTINGS[\"default_device_role_color\"],\n            \"netdev_nb_platform_name\": data[\"platform\"].name if data[\"platform\"] else None,\n            # Kwargs discovered on the Onboarded Device:\n            \"netdev_hostname\": netdev_dict[\"netdev_hostname\"],\n            \"netdev_vendor\": netdev_dict[\"netdev_vendor\"],\n            \"netdev_model\": netdev_dict[\"netdev_model\"],\n            \"netdev_serial_number\": netdev_dict[\"netdev_serial_number\"],\n            \"netdev_mgmt_ifname\": netdev_dict[\"netdev_mgmt_ifname\"],\n            \"netdev_mgmt_pflen\": netdev_dict[\"netdev_mgmt_pflen\"],\n            \"netdev_netmiko_device_type\": netdev_dict[\"netdev_netmiko_device_type\"],\n            \"onboarding_class\": netdev_dict[\"onboarding_class\"],\n            \"driver_addon_result\": netdev_dict[\"driver_addon_result\"],\n        }\n        onboarding_cls = netdev_dict[\"onboarding_class\"]()\n        onboarding_cls.credentials = {\"username\": self.username, \"password\": self.password, \"secret\": self.secret}\n        onboarding_cls.run(onboarding_kwargs=onboarding_kwargs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def parse_url(url):\n    \"\"\"\n    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is\n    performed to parse incomplete urls. Fields not provided will be None.\n    This parser is RFC 3986 compliant.\n\n    :param str url: URL to parse into a :class:`.Url` namedtuple.\n\n    Partly backwards-compatible with :mod:`urlparse`.\n\n    Example::\n\n        >>> parse_url('http://google.com/mail/')\n        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)\n        >>> parse_url('google.com:80')\n        Url(scheme=None, host='google.com', port=80, path=None, ...)\n        >>> parse_url('/foo?bar')\n        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)\n    \"\"\"\n    if not url:\n        # Empty\n        return Url()\n\n    is_string = not isinstance(url, six.binary_type)\n    if not is_string:\n        url = url.decode(\"utf-8\")\n\n    # RFC 3986 doesn't like URLs that have a host but don't start\n    # with a scheme and we support URLs like that so we need to\n    # detect that problem and add an empty scheme indication.\n    # We don't get hurt on path-only URLs here as it's stripped\n    # off and given an empty scheme anyways.\n    if not SCHEME_REGEX.search(url):\n        url = \"//\" + url\n\n    try:\n        iri_ref = rfc3986.IRIReference.from_string(url, encoding=\"utf-8\")\n    except (ValueError, RFC3986Exception):\n        six.raise_from(LocationParseError(url), None)\n\n    def idna_encode(name):\n        if name and any([ord(x) > 128 for x in name]):\n            try:\n                import idna\n            except ImportError:\n                raise LocationParseError(\"Unable to parse URL without the 'idna' module\")\n            try:\n                return idna.encode(name.lower(), strict=True, std3_rules=True)\n            except idna.IDNAError:\n                raise LocationParseError(u\"Name '%s' is not a valid IDNA label\" % name)\n        return name\n\n    has_authority = iri_ref.authority is not None\n    uri_ref = iri_ref.encode(idna_encoder=idna_encode)\n\n    # rfc3986 strips the authority if it's invalid\n    if has_authority and uri_ref.authority is None:\n        raise LocationParseError(url)\n\n    # Only normalize schemes we understand to not break http+unix\n    # or other schemes that don't follow RFC 3986.\n    if uri_ref.scheme is None or uri_ref.scheme.lower() in NORMALIZABLE_SCHEMES:\n        uri_ref = uri_ref.normalize()\n\n    # Validate all URIReference components and ensure that all\n    # components that were set before are still set after\n    # normalization has completed.\n    validator = Validator()\n    try:\n        validator.check_validity_of(\n            *validator.COMPONENT_NAMES\n        ).validate(uri_ref)\n    except ValidationError:\n        six.raise_from(LocationParseError(url), None)\n\n    # For the sake of backwards compatibility we put empty\n    # string values for path if there are any defined values\n    # beyond the path in the URL.\n    # TODO: Remove this when we break backwards compatibility.\n    path = uri_ref.path\n    if not path:\n        if (uri_ref.query is not None\n                or uri_ref.fragment is not None):\n            path = \"\"\n        else:\n            path = None\n\n    # Ensure that each part of the URL is a `str` for\n    # backwards compatibility.\n    def to_input_type(x):\n        if x is None:\n            return None\n        elif not is_string and not isinstance(x, six.binary_type):\n            return x.encode('utf-8')\n        return x\n\n    return Url(\n        scheme=to_input_type(uri_ref.scheme),\n        auth=to_input_type(uri_ref.userinfo),\n        host=to_input_type(uri_ref.host),\n        port=int(uri_ref.port) if uri_ref.port is not None else None,\n        path=to_input_type(path),\n        query=to_input_type(uri_ref.query),\n        fragment=to_input_type(uri_ref.fragment)\n    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_many(self, data, timeout=DEFAULT_TIMEOUT, version=None):\n        safe_data = {}\n        original_keys = {}\n        for key, value in data.items():\n            safe_key = self.make_key(key, version=version)\n            safe_data[safe_key] = value\n            original_keys[safe_key] = key\n        failed_keys = self._cache.set_multi(safe_data, self.get_backend_timeout(timeout))\n        return [original_keys[k] for k in failed_keys]\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def get(self, request: web.Request) -> web.Response:\n        \"\"\"Return a list of persons if request comes from a local IP.\"\"\"\n        try:\n            remote_address = ip_address(request.remote)  # type: ignore[arg-type]\n        except ValueError:\n            return self.json_message(\n                message=\"Invalid remote IP\",\n                status_code=HTTPStatus.BAD_REQUEST,\n                message_code=\"invalid_remote_ip\",\n            )\n\n        hass: HomeAssistant = request.app[\"hass\"]\n        if is_cloud_connection(hass) or not is_local(remote_address):\n            return self.json_message(\n                message=\"Not local\",\n                status_code=HTTPStatus.BAD_REQUEST,\n                message_code=\"not_local\",\n            )\n\n        yaml, storage, _ = hass.data[DOMAIN]\n        persons = [*yaml.async_items(), *storage.async_items()]\n\n        return self.json(\n            {\n                person[ATTR_USER_ID]: {\n                    ATTR_NAME: person[ATTR_NAME],\n                    CONF_PICTURE: person.get(CONF_PICTURE),\n                }\n                for person in persons\n                if person.get(ATTR_USER_ID)\n            }\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_file_transfer_type(sql: str) -> FileTransferType | None:\n        \"\"\"Decide whether a SQL is a file transfer and return its type.\n\n        None is returned if the SQL isn't a file transfer so that this function can be\n        used in an if-statement.\n        \"\"\"\n        if SnowflakeCursor.PUT_SQL_RE.match(sql):\n            return FileTransferType.PUT\n        elif SnowflakeCursor.GET_SQL_RE.match(sql):\n            return FileTransferType.GET\n        return None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def explain(self, using, format=None, **options):\n        q = self.clone()\n        q.explain_info = ExplainInfo(format, options)\n        compiler = q.get_compiler(using=using)\n        return \"\\n\".join(compiler.explain_query())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _generate_kwarg_handlers(context: Context, sig: FunctionSignature) -> List[Any]:\n    # generate kwarg handlers.\n    # since they might come in thru calldata or be default,\n    # allocate them in memory and then fill it in based on calldata or default,\n    # depending on the signature\n    # a kwarg handler looks like\n    # (if (eq _method_id <method_id>)\n    #    copy calldata args to memory\n    #    write default args to memory\n    #    goto external_function_common_ir\n\n    def handler_for(calldata_kwargs, default_kwargs):\n        calldata_args = sig.base_args + calldata_kwargs\n        # create a fake type so that get_element_ptr works\n        calldata_args_t = TupleT(list(arg.typ for arg in calldata_args))\n\n        abi_sig = sig.abi_signature_for_kwargs(calldata_kwargs)\n        method_id = _annotated_method_id(abi_sig)\n\n        calldata_kwargs_ofst = IRnode(\n            4, location=CALLDATA, typ=calldata_args_t, encoding=Encoding.ABI\n        )\n\n        # a sequence of statements to strictify kwargs into memory\n        ret = [\"seq\"]\n\n        # ensure calldata is at least of minimum length\n        args_abi_t = calldata_args_t.abi_type\n        calldata_min_size = args_abi_t.min_size() + 4\n        ret.append([\"assert\", [\"ge\", \"calldatasize\", calldata_min_size]])\n\n        # TODO optimize make_setter by using\n        # TupleT(list(arg.typ for arg in calldata_kwargs + default_kwargs))\n        # (must ensure memory area is contiguous)\n\n        n_base_args = len(sig.base_args)\n\n        for i, arg_meta in enumerate(calldata_kwargs):\n            k = n_base_args + i\n\n            dst = context.lookup_var(arg_meta.name).pos\n\n            lhs = IRnode(dst, location=MEMORY, typ=arg_meta.typ)\n\n            rhs = get_element_ptr(calldata_kwargs_ofst, k, array_bounds_check=False)\n\n            copy_arg = make_setter(lhs, rhs)\n            copy_arg.source_pos = getpos(arg_meta.ast_source)\n            ret.append(copy_arg)\n\n        for x in default_kwargs:\n            dst = context.lookup_var(x.name).pos\n            lhs = IRnode(dst, location=MEMORY, typ=x.typ)\n            lhs.source_pos = getpos(x.ast_source)\n            kw_ast_val = sig.default_values[x.name]  # e.g. `3` in x: int = 3\n            rhs = Expr(kw_ast_val, context).ir_node\n\n            copy_arg = make_setter(lhs, rhs)\n            copy_arg.source_pos = getpos(x.ast_source)\n            ret.append(copy_arg)\n\n        ret.append([\"goto\", sig.external_function_base_entry_label])\n\n        ret = [\"if\", [\"eq\", \"_calldata_method_id\", method_id], ret]\n        return ret\n\n    ret = [\"seq\"]\n\n    keyword_args = sig.default_args\n\n    # allocate variable slots in memory\n    for arg in keyword_args:\n        context.new_variable(arg.name, arg.typ, is_mutable=False)\n\n    for i, _ in enumerate(keyword_args):\n        calldata_kwargs = keyword_args[:i]\n        default_kwargs = keyword_args[i:]\n\n        ret.append(handler_for(calldata_kwargs, default_kwargs))\n\n    ret.append(handler_for(keyword_args, []))\n\n    return ret\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _connect_tls_proxy(self, hostname, conn):\n        \"\"\"\n        Establish a TLS connection to the proxy using the provided SSL context.\n        \"\"\"\n        proxy_config = self.proxy_config\n        ssl_context = proxy_config.ssl_context\n        if ssl_context:\n            # If the user provided a proxy context, we assume CA and client\n            # certificates have already been set\n            return ssl_wrap_socket(\n                sock=conn,\n                server_hostname=hostname,\n                ssl_context=ssl_context,\n            )\n\n        ssl_context = create_proxy_ssl_context(\n            self.ssl_version,\n            self.cert_reqs,\n            self.ca_certs,\n            self.ca_cert_dir,\n            self.ca_cert_data,\n        )\n\n        # If no cert was provided, use only the default options for server\n        # certificate validation\n        return ssl_wrap_socket(\n            sock=conn,\n            ca_certs=self.ca_certs,\n            ca_cert_dir=self.ca_cert_dir,\n            ca_cert_data=self.ca_cert_data,\n            server_hostname=hostname,\n            ssl_context=ssl_context,\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static Status ValidateSavedTensors(const GraphDef& graph_def) {\n  for (const auto& node : graph_def.node()) {\n    const auto node_iterator = node.attr().find(\"value\");\n    if (node_iterator != node.attr().end()) {\n      AttrValue node_value = node_iterator->second;\n      if (node_value.has_tensor()) {\n        const PartialTensorShape node_shape(node_value.tensor().tensor_shape());\n        if (node_shape.num_elements() < 0) {\n          return errors::FailedPrecondition(\n              \"Saved model contains node \\\"\", node.name(), \"\\\" (op \\\"\",\n              node.op(), \"\\\") which initializes from a tensor with \",\n              node_shape.num_elements(), \" elements\");\n        }\n      }\n    } else if (node.op() == \"Const\") {\n      return errors::FailedPrecondition(\n          \"Saved model contains node \\\"\", node.name(),\n          \"\\\" which is a constant tensor but no value has been provided\");\n    }\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& data = ctx->input(0);\n    OP_REQUIRES(ctx, data.dims() <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", data.dims()));\n\n    const Tensor& size_t = ctx->input(1);\n    const Tensor& weights = ctx->input(2);\n\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (data.dims() == 1) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                     ctx, data.flat<Tidx>(), weights.flat<T>(), out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, data.flat<Tidx>(), weights.flat<T>(), out, size));\n      }\n    } else if (data.dims() == 2) {\n      const int64_t num_rows = data.dim_size(0);\n      auto weight_matrix =\n          (weights.NumElements() == 0)\n              ? weights.shaped<T, 2>(gtl::InlinedVector<int64_t, 2>(2, 0))\n              : weights.matrix<T>();\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      if (binary_output_) {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountReduceFunctor<Device, Tidx, T, true>::Compute(\n                     ctx, data.matrix<Tidx>(), weight_matrix, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx,\n            functor::BincountReduceFunctor<Device, Tidx, T, false>::Compute(\n                ctx, data.matrix<Tidx>(), weight_matrix, out, size));\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "            self.confirmChangePassword = function () {\n                if (!CONFIG_ACCESS_CONTROL) return;\n\n                self.updatePassword(self.currentUser().name, self.editor.password()).done(\n                    function () {\n                        // close dialog\n                        self.currentUser(undefined);\n                        self.changePasswordDialog.modal(\"hide\");\n                    }\n                );\n            };\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def clear(self):\n        dag_id = request.form.get('dag_id')\n        task_id = request.form.get('task_id')\n        origin = request.form.get('origin')\n        dag = current_app.dag_bag.get_dag(dag_id)\n\n        execution_date = request.form.get('execution_date')\n        execution_date = timezone.parse(execution_date)\n        confirmed = request.form.get('confirmed') == \"true\"\n        upstream = request.form.get('upstream') == \"true\"\n        downstream = request.form.get('downstream') == \"true\"\n        future = request.form.get('future') == \"true\"\n        past = request.form.get('past') == \"true\"\n        recursive = request.form.get('recursive') == \"true\"\n        only_failed = request.form.get('only_failed') == \"true\"\n\n        dag = dag.sub_dag(\n            task_regex=r\"^{0}$\".format(task_id),\n            include_downstream=downstream,\n            include_upstream=upstream)\n\n        end_date = execution_date if not future else None\n        start_date = execution_date if not past else None\n\n        return self._clear_dag_tis(dag, start_date, end_date, origin,\n                                   recursive=recursive, confirmed=confirmed, only_failed=only_failed)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def authenticate(template, title):\n    \"\"\"\n    Authentication management method.\n    :param template: the template name that should be rendered\n    :returns: rendered final page or a page with error message\n    \"\"\"\n    global AUTH_ISSUERS, SAML_SUPPORT, AUTH_TYPE, COOKIES\n    valid_token_dict = validate_webui_token()\n    if not valid_token_dict:\n        COOKIES.append({'key': 'rucio-requested-path', 'value': request.environ.get('REQUEST_URI')})\n    else:\n        return access_granted(valid_token_dict, template, title)\n\n    # login without any known server config\n    if not AUTH_TYPE:\n        return add_cookies(make_response(render_template(\"select_login_method.html\", oidc_issuers=AUTH_ISSUERS, saml_support=SAML_SUPPORT)))\n    # for AUTH_TYPE predefined by the server continue\n    else:\n        if AUTH_TYPE == 'userpass':\n            return redirect('/login', code=303)\n        elif AUTH_TYPE == 'x509':\n            return x509token_auth(None)\n        elif AUTH_TYPE == 'x509_userpass':\n            if request.environ.get('SSL_CLIENT_VERIFY') == 'SUCCESS':\n                return x509token_auth(None)\n            return render_template(\"no_certificate.html\")\n        elif AUTH_TYPE == 'oidc':\n            return oidc_auth(None, AUTH_ISSUER_WEBUI)\n        elif AUTH_TYPE == 'saml':\n            return saml_auth(\"GET\")\n        return render_template('problem.html', msg='Invalid auth type')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def validate_absolute_path(self, root: str, absolute_path: str) -> Optional[str]:\n        \"\"\"Validate and return the absolute path.\n\n        ``root`` is the configured path for the `StaticFileHandler`,\n        and ``path`` is the result of `get_absolute_path`\n\n        This is an instance method called during request processing,\n        so it may raise `HTTPError` or use methods like\n        `RequestHandler.redirect` (return None after redirecting to\n        halt further processing).  This is where 404 errors for missing files\n        are generated.\n\n        This method may modify the path before returning it, but note that\n        any such modifications will not be understood by `make_static_url`.\n\n        In instance methods, this method's result is available as\n        ``self.absolute_path``.\n\n        .. versionadded:: 3.1\n        \"\"\"\n        # os.path.abspath strips a trailing /.\n        # We must add it back to `root` so that we only match files\n        # in a directory named `root` instead of files starting with\n        # that prefix.\n        root = os.path.abspath(root)\n        if not root.endswith(os.path.sep):\n            # abspath always removes a trailing slash, except when\n            # root is '/'. This is an unusual case, but several projects\n            # have independently discovered this technique to disable\n            # Tornado's path validation and (hopefully) do their own,\n            # so we need to support it.\n            root += os.path.sep\n        # The trailing slash also needs to be temporarily added back\n        # the requested path so a request to root/ will match.\n        if not (absolute_path + os.path.sep).startswith(root):\n            raise HTTPError(403, \"%s is not in root static directory\", self.path)\n        if os.path.isdir(absolute_path) and self.default_filename is not None:\n            # need to look at the request.path here for when path is empty\n            # but there is some prefix to the path that was already\n            # trimmed by the routing\n            if not self.request.path.endswith(\"/\"):\n                self.redirect(self.request.path + \"/\", permanent=True)\n                return None\n            absolute_path = os.path.join(absolute_path, self.default_filename)\n        if not os.path.exists(absolute_path):\n            raise HTTPError(404)\n        if not os.path.isfile(absolute_path):\n            raise HTTPError(403, \"%s is not a file\", self.path)\n        return absolute_path\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _make_cmd(self, tmpfilename, info_dict):\n        cmd = [self.exe, '-O', tmpfilename, '-nv', '--no-cookies', '--compression=auto']\n        if info_dict.get('http_headers') is not None:\n            for key, val in info_dict['http_headers'].items():\n                cmd += ['--header', f'{key}: {val}']\n        cmd += self._option('--limit-rate', 'ratelimit')\n        retry = self._option('--tries', 'retries')\n        if len(retry) == 2:\n            if retry[1] in ('inf', 'infinite'):\n                retry[1] = '0'\n            cmd += retry\n        cmd += self._option('--bind-address', 'source_address')\n        proxy = self.params.get('proxy')\n        if proxy:\n            for var in ('http_proxy', 'https_proxy'):\n                cmd += ['--execute', f'{var}={proxy}']\n        cmd += self._valueless_option('--no-check-certificate', 'nocheckcertificate')\n        cmd += self._configuration_args()\n        cmd += ['--', info_dict['url']]\n        return cmd\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def model_from_yaml(yaml_string, custom_objects=None):\n  \"\"\"Parses a yaml model configuration file and returns a model instance.\n\n  Usage:\n\n  >>> model = tf.keras.Sequential([\n  ...     tf.keras.layers.Dense(5, input_shape=(3,)),\n  ...     tf.keras.layers.Softmax()])\n  >>> try:\n  ...   import yaml\n  ...   config = model.to_yaml()\n  ...   loaded_model = tf.keras.models.model_from_yaml(config)\n  ... except ImportError:\n  ...   pass\n\n  Args:\n      yaml_string: YAML string or open file encoding a model configuration.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n\n  Returns:\n      A Keras model instance (uncompiled).\n\n  Raises:\n      ImportError: if yaml module is not found.\n  \"\"\"\n  if yaml is None:\n    raise ImportError('Requires yaml module installed (`pip install pyyaml`).')\n  # The method unsafe_load only exists in PyYAML 5.x+, so which branch of the\n  # try block is covered by tests depends on the installed version of PyYAML.\n  try:\n    # PyYAML 5.x+\n    config = yaml.unsafe_load(yaml_string)\n  except AttributeError:\n    config = yaml.load(yaml_string)\n  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\n  return deserialize(config, custom_objects=custom_objects)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def handle_html_static(self, request):\n        \"\"\"Handle static html.\"\"\"\n        if not (page := request.path[1:]):\n            page = \"index.html\"\n        file = os.path.join(self.web_path, page)\n        try:\n            with open(file, encoding=\"utf-8\"):\n                return web.FileResponse(file)\n        except (FileNotFoundError, IsADirectoryError) as exc:\n            raise web.HTTPNotFound(reason=\"File not found\") from exc\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void DoImageProjectiveTransformOp(OpKernelContext* ctx,\n                                  const Interpolation& interpolation,\n                                  const Mode& fill_mode) {\n  const Tensor& images_t = ctx->input(0);\n  const Tensor& transform_t = ctx->input(1);\n  OP_REQUIRES(ctx, images_t.shape().dims() == 4,\n              errors::InvalidArgument(\"Input images must have rank 4\"));\n  OP_REQUIRES(ctx,\n              (TensorShapeUtils::IsMatrix(transform_t.shape()) &&\n               (transform_t.dim_size(0) == images_t.dim_size(0) ||\n                transform_t.dim_size(0) == 1) &&\n               transform_t.dim_size(1) == 8),\n              errors::InvalidArgument(\n                  \"Input transform should be num_images x 8 or 1 x 8\"));\n\n  int32_t out_height, out_width;\n  // Kernel is shared by legacy \"ImageProjectiveTransform\" op with 2 args.\n  if (ctx->num_inputs() >= 3) {\n    const Tensor& shape_t = ctx->input(2);\n    OP_REQUIRES(ctx, shape_t.dims() == 1,\n                errors::InvalidArgument(\"output shape must be 1-dimensional\",\n                                        shape_t.shape().DebugString()));\n    OP_REQUIRES(ctx, shape_t.NumElements() == 2,\n                errors::InvalidArgument(\"output shape must have two elements\",\n                                        shape_t.shape().DebugString()));\n    auto shape_vec = shape_t.vec<int32>();\n    out_height = shape_vec(0);\n    out_width = shape_vec(1);\n    OP_REQUIRES(ctx, out_height > 0 && out_width > 0,\n                errors::InvalidArgument(\"output dimensions must be positive\"));\n  } else {\n    // Shape is N (batch size), H (height), W (width), C (channels).\n    out_height = images_t.shape().dim_size(1);\n    out_width = images_t.shape().dim_size(2);\n  }\n\n  T fill_value(0);\n  // Kernel is shared by \"ImageProjectiveTransformV2\" with 3 args.\n  if (ctx->num_inputs() >= 4) {\n    const Tensor& fill_value_t = ctx->input(3);\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(fill_value_t.shape()),\n                errors::InvalidArgument(\"fill_value must be a scalar\",\n                                        fill_value_t.shape().DebugString()));\n    fill_value = static_cast<T>(*(fill_value_t.scalar<float>().data()));\n  }\n\n  Tensor* output_t;\n  OP_REQUIRES_OK(\n      ctx, ctx->allocate_output(0,\n                                TensorShape({images_t.dim_size(0), out_height,\n                                             out_width, images_t.dim_size(3)}),\n                                &output_t));\n  auto output = output_t->tensor<T, 4>();\n  auto images = images_t.tensor<T, 4>();\n  auto transform = transform_t.matrix<float>();\n\n  (FillProjectiveTransform<Device, T>(interpolation))(\n      ctx->eigen_device<Device>(), &output, images, transform, fill_mode,\n      fill_value);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse_header(self, header_plus):\n        \"\"\"\n        Parses the header_plus block of text (the headers plus the\n        first line of the request).\n        \"\"\"\n        index = header_plus.find(b\"\\n\")\n        if index >= 0:\n            first_line = header_plus[:index].rstrip()\n            header = header_plus[index + 1 :]\n        else:\n            first_line = header_plus.rstrip()\n            header = b\"\"\n\n        self.first_line = first_line  # for testing\n\n        lines = get_header_lines(header)\n\n        headers = self.headers\n        for line in lines:\n            index = line.find(b\":\")\n            if index > 0:\n                key = line[:index]\n                if b\"_\" in key:\n                    continue\n                value = line[index + 1 :].strip()\n                key1 = tostr(key.upper().replace(b\"-\", b\"_\"))\n                # If a header already exists, we append subsequent values\n                # seperated by a comma. Applications already need to handle\n                # the comma seperated values, as HTTP front ends might do\n                # the concatenation for you (behavior specified in RFC2616).\n                try:\n                    headers[key1] += tostr(b\", \" + value)\n                except KeyError:\n                    headers[key1] = tostr(value)\n            # else there's garbage in the headers?\n\n        # command, uri, version will be bytes\n        command, uri, version = crack_first_line(first_line)\n        version = tostr(version)\n        command = tostr(command)\n        self.command = command\n        self.version = version\n        (\n            self.proxy_scheme,\n            self.proxy_netloc,\n            self.path,\n            self.query,\n            self.fragment,\n        ) = split_uri(uri)\n        self.url_scheme = self.adj.url_scheme\n        connection = headers.get(\"CONNECTION\", \"\")\n\n        if version == \"1.0\":\n            if connection.lower() != \"keep-alive\":\n                self.connection_close = True\n\n        if version == \"1.1\":\n            # since the server buffers data from chunked transfers and clients\n            # never need to deal with chunked requests, downstream clients\n            # should not see the HTTP_TRANSFER_ENCODING header; we pop it\n            # here\n            te = headers.pop(\"TRANSFER_ENCODING\", \"\")\n            if te.lower() == \"chunked\":\n                self.chunked = True\n                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n                self.body_rcv = ChunkedReceiver(buf)\n            expect = headers.get(\"EXPECT\", \"\").lower()\n            self.expect_continue = expect == \"100-continue\"\n            if connection.lower() == \"close\":\n                self.connection_close = True\n\n        if not self.chunked:\n            try:\n                cl = int(headers.get(\"CONTENT_LENGTH\", 0))\n            except ValueError:\n                cl = 0\n            self.content_length = cl\n            if cl > 0:\n                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n                self.body_rcv = FixedStreamReceiver(cl, buf)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_session_id_by_provider_id(self, provider_id):\n        \"\"\"\n        Return a tuple of logged-in session IDs by provider_id\n        \"\"\"\n        candidates = self._ss.get_user_sessions(self.user)\n\n        session_ids = []\n        for c in candidates:\n            key = c.keys()[0]\n            if c[key].get('provider_id') == provider_id:\n                samlsession = self._data_to_samlsession(key, c[key])\n                session_ids.append(samlsession.session_id.encode('utf-8'))\n\n        return tuple(session_ids)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _keyify(key):\n    return _key_pattern.sub(' ', key.lower())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def events(request):\n   \"\"\"\n   This page presents a list of all the events and links to the event log viewer.\n   \"\"\"\n   if not test_user_authenticated(request): return login(request, next=\"/cobbler_web/events\")\n   events = remote.get_events()\n\n   events2 = []\n   for id in events.keys():\n      (ttime, name, state, read_by) = events[id]\n      events2.append([id,time.asctime(time.localtime(ttime)),name,state])\n\n   def sorter(a,b):\n      return cmp(a[0],b[0])\n   events2.sort(sorter)\n\n   t = get_template('events.tmpl')\n   html = t.render(Context({\n       'results'  : events2,\n       'version'  : remote.version(request.session['token']),\n       'username' : username\n   }))\n   return HttpResponse(html)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _build_write_cert_pair(self, participant, base_name, ctx):\n        subject = participant.subject\n        if self.persistent_state and subject in self.persistent_state:\n            cert = x509.load_pem_x509_certificate(self.persistent_state[subject][\"cert\"], default_backend())\n            pri_key = serialization.load_pem_private_key(\n                self.persistent_state[subject][\"pri_key\"], password=None, backend=default_backend()\n            )\n        else:\n            pri_key, cert = self.get_pri_key_cert(participant)\n            self.persistent_state[subject] = dict(cert=serialize_cert(cert), pri_key=serialize_pri_key(pri_key))\n        dest_dir = self.get_kit_dir(participant, ctx)\n        with open(os.path.join(dest_dir, f\"{base_name}.crt\"), \"wb\") as f:\n            f.write(serialize_cert(cert))\n        with open(os.path.join(dest_dir, f\"{base_name}.key\"), \"wb\") as f:\n            f.write(serialize_pri_key(pri_key))\n        pkcs12 = serialization.pkcs12.serialize_key_and_certificates(\n            subject.encode(\"utf-8\"), pri_key, cert, None, serialization.BestAvailableEncryption(subject.encode(\"utf-8\"))\n        )\n        with open(os.path.join(dest_dir, f\"{base_name}.pfx\"), \"wb\") as f:\n            f.write(pkcs12)\n        with open(os.path.join(dest_dir, \"rootCA.pem\"), \"wb\") as f:\n            f.write(self.serialized_cert)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& indices = context->input(0);\n    const Tensor& values = context->input(1);\n    const Tensor& shape = context->input(2);\n    const Tensor& weights = context->input(3);\n    bool use_weights = weights.NumElements() > 0;\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(indices.shape()),\n                errors::InvalidArgument(\n                    \"Input indices must be a 2-dimensional tensor. Got: \",\n                    indices.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(values.shape()),\n                errors::InvalidArgument(\"Input values must be a vector. Got: \",\n                                        values.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(shape.shape()),\n                errors::InvalidArgument(\"Input shape must be a vector. Got: \",\n                                        shape.shape().DebugString()));\n    OP_REQUIRES(context,\n                values.shape().dim_size(0) == indices.shape().dim_size(0),\n                errors::InvalidArgument(\n                    \"Number of values must match first dimension of indices.\",\n                    \"Got \", values.shape().dim_size(0),\n                    \" values, indices shape: \", indices.shape().DebugString()));\n    OP_REQUIRES(\n        context, shape.shape().dim_size(0) == indices.shape().dim_size(1),\n        errors::InvalidArgument(\n            \"Number of dimensions must match second dimension of indices.\",\n            \"Got \", shape.shape().dim_size(0),\n            \" dimensions, indices shape: \", indices.shape().DebugString()));\n    OP_REQUIRES(context, shape.NumElements() > 0,\n                errors::InvalidArgument(\n                    \"The shape argument requires at least one element.\"));\n\n    if (use_weights) {\n      OP_REQUIRES(\n          context, weights.shape() == values.shape(),\n          errors::InvalidArgument(\n              \"Weights and values must have the same shape. Weight shape: \",\n              weights.shape().DebugString(),\n              \"; values shape: \", values.shape().DebugString()));\n    }\n\n    bool is_1d = shape.NumElements() == 1;\n    auto shape_vector = shape.flat<int64_t>();\n    int num_batches = is_1d ? 1 : shape_vector(0);\n    int num_values = values.NumElements();\n\n    const auto indices_values = indices.matrix<int64_t>();\n    const auto values_values = values.flat<T>();\n    const auto weight_values = weights.flat<W>();\n\n    auto per_batch_counts = BatchedMap<W>(num_batches);\n\n    T max_value = 0;\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      int batch = is_1d ? 0 : indices_values(idx, 0);\n      if (batch >= num_batches) {\n        OP_REQUIRES(context, batch < num_batches,\n                    errors::InvalidArgument(\n                        \"Indices value along the first dimension must be \",\n                        \"lower than the first index of the shape.\", \"Got \",\n                        batch, \" as batch and \", num_batches,\n                        \" as the first dimension of the shape.\"));\n      }\n      const auto& value = values_values(idx);\n      if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {\n        if (binary_output_) {\n          per_batch_counts[batch][value] = 1;\n        } else if (use_weights) {\n          per_batch_counts[batch][value] += weight_values(idx);\n        } else {\n          per_batch_counts[batch][value]++;\n        }\n        if (value > max_value) {\n          max_value = value;\n        }\n      }\n    }\n\n    int num_output_values = GetOutputSize(max_value, maxlength_, minlength_);\n    OP_REQUIRES_OK(context, OutputSparse<W>(per_batch_counts, num_output_values,\n                                            is_1d, context));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def setup(self, task_metadata: Dict) -> None:\n        \"\"\"\n        Invokes Terraform to provision supporting resources for the instance\n\n        \"\"\"\n\n        state_file = self._get_tf_statefile_path(task_metadata)\n\n        # Init Terraform\n        await self._run_async_subprocess([\"terraform\", \"init\"], cwd=self._TF_DIR, log_output=True)\n\n        # Apply Terraform Plan\n        base_cmd = [\n            \"terraform\",\n            \"apply\",\n            \"-auto-approve\",\n            f\"-state={state_file}\",\n        ]\n\n        self.infra_vars = [\n            \"-var=name=covalent-task-{dispatch_id}-{node_id}\".format(\n                dispatch_id=task_metadata[\"dispatch_id\"],\n                node_id=task_metadata[\"node_id\"],\n            ),\n            f\"-var=instance_type={self.instance_type}\",\n            f\"-var=disk_size={self.volume_size}\",\n            f\"-var=key_file={self.ssh_key_file}\",\n            f\"-var=key_name={self.key_name}\",\n        ]\n\n        if self.region:\n            self.infra_vars += [\n                f\"-var=aws_region={self.region}\",\n            ]\n\n        if self.profile:\n            self.infra_vars += [f\"-var=aws_profile={self.profile}\"]\n\n        if self.credentials_file:\n            self.infra_vars += [f\"-var=aws_credentials={self.credentials_file}\"]\n\n        if self.vpc:\n            self.infra_vars += [f\"-var=vpc_id={self.vpc}\"]\n        if self.subnet:\n            self.infra_vars += [f\"-var=subnet_id={self.subnet}\"]\n\n        cmd = base_cmd + self.infra_vars\n\n        app_log.debug(f\"Running Terraform setup command: {cmd}\")\n\n        await self._run_async_subprocess(cmd, cwd=self._TF_DIR, log_output=True)\n\n        # Get Hostname from Terraform Output\n        self.hostname = await self._get_tf_output(\"hostname\", state_file)\n\n        # Get Username from Terraform Output\n        self.username = await self._get_tf_output(\"username\", state_file)\n\n        # Get Remote Cache from Terraform Output\n        self.remote_cache = await self._get_tf_output(\"remote_cache\", state_file)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def post(self, request, *args, **kwargs):\n        \"\"\"\n        Handles POST request.\n        \"\"\"\n        return_url = request.POST.get(\"returnTo\", \"/\")\n        terms_ids = request.POST.getlist(\"terms\")\n\n        parsed = urlparse(return_url)\n        if parsed.hostname and parsed.hostname not in settings.ALLOWED_HOSTS:\n            # Make sure the return url is a relative path or a trusted hostname\n            return_url = '/'\n\n        if not terms_ids:  # pragma: nocover\n            return HttpResponseRedirect(return_url)\n\n        if request.user.is_authenticated:\n            user = request.user\n        else:\n            # Get user out of saved pipeline from django-socialauth\n            if \"partial_pipeline\" in request.session:\n                user_pk = request.session[\"partial_pipeline\"][\"kwargs\"][\"user\"][\"pk\"]\n                user = User.objects.get(id=user_pk)\n            else:\n                return HttpResponseRedirect(\"/\")\n\n        store_ip_address = getattr(settings, \"TERMS_STORE_IP_ADDRESS\", True)\n        if store_ip_address:\n            ip_address = request.META.get(\n                getattr(settings, \"TERMS_IP_HEADER_NAME\", DEFAULT_TERMS_IP_HEADER_NAME)\n            )\n            if \",\" in ip_address:\n                ip_address = ip_address.split(\",\")[0].strip()\n        else:\n            ip_address = \"\"\n\n        for terms_id in terms_ids:\n            try:\n                new_user_terms = UserTermsAndConditions(\n                    user=user,\n                    terms=TermsAndConditions.objects.get(pk=int(terms_id)),\n                    ip_address=ip_address,\n                )\n                new_user_terms.save()\n            except IntegrityError:  # pragma: nocover\n                pass\n\n        return HttpResponseRedirect(return_url)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, url):\n        Error.__init__(self, 302, \"redirect\")\n        self.url = url\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    auto x = ctx->input(0);\n    auto i = ctx->input(1);\n    auto v = ctx->input(2);\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(i.shape()),\n                errors::InvalidArgument(\"i must be a vector. \",\n                                        i.shape().DebugString()));\n    OP_REQUIRES(ctx, x.dims() == v.dims(),\n                errors::InvalidArgument(\n                    \"x and v shape doesn't match (ranks differ): \",\n                    x.shape().DebugString(), \" vs. \", v.shape().DebugString()));\n    for (int i = 1; i < x.dims(); ++i) {\n      OP_REQUIRES(\n          ctx, x.dim_size(i) == v.dim_size(i),\n          errors::InvalidArgument(\"x and v shape doesn't match at index \", i,\n                                  \" : \", x.shape().DebugString(), \" vs. \",\n                                  v.shape().DebugString()));\n    }\n    OP_REQUIRES(ctx, i.dim_size(0) == v.dim_size(0),\n                errors::InvalidArgument(\n                    \"i and x shape doesn't match at index 0: \",\n                    i.shape().DebugString(), \" vs. \", v.shape().DebugString()));\n\n    Tensor y = x;  // This creates an alias intentionally.\n    // Skip processing if tensors are empty.\n    if (x.NumElements() > 0 || v.NumElements() > 0) {\n      OP_REQUIRES_OK(ctx, DoCompute(ctx, i, v, &y));\n    }\n    ctx->set_output(0, y);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self):\n        if not self.should_run():\n            print(\"bower dependencies up to date\")\n            return\n        \n        if self.should_run_npm():\n            print(\"installing build dependencies with npm\")\n            run(['npm', 'install'], cwd=repo_root)\n            os.utime(self.node_modules, None)\n        \n        env = os.environ.copy()\n        env['PATH'] = npm_path\n        \n        try:\n            run(\n                ['bower', 'install', '--allow-root', '--config.interactive=false'],\n                cwd=repo_root,\n                env=env\n            )\n        except OSError as e:\n            print(\"Failed to run bower: %s\" % e, file=sys.stderr)\n            print(\"You can install js dependencies with `npm install`\", file=sys.stderr)\n            raise\n        # self.npm_components()\n        os.utime(self.bower_dir, None)\n        # update package data in case this created new files\n        update_package_data(self.distribution)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def complete_reg(request):\n    \"\"\"Completes the registeration, called by API\"\"\"\n    try:\n        data = cbor.decode(request.body)\n\n        client_data = CollectedClientData(data['clientDataJSON'])\n        att_obj = AttestationObject((data['attestationObject']))\n        server = getServer()\n        auth_data = server.register_complete(\n            request.session['fido_state'],\n            client_data,\n            att_obj\n        )\n        encoded = websafe_encode(auth_data.credential_data)\n        uk = User_Keys()\n        uk.username = request.user.username\n        uk.properties = {\"device\": encoded, \"type\": att_obj.fmt, }\n        uk.owned_by_enterprise = getattr(settings, \"MFA_OWNED_BY_ENTERPRISE\", False)\n        uk.key_type = \"FIDO2\"\n        uk.save()\n        return HttpResponse(simplejson.dumps({'status': 'OK'}))\n    except Exception as exp:\n        import traceback\n        print(traceback.format_exc())\n        try:\n            from raven.contrib.django.raven_compat.models import client\n            client.captureException()\n        except:\n            pass\n        return HttpResponse(simplejson.dumps({'status': 'ERR', \"message\": \"Error on server, please try again later\"}))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def oauth_authorized(self, provider):\n        log.debug(\"Authorized init\")\n        resp = self.appbuilder.sm.oauth_remotes[provider].authorize_access_token()\n        if resp is None:\n            flash(u\"You denied the request to sign in.\", \"warning\")\n            return redirect(self.appbuilder.get_url_for_login)\n        log.debug(\"OAUTH Authorized resp: {0}\".format(resp))\n        # Retrieves specific user info from the provider\n        try:\n            self.appbuilder.sm.set_oauth_session(provider, resp)\n            userinfo = self.appbuilder.sm.oauth_user_info(provider, resp)\n        except Exception as e:\n            log.error(\"Error returning OAuth user info: {0}\".format(e))\n            user = None\n        else:\n            log.debug(\"User info retrieved from {0}: {1}\".format(provider, userinfo))\n            # User email is not whitelisted\n            if provider in self.appbuilder.sm.oauth_whitelists:\n                whitelist = self.appbuilder.sm.oauth_whitelists[provider]\n                allow = False\n                for e in whitelist:\n                    if re.search(e, userinfo[\"email\"]):\n                        allow = True\n                        break\n                if not allow:\n                    flash(u\"You are not authorized.\", \"warning\")\n                    return redirect(self.appbuilder.get_url_for_login)\n            else:\n                log.debug(\"No whitelist for OAuth provider\")\n            user = self.appbuilder.sm.auth_user_oauth(userinfo)\n\n        if user is None:\n            flash(as_unicode(self.invalid_login_message), \"warning\")\n            return redirect(self.appbuilder.get_url_for_login)\n        else:\n            login_user(user)\n            try:\n                state = jwt.decode(\n                    request.args[\"state\"],\n                    self.appbuilder.app.config[\"SECRET_KEY\"],\n                    algorithms=[\"HS256\"],\n                )\n            except jwt.InvalidTokenError:\n                raise Exception(\"State signature is not valid!\")\n\n            try:\n                next_url = state[\"next\"][0] or self.appbuilder.get_url_for_index\n            except (KeyError, IndexError):\n                next_url = self.appbuilder.get_url_for_index\n\n            return redirect(next_url)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void RestoreTensor(OpKernelContext* context,\n                   checkpoint::TensorSliceReader::OpenTableFunction open_func,\n                   int preferred_shard, bool restore_slice, int restore_index) {\n  const Tensor& file_pattern_t = context->input(0);\n  {\n    const int64_t size = file_pattern_t.NumElements();\n    OP_REQUIRES(\n        context, size == 1,\n        errors::InvalidArgument(\n            \"Input 0 (file_pattern) must be a string scalar; got a tensor of \",\n            size, \"elements\"));\n  }\n  const string& file_pattern = file_pattern_t.flat<tstring>()(0);\n\n  const Tensor& tensor_name_t = context->input(1);\n  const string& tensor_name = tensor_name_t.flat<tstring>()(restore_index);\n\n  // If we cannot find a cached reader we will allocate our own.\n  std::unique_ptr<checkpoint::TensorSliceReader> allocated_reader;\n\n  const checkpoint::TensorSliceReader* reader = nullptr;\n\n  if (context->slice_reader_cache()) {\n    reader = context->slice_reader_cache()->GetReader(file_pattern, open_func,\n                                                      preferred_shard);\n  }\n  if (!reader) {\n    allocated_reader.reset(new checkpoint::TensorSliceReader(\n        file_pattern, open_func, preferred_shard));\n    reader = allocated_reader.get();\n  }\n  OP_REQUIRES_OK(context, CHECK_NOTNULL(reader)->status());\n\n  // Get the shape and type from the save file.\n  DataType type;\n  TensorShape saved_shape;\n  OP_REQUIRES(\n      context, reader->HasTensor(tensor_name, &saved_shape, &type),\n      errors::NotFound(\"Tensor name \\\"\", tensor_name,\n                       \"\\\" not found in checkpoint files \", file_pattern));\n  OP_REQUIRES(\n      context, type == context->expected_output_dtype(restore_index),\n      errors::InvalidArgument(\"Expected to restore a tensor of type \",\n                              DataTypeString(context->expected_output_dtype(0)),\n                              \", got a tensor of type \", DataTypeString(type),\n                              \" instead: tensor_name = \", tensor_name));\n\n  // Shape of the output and slice to load.\n  TensorShape output_shape(saved_shape);\n  TensorSlice slice_to_load(saved_shape.dims());\n  if (restore_slice) {\n    const tstring& shape_spec =\n        context->input(2).flat<tstring>()(restore_index);\n    if (!shape_spec.empty()) {\n      TensorShape parsed_shape;\n      OP_REQUIRES_OK(context, checkpoint::ParseShapeAndSlice(\n                                  shape_spec, &parsed_shape, &slice_to_load,\n                                  &output_shape));\n      OP_REQUIRES(\n          context, parsed_shape.IsSameSize(saved_shape),\n          errors::InvalidArgument(\n              \"Shape in shape_and_slice spec does not match the shape in the \"\n              \"save file: \",\n              parsed_shape.DebugString(),\n              \", save file shape: \", saved_shape.DebugString()));\n    }\n  }\n\n  Tensor* t = nullptr;\n  OP_REQUIRES_OK(context,\n                 context->allocate_output(restore_index, output_shape, &t));\n\n  if (output_shape.num_elements() == 0) return;\n\n#define READER_COPY(T)                                                \\\n  case DataTypeToEnum<T>::value:                                      \\\n    OP_REQUIRES(context,                                              \\\n                reader->CopySliceData(tensor_name, slice_to_load,     \\\n                                      t->flat<T>().data()),           \\\n                errors::InvalidArgument(\"Error copying slice data\")); \\\n    break;\n\n  switch (type) {\n    TF_CALL_SAVE_RESTORE_TYPES(READER_COPY)\n    default:\n      context->SetStatus(errors::Unimplemented(\n          \"Restoring data type \", DataTypeString(type), \" not yet supported\"));\n  }\n#undef READER_COPY\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def validate_node_sign(signature_node, elem, cert=None, fingerprint=None, fingerprintalg='sha1', validatecert=False, debug=False):\n        \"\"\"\n        Validates a signature node.\n\n        :param signature_node: The signature node\n        :type: Node\n\n        :param xml: The element we should validate\n        :type: Document\n\n        :param cert: The public cert\n        :type: string\n\n        :param fingerprint: The fingerprint of the public cert\n        :type: string\n\n        :param fingerprintalg: The algorithm used to build the fingerprint\n        :type: string\n\n        :param validatecert: If true, will verify the signature and if the cert is valid.\n        :type: bool\n\n        :param debug: Activate the xmlsec debug\n        :type: bool\n\n        :param raise_exceptions: Whether to return false on failure or raise an exception\n        :type raise_exceptions: Boolean\n        \"\"\"\n        error_callback_method = None\n        if debug:\n            error_callback_method = print_xmlsec_errors\n        xmlsec.set_error_callback(error_callback_method)\n\n        xmlsec.addIDs(elem, [\"ID\"])\n\n        if (cert is None or cert == '') and fingerprint:\n            x509_certificate_nodes = OneLogin_Saml2_Utils.query(signature_node, '//ds:Signature/ds:KeyInfo/ds:X509Data/ds:X509Certificate')\n            if len(x509_certificate_nodes) > 0:\n                x509_certificate_node = x509_certificate_nodes[0]\n                x509_cert_value = x509_certificate_node.text\n                x509_fingerprint_value = OneLogin_Saml2_Utils.calculate_x509_fingerprint(x509_cert_value, fingerprintalg)\n                if fingerprint == x509_fingerprint_value:\n                    cert = OneLogin_Saml2_Utils.format_cert(x509_cert_value)\n\n        # Check if Reference URI is empty\n        # reference_elem = OneLogin_Saml2_Utils.query(signature_node, '//ds:Reference')\n        # if len(reference_elem) > 0:\n        #    if reference_elem[0].get('URI') == '':\n        #        reference_elem[0].set('URI', '#%s' % signature_node.getparent().get('ID'))\n\n        if cert is None or cert == '':\n            raise OneLogin_Saml2_Error(\n                'Could not validate node signature: No certificate provided.',\n                OneLogin_Saml2_Error.CERT_NOT_FOUND\n            )\n\n        file_cert = OneLogin_Saml2_Utils.write_temp_file(cert)\n\n        if validatecert:\n            mngr = xmlsec.KeysMngr()\n            mngr.loadCert(file_cert.name, xmlsec.KeyDataFormatCertPem, xmlsec.KeyDataTypeTrusted)\n            dsig_ctx = xmlsec.DSigCtx(mngr)\n        else:\n            dsig_ctx = xmlsec.DSigCtx()\n            dsig_ctx.signKey = xmlsec.Key.load(file_cert.name, xmlsec.KeyDataFormatCertPem, None)\n\n        file_cert.close()\n\n        dsig_ctx.setEnabledKeyData([xmlsec.KeyDataX509])\n\n        try:\n            dsig_ctx.verify(signature_node)\n        except Exception as err:\n            raise OneLogin_Saml2_ValidationError(\n                'Signature validation failed. SAML Response rejected. %s',\n                OneLogin_Saml2_ValidationError.INVALID_SIGNATURE,\n                err.__str__()\n            )\n\n        return True\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def snake_case(value: str) -> str:\n    value = re.sub(r\"([A-Z]{2,})([A-Z][a-z]|[ -_]|$)\", lambda m: m.group(1).title() + m.group(2), value.strip())\n    value = re.sub(r\"(^|[ _-])([A-Z])\", lambda m: m.group(1) + m.group(2).lower(), value)\n    return stringcase.snakecase(value)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status TensorSliceWriter::SaveData(const T* data, int64_t num_elements,\n                                   SavedSlice* ss) {\n  size_t size_bound =\n      ss->ByteSize() + kTensorProtoHeaderBytes +\n      (MaxBytesPerElement(DataTypeToEnum<T>::value) * num_elements);\n  if (size_bound > kMaxMessageBytes) {\n    return errors::InvalidArgument(\n        \"Tensor slice is too large to serialize (conservative estimate: \",\n        size_bound, \" bytes)\");\n  }\n  Fill(data, num_elements, ss->mutable_data());\n  DCHECK_GE(ss->ByteSize(), 0);\n  DCHECK_LE(ss->ByteSize(), size_bound);\n  return OkStatus();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_valid_filename(s):\n    \"\"\"\n    Return the given string converted to a string that can be used for a clean\n    filename. Remove leading and trailing spaces; convert other spaces to\n    underscores; and remove anything that is not an alphanumeric, dash,\n    underscore, or dot.\n    >>> get_valid_filename(\"john's portrait in 2004.jpg\")\n    'johns_portrait_in_2004.jpg'\n    \"\"\"\n    s = str(s).strip().replace(' ', '_')\n    return re.sub(r'(?u)[^-\\w.]', '', s)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  static void launch(OpKernelContext* context, const PoolParameters& params,\n                     const Tensor& grad_in, const Tensor& argmax,\n                     Tensor* grad_out, const bool include_batch_in_index) {\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n\n    auto shard = [&grad_in, &argmax, &grad_out, include_batch_in_index](\n                     int64 start, int64 limit) {\n      const int64 batch_size =\n          GetTensorDim(grad_out->shape(), FORMAT_NHWC, 'N');\n      const int64 output_size_per_batch = grad_out->NumElements() / batch_size;\n      const int64 input_size_per_batch = grad_in.NumElements() / batch_size;\n\n      {\n        auto grad_out_flat = grad_out->flat<T>();\n        auto argmax_flat = argmax.flat<int64>();\n        auto grad_in_flat = grad_in.flat<T>();\n\n        const int64 output_start = start * output_size_per_batch;\n        const int64 output_end = limit * output_size_per_batch;\n        EigenMatrixMap inputShard(grad_out_flat.data() + output_start, 1,\n                                  output_end - output_start);\n        inputShard.setConstant(T(0));\n\n        const int input_start = start * input_size_per_batch;\n        const int input_end = limit * input_size_per_batch;\n        for (int64 index = input_start; index < input_end; index++) {\n          int64 grad_out_index = argmax_flat(index);\n          if (!include_batch_in_index) {\n            const int64 cur_batch = index / input_size_per_batch;\n            grad_out_index += cur_batch * output_size_per_batch;\n          }\n          CHECK(grad_out_index >= output_start && grad_out_index < output_end)\n              << \"Invalid output gradient index: \" << grad_out_index << \", \"\n              << output_start << \", \" << output_end;\n          grad_out_flat(grad_out_index) += grad_in_flat(index);\n        }\n      }\n    };\n\n    const int64 batch_size = GetTensorDim(grad_out->shape(), FORMAT_NHWC, 'N');\n    const int64 shard_cost = grad_out->NumElements() / batch_size;\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          shard_cost, shard);\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def wget_sync(self, repo):\n\n        \"\"\"\n        Handle mirroring of directories using wget\n        \"\"\"\n\n        repo_mirror = repo.mirror.strip()\n\n        if repo.rpm_list != \"\" and repo.rpm_list != []:\n            self.logger.warning(\"--rpm-list is not supported for wget'd repositories\")\n\n        # FIXME: don't hardcode\n        dest_path = os.path.join(self.settings.webdir + \"/repo_mirror\", repo.name)\n\n        # FIXME: wrapper for subprocess that logs to logger\n        cmd = \"wget -N -np -r -l inf -nd -P %s %s\" % (dest_path, repo_mirror)\n        rc = utils.subprocess_call(self.logger, cmd)\n\n        if rc != 0:\n            utils.die(self.logger, \"cobbler reposync failed\")\n        os.path.walk(dest_path, self.createrepo_walker, repo)\n        self.create_local_file(dest_path, repo)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n        key = self.make_key(key, version=version)\n        if not self._cache.set(key, value, self.get_backend_timeout(timeout)):\n            # make sure the key doesn't keep its old value in case of failure to set (memcached's 1MB limit)\n            self._cache.delete(key)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def url_for(  # type: ignore[override]\n        self,\n        *,\n        filename: PathLike,\n        append_version: Optional[bool] = None,\n    ) -> URL:\n        if append_version is None:\n            append_version = self._append_version\n        filename = str(filename).lstrip(\"/\")\n\n        url = URL.build(path=self._prefix, encoded=True)\n        # filename is not encoded\n        if YARL_VERSION < (1, 6):\n            url = url / filename.replace(\"%\", \"%25\")\n        else:\n            url = url / filename\n\n        if append_version:\n            try:\n                filepath = self._directory.joinpath(filename).resolve()\n                if not self._follow_symlinks:\n                    filepath.relative_to(self._directory)\n            except (ValueError, FileNotFoundError):\n                # ValueError for case when path point to symlink\n                # with follow_symlinks is False\n                return url  # relatively safe\n            if filepath.is_file():\n                # TODO cache file content\n                # with file watcher for cache invalidation\n                with filepath.open(\"rb\") as f:\n                    file_bytes = f.read()\n                h = self._get_file_hash(file_bytes)\n                url = url.with_query({self.VERSION_KEY: h})\n                return url\n        return url\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def make_nonce(cls):\n        \"\"\"Generate pseudorandom number.\"\"\"\n        return str(random.randint(0, 100000000))\n "}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def main():\n    cli_args = create_parser().parse_args()\n\n    if cli_args.quiet:\n        logger.setLevel(logging.ERROR)\n    elif cli_args.debug:\n        logger.setLevel(logging.DEBUG)\n    elif cli_args.verbose:\n        logger.setLevel(logging.INFO)\n\n    # parse extra options\n    extra_input_options = _split_options(cli_args.extra_input_options)\n    extra_output_options = _split_options(cli_args.extra_output_options)\n\n    ffmpeg_normalize = FFmpegNormalize(\n        normalization_type=cli_args.normalization_type,\n        target_level=cli_args.target_level,\n        print_stats=cli_args.print_stats,\n        loudness_range_target=cli_args.loudness_range_target,\n        # threshold=cli_args.threshold,\n        true_peak=cli_args.true_peak,\n        offset=cli_args.offset,\n        dual_mono=cli_args.dual_mono,\n        dynamic=cli_args.dynamic,\n        audio_codec=cli_args.audio_codec,\n        audio_bitrate=cli_args.audio_bitrate,\n        sample_rate=cli_args.sample_rate,\n        keep_original_audio=cli_args.keep_original_audio,\n        pre_filter=cli_args.pre_filter,\n        post_filter=cli_args.post_filter,\n        video_codec=cli_args.video_codec,\n        video_disable=cli_args.video_disable,\n        subtitle_disable=cli_args.subtitle_disable,\n        metadata_disable=cli_args.metadata_disable,\n        chapters_disable=cli_args.chapters_disable,\n        extra_input_options=extra_input_options,\n        extra_output_options=extra_output_options,\n        output_format=cli_args.output_format,\n        dry_run=cli_args.dry_run,\n        progress=cli_args.progress,\n    )\n\n    if (\n        cli_args.output is not None\n        and len(cli_args.output) > 0\n        and len(cli_args.input) > len(cli_args.output)\n    ):\n        logger.warning(\n            \"There are more input files than output file names given. \"\n            \"Please specify one output file name per input file using -o <output1> <output2> ... \"\n            \"Will apply default file naming for the remaining ones.\"\n        )\n\n    for index, input_file in enumerate(cli_args.input):\n        if cli_args.output is not None and index < len(cli_args.output):\n            if cli_args.output_folder and cli_args.output_folder != \"normalized\":\n                logger.warning(\n                    \"Output folder {} is ignored for input file {}\".format(\n                        cli_args.output_folder, input_file\n                    )\n                )\n            output_file = cli_args.output[index]\n            output_dir = os.path.dirname(output_file)\n            if output_dir != \"\" and not os.path.isdir(output_dir):\n                raise FFmpegNormalizeError(\n                    f\"Output file path {output_dir} does not exist\"\n                )\n        else:\n            output_file = os.path.join(\n                cli_args.output_folder,\n                os.path.splitext(os.path.basename(input_file))[0]\n                + \".\"\n                + cli_args.extension,\n            )\n            if not os.path.isdir(cli_args.output_folder) and not cli_args.dry_run:\n                logger.warning(\n                    \"Output directory '{}' does not exist, will create\".format(\n                        cli_args.output_folder\n                    )\n                )\n                os.makedirs(cli_args.output_folder)\n\n        if os.path.exists(output_file) and not cli_args.force:\n            logger.error(\n                \"Output file {} already exists, skipping. Use -f to force overwriting.\".format(\n                    output_file\n                )\n            )\n        else:\n            ffmpeg_normalize.add_media_file(input_file, output_file)\n\n    ffmpeg_normalize.run_normalization()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_http_response_with_retries(\n    method,\n    url,\n    max_retries,\n    backoff_factor,\n    backoff_jitter,\n    retry_codes,\n    raise_on_status=True,\n    **kwargs,\n):\n    \"\"\"\n    Performs an HTTP request using Python's `requests` module with an automatic retry policy.\n\n    :param method: a string indicating the method to use, e.g. \"GET\", \"POST\", \"PUT\".\n    :param url: the target URL address for the HTTP request.\n    :param max_retries: Maximum total number of retries.\n    :param backoff_factor: a time factor for exponential backoff. e.g. value 5 means the HTTP\n      request will be retried with interval 5, 10, 20... seconds. A value of 0 turns off the\n      exponential backoff.\n    :param backoff_jitter: A random jitter to add to the backoff interval.\n    :param retry_codes: a list of HTTP response error codes that qualifies for retry.\n    :param raise_on_status: whether to raise an exception, or return a response, if status falls\n      in retry_codes range and retries have been exhausted.\n    :param kwargs: Additional keyword arguments to pass to `requests.Session.request()`\n\n    :return: requests.Response object.\n    \"\"\"\n    session = _get_request_session(\n        max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status\n    )\n    return session.request(method, url, **kwargs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "inline void Conv3DTranspose(\n    const Conv3DTransposeParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& bias_shape,\n    const float* bias_data, const RuntimeShape& output_shape,\n    float* output_data) {\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int stride_depth = params.stride_depth;\n  const int pad_width = params.padding_values.width;\n  const int pad_height = params.padding_values.height;\n  const int pad_depth = params.padding_values.depth;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_num_channels = MatchingDim(input_shape, 4, filter_shape, 4);\n  const int output_num_channels = output_shape.Dims(4);\n  const int input_depth = input_shape.Dims(1);\n  const int input_height = input_shape.Dims(2);\n  const int input_width = input_shape.Dims(3);\n  const int filter_depth = filter_shape.Dims(0);\n  const int filter_height = filter_shape.Dims(1);\n  const int filter_width = filter_shape.Dims(2);\n  const int output_depth = output_shape.Dims(1);\n  const int output_height = output_shape.Dims(2);\n  const int output_width = output_shape.Dims(3);\n  if (bias_data) {\n    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_num_channels);\n  }\n\n  // Initializes the output array to zero.\n  const int num_elements = output_shape.FlatSize();\n  for (int i = 0; i < num_elements; i++) {\n    output_data[i] = 0.0f;\n  }\n\n  // Loop through input elements one at a time.\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int in_d = 0; in_d < input_depth; ++in_d) {\n      for (int in_y = 0; in_y < input_height; ++in_y) {\n        for (int in_x = 0; in_x < input_width; ++in_x) {\n          for (int in_channel = 0; in_channel < input_num_channels;\n               ++in_channel) {\n            // Loop through the output elements it will influence.\n            const int out_x_origin = (in_x * stride_width) - pad_width;\n            const int out_y_origin = (in_y * stride_height) - pad_height;\n            const int out_d_origin = (in_d * stride_depth) - pad_depth;\n            for (int filter_d = 0; filter_d < filter_depth; ++filter_d) {\n              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {\n                  for (int out_channel = 0; out_channel < output_num_channels;\n                       ++out_channel) {\n                    // Compute output element location.\n                    const int out_x =\n                        out_x_origin + params.dilation_width * filter_x;\n                    const int out_y =\n                        out_y_origin + params.dilation_height * filter_y;\n                    const int out_d =\n                        out_d_origin + params.dilation_depth * filter_d;\n                    // We cannot accumulate out of bounds.\n                    if ((out_x >= 0) && (out_x < output_width) &&\n                        (out_y >= 0) && (out_y < output_height) &&\n                        (out_d >= 0) && (out_d < output_depth)) {\n                      float input_value = input_data[Offset(\n                          input_shape, batch, in_d, in_y, in_x, in_channel)];\n                      float filter_value = filter_data[Offset(\n                          filter_shape, filter_d, filter_y, filter_x,\n                          out_channel, in_channel)];\n                      output_data[Offset(output_shape, batch, out_d, out_y,\n                                         out_x, out_channel)] +=\n                          input_value * filter_value;\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n\n  const float float_activation_min = params.float_activation_min;\n  const float float_activation_max = params.float_activation_max;\n  float* data_ptr = output_data;\n  if (bias_data) {\n    const int outer_size =\n        batches * output_depth * output_height * output_width;\n    const int num_channels = input_shape.Dims(4);\n    for (int n = 0; n < outer_size; ++n) {\n      for (int c = 0; c < output_num_channels; ++c) {\n        data_ptr[c] = ActivationFunctionWithMinMax(data_ptr[c] + bias_data[c],\n                                                   float_activation_min,\n                                                   float_activation_max);\n      }\n      data_ptr += num_channels;\n    }\n  } else {\n    const int flat_size = output_shape.FlatSize();\n    for (int i = 0; i < flat_size; ++i) {\n      data_ptr[i] = ActivationFunctionWithMinMax(\n          data_ptr[i], float_activation_min, float_activation_max);\n    }\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def get_file(\n        self,\n        url: str,\n        output_stream: BinaryIO,\n        max_size: Optional[int] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:\n        \"\"\"GETs a file from a given URL\n        Args:\n            url: The URL to GET\n            output_stream: File to write the response body to.\n            headers: A map from header name to a list of values for that header\n        Returns:\n            A tuple of the file length, dict of the response\n            headers, absolute URI of the response and HTTP response code.\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            SynapseError: if the response is not a 2xx, the remote file is too large, or\n               another exception happens during the download.\n        \"\"\"\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", url, headers=Headers(actual_headers))\n\n        resp_headers = dict(response.headers.getAllRawHeaders())\n\n        if (\n            b\"Content-Length\" in resp_headers\n            and max_size\n            and int(resp_headers[b\"Content-Length\"][0]) > max_size\n        ):\n            logger.warning(\"Requested URL is too large > %r bytes\" % (max_size,))\n            raise SynapseError(\n                502,\n                \"Requested file is too large > %r bytes\" % (max_size,),\n                Codes.TOO_LARGE,\n            )\n\n        if response.code > 299:\n            logger.warning(\"Got %d when downloading %s\" % (response.code, url))\n            raise SynapseError(502, \"Got error %d\" % (response.code,), Codes.UNKNOWN)\n\n        # TODO: if our Content-Type is HTML or something, just read the first\n        # N bytes into RAM rather than saving it all to disk only to read it\n        # straight back in again\n\n        try:\n            length = await make_deferred_yieldable(\n                readBodyToFile(response, output_stream, max_size)\n            )\n        except SynapseError:\n            # This can happen e.g. because the body is too large.\n            raise\n        except Exception as e:\n            raise SynapseError(502, (\"Failed to download remote body: %s\" % e)) from e\n\n        return (\n            length,\n            resp_headers,\n            response.request.absoluteURI.decode(\"ascii\"),\n            response.code,\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, source, *args, **kwargs):\n        self.source = source\n        self.args = args\n        if len(args) == 2 and isinstance(args[1], (string_types, tuple, list)):\n            self.rmatch = args[0]\n            self.vmatch = args[1]\n            self.vdict = None\n            self.attr = None\n        elif len(args) == 2 and isinstance(args[1], dict):\n            self.rmatch = args[0]\n            self.vmatch = None\n            self.vdict = args[1]\n            self.attr = None\n        elif len(args) == 3:\n            self.rmatch = args[0]\n            self.vmatch = args[1]\n            self.vdict = None\n            self.attr = args[2]\n        else:\n            assert False, 'bad parameters'\n        self.missing = kwargs.get('missing', None)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void CalculateOutputIndexRowSplit(\n      const RowPartitionTensor& row_split,\n      const vector<INDEX_TYPE>& parent_output_index,\n      INDEX_TYPE output_index_multiplier, INDEX_TYPE output_size,\n      vector<INDEX_TYPE>* result) {\n    INDEX_TYPE row_split_size = row_split.size();\n    if (row_split_size > 0) {\n      result->reserve(row_split(row_split_size - 1));\n    }\n    for (INDEX_TYPE i = 0; i < row_split_size - 1; ++i) {\n      INDEX_TYPE row_length = row_split(i + 1) - row_split(i);\n      INDEX_TYPE real_length = std::min(output_size, row_length);\n      INDEX_TYPE parent_output_index_current = parent_output_index[i];\n\n      if (parent_output_index_current == -1) {\n        real_length = 0;\n      }\n      for (INDEX_TYPE j = 0; j < real_length; ++j) {\n        result->push_back(parent_output_index_current);\n        parent_output_index_current += output_index_multiplier;\n      }\n      for (INDEX_TYPE j = 0; j < row_length - real_length; ++j) {\n        result->push_back(-1);\n      }\n    }\n    if (row_split_size > 0) {\n      DCHECK_EQ(result->size(), row_split(row_split_size - 1));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def send(self, *args, **kwargs) -> Response:\n        \"\"\"\n        Send a request with automatic retry behavior for the following status codes:\n\n        - 429 CloudFlare-style rate limiting\n        - 502 Bad Gateway\n        - 503 Service unavailable\n        \"\"\"\n\n        api_request = partial(super().send, *args, **kwargs)\n\n        response = await self._send_with_retry(\n            request=api_request,\n            retry_codes={\n                status.HTTP_429_TOO_MANY_REQUESTS,\n                status.HTTP_503_SERVICE_UNAVAILABLE,\n                status.HTTP_502_BAD_GATEWAY,\n                status.HTTP_408_REQUEST_TIMEOUT,\n                *PREFECT_CLIENT_RETRY_EXTRA_CODES.value(),\n            },\n            retry_exceptions=(\n                httpx.ReadTimeout,\n                httpx.PoolTimeout,\n                httpx.ConnectTimeout,\n                # `ConnectionResetError` when reading socket raises as a `ReadError`\n                httpx.ReadError,\n                # Sockets can be closed during writes resulting in a `WriteError`\n                httpx.WriteError,\n                # Uvicorn bug, see https://github.com/PrefectHQ/prefect/issues/7512\n                httpx.RemoteProtocolError,\n                # HTTP2 bug, see https://github.com/PrefectHQ/prefect/issues/7442\n                httpx.LocalProtocolError,\n            ),\n        )\n\n        # Convert to a Prefect response to add nicer errors messages\n        response = PrefectResponse.from_httpx_response(response)\n\n        # Always raise bad responses\n        # NOTE: We may want to remove this and handle responses per route in the\n        #       `PrefectClient`\n        response.raise_for_status()\n\n        return response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, **kwargs):\n        self.basic_auth = get_anymail_setting('webhook_authorization', default=[],\n                                              kwargs=kwargs)  # no esp_name -- auth is shared between ESPs\n        # Allow a single string:\n        if isinstance(self.basic_auth, six.string_types):\n            self.basic_auth = [self.basic_auth]\n        if self.warn_if_no_basic_auth and len(self.basic_auth) < 1:\n            warnings.warn(\n                \"Your Anymail webhooks are insecure and open to anyone on the web. \"\n                \"You should set WEBHOOK_AUTHORIZATION in your ANYMAIL settings. \"\n                \"See 'Securing webhooks' in the Anymail docs.\",\n                AnymailInsecureWebhookWarning)\n        # noinspection PyArgumentList\n        super(AnymailBasicAuthMixin, self).__init__(**kwargs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& in0 = ctx->input(0);\n    const Tensor& in1 = ctx->input(1);\n\n    ValidateInputTensors(ctx, in0, in1);\n\n    MatMulBCast bcast(in0.shape().dim_sizes(), in1.shape().dim_sizes());\n    OP_REQUIRES(\n        ctx, bcast.IsValid(),\n        errors::InvalidArgument(\n            \"In[0] and In[1] must have compatible batch dimensions: \",\n            in0.shape().DebugString(), \" vs. \", in1.shape().DebugString()));\n\n    TensorShape out_shape = bcast.output_batch_shape();\n    auto batch_size = bcast.output_batch_size();\n    auto d0 = in0.dim_size(in0.dims() - 2);  // Band size.\n    auto d1 = in0.dim_size(in0.dims() - 1);\n    Tensor in0_reshaped;\n    OP_REQUIRES(\n        ctx,\n        in0_reshaped.CopyFrom(in0, TensorShape({bcast.x_batch_size(), d0, d1})),\n        errors::Internal(\"Failed to reshape In[0] from \",\n                         in0.shape().DebugString()));\n    auto d2 = in1.dim_size(in1.dims() - 2);\n    auto d3 = in1.dim_size(in1.dims() - 1);\n    Tensor in1_reshaped;\n    OP_REQUIRES(\n        ctx,\n        in1_reshaped.CopyFrom(in1, TensorShape({bcast.y_batch_size(), d2, d3})),\n        errors::Internal(\"Failed to reshape In[1] from \",\n                         in1.shape().DebugString()));\n    OP_REQUIRES(ctx, d1 == d2,\n                errors::InvalidArgument(\n                    \"In[0] mismatch In[1] shape: \", d1, \" vs. \", d2, \": \",\n                    in0.shape().DebugString(), \" \", in1.shape().DebugString(),\n                    \" \", lower_, \" \", adjoint_));\n    out_shape.AddDim(d1);\n    out_shape.AddDim(d3);\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));\n    if (out->NumElements() == 0) {\n      return;\n    }\n    Tensor out_reshaped;\n    OP_REQUIRES(ctx,\n                out_reshaped.CopyFrom(*out, TensorShape({batch_size, d1, d3})),\n                errors::Internal(\"Failed to reshape output from \",\n                                 out->shape().DebugString()));\n    LaunchBatchBandedTriangularSolve<Scalar>::Launch(\n        ctx, in0_reshaped, in1_reshaped, adjoint_, lower_, bcast,\n        &out_reshaped);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def LoadSettingsFile(filename=SETTINGS_FILE):\n    \"\"\"Loads settings file in yaml format given file name.\n\n    :param filename: path for settings file. 'settings.yaml' by default.\n    :type filename: str.\n    :raises: SettingsError\n    \"\"\"\n    try:\n        with open(filename) as stream:\n            data = load(stream, Loader=Loader)\n    except (YAMLError, OSError) as e:\n        raise SettingsError(e)\n    return data\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status Examples::Initialize(OpKernelContext* const context,\n                            const ModelWeights& weights,\n                            const int num_sparse_features,\n                            const int num_sparse_features_with_values,\n                            const int num_dense_features) {\n  num_features_ = num_sparse_features + num_dense_features;\n\n  OpInputList sparse_example_indices_inputs;\n  TF_RETURN_IF_ERROR(context->input_list(\"sparse_example_indices\",\n                                         &sparse_example_indices_inputs));\n  if (sparse_example_indices_inputs.size() != num_sparse_features)\n    return errors::InvalidArgument(\n        \"Expected \", num_sparse_features,\n        \" tensors in sparse_example_indices but got \",\n        sparse_example_indices_inputs.size());\n  OpInputList sparse_feature_indices_inputs;\n  TF_RETURN_IF_ERROR(context->input_list(\"sparse_feature_indices\",\n                                         &sparse_feature_indices_inputs));\n  if (sparse_feature_indices_inputs.size() != num_sparse_features)\n    return errors::InvalidArgument(\n        \"Expected \", num_sparse_features,\n        \" tensors in sparse_feature_indices but got \",\n        sparse_feature_indices_inputs.size());\n  OpInputList sparse_feature_values_inputs;\n  if (num_sparse_features_with_values > 0) {\n    TF_RETURN_IF_ERROR(context->input_list(\"sparse_feature_values\",\n                                           &sparse_feature_values_inputs));\n    if (sparse_feature_values_inputs.size() != num_sparse_features_with_values)\n      return errors::InvalidArgument(\n          \"Expected \", num_sparse_features_with_values,\n          \" tensors in sparse_feature_values but got \",\n          sparse_feature_values_inputs.size());\n  }\n\n  const Tensor* example_weights_t;\n  TF_RETURN_IF_ERROR(context->input(\"example_weights\", &example_weights_t));\n  auto example_weights = example_weights_t->flat<float>();\n\n  if (example_weights.size() >= std::numeric_limits<int>::max()) {\n    return errors::InvalidArgument(strings::Printf(\n        \"Too many examples in a mini-batch: %zu > %d\", example_weights.size(),\n        std::numeric_limits<int>::max()));\n  }\n\n  // The static_cast here is safe since num_examples can be at max an int.\n  const int num_examples = static_cast<int>(example_weights.size());\n  const Tensor* example_labels_t;\n  TF_RETURN_IF_ERROR(context->input(\"example_labels\", &example_labels_t));\n  auto example_labels = example_labels_t->flat<float>();\n  if (example_labels.size() != num_examples) {\n    return errors::InvalidArgument(\"Expected \", num_examples,\n                                   \" example labels but got \",\n                                   example_labels.size());\n  }\n\n  OpInputList dense_features_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"dense_features\", &dense_features_inputs));\n\n  examples_.clear();\n  examples_.resize(num_examples);\n  probabilities_.resize(num_examples);\n  sampled_index_.resize(num_examples);\n  sampled_count_.resize(num_examples);\n  for (int example_id = 0; example_id < num_examples; ++example_id) {\n    Example* const example = &examples_[example_id];\n    example->sparse_features_.resize(num_sparse_features);\n    example->dense_vectors_.resize(num_dense_features);\n    example->example_weight_ = example_weights(example_id);\n    example->example_label_ = example_labels(example_id);\n  }\n  const DeviceBase::CpuWorkerThreads& worker_threads =\n      *context->device()->tensorflow_cpu_worker_threads();\n  TF_RETURN_IF_ERROR(CreateSparseFeatureRepresentation(\n      worker_threads, num_examples, num_sparse_features, weights,\n      sparse_example_indices_inputs, sparse_feature_indices_inputs,\n      sparse_feature_values_inputs, &examples_));\n  TF_RETURN_IF_ERROR(CreateDenseFeatureRepresentation(\n      worker_threads, num_examples, num_dense_features, weights,\n      dense_features_inputs, &examples_));\n  TF_RETURN_IF_ERROR(ComputeSquaredNormPerExample(\n      worker_threads, num_examples, num_sparse_features, num_dense_features,\n      &examples_));\n  return OkStatus();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render(self):\n        if len(self.params) == 1:\n            # Retrieve the user portrait\n            user = decleanId(self.params[0])\n            portrait = self.portal_membership.getPersonalPortrait(user)\n        elif len(self.params) == 0:\n            current_user_id = self.portal_membership.getAuthenticatedMember().getId()\n            portrait = self.portal_membership.getPersonalPortrait(current_user_id)\n        else:\n            raise Exception(\n                \"Must supply exactly zero (own portrait) or one parameter \" \"(user id)\"\n            )\n        # User uploaded portraits have a meta_type of \"Image\"\n        if not portrait or isDefaultPortrait(portrait):\n            self.request.response.setStatus(404)\n            return None\n\n        self.request.response.setStatus(200)\n        self.request.response.setHeader(\"Content-Type\", portrait.content_type)\n\n        return stream_data(portrait)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def parse_url(url):\n    \"\"\"\n    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is\n    performed to parse incomplete urls. Fields not provided will be None.\n    This parser is RFC 3986 compliant.\n\n    The parser logic and helper functions are based heavily on\n    work done in the ``rfc3986`` module.\n\n    :param str url: URL to parse into a :class:`.Url` namedtuple.\n\n    Partly backwards-compatible with :mod:`urlparse`.\n\n    Example::\n\n        >>> parse_url('http://google.com/mail/')\n        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)\n        >>> parse_url('google.com:80')\n        Url(scheme=None, host='google.com', port=80, path=None, ...)\n        >>> parse_url('/foo?bar')\n        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)\n    \"\"\"\n    if not url:\n        # Empty\n        return Url()\n\n    source_url = url\n    if not SCHEME_RE.search(url):\n        url = \"//\" + url\n\n    try:\n        scheme, authority, path, query, fragment = URI_RE.match(url).groups()\n        normalize_uri = scheme is None or scheme.lower() in NORMALIZABLE_SCHEMES\n\n        if scheme:\n            scheme = scheme.lower()\n\n        if authority:\n            auth, host, port = SUBAUTHORITY_RE.match(authority).groups()\n            if auth and normalize_uri:\n                auth = _encode_invalid_chars(auth, USERINFO_CHARS)\n            if port == \"\":\n                port = None\n        else:\n            auth, host, port = None, None, None\n\n        if port is not None:\n            port = int(port)\n            if not (0 <= port <= 65535):\n                raise LocationParseError(url)\n\n        host = _normalize_host(host, scheme)\n\n        if normalize_uri and path:\n            path = _remove_path_dot_segments(path)\n            path = _encode_invalid_chars(path, PATH_CHARS)\n        if normalize_uri and query:\n            query = _encode_invalid_chars(query, QUERY_CHARS)\n        if normalize_uri and fragment:\n            fragment = _encode_invalid_chars(fragment, FRAGMENT_CHARS)\n\n    except (ValueError, AttributeError):\n        return six.raise_from(LocationParseError(source_url), None)\n\n    # For the sake of backwards compatibility we put empty\n    # string values for path if there are any defined values\n    # beyond the path in the URL.\n    # TODO: Remove this when we break backwards compatibility.\n    if not path:\n        if query is not None or fragment is not None:\n            path = \"\"\n        else:\n            path = None\n\n    # Ensure that each part of the URL is a `str` for\n    # backwards compatibility.\n    if isinstance(url, six.text_type):\n        ensure_func = six.ensure_text\n    else:\n        ensure_func = six.ensure_str\n\n    def ensure_type(x):\n        return x if x is None else ensure_func(x)\n\n    return Url(\n        scheme=ensure_type(scheme),\n        auth=ensure_type(auth),\n        host=ensure_type(host),\n        port=port,\n        path=ensure_type(path),\n        query=ensure_type(query),\n        fragment=ensure_type(fragment),\n    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_url(url):\n    if os.path.exists(url):\n        with open(url, 'r') as opener:\n            content = opener.read()\n    else:\n        content = url\n\n    urls = []\n    for _url in content.splitlines():\n        if not _url:\n            continue\n\n        urls.append((_validate(_url), _url))\n    \n    return urls\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def cancel(self, request_id):\n        method, okey, seq = request_id\n        if request_id in self._futures:\n            for seq_item in self._pending[okey]:\n                if seq_item.method == method and seq_item.seq == seq:\n                    self._pending[okey].remove(seq_item)\n            if self._pending[okey]:\n                heapq.heapify(self._pending[okey])\n            else:\n                del self._pending[okey]\n            job = self._jobs.get(request_id, None)\n            if job:\n                # aiojobs cancels the internal task upon job closing.\n                await job.close()\n        else:\n            self._log.warning('cancellation of unknown or '\n                              'not sent yet request: %r', request_id)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def to_dot_graph(\n        self,\n        priorities=True,\n        addnodes=[],\n        savefn=None,\n        savelayout=\"plain\",\n        saveformat=None,\n        color_edges=True,\n    ):\n        \"\"\"\n        Converts this transform graph to the graphviz_ DOT format.\n\n        Optionally saves it (requires `graphviz`_ be installed and on your path).\n\n        .. _graphviz: http://www.graphviz.org/\n\n        Parameters\n        ----------\n        priorities : bool\n            If `True`, show the priority values for each transform.  Otherwise,\n            the will not be included in the graph.\n        addnodes : sequence of str\n            Additional coordinate systems to add (this can include systems\n            already in the transform graph, but they will only appear once).\n        savefn : None or str\n            The file name to save this graph to or `None` to not save\n            to a file.\n        savelayout : str\n            The graphviz program to use to layout the graph (see\n            graphviz_ for details) or 'plain' to just save the DOT graph\n            content. Ignored if ``savefn`` is `None`.\n        saveformat : str\n            The graphviz output format. (e.g. the ``-Txxx`` option for\n            the command line program - see graphviz docs for details).\n            Ignored if ``savefn`` is `None`.\n        color_edges : bool\n            Color the edges between two nodes (frames) based on the type of\n            transform. ``FunctionTransform``: red, ``StaticMatrixTransform``:\n            blue, ``DynamicMatrixTransform``: green.\n\n        Returns\n        -------\n        dotgraph : str\n            A string with the DOT format graph.\n        \"\"\"\n        # We store nodes as `dict` keys because differently from `list` uniqueness is\n        # guaranteed and differently from `set` insertion order is preserved.\n        nodes = {}\n        for node, node_graph in self._graph.items():\n            nodes[node] = None\n            nodes |= {node: None for node in node_graph}\n        nodes |= {node: None for node in addnodes}\n        nodenames = []\n        invclsaliases = {\n            f: [k for k, v in self._cached_names.items() if v == f]\n            for f in self.frame_set\n        }\n        for n in nodes:\n            if n in invclsaliases:\n                aliases = \"`\\\\n`\".join(invclsaliases[n])\n                nodenames.append(\n                    '{0} [shape=oval label=\"{0}\\\\n`{1}`\"]'.format(n.__name__, aliases)\n                )\n            else:\n                nodenames.append(n.__name__ + \"[ shape=oval ]\")\n\n        edgenames = []\n        # Now the edges\n        for a, agraph in self._graph.items():\n            for b, transform in agraph.items():\n                pri = getattr(transform, \"priority\", 1)\n                color = trans_to_color[transform.__class__] if color_edges else \"black\"\n                edgenames.append((a.__name__, b.__name__, pri, color))\n\n        # generate simple dot format graph\n        lines = [\"digraph AstropyCoordinateTransformGraph {\"]\n        lines.append(\"graph [rankdir=LR]\")\n        lines.append(\"; \".join(nodenames) + \";\")\n        for enm1, enm2, weights, color in edgenames:\n            labelstr_fmt = \"[ {0} {1} ]\"\n            priority_part = f'label = \"{weights}\"' if priorities else \"\"\n            color_part = f'color = \"{color}\"'\n            labelstr = labelstr_fmt.format(priority_part, color_part)\n            lines.append(f\"{enm1} -> {enm2}{labelstr};\")\n        lines.append(\"\")\n        lines.append(\"overlap=false\")\n        lines.append(\"}\")\n        dotgraph = \"\\n\".join(lines)\n\n        if savefn is not None:\n            if savelayout == \"plain\":\n                with open(savefn, \"w\") as f:\n                    f.write(dotgraph)\n            else:\n                args = [savelayout]\n                if saveformat is not None:\n                    args.append(\"-T\" + saveformat)\n                proc = subprocess.Popen(\n                    args,\n                    stdin=subprocess.PIPE,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                )\n                stdout, stderr = proc.communicate(dotgraph)\n                if proc.returncode != 0:\n                    raise OSError(\"problem running graphviz: \\n\" + stderr)\n\n                with open(savefn, \"w\") as f:\n                    f.write(stdout)\n\n        return dotgraph\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def value_from_datadict(self, data, files, name):\n        \"File widgets take data from FILES, not POST\"\n        return files.get(name)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "uint8* Decode(const void* srcdata, int datasize,\n              const std::function<uint8*(int, int, int, int)>& allocate_output,\n              string* error_string, bool expand_animations) {\n  int error_code = D_GIF_SUCCEEDED;\n  InputBufferInfo info = {reinterpret_cast<const uint8*>(srcdata), datasize};\n  GifFileType* gif_file =\n      DGifOpen(static_cast<void*>(&info), &input_callback, &error_code);\n  const auto cleanup = gtl::MakeCleanup([gif_file]() {\n    int error_code = D_GIF_SUCCEEDED;\n    if (gif_file && DGifCloseFile(gif_file, &error_code) != GIF_OK) {\n      LOG(WARNING) << \"Fail to close gif file, reason: \"\n                   << GifErrorStringNonNull(error_code);\n    }\n  });\n  if (error_code != D_GIF_SUCCEEDED) {\n    *error_string = absl::StrCat(\"failed to open gif file: \",\n                                 GifErrorStringNonNull(error_code));\n    return nullptr;\n  }\n  if (DGifSlurp(gif_file) != GIF_OK) {\n    *error_string = absl::StrCat(\"failed to slurp gif file: \",\n                                 GifErrorStringNonNull(gif_file->Error));\n    return nullptr;\n  }\n  if (gif_file->ImageCount <= 0) {\n    *error_string = \"gif file does not contain any image\";\n    return nullptr;\n  }\n\n  int target_num_frames = gif_file->ImageCount;\n\n  // Don't request more memory than needed for each frame, preventing OOM\n  int max_frame_width = 0;\n  int max_frame_height = 0;\n  for (int k = 0; k < target_num_frames; k++) {\n    SavedImage* si = &gif_file->SavedImages[k];\n    if (max_frame_height < si->ImageDesc.Height)\n      max_frame_height = si->ImageDesc.Height;\n    if (max_frame_width < si->ImageDesc.Width)\n      max_frame_width = si->ImageDesc.Width;\n  }\n\n  const int width = max_frame_width;\n  const int height = max_frame_height;\n  const int channel = 3;\n  if (!expand_animations) target_num_frames = 1;\n\n  uint8* const dstdata =\n      allocate_output(target_num_frames, width, height, channel);\n  if (!dstdata) return nullptr;\n  for (int k = 0; k < target_num_frames; k++) {\n    uint8* this_dst = dstdata + k * width * channel * height;\n\n    SavedImage* this_image = &gif_file->SavedImages[k];\n    GifImageDesc* img_desc = &this_image->ImageDesc;\n\n    // The Graphics Control Block tells us which index in the color map\n    // correspond to \"transparent color\", i.e. no need to update the pixel\n    // on the canvas. The \"transparent color index\" is specific to each\n    // sub-frame.\n    GraphicsControlBlock gcb;\n    DGifSavedExtensionToGCB(gif_file, k, &gcb);\n\n    int imgLeft = img_desc->Left;\n    int imgTop = img_desc->Top;\n    int imgRight = img_desc->Left + img_desc->Width;\n    int imgBottom = img_desc->Top + img_desc->Height;\n\n    if (k > 0) {\n      uint8* last_dst = dstdata + (k - 1) * width * channel * height;\n      for (int i = 0; i < height; ++i) {\n        uint8* p_dst = this_dst + i * width * channel;\n        uint8* l_dst = last_dst + i * width * channel;\n        for (int j = 0; j < width; ++j) {\n          p_dst[j * channel + 0] = l_dst[j * channel + 0];\n          p_dst[j * channel + 1] = l_dst[j * channel + 1];\n          p_dst[j * channel + 2] = l_dst[j * channel + 2];\n        }\n      }\n    }\n\n    if (img_desc->Left != 0 || img_desc->Top != 0 || img_desc->Width != width ||\n        img_desc->Height != height) {\n      // If the first frame does not fill the entire canvas then fill the\n      // unoccupied canvas with zeros (black).\n      if (k == 0) {\n        for (int i = 0; i < height; ++i) {\n          uint8* p_dst = this_dst + i * width * channel;\n          for (int j = 0; j < width; ++j) {\n            p_dst[j * channel + 0] = 0;\n            p_dst[j * channel + 1] = 0;\n            p_dst[j * channel + 2] = 0;\n          }\n        }\n      }\n\n      imgLeft = std::max(imgLeft, 0);\n      imgTop = std::max(imgTop, 0);\n      imgRight = std::min(imgRight, width);\n      imgBottom = std::min(imgBottom, height);\n    }\n\n    ColorMapObject* color_map = this_image->ImageDesc.ColorMap\n                                    ? this_image->ImageDesc.ColorMap\n                                    : gif_file->SColorMap;\n    if (color_map == nullptr) {\n      *error_string = absl::StrCat(\"missing color map for frame \", k);\n      return nullptr;\n    }\n\n    for (int i = imgTop; i < imgBottom; ++i) {\n      uint8* p_dst = this_dst + i * width * channel;\n      for (int j = imgLeft; j < imgRight; ++j) {\n        GifByteType color_index =\n            this_image->RasterBits[(i - img_desc->Top) * (img_desc->Width) +\n                                   (j - img_desc->Left)];\n\n        if (color_index >= color_map->ColorCount) {\n          *error_string = absl::StrCat(\"found color index \", color_index,\n                                       \" outside of color map range \",\n                                       color_map->ColorCount);\n          return nullptr;\n        }\n\n        if (color_index == gcb.TransparentColor) {\n          // Use the pixel from the previous frame. In other words, no need to\n          // update our canvas for this pixel.\n          continue;\n        }\n\n        const GifColorType& gif_color = color_map->Colors[color_index];\n        p_dst[j * channel + 0] = gif_color.Red;\n        p_dst[j * channel + 1] = gif_color.Green;\n        p_dst[j * channel + 2] = gif_color.Blue;\n      }\n    }\n  }\n\n  return dstdata;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def authorize_and_redirect(request):\n    if not request.GET.get(\"redirect\"):\n        return HttpResponse(\"You need to pass a url to ?redirect=\", status=401)\n    if not request.META.get(\"HTTP_REFERER\"):\n        return HttpResponse('You need to make a request that includes the \"Referer\" header.', status=400)\n\n    referer_url = urlparse(request.META[\"HTTP_REFERER\"])\n    redirect_url = urlparse(request.GET[\"redirect\"])\n\n    if referer_url.hostname != redirect_url.hostname:\n        return HttpResponse(f\"Can only redirect to the same domain as the referer: {referer_url.hostname}\", status=400)\n\n    if referer_url.scheme != redirect_url.scheme:\n        return HttpResponse(f\"Can only redirect to the same scheme as the referer: {referer_url.scheme}\", status=400)\n\n    if referer_url.port != redirect_url.port:\n        return HttpResponse(\n            f\"Can only redirect to the same port as the referer: {referer_url.port or 'no port in URL'}\", status=400\n        )\n\n    return render_template(\n        \"authorize_and_redirect.html\",\n        request=request,\n        context={\"domain\": redirect_url.hostname, \"redirect_url\": request.GET[\"redirect\"]},\n    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def include_is_allowed(filepath):\n    for root in settings.ALLOWED_INCLUDE_ROOTS:\n        if filepath.startswith(root):\n            return True\n    return False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def urlize(text, trim_url_limit=None, nofollow=False, autoescape=False):\n    \"\"\"\n    Converts any URLs in text into clickable links.\n\n    Works on http://, https://, www. links, and also on links ending in one of\n    the original seven gTLDs (.com, .edu, .gov, .int, .mil, .net, and .org).\n    Links can have trailing punctuation (periods, commas, close-parens) and\n    leading punctuation (opening parens) and it'll still do the right thing.\n\n    If trim_url_limit is not None, the URLs in the link text longer than this\n    limit will be truncated to trim_url_limit-3 characters and appended with\n    an ellipsis.\n\n    If nofollow is True, the links will get a rel=\"nofollow\" attribute.\n\n    If autoescape is True, the link text and URLs will be autoescaped.\n    \"\"\"\n    safe_input = isinstance(text, SafeData)\n\n    def trim_url(x, limit=trim_url_limit):\n        if limit is None or len(x) <= limit:\n            return x\n        return '%s...' % x[:max(0, limit - 3)]\n\n    def unescape(text, trail):\n        \"\"\"\n        If input URL is HTML-escaped, unescape it so as we can safely feed it to\n        smart_urlquote. For example:\n        http://example.com?x=1&amp;y=&lt;2&gt; => http://example.com?x=1&y=<2>\n        \"\"\"\n        unescaped = (text + trail).replace(\n            '&amp;', '&').replace('&lt;', '<').replace(\n            '&gt;', '>').replace('&quot;', '\"').replace('&#39;', \"'\")\n        if trail and unescaped.endswith(trail):\n            # Remove trail for unescaped if it was not consumed by unescape\n            unescaped = unescaped[:-len(trail)]\n        elif trail == ';':\n            # Trail was consumed by unescape (as end-of-entity marker), move it to text\n            text += trail\n            trail = ''\n        return text, unescaped, trail\n\n    def trim_punctuation(lead, middle, trail):\n        \"\"\"\n        Trim trailing and wrapping punctuation from `middle`. Return the items\n        of the new state.\n        \"\"\"\n        # Continue trimming until middle remains unchanged.\n        trimmed_something = True\n        while trimmed_something:\n            trimmed_something = False\n\n            # Trim trailing punctuation.\n            match = TRAILING_PUNCTUATION_RE.match(middle)\n            if match:\n                middle = match.group(1)\n                trail = match.group(2) + trail\n                trimmed_something = True\n\n            # Trim wrapping punctuation.\n            for opening, closing in WRAPPING_PUNCTUATION:\n                if middle.startswith(opening):\n                    middle = middle[len(opening):]\n                    lead += opening\n                    trimmed_something = True\n                # Keep parentheses at the end only if they're balanced.\n                if (middle.endswith(closing) and\n                        middle.count(closing) == middle.count(opening) + 1):\n                    middle = middle[:-len(closing)]\n                    trail = closing + trail\n                    trimmed_something = True\n        return lead, middle, trail\n\n    words = word_split_re.split(force_text(text))\n    for i, word in enumerate(words):\n        if '.' in word or '@' in word or ':' in word:\n            # lead: Current punctuation trimmed from the beginning of the word.\n            # middle: Current state of the word.\n            # trail: Current punctuation trimmed from the end of the word.\n            lead, middle, trail = '', word, ''\n            # Deal with punctuation.\n            lead, middle, trail = trim_punctuation(lead, middle, trail)\n\n            # Make URL we want to point to.\n            url = None\n            nofollow_attr = ' rel=\"nofollow\"' if nofollow else ''\n            if simple_url_re.match(middle):\n                middle, middle_unescaped, trail = unescape(middle, trail)\n                url = smart_urlquote(middle_unescaped)\n            elif simple_url_2_re.match(middle):\n                middle, middle_unescaped, trail = unescape(middle, trail)\n                url = smart_urlquote('http://%s' % middle_unescaped)\n            elif ':' not in middle and simple_email_re.match(middle):\n                local, domain = middle.rsplit('@', 1)\n                try:\n                    domain = domain.encode('idna').decode('ascii')\n                except UnicodeError:\n                    continue\n                url = 'mailto:%s@%s' % (local, domain)\n                nofollow_attr = ''\n\n            # Make link.\n            if url:\n                trimmed = trim_url(middle)\n                if autoescape and not safe_input:\n                    lead, trail = escape(lead), escape(trail)\n                    trimmed = escape(trimmed)\n                middle = '<a href=\"%s\"%s>%s</a>' % (escape(url), nofollow_attr, trimmed)\n                words[i] = mark_safe('%s%s%s' % (lead, middle, trail))\n            else:\n                if safe_input:\n                    words[i] = mark_safe(word)\n                elif autoescape:\n                    words[i] = escape(word)\n        elif safe_input:\n            words[i] = mark_safe(word)\n        elif autoescape:\n            words[i] = escape(word)\n    return ''.join(words)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n        key = self.make_key(key, version=version)\n        return self._cache.add(key, value, self.get_backend_timeout(timeout))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _redirect_safe(self, url, default=None):\n        \"\"\"Redirect if url is on our PATH\n\n        Full-domain redirects are allowed if they pass our CORS origin checks.\n\n        Otherwise use default (self.base_url if unspecified).\n        \"\"\"\n        if default is None:\n            default = self.base_url\n        parsed = urlparse(url)\n        if parsed.netloc or not (parsed.path + '/').startswith(self.base_url):\n            # require that next_url be absolute path within our path\n            allow = False\n            # OR pass our cross-origin check\n            if parsed.netloc:\n                # if full URL, run our cross-origin check:\n                origin = '%s://%s' % (parsed.scheme, parsed.netloc)\n                origin = origin.lower()\n                if self.allow_origin:\n                    allow = self.allow_origin == origin\n                elif self.allow_origin_pat:\n                    allow = bool(self.allow_origin_pat.match(origin))\n            if not allow:\n                # not allowed, use default\n                self.log.warning(\"Not allowing login redirect to %r\" % url)\n                url = default\n        self.redirect(url)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_columns(self) -> list[str]:\n        \"\"\"A list of columns to show in the Add form.\n\n        This dynamically calculates additional fields from providers and add\n        them to the backing list. This calculation is done exactly once (by\n        checking we're referencing the class-level variable instead of the\n        instance-level), and only after we enter the request context (to skip\n        superfuluous checks done by Flask-Appbuilder on startup).\n        \"\"\"\n        if self._add_columns is type(self)._add_columns and has_request_context():\n            self._add_columns = [*self._add_columns, *(k for k, _ in self._iter_extra_field_names())]\n        return self._add_columns\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _check_signature(self, decoded_xml, item, node_name=NODE_NAME, origdoc=None, id_attr='', must=False, only_valid_cert=False, issuer=None):\n        try:\n            _issuer = item.issuer.text.strip()\n        except AttributeError:\n            _issuer = None\n\n        if _issuer is None:\n            try:\n                _issuer = issuer.text.strip()\n            except AttributeError:\n                _issuer = None\n\n        # More trust in certs from metadata then certs in the XML document\n        if self.metadata:\n            try:\n                _certs = self.metadata.certs(_issuer, 'any', 'signing')\n            except KeyError:\n                _certs = []\n            certs = []\n\n            for cert in _certs:\n                if isinstance(cert, six.string_types):\n                    content = pem_format(cert)\n                    tmp = make_temp(content,\n                                    suffix=\".pem\",\n                                    decode=False,\n                                    delete_tmpfiles=self.delete_tmpfiles)\n                    certs.append(tmp)\n                else:\n                    certs.append(cert)\n        else:\n            certs = []\n\n        if not certs and not self.only_use_keys_in_metadata:\n            logger.debug('==== Certs from instance ====')\n            certs = [\n                make_temp(content=pem_format(cert),\n                          suffix=\".pem\",\n                          decode=False,\n                          delete_tmpfiles=self.delete_tmpfiles)\n                for cert in cert_from_instance(item)\n            ]\n        else:\n            logger.debug('==== Certs from metadata ==== %s: %s ====', _issuer, certs)\n\n        if not certs:\n            raise MissingKey(_issuer)\n\n        verified = False\n        last_pem_file = None\n\n        for pem_fd in certs:\n            try:\n                last_pem_file = pem_fd.name\n                if self.verify_signature(\n                        decoded_xml,\n                        pem_fd.name,\n                        node_name=node_name,\n                        node_id=item.id,\n                        id_attr=id_attr):\n                    verified = True\n                    break\n            except XmlsecError as exc:\n                logger.error('check_sig: %s', exc)\n                pass\n            except Exception as exc:\n                logger.error('check_sig: %s', exc)\n                raise\n\n        if verified or only_valid_cert:\n            if not self.cert_handler.verify_cert(last_pem_file):\n                raise CertificateError('Invalid certificate!')\n        else:\n            raise SignatureError('Failed to verify signature')\n\n        return item\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "function notify_update() {\n  jsonWrapper(URL_NOTIFY_GET_NEW+notify_latest_id+'/', function (data) {\n    if (data.success) {\n      $('.notification-cnt').html(data.total_count);\n      if (data.objects.length> 0) {\n        $('.notifications-empty').hide();\n      }\n      if (data.total_count > 0) {\n        $('.notification-cnt').addClass('badge-important');\n      } else {\n        $('.notification-cnt').removeClass('badge-important');\n      }\n      for (var i=data.objects.length-1; i >=0 ; i--) {\n        n = data.objects[i];\n        notify_latest_id = n.pk>notify_latest_id ? n.pk:notify_latest_id;\n        notify_oldest_id = (n.pk<notify_oldest_id || notify_oldest_id==0) ? n.pk:notify_oldest_id;\n        if (n.occurrences > 1) {\n          element = $('<div><a href=\"'+URL_NOTIFY_GOTO+n.pk+'/\"><div>'+n.message+'</div><div class=\"since\">'+n.occurrences_msg+' - ' + n.since + '</div></a></div>')\n        } else {\n          element = $('<div><a href=\"'+URL_NOTIFY_GOTO+n.pk+'/\"><div>'+n.message+'</div><div class=\"since\">'+n.since+'</div></a></div>');\n        }\n        element.addClass('dropdown-item notification-item');\n        element.insertAfter('.notification-before-list');\n      }\n    }\n  });\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_object_element(dataset, seq_no, rel_path, download_link):\n    \"\"\"If rel_path and download_link are not None, we are called from scope.\n    Otherwise we are called from ID and need to run SQL query to fetch these attrs.\"\"\"\n\n    if rel_path is None:\n        query = \"SELECT rel_path, download_link FROM \" + \\\n        dataset + \\\n        \" WHERE sequence_no = %s\"\n\n        cnx = mysql.connector.connect(user=DB_USER,\n                                    password=DB_PASSWORD,\n                                    host=DB_HOST,\n                                    database=DB_DBNAME,\n                                    port=DB_PORT)\n        cursor = cnx.cursor()\n        cursor.execute(query, (seq_no,))\n\n        row = cursor.fetchone()\n\n        if not row:\n            return None\n\n        rel_path, download_link = row[0], row[1]\n\n    if LOCAL_OBJ_URI:\n        src_uri = 'file://' + os.path.join(DATAROOT, dataset, rel_path)\n    else:\n        src_uri = url_for('.get_object_src_http', dataset=dataset, rel_path=rel_path)\n\n    return '<object id={} src={} hyperfind.external-link={} />' \\\n        .format(\n        quoteattr(url_for('.get_object_id', dataset=dataset, seq_no=seq_no)),\n        quoteattr(src_uri),\n        quoteattr(download_link))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static void SpatialMaxPoolWithArgMaxHelper(\n    OpKernelContext* context, Tensor* output, Tensor* output_arg_max,\n    Tensor* input_backprop, const Tensor& tensor_in, const Tensor& out_backprop,\n    const PoolParameters& params, const bool include_batch_in_index) {\n  if (input_backprop != nullptr) {\n    OP_REQUIRES(\n        context, include_batch_in_index,\n        errors::Internal(\n            \"SpatialMaxPoolWithArgMaxHelper requires include_batch_in_index \"\n            \"to be True when input_backprop != nullptr\"));\n    OP_REQUIRES(\n        context, (std::is_same<Targmax, int64>::value),\n        errors::Internal(\"SpatialMaxPoolWithArgMaxHelper requires Targmax \"\n                         \"to be int64 when input_backprop != nullptr\"));\n  }\n\n  typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n      ConstEigenMatrixMap;\n  typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n      EigenMatrixMap;\n  typedef Eigen::Map<Eigen::Matrix<Targmax, Eigen::Dynamic, Eigen::Dynamic>>\n      EigenIndexMatrixMap;\n\n  ConstEigenMatrixMap in_mat(\n      tensor_in.flat<T>().data(), params.depth,\n      params.tensor_in_cols * params.tensor_in_rows * params.tensor_in_batch);\n  EigenMatrixMap out_mat(\n      output->flat<T>().data(), params.depth,\n      params.out_width * params.out_height * params.tensor_in_batch);\n  EigenIndexMatrixMap out_arg_max_mat(\n      output_arg_max->flat<Targmax>().data(), params.depth,\n      params.out_width * params.out_height * params.tensor_in_batch);\n\n  const DeviceBase::CpuWorkerThreads& worker_threads =\n      *(context->device()->tensorflow_cpu_worker_threads());\n\n  // The following code basically does the following:\n  // 1. Flattens the input and output tensors into two dimensional arrays.\n  //    tensor_in_as_matrix:\n  //      depth by (tensor_in_cols * tensor_in_rows * tensor_in_batch)\n  //    output_as_matrix:\n  //      depth by (out_width * out_height * tensor_in_batch)\n  //\n  // 2. Walks through the set of columns in the flattened tensor_in_as_matrix,\n  //    and updates the corresponding column(s) in output_as_matrix with the\n  //    max value.\n  auto shard = [&params, &in_mat, &out_mat, &out_arg_max_mat, &input_backprop,\n                &output_arg_max, &out_backprop,\n                include_batch_in_index](int64_t start, int64_t limit) {\n    const int32_t depth = params.depth;\n    const int32_t in_rows = params.tensor_in_rows;\n    const int32_t in_cols = params.tensor_in_cols;\n    const int32_t pad_top = params.pad_top;\n    const int32_t pad_left = params.pad_left;\n    const int32_t window_rows = params.window_rows;\n    const int32_t window_cols = params.window_cols;\n    const int32_t row_stride = params.row_stride;\n    const int32_t col_stride = params.col_stride;\n    const int32_t out_height = params.out_height;\n    const int32_t out_width = params.out_width;\n\n    {\n      // Initializes the output tensor with MIN<T>.\n      const int32_t output_image_size = out_height * out_width * depth;\n      EigenMatrixMap out_shard(out_mat.data() + start * output_image_size, 1,\n                               (limit - start) * output_image_size);\n      out_shard.setConstant(Eigen::NumTraits<T>::lowest());\n      EigenIndexMatrixMap out_arg_max_shard(\n          out_arg_max_mat.data() + start * output_image_size, 1,\n          (limit - start) * output_image_size);\n      out_arg_max_shard.setConstant(kInvalidMaxPoolingIndex);\n    }\n\n    for (int64_t b = start; b < limit; ++b) {\n      for (int h = 0; h < in_rows; ++h) {\n        for (int w = 0; w < in_cols; ++w) {\n          // (h_start, h_end) * (w_start, w_end) is the range that the input\n          // vector projects to.\n          const int hpad = h + pad_top;\n          const int wpad = w + pad_left;\n          const int h_start =\n              (hpad < window_rows) ? 0 : (hpad - window_rows) / row_stride + 1;\n          const int h_end = std::min(hpad / row_stride + 1, out_height);\n          const int w_start =\n              (wpad < window_cols) ? 0 : (wpad - window_cols) / col_stride + 1;\n          const int w_end = std::min(wpad / col_stride + 1, out_width);\n          // compute elementwise max\n          const int64_t in_index = (b * in_rows + h) * in_cols + w;\n          for (int ph = h_start; ph < h_end; ++ph) {\n            const int64_t out_index_base = (b * out_height + ph) * out_width;\n            for (int pw = w_start; pw < w_end; ++pw) {\n              const int64_t out_index = out_index_base + pw;\n              /// NOTES(zhengxq): not using the eigen matrix operation for\n              /// now.\n              for (int d = 0; d < depth; ++d) {\n                const T& input_ref = in_mat.coeffRef(d, in_index);\n                T& output_ref = out_mat.coeffRef(d, out_index);\n                Targmax& out_arg_max_ref =\n                    out_arg_max_mat.coeffRef(d, out_index);\n                if (output_ref < input_ref ||\n                    out_arg_max_ref == kInvalidMaxPoolingIndex) {\n                  output_ref = input_ref;\n                  if (include_batch_in_index) {\n                    out_arg_max_ref = in_index * depth + d;\n                  } else {\n                    out_arg_max_ref = (h * in_cols + w) * depth + d;\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n\n    if (input_backprop != nullptr) {\n      auto input_backprop_flat = input_backprop->flat<T>();\n      auto out_arg_max_flat = output_arg_max->flat<int64>();\n      auto out_backprop_flat = out_backprop.flat<T>();\n\n      // Initialize output to 0.\n      const int64_t in_size = in_rows * in_cols * depth;\n      const int64_t in_start = start * in_size;\n      const int64_t in_end = limit * in_size;\n      EigenMatrixMap in_shard(input_backprop_flat.data() + in_start, 1,\n                              in_end - in_start);\n      in_shard.setConstant(T(0));\n\n      // Backpropagate.\n      const int out_size = out_height * out_width * depth;\n      const int out_start = start * out_size;\n      const int out_end = limit * out_size;\n      for (int index = out_start; index < out_end; ++index) {\n        int input_backprop_index = out_arg_max_flat(index);\n        // Although this check is in the inner loop, it is worth its value\n        // so we don't end up with memory corruptions. Our benchmark shows that\n        // the performance impact is quite small\n        // CHECK(input_backprop_index >= in_start && input_backprop_index <\n        // in_end)\n        FastBoundsCheck(input_backprop_index - in_start, in_end - in_start);\n        if (index < out_backprop.NumElements()) {\n          input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n        }\n      }\n    }\n  };\n\n  const int64_t shard_cost = params.tensor_in_rows * params.tensor_in_cols *\n                             params.depth * params.window_rows *\n                             params.window_cols;\n  Shard(worker_threads.num_threads, worker_threads.workers,\n        params.tensor_in_batch, shard_cost, shard);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _notify(self, assoc, attempt):\n        \"\"\"\n        Sends data about a new association (and, if necessary, the associated invites)\n        to the associated MXID's homeserver.\n\n        :param assoc: The association to send down to the homeserver.\n        :type assoc: dict[str, any]\n        :param attempt: The number of previous attempts to send this association.\n        :type attempt: int\n        \"\"\"\n        mxid = assoc[\"mxid\"]\n        mxid_parts = mxid.split(\":\", 1)\n        if len(mxid_parts) != 2:\n            logger.error(\n                \"Can't notify on bind for unparseable mxid %s. Not retrying.\",\n                assoc[\"mxid\"],\n            )\n            return\n\n        post_url = \"matrix://%s/_matrix/federation/v1/3pid/onbind\" % (\n            mxid_parts[1],\n        )\n\n        logger.info(\"Making bind callback to: %s\", post_url)\n\n        # Make a POST to the chosen Synapse server\n        http_client = FederationHttpClient(self.sydent)\n        try:\n            response = yield http_client.post_json_get_nothing(post_url, assoc, {})\n        except Exception as e:\n            self._notifyErrback(assoc, attempt, e)\n            return\n\n        # If the request failed, try again with exponential backoff\n        if response.code != 200:\n            self._notifyErrback(\n                assoc, attempt, \"Non-OK error code received (%d)\" % response.code\n            )\n        else:\n            logger.info(\"Successfully notified on bind for %s\" % (mxid,))\n\n            # Skip the deletion step if instructed so by the config.\n            if not self.sydent.delete_tokens_on_bind:\n                return\n\n            # Only remove sent tokens when they've been successfully sent.\n            try:\n                joinTokenStore = JoinTokenStore(self.sydent)\n                joinTokenStore.deleteTokens(assoc[\"medium\"], assoc[\"address\"])\n                logger.info(\n                    \"Successfully deleted invite for %s from the store\",\n                    assoc[\"address\"],\n                )\n            except Exception as e:\n                logger.exception(\n                    \"Couldn't remove invite for %s from the store\",\n                    assoc[\"address\"],\n                )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _truncate_html(self, length, truncate, text, truncate_len, words):\n        \"\"\"\n        Truncate HTML to a certain number of chars (not counting tags and\n        comments), or, if words is True, then to a certain number of words.\n        Close opened tags if they were correctly closed in the given HTML.\n\n        Preserve newlines in the HTML.\n        \"\"\"\n        if words and length <= 0:\n            return ''\n\n        html4_singlets = (\n            'br', 'col', 'link', 'base', 'img',\n            'param', 'area', 'hr', 'input'\n        )\n\n        # Count non-HTML chars/words and keep note of open tags\n        pos = 0\n        end_text_pos = 0\n        current_len = 0\n        open_tags = []\n\n        regex = re_words if words else re_chars\n\n        while current_len <= length:\n            m = regex.search(text, pos)\n            if not m:\n                # Checked through whole string\n                break\n            pos = m.end(0)\n            if m[1]:\n                # It's an actual non-HTML word or char\n                current_len += 1\n                if current_len == truncate_len:\n                    end_text_pos = pos\n                continue\n            # Check for tag\n            tag = re_tag.match(m[0])\n            if not tag or current_len >= truncate_len:\n                # Don't worry about non tags or tags after our truncate point\n                continue\n            closing_tag, tagname, self_closing = tag.groups()\n            # Element names are always case-insensitive\n            tagname = tagname.lower()\n            if self_closing or tagname in html4_singlets:\n                pass\n            elif closing_tag:\n                # Check for match in open tags list\n                try:\n                    i = open_tags.index(tagname)\n                except ValueError:\n                    pass\n                else:\n                    # SGML: An end tag closes, back to the matching start tag,\n                    # all unclosed intervening start tags with omitted end tags\n                    open_tags = open_tags[i + 1:]\n            else:\n                # Add it to the start of the open tags list\n                open_tags.insert(0, tagname)\n\n        if current_len <= length:\n            return text\n        out = text[:end_text_pos]\n        truncate_text = self.add_truncation_text('', truncate)\n        if truncate_text:\n            out += truncate_text\n        # Close any tags still open\n        for tag in open_tags:\n            out += '</%s>' % tag\n        # Return string\n        return out\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    Tensor* output_tensor;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(c, c->allocate_output(0, {}, &output_tensor, attr));\n    PartialTensorShape element_shape;\n    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(1), &element_shape));\n    TensorList output_list;\n    const Tensor& t = c->input(0);\n    output_list.element_dtype = t.dtype();\n    OP_REQUIRES(c, TensorShapeUtils::IsVectorOrHigher(t.shape()),\n                errors::InvalidArgument(\n                    \"Tensor must be at least a vector, but saw shape: \",\n                    t.shape().DebugString()));\n    TensorShape output_shape(t.shape());\n    output_shape.RemoveDim(0);\n    OP_REQUIRES(c, element_shape.IsCompatibleWith(output_shape),\n                errors::InvalidArgument(\n                    \"Specified a list with shape \", element_shape.DebugString(),\n                    \" from a tensor with shape \", output_shape.DebugString()));\n    output_list.element_shape = element_shape;\n    output_list.tensors().reserve(t.shape().dim_size(0));\n\n    const auto copy_tensor = IsPluggableDevice(c)\n                                 ? &CopyTensorPluggableDevice<T>\n                                 : &CopyTensor<Device, T>;\n\n    for (int i = 0; i < t.shape().dim_size(0); ++i) {\n      Tensor tmp = t.Slice(i, i + 1);\n      TensorShape tmp_shape = tmp.shape();\n      tmp_shape.RemoveDim(0);\n      OP_REQUIRES(c, tmp.CopyFrom(tmp, tmp_shape),\n                  errors::Unknown(\"Unexpected shape error.\"));\n      // TODO(apassos) maybe not always align; but weird compiler bugs seem to\n      // prevent this.\n      Tensor aligned;\n      OP_REQUIRES_OK(c, c->allocate_temp(tmp.dtype(), tmp.shape(), &aligned));\n      copy_tensor(c, tmp, aligned);\n      output_list.tensors().push_back(aligned);\n    }\n    output_tensor->scalar<Variant>()() = std::move(output_list);\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def load_pkcs12(buffer, passphrase=None):\n    \"\"\"\n    Load a PKCS12 object from a buffer\n\n    :param buffer: The buffer the certificate is stored in\n    :param passphrase: (Optional) The password to decrypt the PKCS12 lump\n    :returns: The PKCS12 object\n    \"\"\"\n    passphrase = _text_to_bytes_and_warn(\"passphrase\", passphrase)\n\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode(\"ascii\")\n\n    bio = _new_mem_buf(buffer)\n\n    # Use null passphrase if passphrase is None or empty string. With PKCS#12\n    # password based encryption no password and a zero length password are two\n    # different things, but OpenSSL implementation will try both to figure out\n    # which one works.\n    if not passphrase:\n        passphrase = _ffi.NULL\n\n    p12 = _lib.d2i_PKCS12_bio(bio, _ffi.NULL)\n    if p12 == _ffi.NULL:\n        _raise_current_error()\n    p12 = _ffi.gc(p12, _lib.PKCS12_free)\n\n    pkey = _ffi.new(\"EVP_PKEY**\")\n    cert = _ffi.new(\"X509**\")\n    cacerts = _ffi.new(\"Cryptography_STACK_OF_X509**\")\n\n    parse_result = _lib.PKCS12_parse(p12, passphrase, pkey, cert, cacerts)\n    if not parse_result:\n        _raise_current_error()\n\n    cacerts = _ffi.gc(cacerts[0], _lib.sk_X509_free)\n\n    # openssl 1.0.0 sometimes leaves an X509_check_private_key error in the\n    # queue for no particular reason.  This error isn't interesting to anyone\n    # outside this function.  It's not even interesting to us.  Get rid of it.\n    try:\n        _raise_current_error()\n    except Error:\n        pass\n\n    if pkey[0] == _ffi.NULL:\n        pykey = None\n    else:\n        pykey = PKey.__new__(PKey)\n        pykey._pkey = _ffi.gc(pkey[0], _lib.EVP_PKEY_free)\n\n    if cert[0] == _ffi.NULL:\n        pycert = None\n        friendlyname = None\n    else:\n        pycert = X509.__new__(X509)\n        pycert._x509 = _ffi.gc(cert[0], _lib.X509_free)\n\n        friendlyname_length = _ffi.new(\"int*\")\n        friendlyname_buffer = _lib.X509_alias_get0(\n            cert[0], friendlyname_length\n        )\n        friendlyname = _ffi.buffer(\n            friendlyname_buffer, friendlyname_length[0]\n        )[:]\n        if friendlyname_buffer == _ffi.NULL:\n            friendlyname = None\n\n    pycacerts = []\n    for i in range(_lib.sk_X509_num(cacerts)):\n        pycacert = X509.__new__(X509)\n        pycacert._x509 = _lib.sk_X509_value(cacerts, i)\n        pycacerts.append(pycacert)\n    if not pycacerts:\n        pycacerts = None\n\n    pkcs12 = PKCS12.__new__(PKCS12)\n    pkcs12._pkey = pykey\n    pkcs12._cert = pycert\n    pkcs12._cacerts = pycacerts\n    pkcs12._friendlyname = friendlyname\n    return pkcs12\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& x = context->input(0);\n    const Tensor& y = context->input(1);\n    const float min_x = context->input(2).flat<float>()(0);\n    const float max_x = context->input(3).flat<float>()(0);\n    const float min_y = context->input(4).flat<float>()(0);\n    const float max_y = context->input(5).flat<float>()(0);\n\n    BCast bcast(BCast::FromShape(x.shape()), BCast::FromShape(y.shape()));\n    if (!bcast.IsValid()) {\n      context->SetStatus(errors::InvalidArgument(\n          \"Incompatible shapes: \", x.shape().DebugString(), \" vs. \",\n          y.shape().DebugString()));\n      return;\n    }\n    Tensor* z;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0, BCast::ToShape(bcast.output_shape()), &z));\n\n    // Make sure that we have valid quantization ranges for the input buffers.\n    // If the difference between the min and max is negative or zero, it makes\n    // it hard to do meaningful intermediate operations on the values.\n    OP_REQUIRES(context, (max_x > min_x),\n                errors::InvalidArgument(\"max_x must be larger than min_a.\"));\n    OP_REQUIRES(context, (max_y > min_y),\n                errors::InvalidArgument(\"max_x must be larger than min_b.\"));\n    const int32 offset_x = FloatToQuantizedUnclamped<T>(0.0f, min_x, max_x);\n    const int32 offset_y = FloatToQuantizedUnclamped<T>(0.0f, min_y, max_y);\n    const T* x_data = x.flat<T>().data();\n    const T* y_data = y.flat<T>().data();\n    Toutput* z_data = z->flat<Toutput>().data();\n\n    const int ndims = bcast.x_reshape().size();\n    if (ndims <= 1) {\n      if (x.NumElements() == 1) {\n        ScalarMultiply<T, Toutput>(context, y_data, offset_y, y.NumElements(),\n                                   x_data[0], offset_x, z_data);\n      } else if (y.NumElements() == 1) {\n        ScalarMultiply<T, Toutput>(context, x_data, offset_x, x.NumElements(),\n                                   y_data[0], offset_y, z_data);\n      } else {\n        VectorMultiply<T, Toutput>(context, x_data, offset_x, y_data, offset_y,\n                                   x.NumElements(), z_data);\n      }\n    } else if (ndims == 2) {\n      const T* vector_data;\n      int64 vector_num_elements;\n      int32 vector_offset;\n      const T* tensor_data;\n      int64 tensor_num_elements;\n      int32 tensor_offset;\n      if (x.NumElements() < y.NumElements()) {\n        vector_data = x_data;\n        vector_num_elements = x.NumElements();\n        vector_offset = offset_x;\n        tensor_data = y_data;\n        tensor_num_elements = y.NumElements();\n        tensor_offset = offset_y;\n      } else {\n        vector_data = y_data;\n        vector_num_elements = y.NumElements();\n        vector_offset = offset_y;\n        tensor_data = x_data;\n        tensor_num_elements = x.NumElements();\n        tensor_offset = offset_x;\n      }\n      VectorTensorMultiply<T, Toutput>(\n          vector_data, vector_offset, vector_num_elements, tensor_data,\n          tensor_offset, tensor_num_elements, z_data);\n    } else {\n      LOG(INFO) << \"ndims=\" << ndims;\n      LOG(INFO) << \"bcast.x_reshape()=\"\n                << TensorShape(bcast.x_reshape()).DebugString();\n      LOG(INFO) << \"bcast.y_reshape()=\"\n                << TensorShape(bcast.y_reshape()).DebugString();\n      LOG(INFO) << \"bcast.x_bcast()=\"\n                << TensorShape(bcast.x_bcast()).DebugString();\n      LOG(INFO) << \"bcast.y_bcast()=\"\n                << TensorShape(bcast.y_bcast()).DebugString();\n\n      context->SetStatus(errors::Unimplemented(\n          \"Broadcast between \", context->input(0).shape().DebugString(),\n          \" and \", context->input(1).shape().DebugString(),\n          \" is not supported yet.\"));\n      return;\n    }\n\n    float min_z_value;\n    float max_z_value;\n    QuantizationRangeForMultiplication<T, T, Toutput>(\n        min_x, max_x, min_y, max_y, &min_z_value, &max_z_value);\n    Tensor* z_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &z_min));\n    z_min->flat<float>()(0) = min_z_value;\n\n    Tensor* z_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &z_max));\n    z_max->flat<float>()(0) = max_z_value;\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def value_from_datadict(self, data, files, name):\n        \"File widgets take data from FILES, not POST\"\n        return files.get(name)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def as_sql(self, compiler, connection):\n        sql, params = compiler.compile(self.lhs)\n        lhs_output_field = self.lhs.output_field\n        if isinstance(lhs_output_field, DateTimeField):\n            tzname = self.get_tzname()\n            sql = connection.ops.datetime_extract_sql(self.lookup_name, sql, tzname)\n        elif self.tzinfo is not None:\n            raise ValueError('tzinfo can only be used with DateTimeField.')\n        elif isinstance(lhs_output_field, DateField):\n            sql = connection.ops.date_extract_sql(self.lookup_name, sql)\n        elif isinstance(lhs_output_field, TimeField):\n            sql = connection.ops.time_extract_sql(self.lookup_name, sql)\n        elif isinstance(lhs_output_field, DurationField):\n            if not connection.features.has_native_duration_field:\n                raise ValueError('Extract requires native DurationField database support.')\n            sql = connection.ops.time_extract_sql(self.lookup_name, sql)\n        else:\n            # resolve_expression has already validated the output_field so this\n            # assert should never be hit.\n            assert False, \"Tried to Extract from an invalid type.\"\n        return sql, params\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def handle(self, body):\n        \"\"\"\n        Synchronous message processing.\n        \"\"\"\n        # Set script prefix from message root_path, turning None into empty string\n        script_prefix = self.scope.get(\"root_path\", \"\") or \"\"\n        if settings.FORCE_SCRIPT_NAME:\n            script_prefix = settings.FORCE_SCRIPT_NAME\n        set_script_prefix(script_prefix)\n        signals.request_started.send(sender=self.__class__, scope=self.scope)\n        # Run request through view system\n        try:\n            request = self.request_class(self.scope, body)\n        except UnicodeDecodeError:\n            logger.warning(\n                \"Bad Request (UnicodeDecodeError)\",\n                exc_info=sys.exc_info(),\n                extra={\"status_code\": 400},\n            )\n            response = http.HttpResponseBadRequest()\n        except RequestTimeout:\n            # Parsing the request failed, so the response is a Request Timeout error\n            response = HttpResponse(\"408 Request Timeout (upload too slow)\", status=408)\n        except RequestAborted:\n            # Client closed connection on us mid request. Abort!\n            return\n        except RequestDataTooBig:\n            response = HttpResponse(\"413 Payload too large\", status=413)\n        else:\n            response = self.get_response(request)\n            # Fix chunk size on file responses\n            if isinstance(response, FileResponse):\n                response.block_size = 1024 * 512\n        # Transform response into messages, which we yield back to caller\n        for response_message in self.encode_response(response):\n            self.send(response_message)\n        # Close the response now we're done with it\n        response.close()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _construct_forms(self):\n        # instantiate all the forms and put them in self.forms\n        self.forms = []\n        for i in xrange(self.total_form_count()):\n            self.forms.append(self._construct_form(i))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def send(self, conn: \"Connection\") -> \"ClientResponse\":\n        # Specify request target:\n        # - CONNECT request must send authority form URI\n        # - not CONNECT proxy must send absolute form URI\n        # - most common is origin form URI\n        if self.method == hdrs.METH_CONNECT:\n            connect_host = self.url.raw_host\n            assert connect_host is not None\n            if helpers.is_ipv6_address(connect_host):\n                connect_host = f\"[{connect_host}]\"\n            path = f\"{connect_host}:{self.url.port}\"\n        elif self.proxy and not self.is_ssl():\n            path = str(self.url)\n        else:\n            path = self.url.raw_path\n            if self.url.raw_query_string:\n                path += \"?\" + self.url.raw_query_string\n\n        protocol = conn.protocol\n        assert protocol is not None\n        writer = StreamWriter(\n            protocol,\n            self.loop,\n            on_chunk_sent=functools.partial(\n                self._on_chunk_request_sent, self.method, self.url\n            ),\n            on_headers_sent=functools.partial(\n                self._on_headers_request_sent, self.method, self.url\n            ),\n        )\n\n        if self.compress:\n            writer.enable_compression(self.compress)\n\n        if self.chunked is not None:\n            writer.enable_chunking()\n\n        # set default content-type\n        if (\n            self.method in self.POST_METHODS\n            and hdrs.CONTENT_TYPE not in self.skip_auto_headers\n            and hdrs.CONTENT_TYPE not in self.headers\n        ):\n            self.headers[hdrs.CONTENT_TYPE] = \"application/octet-stream\"\n\n        # set the connection header\n        connection = self.headers.get(hdrs.CONNECTION)\n        if not connection:\n            if self.keep_alive():\n                if self.version == HttpVersion10:\n                    connection = \"keep-alive\"\n            else:\n                if self.version == HttpVersion11:\n                    connection = \"close\"\n\n        if connection is not None:\n            self.headers[hdrs.CONNECTION] = connection\n\n        # status + headers\n        status_line = \"{0} {1} HTTP/{2[0]}.{2[1]}\".format(\n            self.method, path, self.version\n        )\n        await writer.write_headers(status_line, self.headers)\n\n        self._writer = self.loop.create_task(self.write_bytes(writer, conn))\n\n        response_class = self.response_class\n        assert response_class is not None\n        self.response = response_class(\n            self.method,\n            self.original_url,\n            writer=self._writer,\n            continue100=self._continue,\n            timer=self._timer,\n            request_info=self.request_info,\n            traces=self._traces,\n            loop=self.loop,\n            session=self._session,\n        )\n        return self.response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def on_PUT(self, origin, content, query, room_id):\n        content = await self.handler.on_exchange_third_party_invite_request(\n            room_id, content\n        )\n        return 200, content\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def save_template(cls, db: Session, zip_file: ZipFile) -> None:\n        \"\"\"\n        Extracts and validates the contents of a zip file containing a\n        custom connector template, registers the template, and saves it to the database.\n        \"\"\"\n\n        config_contents = None\n        dataset_contents = None\n        icon_contents = None\n        function_contents = None\n\n        for info in zip_file.infolist():\n            try:\n                file_contents = zip_file.read(info).decode()\n            except UnicodeDecodeError:\n                # skip any hidden metadata files that can't be decoded with UTF-8\n                logger.debug(f\"Unable to decode the file: {info.filename}\")\n                continue\n\n            if info.filename.endswith(\"config.yml\"):\n                if not config_contents:\n                    config_contents = file_contents\n                else:\n                    raise ValidationError(\n                        \"Multiple files ending with config.yml found, only one is allowed.\"\n                    )\n            elif info.filename.endswith(\"dataset.yml\"):\n                if not dataset_contents:\n                    dataset_contents = file_contents\n                else:\n                    raise ValidationError(\n                        \"Multiple files ending with dataset.yml found, only one is allowed.\"\n                    )\n            elif info.filename.endswith(\".svg\"):\n                if not icon_contents:\n                    icon_contents = str_to_b64_str(file_contents)\n                else:\n                    raise ValidationError(\n                        \"Multiple svg files found, only one is allowed.\"\n                    )\n            elif info.filename.endswith(\".py\"):\n                if not function_contents:\n                    function_contents = file_contents\n                else:\n                    raise ValidationError(\n                        \"Multiple Python (.py) files found, only one is allowed.\"\n                    )\n\n        if not config_contents:\n            raise ValidationError(\"Zip file does not contain a config.yml file.\")\n\n        if not dataset_contents:\n            raise ValidationError(\"Zip file does not contain a dataset.yml file.\")\n\n        # early validation of SaaS config and dataset\n        saas_config = SaaSConfig(**load_config_from_string(config_contents))\n        Dataset(**load_dataset_from_string(dataset_contents))\n\n        # extract connector_type, human_readable, and replaceable values from the SaaS config\n        connector_type = saas_config.type\n        human_readable = saas_config.name\n        replaceable = saas_config.replaceable\n\n        # if the incoming connector is flagged as replaceable we will update the version to match\n        # that of the existing connector template this way the custom connector template can be\n        # removed once a newer version is bundled with Fides\n        if replaceable:\n            existing_connector = (\n                FileConnectorTemplateLoader.get_connector_templates().get(\n                    connector_type\n                )\n            )\n            if existing_connector:\n                existing_config = SaaSConfig(\n                    **load_config_from_string(existing_connector.config)\n                )\n                config_contents = replace_version(\n                    config_contents, existing_config.version\n                )\n\n        template = CustomConnectorTemplate(\n            key=connector_type,\n            name=human_readable,\n            config=config_contents,\n            dataset=dataset_contents,\n            icon=icon_contents,\n            functions=function_contents,\n            replaceable=replaceable,\n        )\n\n        # attempt to register the template, raises an exception if validation fails\n        CustomConnectorTemplateLoader._register_template(template)\n\n        # save the custom connector to the database if it passed validation\n        CustomConnectorTemplate.create_or_update(\n            db=db,\n            data={\n                \"key\": connector_type,\n                \"name\": human_readable,\n                \"config\": config_contents,\n                \"dataset\": dataset_contents,\n                \"icon\": icon_contents,\n                \"functions\": function_contents,\n                \"replaceable\": replaceable,\n            },\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_safe_url(url, host=None):\n    \"\"\"\n    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to\n    a different host).\n\n    Always returns ``False`` on an empty url.\n    \"\"\"\n    if not url:\n        return False\n    netloc = urlparse.urlparse(url)[1]\n    return not netloc or netloc == host\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_session_id_by_provider_id(self, provider_id):\n        \"\"\"\n        Return a tuple of logged-in session IDs by provider_id\n        \"\"\"\n        candidates = self._ss.get_user_sessions(self.user)\n\n        session_ids = []\n        for c in candidates:\n            key = c.keys()[0]\n            if c[key].get('provider_id') == provider_id:\n                samlsession = self._data_to_samlsession(key, c[key])\n                session_ids.append(samlsession.session_id.encode('utf-8'))\n\n        return tuple(session_ids)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void ProcessMedianKernel(const Context& dev_ctx,\n                         const DenseTensor& x,\n                         DenseTensor* out,\n                         DenseTensor* median_index) {\n  const T* x_data = x.data<T>();\n  T* out_data = dev_ctx.template Alloc<T>(out);\n  int64_t* m_data = dev_ctx.template Alloc<int64_t>(median_index);\n\n  int64_t numel = x.numel();\n  auto x_dim = x.dims();\n  int64_t x_rank = x_dim.size();\n  int64_t stride = x_dim[x_rank - 1];\n  int64_t pre_dim = numel / stride;\n  int64_t i = 0;\n\n  int64_t max_valid_num = 0;\n  std::vector<int64_t> nan_counts;\n  bool ignore_nan = true;\n  if (ignore_nan) {\n    int64_t total_nan_num = 0;\n    std::vector<T> col_vec;\n    col_vec.reserve(stride);\n    col_vec.resize(stride);\n    nan_counts.clear();\n    nan_counts.reserve(pre_dim);\n    nan_counts.resize(pre_dim);\n    for (int64_t i = 0; i < pre_dim; i++) {\n      col_vec.clear();\n      col_vec.insert(\n          col_vec.begin(), x_data + i * stride, x_data + (i + 1) * stride);\n      nan_counts[i] =\n          std::count_if(col_vec.begin(), col_vec.end(), [&](const T& val) {\n            return std::isnan(static_cast<float>(val));\n          });\n      total_nan_num += nan_counts[i];\n      if (stride - nan_counts[i] > max_valid_num)\n        max_valid_num = stride - nan_counts[i];\n    }\n    // all elems are nan\n    if (total_nan_num == numel) {\n      for (i = 0; i < pre_dim; i++) {\n        out_data[i] = std::numeric_limits<T>::quiet_NaN();\n        m_data[2 * i] = -1;\n        m_data[2 * i + 1] = -1;\n      }\n      return;\n    }\n    ignore_nan = total_nan_num > 0;\n  }\n\n  int64_t sort_k = ignore_nan ? max_valid_num : ((stride >> 1) + 1);\n  CalcMedianFunc<T, Context>(dev_ctx,\n                             x,\n                             nan_counts,\n                             ignore_nan,\n                             sort_k,\n                             stride,\n                             pre_dim,\n                             out_data,\n                             m_data);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def decorated(*args, **kwargs):\n            __tracebackhide__ = True  # Hide from pytest traceback.\n\n            appbuilder = current_app.appbuilder\n\n            dag_id = (\n                request.args.get(\"dag_id\")\n                or request.form.get(\"dag_id\")\n                or (request.is_json and request.json.get(\"dag_id\"))\n                or None\n            )\n            if appbuilder.sm.check_authorization(permissions, dag_id):\n                return func(*args, **kwargs)\n            elif not g.user.is_anonymous and not g.user.perms:\n                return (\n                    render_template(\n                        \"airflow/no_roles_permissions.html\",\n                        hostname=get_hostname()\n                        if conf.getboolean(\"webserver\", \"EXPOSE_HOSTNAME\")\n                        else \"redact\",\n                        logout_url=appbuilder.get_url_for_logout,\n                    ),\n                    403,\n                )\n            else:\n                access_denied = \"Access is Denied\"\n                flash(access_denied, \"danger\")\n            return redirect(\n                url_for(\n                    appbuilder.sm.auth_view.__class__.__name__ + \".login\",\n                    next=request.url,\n                )\n            )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_many(self, data, timeout=DEFAULT_TIMEOUT, version=None):\n        safe_data = {}\n        original_keys = {}\n        for key, value in data.items():\n            safe_key = self.make_key(key, version=version)\n            safe_data[safe_key] = value\n            original_keys[safe_key] = key\n        failed_keys = self._cache.set_multi(safe_data, self.get_backend_timeout(timeout))\n        return [original_keys[k] for k in failed_keys]\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def creatorForNetloc(self, hostname, port):\n        \"\"\"\n        Create a L{client connection creator\n        <twisted.internet.interfaces.IOpenSSLClientConnectionCreator>} for a\n        given network location.\n\n        @param tls: The TLS protocol to create a connection for.\n        @type tls: L{twisted.protocols.tls.TLSMemoryBIOProtocol}\n\n        @param hostname: The hostname part of the URI.\n        @type hostname: L{bytes}\n\n        @param port: The port part of the URI.\n        @type port: L{int}\n\n        @return: a connection creator with appropriate verification\n            restrictions set\n        @rtype: L{client connection creator\n            <twisted.internet.interfaces.IOpenSSLClientConnectionCreator>}\n        \"\"\"\n        return optionsForClientTLS(hostname.decode(\"ascii\"))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def gen_response_objects(self):\n        for o in self.event_json['Event']['Object']:\n            self.response += object_to_entity(o)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def default(self, path=b\"\", **kwargs):\r\n        # Check permissions on path/repo\r\n        repo, path = self.app.store.get_repo_path(path)\r\n        # Check if path exists with fstats\r\n        path_obj = repo.fstat(path)\r\n        # Check user's permissions\r\n        is_maintainer()\r\n\r\n        # validate form\r\n        form = DeleteRepoForm()\r\n        if not form.validate():\r\n            raise cherrypy.HTTPError(400, form.error_message)\r\n\r\n        # Validate the name\r\n        if form.confirm.data != path_obj.display_name:\r\n            _logger.info(\"do not delete repo, bad confirmation %r != %r\", form.confirm.data, path_obj.display_name)\r\n            raise cherrypy.HTTPError(400, 'bad confirmation')\r\n\r\n        # Delete repository in background using a schedule task.\r\n        scheduled = cherrypy.engine.publish('schedule_task', repo.delete, path)\r\n        assert scheduled\r\n        raise cherrypy.HTTPRedirect(form.redirect.data)\r\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def load_djpeg(self):\n\n        # ALTERNATIVE: handle JPEGs via the IJG command line utilities\n\n        import tempfile, os\n        file = tempfile.mktemp()\n        os.system(\"djpeg %s >%s\" % (self.filename, file))\n\n        try:\n            self.im = Image.core.open_ppm(file)\n        finally:\n            try: os.unlink(file)\n            except: pass\n\n        self.mode = self.im.mode\n        self.size = self.im.size\n\n        self.tile = []\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_value(section: str, option: str) -> Response:\n    serializer = {\n        \"text/plain\": _config_to_text,\n        \"application/json\": _config_to_json,\n    }\n    return_type = request.accept_mimetypes.best_match(serializer.keys())\n    if return_type not in serializer:\n        return Response(status=HTTPStatus.NOT_ACCEPTABLE)\n    elif conf.getboolean(\"webserver\", \"expose_config\"):\n        if not conf.has_option(section, option):\n            raise NotFound(\n                \"Config not found.\", detail=f\"The option [{section}/{option}] is not found in config.\"\n            )\n        value = conf.get(section, option)\n\n        config = Config(\n            sections=[ConfigSection(name=section, options=[ConfigOption(key=option, value=value)])]\n        )\n        config_text = serializer[return_type](config)\n        return Response(config_text, headers={\"Content-Type\": return_type})\n    else:\n        raise PermissionDenied(\n            detail=(\n                \"Your Airflow administrator chose not to expose the configuration, most likely for security\"\n                \" reasons.\"\n            )\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _write(self, contents: dict):\n        LOGGER.debug(f'Writing to {self.path}')\n        with open(self.path, 'w') as fp:\n            fp.write(json.dumps(contents))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, value_type: VyperType, length: int):\n        if not 0 < length < 2**256:\n            raise InvalidType(\"Array length is invalid\")\n\n        super().__init__(UINT256_T, value_type)\n        self.length = length\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _load_from(self, data: bytes) -> None:\n        if data.strip() == b'':\n            data = XMP_EMPTY  # on some platforms lxml chokes on empty documents\n\n        def basic_parser(xml):\n            return parse(BytesIO(xml))\n\n        def strip_illegal_bytes_parser(xml):\n            return parse(BytesIO(re_xml_illegal_bytes.sub(b'', xml)))\n\n        def recovery_parser(xml):\n            parser = XMLParser(recover=True)\n            return parse(BytesIO(xml), parser)\n\n        def replace_with_empty_xmp(_xml=None):\n            log.warning(\"Error occurred parsing XMP, replacing with empty XMP.\")\n            return basic_parser(XMP_EMPTY)\n\n        if self.overwrite_invalid_xml:\n            parsers: Iterable[Callable] = [\n                basic_parser,\n                strip_illegal_bytes_parser,\n                recovery_parser,\n                replace_with_empty_xmp,\n            ]\n        else:\n            parsers = [basic_parser]\n\n        for parser in parsers:\n            try:\n                self._xmp = parser(data)\n            except (XMLSyntaxError if self.overwrite_invalid_xml else NeverRaise) as e:\n                if str(e).startswith(\"Start tag expected, '<' not found\") or str(\n                    e\n                ).startswith(\"Document is empty\"):\n                    self._xmp = replace_with_empty_xmp()\n                    break\n            else:\n                break\n\n        try:\n            pis = self._xmp.xpath('/processing-instruction()')\n            for pi in pis:\n                etree.strip_tags(self._xmp, pi.tag)\n            self._get_rdf_root()\n        except (Exception if self.overwrite_invalid_xml else NeverRaise) as e:\n            log.warning(\"Error occurred parsing XMP\", exc_info=e)\n            self._xmp = replace_with_empty_xmp()\n        return\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    const TensorList* input_list = nullptr;\n    OP_REQUIRES_OK(c, GetInputList(c, 0, &input_list));\n    int32_t size = c->input(1).scalar<int32>()();\n    OP_REQUIRES(\n        c, size >= 0,\n        errors::InvalidArgument(\n            \"TensorListSlice expects size to be non-negative. Got: \", size));\n\n    std::unique_ptr<Tensor> maybe_result =\n        c->forward_input(0, 0, DT_VARIANT, TensorShape{},\n                         c->input_memory_type(0), AllocatorAttributes());\n    if (maybe_result != nullptr) {\n      TensorList* out = maybe_result->scalar<Variant>()().get<TensorList>();\n      if (out->RefCountIsOne()) {\n        // We are able to forward the input.\n        out->tensors().resize(size, Tensor(DT_INVALID));\n        c->set_output(0, *maybe_result);\n        return;\n      }\n    }\n\n    // We were not able to forward the input.  Will have to resize from scratch.\n    Tensor* result;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape{}, &result, attr));\n    TensorList output_list;\n    output_list.element_shape = input_list->element_shape;\n    output_list.element_dtype = input_list->element_dtype;\n    output_list.max_num_elements = input_list->max_num_elements;\n    if (size > input_list->tensors().size()) {\n      output_list.tensors().insert(output_list.tensors().begin(),\n                                   input_list->tensors().begin(),\n                                   input_list->tensors().end());\n      // Add DT_INVALID tensors to the end of the list if the requested size\n      // is larger than the list length.\n      output_list.tensors().resize(size, Tensor(DT_INVALID));\n    } else {\n      output_list.tensors().insert(output_list.tensors().begin(),\n                                   input_list->tensors().begin(),\n                                   input_list->tensors().begin() + size);\n    }\n    result->scalar<Variant>()() = std::move(output_list);\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def process_request(self, request, spider):\n        if 'splash' not in request.meta:\n            return\n\n        if request.method not in {'GET', 'POST'}:\n            logger.warning(\n                \"Currently only GET and POST requests are supported by \"\n                \"SplashMiddleware; %(request)s will be handled without Splash\",\n                {'request': request},\n                extra={'spider': spider}\n            )\n            return request\n\n        if request.meta.get(\"_splash_processed\"):\n            # don't process the same request more than once\n            return\n\n        splash_options = request.meta['splash']\n        request.meta['_splash_processed'] = True\n\n        slot_policy = splash_options.get('slot_policy', self.slot_policy)\n        self._set_download_slot(request, request.meta, slot_policy)\n\n        args = splash_options.setdefault('args', {})\n\n        if '_replaced_args' in splash_options:\n            # restore arguments before sending request to the downloader\n            load_args = {}\n            save_args = []\n            local_arg_fingerprints = {}\n            for name in splash_options['_replaced_args']:\n                fp = args[name]\n                # Use remote Splash argument cache: if Splash key\n                # for a value is known then don't send the value to Splash;\n                # if it is unknown then try to save the value on server using\n                # ``save_args``.\n                if fp in self._remote_keys:\n                    load_args[name] = self._remote_keys[fp]\n                    del args[name]\n                else:\n                    save_args.append(name)\n                    args[name] = self._argument_values[fp]\n\n                local_arg_fingerprints[name] = fp\n\n            if load_args:\n                args['load_args'] = load_args\n            if save_args:\n                args['save_args'] = save_args\n            splash_options['_local_arg_fingerprints'] = local_arg_fingerprints\n\n            del splash_options['_replaced_args']  # ??\n\n        args.setdefault('url', request.url)\n        if request.method == 'POST':\n            args.setdefault('http_method', request.method)\n            # XXX: non-UTF8 request bodies are not supported now\n            args.setdefault('body', request.body.decode('utf8'))\n\n        if not splash_options.get('dont_send_headers'):\n            headers = scrapy_headers_to_unicode_dict(request.headers)\n            if headers:\n                args.setdefault('headers', headers)\n\n        body = json.dumps(args, ensure_ascii=False, sort_keys=True, indent=4)\n        # print(body)\n\n        if 'timeout' in args:\n            # User requested a Splash timeout explicitly.\n            #\n            # We can't catch a case when user requested `download_timeout`\n            # explicitly because a default value for `download_timeout`\n            # is set by DownloadTimeoutMiddleware.\n            #\n            # As user requested Splash timeout explicitly, we shouldn't change\n            # it. Another reason not to change the requested Splash timeout is\n            # because it may cause a validation error on the remote end.\n            #\n            # But we can change Scrapy `download_timeout`: increase\n            # it when it's too small. Decreasing `download_timeout` is not\n            # safe.\n\n            timeout_requested = float(args['timeout'])\n            timeout_expected = timeout_requested + self.splash_extra_timeout\n\n            # no timeout means infinite timeout\n            timeout_current = request.meta.get('download_timeout', 1e6)\n\n            if timeout_expected > timeout_current:\n                request.meta['download_timeout'] = timeout_expected\n\n        endpoint = splash_options.setdefault('endpoint', self.default_endpoint)\n        splash_base_url = splash_options.get('splash_url', self.splash_base_url)\n        splash_url = urljoin(splash_base_url, endpoint)\n\n        headers = Headers({'Content-Type': 'application/json'})\n        headers.update(splash_options.get('splash_headers', {}))\n        new_request = request.replace(\n            url=splash_url,\n            method='POST',\n            body=body,\n            headers=headers,\n            priority=request.priority + self.rescheduling_priority_adjust\n        )\n        self.crawler.stats.inc_value('splash/%s/request_count' % endpoint)\n        return new_request\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& handle = ctx->input(0);\n    const string& name = handle.scalar<tstring>()();\n    Tensor val;\n    OP_REQUIRES_OK(ctx, ctx->session_state()->GetTensor(name, &val));\n    ctx->set_output(0, val);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def decrypt(crypto: bytes, priv_key: key.PrivateKey) -> bytes:\n    r\"\"\"Decrypts the given message using PKCS#1 v1.5\n\n    The decryption is considered 'failed' when the resulting cleartext doesn't\n    start with the bytes 00 02, or when the 00 byte between the padding and\n    the message cannot be found.\n\n    :param crypto: the crypto text as returned by :py:func:`rsa.encrypt`\n    :param priv_key: the :py:class:`rsa.PrivateKey` to decrypt with.\n    :raise DecryptionError: when the decryption fails. No details are given as\n        to why the code thinks the decryption fails, as this would leak\n        information about the private key.\n\n\n    >>> import rsa\n    >>> (pub_key, priv_key) = rsa.newkeys(256)\n\n    It works with strings:\n\n    >>> crypto = encrypt(b'hello', pub_key)\n    >>> decrypt(crypto, priv_key)\n    b'hello'\n\n    And with binary data:\n\n    >>> crypto = encrypt(b'\\x00\\x00\\x00\\x00\\x01', pub_key)\n    >>> decrypt(crypto, priv_key)\n    b'\\x00\\x00\\x00\\x00\\x01'\n\n    Altering the encrypted information will *likely* cause a\n    :py:class:`rsa.pkcs1.DecryptionError`. If you want to be *sure*, use\n    :py:func:`rsa.sign`.\n\n\n    .. warning::\n\n        Never display the stack trace of a\n        :py:class:`rsa.pkcs1.DecryptionError` exception. It shows where in the\n        code the exception occurred, and thus leaks information about the key.\n        It's only a tiny bit of information, but every bit makes cracking the\n        keys easier.\n\n    >>> crypto = encrypt(b'hello', pub_key)\n    >>> crypto = crypto[0:5] + b'X' + crypto[6:] # change a byte\n    >>> decrypt(crypto, priv_key)\n    Traceback (most recent call last):\n    ...\n    rsa.pkcs1.DecryptionError: Decryption failed\n\n    \"\"\"\n\n    blocksize = common.byte_size(priv_key.n)\n    encrypted = transform.bytes2int(crypto)\n    decrypted = priv_key.blinded_decrypt(encrypted)\n    cleartext = transform.int2bytes(decrypted, blocksize)\n\n    # Detect leading zeroes in the crypto. These are not reflected in the\n    # encrypted value (as leading zeroes do not influence the value of an\n    # integer). This fixes CVE-2020-13757.\n    if len(crypto) > blocksize:\n        raise DecryptionError('Decryption failed')\n\n    # If we can't find the cleartext marker, decryption failed.\n    if cleartext[0:2] != b'\\x00\\x02':\n        raise DecryptionError('Decryption failed')\n\n    # Find the 00 separator between the padding and the message\n    try:\n        sep_idx = cleartext.index(b'\\x00', 2)\n    except ValueError:\n        raise DecryptionError('Decryption failed')\n\n    return cleartext[sep_idx + 1:]\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void pointZZ_pAdd(PointZZ_p * rop, const PointZZ_p * op1, const PointZZ_p * op2, const CurveZZ_p * curve) {\n    mpz_t xdiff, ydiff, lambda;\n    mpz_inits(xdiff, ydiff, lambda, NULL);\n\n    // calculate lambda\n    mpz_sub(ydiff, op2->y, op1->y);\n    mpz_sub(xdiff, op2->x, op1->x);\n    mpz_invert(xdiff, xdiff, curve->p);  // TODO check status\n    mpz_mul(lambda, ydiff, xdiff);\n    mpz_mod(lambda, lambda, curve->p);\n\n    // calculate resulting x coord\n    mpz_mul(rop->x, lambda, lambda);\n    mpz_sub(rop->x, rop->x, op1->x);\n    mpz_sub(rop->x, rop->x, op2->x);\n    mpz_mod(rop->x, rop->x, curve->p);\n\n    //calculate resulting y coord\n    mpz_sub(rop->y, op1->x, rop->x);\n    mpz_mul(rop->y, lambda, rop->y);\n    mpz_sub(rop->y, rop->y, op1->y);\n    mpz_mod(rop->y, rop->y, curve->p);\n\n    mpz_clears(xdiff, ydiff, lambda, NULL);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "async def indieauth_done(request, datasette):\n    from datasette.utils.asgi import Response\n\n    state = request.args.get(\"state\") or \"\"\n    code = request.args.get(\"code\")\n    try:\n        state_bits = datasette.unsign(state, DATASETTE_INDIEAUTH_STATE)\n    except itsdangerous.BadSignature:\n        return await indieauth_page(\n            request, datasette, error=\"Invalid state\", status=400\n        )\n    authorization_endpoint = state_bits[\"a\"]\n\n    urls = Urls(request, datasette)\n\n    # code_verifier should be in a signed cookie\n    code_verifier = None\n    if \"ds_indieauth\" in request.cookies:\n        try:\n            cookie_bits = datasette.unsign(\n                request.cookies[\"ds_indieauth\"], DATASETTE_INDIEAUTH_COOKIE\n            )\n            code_verifier = cookie_bits[\"v\"]\n        except itsdangerous.BadSignature:\n            pass\n    if not code_verifier:\n        return await indieauth_page(\n            request, datasette, error=\"Invalid ds_indieauth cookie\"\n        )\n\n    data = {\n        \"grant_type\": \"authorization_code\",\n        \"code\": code,\n        \"client_id\": urls.client_id,\n        \"redirect_uri\": urls.redirect_uri,\n        \"code_verifier\": code_verifier,\n    }\n    async with httpx.AsyncClient() as client:\n        response = await client.post(authorization_endpoint, data=data)\n\n    if response.status_code == 200:\n        body = response.text\n        try:\n            info = json.loads(body)\n        except ValueError:\n            info = dict(urllib.parse.parse_qsl(body))\n        if \"me\" not in info:\n            return await indieauth_page(\n                request,\n                datasette,\n                error=\"Invalid authorization_code response from authorization server\",\n            )\n        me = info[\"me\"]\n        actor = {\n            \"me\": me,\n            \"display\": display_url(me),\n        }\n        if \"scope\" in info:\n            actor[\"indieauth_scope\"] = info[\"scope\"]\n\n        if \"profile\" in info and isinstance(info[\"profile\"], dict):\n            actor.update(info[\"profile\"])\n        response = Response.redirect(datasette.urls.instance())\n        response.set_cookie(\n            \"ds_actor\",\n            datasette.sign(\n                {\"a\": actor},\n                \"actor\",\n            ),\n        )\n        return response\n    else:\n        return await indieauth_page(\n            request,\n            datasette,\n            error=\"Invalid response from authorization server\",\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def encrypt_item(table_name, aws_cmk_id, meta_table_name, material_name):\n    \"\"\"Demonstrate use of EncryptedTable to transparently encrypt an item.\"\"\"\n    index_key = {\"partition_attribute\": \"is this\", \"sort_attribute\": 55}\n    plaintext_item = {\n        \"example\": \"data\",\n        \"some numbers\": 99,\n        \"and some binary\": Binary(b\"\\x00\\x01\\x02\"),\n        \"leave me\": \"alone\",  # We want to ignore this attribute\n    }\n    # Collect all of the attributes that will be encrypted (used later).\n    encrypted_attributes = set(plaintext_item.keys())\n    encrypted_attributes.remove(\"leave me\")\n    # Collect all of the attributes that will not be encrypted (used later).\n    unencrypted_attributes = set(index_key.keys())\n    unencrypted_attributes.add(\"leave me\")\n    # Add the index pairs to the item.\n    plaintext_item.update(index_key)\n\n    # Create a normal table resource for the meta store.\n    meta_table = boto3.resource(\"dynamodb\").Table(meta_table_name)  # generated code confuse pylint: disable=no-member\n    # Create a crypto materials provider for the meta store using the specified AWS KMS key.\n    aws_kms_cmp = AwsKmsCryptographicMaterialsProvider(key_id=aws_cmk_id)\n    # Create a meta store using the AWS KMS crypto materials provider.\n    meta_store = MetaStore(table=meta_table, materials_provider=aws_kms_cmp)\n    # Create a most recent provider using the meta store.\n    most_recent_cmp = MostRecentProvider(\n        provider_store=meta_store,\n        material_name=material_name,\n        version_ttl=600.0,  # Check for a new material version every five minutes.\n    )\n    # Create a normal table resource.\n    table = boto3.resource(\"dynamodb\").Table(table_name)  # generated code confuse pylint: disable=no-member\n    # Create attribute actions that tells the encrypted table to encrypt all attributes except one.\n    actions = AttributeActions(\n        default_action=CryptoAction.ENCRYPT_AND_SIGN, attribute_actions={\"leave me\": CryptoAction.DO_NOTHING}\n    )\n    # Use these objects to create an encrypted table resource.\n    encrypted_table = EncryptedTable(table=table, materials_provider=most_recent_cmp, attribute_actions=actions)\n\n    # Put the item to the table, using the encrypted table resource to transparently encrypt it.\n    encrypted_table.put_item(Item=plaintext_item)\n\n    # Get the encrypted item using the standard table resource.\n    encrypted_item = table.get_item(Key=index_key)[\"Item\"]\n\n    # Get the item using the encrypted table resource, transparently decyrpting it.\n    decrypted_item = encrypted_table.get_item(Key=index_key)[\"Item\"]\n\n    # Verify that all of the attributes are different in the encrypted item\n    for name in encrypted_attributes:\n        assert encrypted_item[name] != plaintext_item[name]\n        assert decrypted_item[name] == plaintext_item[name]\n\n    # Verify that all of the attributes that should not be encrypted were not.\n    for name in unencrypted_attributes:\n        assert decrypted_item[name] == encrypted_item[name] == plaintext_item[name]\n\n    # Clean up the item\n    encrypted_table.delete_item(Key=index_key)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_email(value: Union[str]) -> Tuple[str, str]:\n    \"\"\"\n    Email address validation using https://pypi.org/project/email-validator/\n    Notes:\n    * raw ip address (literal) domain parts are not allowed.\n    * \"John Doe <local_part@domain.com>\" style \"pretty\" email addresses are processed\n    * spaces are striped from the beginning and end of addresses but no error is raised\n    \"\"\"\n    if email_validator is None:\n        import_email_validator()\n    m = pretty_email_regex.fullmatch(value)\n    name: Union[str, None] = None\n    if m:\n        name, value = m.groups()\n    email = value.strip()\n    try:\n        parts = email_validator.validate_email(email, check_deliverability=False)\n    except email_validator.EmailNotValidError as e:\n        raise errors.EmailError from e\n\n    if hasattr(parts, 'normalized'):\n        # email-validator >= 2\n        email = parts.normalized\n        assert email is not None\n        name = name or parts.local_part\n        return name, email\n    else:\n        # email-validator >1, <2\n        at_index = email.index('@')\n        local_part = email[:at_index]  # RFC 5321, local part must be case-sensitive.\n        global_part = email[at_index:].lower()\n\n        return name or local_part, local_part + global_part\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def login():\n    form = forms.UserForm()\n    if form.validate_on_submit():\n        db = get_db()\n        user = db.search(\n            (Query().username == form.username.data) & (Query().type == \"user\")\n        )\n\n        if user and check_password_hash(user[0][\"hashed_password\"], form.password.data):\n            user = User.from_db(user[0])\n            login_user(user, remember=True)\n            flash(\"Login successful!\", \"success\")\n\n            next_url = request.args.get(\"next\")\n            return redirect(next_url or \"/\")\n\n        flash(\"Invalid credentials\", \"error\")\n        return redirect(\"/login\")\n    return render_template(\"users/login.html\", form=form, title=\"Login\")\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_request_kwargs(cls, form, url=None, **kwargs):\n        \"\"\"Extract input data from the form.\"\"\"\n        method = str(form.get(\"method\", \"get\"))\n        action = form.get(\"action\")\n        url = urllib.parse.urljoin(url, action)\n        if url is None:  # This happens when both `action` and `url` are None.\n            raise ValueError('no URL to submit to')\n\n        # read https://www.w3.org/TR/html52/sec-forms.html\n        if method.lower() == \"get\":\n            data = kwargs.pop(\"params\", dict())\n        else:\n            data = kwargs.pop(\"data\", dict())\n        files = kwargs.pop(\"files\", dict())\n\n        # Use a list of 2-tuples to better reflect the behavior of browser QSL.\n        # Requests also retains order when encoding form data in 2-tuple lists.\n        data = [(k, v) for k, v in data.items()]\n\n        multipart = form.get(\"enctype\", \"\") == \"multipart/form-data\"\n\n        # Process form tags in the order that they appear on the page,\n        # skipping those tags that do not have a name-attribute.\n        selector = \",\".join(f\"{tag}[name]\" for tag in\n                            (\"input\", \"button\", \"textarea\", \"select\"))\n        for tag in form.select(selector):\n            name = tag.get(\"name\")  # name-attribute of tag\n\n            # Skip disabled elements, since they should not be submitted.\n            if tag.has_attr('disabled'):\n                continue\n\n            if tag.name == \"input\":\n                if tag.get(\"type\", \"\").lower() in (\"radio\", \"checkbox\"):\n                    if \"checked\" not in tag.attrs:\n                        continue\n                    value = tag.get(\"value\", \"on\")\n                else:\n                    # browsers use empty string for inputs with missing values\n                    value = tag.get(\"value\", \"\")\n\n                # If the enctype is not multipart, the filename is put in\n                # the form as a text input and the file is not sent.\n                if tag.get(\"type\", \"\").lower() == \"file\" and multipart:\n                    filepath = value\n                    if filepath != \"\" and isinstance(filepath, str):\n                        content = open(filepath, \"rb\")\n                    else:\n                        content = \"\"\n                    filename = os.path.basename(filepath)\n                    # If value is the empty string, we still pass it\n                    # for consistency with browsers (see\n                    # https://github.com/MechanicalSoup/MechanicalSoup/issues/250).\n                    files[name] = (filename, content)\n                else:\n                    data.append((name, value))\n\n            elif tag.name == \"button\":\n                if tag.get(\"type\", \"\").lower() in (\"button\", \"reset\"):\n                    continue\n                else:\n                    data.append((name, tag.get(\"value\", \"\")))\n\n            elif tag.name == \"textarea\":\n                data.append((name, tag.text))\n\n            elif tag.name == \"select\":\n                # If the value attribute is not specified, the content will\n                # be passed as a value instead.\n                options = tag.select(\"option\")\n                selected_values = [i.get(\"value\", i.text) for i in options\n                                   if \"selected\" in i.attrs]\n                if \"multiple\" in tag.attrs:\n                    for value in selected_values:\n                        data.append((name, value))\n                elif selected_values:\n                    # A standard select element only allows one option to be\n                    # selected, but browsers pick last if somehow multiple.\n                    data.append((name, selected_values[-1]))\n                elif options:\n                    # Selects the first option if none are selected\n                    first_value = options[0].get(\"value\", options[0].text)\n                    data.append((name, first_value))\n\n        if method.lower() == \"get\":\n            kwargs[\"params\"] = data\n        else:\n            kwargs[\"data\"] = data\n\n        # The following part of the function is here to respect the\n        # enctype specified by the form, i.e. force sending multipart\n        # content. Since Requests doesn't have yet a feature to choose\n        # enctype, we have to use tricks to make it behave as we want\n        # This code will be updated if Requests implements it.\n        if multipart and not files:\n            # Requests will switch to \"multipart/form-data\" only if\n            # files pass the `if files:` test, so in this case we use\n            # a modified dict that passes the if test even if empty.\n            class DictThatReturnsTrue(dict):\n                def __bool__(self):\n                    return True\n                __nonzero__ = __bool__\n\n            files = DictThatReturnsTrue()\n\n        return cls._get_request_kwargs(method, url, files=files, **kwargs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(\n        self, manifest: Optional[dict] = None, manifest_url: Optional[str] = None\n    ):\n        import yaml\n\n        if manifest and manifest_url:\n            raise ValueError(\"You cannot provide both a manifest and a manifest_url\")\n        elif manifest:\n            pass\n        elif manifest_url:\n            response = requests.get(manifest_url).text\n            manifest = yaml.load(response, Loader=yaml.Loader)\n        else:\n            raise ValueError(\"You must provide either a manifest or a manifest_url\")\n\n        if manifest[\"api\"][\"type\"] != \"openapi\":\n            raise ValueError(\n                f'API type must be \"openapi\", not \"{manifest[\"api\"][\"type\"]}\"'\n            )\n\n        if manifest[\"auth\"][\"type\"] != \"none\":\n            raise ValueError(\"Authentication cannot be supported for ChatGPT plugins\")\n\n        self.openapi = OpenAPIToolSpec(url=manifest[\"api\"][\"url\"])\n\n        self.plugin_description = f\"\"\"\n            'human_description': {manifest['description_for_human']}\n            'model_description': {manifest['description_for_model']}\n        \"\"\"\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_argparser():\n    \"\"\"Get the argparse parser.\"\"\"\n    parser = argparse.ArgumentParser(prog='qutebrowser',\n                                     description=qutebrowser.__description__)\n    parser.add_argument('-B', '--basedir', help=\"Base directory for all \"\n                        \"storage.\")\n    parser.add_argument('-C', '--config-py', help=\"Path to config.py.\",\n                        metavar='CONFIG')\n    parser.add_argument('-V', '--version', help=\"Show version and quit.\",\n                        action='store_true')\n    parser.add_argument('-s', '--set', help=\"Set a temporary setting for \"\n                        \"this session.\", nargs=2, action='append',\n                        dest='temp_settings', default=[],\n                        metavar=('OPTION', 'VALUE'))\n    parser.add_argument('-r', '--restore', help=\"Restore a named session.\",\n                        dest='session')\n    parser.add_argument('-R', '--override-restore', help=\"Don't restore a \"\n                        \"session even if one would be restored.\",\n                        action='store_true')\n    parser.add_argument('--target', choices=['auto', 'tab', 'tab-bg',\n                                             'tab-silent', 'tab-bg-silent',\n                                             'window', 'private-window'],\n                        help=\"How URLs should be opened if there is already a \"\n                             \"qutebrowser instance running.\")\n    parser.add_argument('--backend', choices=['webkit', 'webengine'],\n                        help=\"Which backend to use.\")\n    parser.add_argument('--desktop-file-name',\n                        default=\"org.qutebrowser.qutebrowser\",\n                        help=\"Set the base name of the desktop entry for this \"\n                        \"application. Used to set the app_id under Wayland. See \"\n                        \"https://doc.qt.io/qt-5/qguiapplication.html#desktopFileName-prop\")\n\n    parser.add_argument('--json-args', help=argparse.SUPPRESS)\n    parser.add_argument('--temp-basedir-restarted',\n                        help=argparse.SUPPRESS,\n                        action='store_true')\n\n    # WORKAROUND to be able to restart from older qutebrowser versions into this one.\n    # Should be removed at some point.\n    parser.add_argument('--enable-webengine-inspector',\n                        help=argparse.SUPPRESS,\n                        action='store_true')\n\n    debug = parser.add_argument_group('debug arguments')\n    debug.add_argument('-l', '--loglevel', dest='loglevel',\n                       help=\"Override the configured console loglevel\",\n                       choices=['critical', 'error', 'warning', 'info',\n                                'debug', 'vdebug'])\n    debug.add_argument('--logfilter', type=logfilter_error,\n                       help=\"Comma-separated list of things to be logged \"\n                       \"to the debug log on stdout.\")\n    debug.add_argument('--loglines',\n                       help=\"How many lines of the debug log to keep in RAM \"\n                       \"(-1: unlimited).\",\n                       default=2000, type=int)\n    debug.add_argument('-d', '--debug', help=\"Turn on debugging options.\",\n                       action='store_true')\n    debug.add_argument('--json-logging', action='store_true', help=\"Output log\"\n                       \" lines in JSON format (one object per line).\")\n    debug.add_argument('--nocolor', help=\"Turn off colored logging.\",\n                       action='store_false', dest='color')\n    debug.add_argument('--force-color', help=\"Force colored logging\",\n                       action='store_true')\n    debug.add_argument('--nowindow', action='store_true', help=\"Don't show \"\n                       \"the main window.\")\n    debug.add_argument('-T', '--temp-basedir', action='store_true', help=\"Use \"\n                       \"a temporary basedir.\")\n    debug.add_argument('--no-err-windows', action='store_true', help=\"Don't \"\n                       \"show any error windows (used for tests/smoke.py).\")\n    debug.add_argument('--qt-arg', help=\"Pass an argument with a value to Qt. \"\n                       \"For example, you can do \"\n                       \"`--qt-arg geometry 650x555+200+300` to set the window \"\n                       \"geometry.\", nargs=2, metavar=('NAME', 'VALUE'),\n                       action='append')\n    debug.add_argument('--qt-flag', help=\"Pass an argument to Qt as flag.\",\n                       nargs=1, action='append')\n    debug.add_argument('-D', '--debug-flag', type=debug_flag_error,\n                       default=[], help=\"Pass name of debugging feature to be\"\n                       \" turned on.\", action='append', dest='debug_flags')\n    parser.add_argument('command', nargs='*', help=\"Commands to execute on \"\n                        \"startup.\", metavar=':command')\n    # URLs will actually be in command\n    parser.add_argument('url', nargs='*', help=\"URLs to open on startup \"\n                        \"(empty as a window separator).\")\n    return parser\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def check_xsrf_cookie(self):\n        \"\"\"Verifies that the ``_xsrf`` cookie matches the ``_xsrf`` argument.\n\n        To prevent cross-site request forgery, we set an ``_xsrf``\n        cookie and include the same value as a non-cookie\n        field with all ``POST`` requests. If the two do not match, we\n        reject the form submission as a potential forgery.\n\n        The ``_xsrf`` value may be set as either a form field named ``_xsrf``\n        or in a custom HTTP header named ``X-XSRFToken`` or ``X-CSRFToken``\n        (the latter is accepted for compatibility with Django).\n\n        See http://en.wikipedia.org/wiki/Cross-site_request_forgery\n\n        Prior to release 1.1.1, this check was ignored if the HTTP header\n        ``X-Requested-With: XMLHTTPRequest`` was present.  This exception\n        has been shown to be insecure and has been removed.  For more\n        information please see\n        http://www.djangoproject.com/weblog/2011/feb/08/security/\n        http://weblog.rubyonrails.org/2011/2/8/csrf-protection-bypass-in-ruby-on-rails\n        \"\"\"\n        token = (self.get_argument(\"_xsrf\", None) or\n                 self.request.headers.get(\"X-Xsrftoken\") or\n                 self.request.headers.get(\"X-Csrftoken\"))\n        if not token:\n            raise HTTPError(403, \"'_xsrf' argument missing from POST\")\n        if not _time_independent_equals(utf8(self.xsrf_token), utf8(token)):\n            raise HTTPError(403, \"XSRF cookie does not match POST argument\")\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    OpInputList indices_list_in;\n    OP_REQUIRES_OK(context, context->input_list(\"indices\", &indices_list_in));\n    OpInputList values_list_in;\n    OP_REQUIRES_OK(context, context->input_list(\"values\", &values_list_in));\n    OpInputList shapes_list_in;\n    OP_REQUIRES_OK(context, context->input_list(\"shapes\", &shapes_list_in));\n    OpInputList dense_list_in;\n    OP_REQUIRES_OK(context,\n                   context->input_list(\"dense_inputs\", &dense_list_in));\n\n    OP_REQUIRES_OK(context, ValidateInput(indices_list_in, values_list_in,\n                                          shapes_list_in, dense_list_in));\n\n    const Tensor* num_buckets_t;\n    OP_REQUIRES_OK(context, context->input(\"num_buckets\", &num_buckets_t));\n    const int64 num_buckets = num_buckets_t->scalar<int64>()();\n\n    const Tensor* strong_hash_t;\n    OP_REQUIRES_OK(context, context->input(\"strong_hash\", &strong_hash_t));\n    const bool strong_hash = strong_hash_t->scalar<bool>()();\n\n    const Tensor* salt_t;\n    OP_REQUIRES_OK(context, context->input(\"salt\", &salt_t));\n    const auto salt = salt_t->flat<int64>();\n    std::vector<int64> key_{salt(0), salt(1)};\n\n    std::vector<std::unique_ptr<ColumnInterface<int64>>> columns =\n        GenerateKeyedColumnsFromInput<int64>(indices_list_in, values_list_in,\n                                             shapes_list_in, dense_list_in,\n                                             key_);\n    Tensor* indices_out;\n    Tensor* values_out;\n    Tensor* shape_out;\n    const int64 batch_size = CalculateBatchSize(shapes_list_in, dense_list_in);\n    std::vector<int64> output_start_indices(batch_size);\n    OP_REQUIRES_OK(\n        context,\n        CreateOutputTensors(columns, batch_size, context, &indices_out,\n                            &values_out, &shape_out, &output_start_indices));\n    const tstring unused_sep;\n    HashCrosserV2 crosser(columns, num_buckets, 0, unused_sep);\n    OutputUpdater<int64> updater(output_start_indices, indices_out, values_out);\n    auto do_work = [&columns, crosser, updater, strong_hash](int64 begin,\n                                                             int64 end) {\n      for (int b = begin; b < end; b++) {\n        ProductIterator<int64> product_iterator(columns, b);\n        int64 cross_count = 0;\n        while (product_iterator.HasNext()) {\n          const auto permutation = product_iterator.Next();\n          updater.Update(b, cross_count,\n                         crosser.Generate(b, permutation, strong_hash));\n          cross_count++;\n        }\n      }\n    };\n\n    auto* worker_threads = context->device()->tensorflow_cpu_worker_threads();\n    // TODO(zakaria): optimize kCostPerUnit\n    const int kCostPerUnit = 5000 * indices_list_in.size();\n    Shard(worker_threads->num_threads, worker_threads->workers, batch_size,\n          kCostPerUnit, do_work);\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def archive(request, bookmark_id: int):\n    try:\n        bookmark = Bookmark.objects.get(pk=bookmark_id, owner=request.user)\n    except Bookmark.DoesNotExist:\n        raise Http404('Bookmark does not exist')\n\n    archive_bookmark(bookmark)\n    return_url = request.GET.get('return_url')\n    return_url = return_url if return_url else reverse('bookmarks:index')\n    return HttpResponseRedirect(return_url)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _expand_user_properties(self, template):\n        # Make sure username and servername match the restrictions for DNS labels\n        # Note: '-' is not in safe_chars, as it is being used as escape character\n        safe_chars = set(string.ascii_lowercase + string.digits)\n\n        # Set servername based on whether named-server initialised\n        if self.name:\n            # use two -- to ensure no collision possibilities\n            # are created by an ambiguous boundary between username and\n            # servername.\n            # -- cannot occur in a string where - is the escape char.\n            servername = '--{}'.format(self.name)\n            safe_servername = '--{}'.format(escapism.escape(self.name, safe=safe_chars, escape_char='-').lower())\n        else:\n            servername = ''\n            safe_servername = ''\n\n        legacy_escaped_username = ''.join([s if s in safe_chars else '-' for s in self.user.name.lower()])\n        safe_username = escapism.escape(self.user.name, safe=safe_chars, escape_char='-').lower()\n        return template.format(\n            userid=self.user.id,\n            username=safe_username,\n            unescaped_username=self.user.name,\n            legacy_escape_username=legacy_escaped_username,\n            servername=safe_servername,\n            unescaped_servername=servername,\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteDepthToSpaceParams*>(node->builtin_data);\n\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n\n  auto data_type = output->type;\n  TF_LITE_ENSURE(context,\n                 data_type == kTfLiteFloat32 || data_type == kTfLiteInt8);\n  TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n\n  const int block_size = params->block_size;\n  const int input_height = input->dims->data[kHeightRank];\n  const int input_width = input->dims->data[kWidthRank];\n  const int input_channels = input->dims->data[kDepthRank];\n  int output_height = input_height * block_size;\n  int output_width = input_width * block_size;\n  int output_channels = input_channels / block_size / block_size;\n\n  TF_LITE_ENSURE_EQ(context, input_height, output_height / block_size);\n  TF_LITE_ENSURE_EQ(context, input_width, output_width / block_size);\n  TF_LITE_ENSURE_EQ(context, input_channels,\n                    output_channels * block_size * block_size);\n\n  // We must update the output tensor dimensions.\n  // The dims storage is expected to be the same area in memory\n  // for both TfLiteTensor and TfLiteEvalTensor.  This is important\n  // because TfLiteTensor in the MicroInterpreter is a temporary\n  // allocation.  For the KernelRunner interpreter, TfLiteEvalTensor\n  // is a temporary allocation.  We must therefore relocate the dims\n  // from the FlatBuffer to the persistant storage arena.\n  TfLiteEvalTensor* output_eval =\n      tflite::micro::GetEvalOutput(context, node, kOutputTensor);\n  TF_LITE_ENSURE_OK(context, tflite::micro::CreateWritableTensorDimsWithCopy(\n                                 context, output, output_eval));\n  output->dims->data[kBatchRank] = input->dims->data[kBatchRank];\n  output->dims->data[kHeightRank] = output_height;\n  output->dims->data[kWidthRank] = output_width;\n  output->dims->data[kDepthRank] = output_channels;\n\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void TensorSliceReader::LoadShard(int shard) const {\n  CHECK_LT(shard, sss_.size());\n  if (sss_[shard] || !status_.ok()) {\n    return;  // Already loaded, or invalid.\n  }\n  string value;\n  SavedTensorSlices sts;\n  const string fname = fnames_[shard];\n  VLOG(1) << \"Reading meta data from file \" << fname << \"...\";\n  Table* table;\n  Status s = open_function_(fname, &table);\n  if (!s.ok()) {\n    status_ = errors::DataLoss(\"Unable to open table file \", fname, \": \",\n                               s.ToString());\n    return;\n  }\n  sss_[shard].reset(table);\n  if (!(table->Get(kSavedTensorSlicesKey, &value) &&\n        ParseProtoUnlimited(&sts, value))) {\n    status_ = errors::Internal(\n        \"Failed to find the saved tensor slices at the beginning of the \"\n        \"checkpoint file: \",\n        fname);\n    return;\n  }\n  status_ = CheckVersions(sts.meta().versions(), TF_CHECKPOINT_VERSION,\n                          TF_CHECKPOINT_VERSION_MIN_PRODUCER, \"Checkpoint\",\n                          \"checkpoint\");\n  if (!status_.ok()) return;\n  for (const SavedSliceMeta& ssm : sts.meta().tensor()) {\n    TensorShape ssm_shape(ssm.shape());\n    for (const TensorSliceProto& tsp : ssm.slice()) {\n      TensorSlice ss_slice(tsp);\n      status_ = RegisterTensorSlice(ssm.name(), ssm_shape, ssm.type(), fname,\n                                    ss_slice, &tensors_);\n      if (!status_.ok()) return;\n    }\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, **kwargs):\n        super().__init__(strip=True, **kwargs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_base_command(self, cwd, use_dash_c):\n        if use_dash_c:\n            return ['git', '-C', cwd]\n\n        while cwd and cwd != os.sep:\n            gitdir = os.path.join(cwd, '.git')\n\n            if os.path.isdir(gitdir):\n                return ['git', '--git-dir=%s' % gitdir, '--work-tree=%s' % cwd]\n\n            cwd = os.path.dirname(cwd)\n\n        return None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def authenticate(self, context, auth=None):\n        \"\"\"Authenticate credentials and return a token.\n\n        Accept auth as a dict that looks like::\n\n            {\n                \"auth\":{\n                    \"passwordCredentials\":{\n                        \"username\":\"test_user\",\n                        \"password\":\"mypass\"\n                    },\n                    \"tenantName\":\"customer-x\"\n                }\n            }\n\n        In this case, tenant is optional, if not provided the token will be\n        considered \"unscoped\" and can later be used to get a scoped token.\n\n        Alternatively, this call accepts auth with only a token and tenant\n        that will return a token that is scoped to that tenant.\n        \"\"\"\n\n        token_id = uuid.uuid4().hex\n        if 'passwordCredentials' in auth:\n            user_id = auth['passwordCredentials'].get('userId', None)\n            username = auth['passwordCredentials'].get('username', '')\n            password = auth['passwordCredentials'].get('password', '')\n            tenant_name = auth.get('tenantName', None)\n\n            if username:\n                try:\n                    user_ref = self.identity_api.get_user_by_name(\n                        context=context, user_name=username)\n                    user_id = user_ref['id']\n                except exception.UserNotFound:\n                    raise exception.Unauthorized()\n\n            # more compat\n            tenant_id = auth.get('tenantId', None)\n            if tenant_name:\n                try:\n                    tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                    tenant_id = tenant_ref['id']\n                except exception.TenantNotFound:\n                    raise exception.Unauthorized()\n\n            try:\n                auth_info = self.identity_api.authenticate(context=context,\n                                                           user_id=user_id,\n                                                           password=password,\n                                                           tenant_id=tenant_id)\n                (user_ref, tenant_ref, metadata_ref) = auth_info\n\n                # If the user is disabled don't allow them to authenticate\n                if not user_ref.get('enabled', True):\n                    LOG.warning('User %s is disabled' % user_id)\n                    raise exception.Unauthorized()\n            except AssertionError as e:\n                raise exception.Unauthorized(e.message)\n\n            token_ref = self.token_api.create_token(\n                context,\n                token_id,\n                dict(id=token_id,\n                     user=user_ref,\n                     tenant=tenant_ref,\n                     metadata=metadata_ref))\n            if tenant_ref:\n                catalog_ref = self.catalog_api.get_catalog(\n                    context=context,\n                    user_id=user_ref['id'],\n                    tenant_id=tenant_ref['id'],\n                    metadata=metadata_ref)\n            else:\n                catalog_ref = {}\n\n        elif 'token' in auth:\n            token = auth['token'].get('id', None)\n\n            tenant_name = auth.get('tenantName')\n\n            # more compat\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                    context=context, tenant_name=tenant_name)\n                tenant_id = tenant_ref['id']\n            else:\n                tenant_id = auth.get('tenantId', None)\n\n            try:\n                old_token_ref = self.token_api.get_token(context=context,\n                                                         token_id=token)\n            except exception.NotFound:\n                raise exception.Unauthorized()\n\n            user_ref = old_token_ref['user']\n\n            # If the user is disabled don't allow them to authenticate\n            current_user_ref = self.identity_api.get_user(\n                context=context,\n                user_id=user_ref['id'])\n            if not current_user_ref.get('enabled', True):\n                LOG.warning('User %s is disabled' % user_ref['id'])\n                raise exception.Unauthorized()\n\n            tenants = self.identity_api.get_tenants_for_user(context,\n                                                             user_ref['id'])\n            if tenant_id and tenant_id not in tenants:\n                raise exception.Unauthorized()\n\n            try:\n                tenant_ref = self.identity_api.get_tenant(\n                    context=context,\n                    tenant_id=tenant_id)\n                metadata_ref = self.identity_api.get_metadata(\n                    context=context,\n                    user_id=user_ref['id'],\n                    tenant_id=tenant_ref['id'])\n                catalog_ref = self.catalog_api.get_catalog(\n                    context=context,\n                    user_id=user_ref['id'],\n                    tenant_id=tenant_ref['id'],\n                    metadata=metadata_ref)\n            except exception.TenantNotFound:\n                tenant_ref = None\n                metadata_ref = {}\n                catalog_ref = {}\n\n            token_ref = self.token_api.create_token(\n                context, token_id, dict(id=token_id,\n                                        user=user_ref,\n                                        tenant=tenant_ref,\n                                        metadata=metadata_ref,\n                                        expires=old_token_ref['expires']))\n\n        # TODO(termie): optimize this call at some point and put it into the\n        #               the return for metadata\n        # fill out the roles in the metadata\n        roles_ref = []\n        for role_id in metadata_ref.get('roles', []):\n            roles_ref.append(self.identity_api.get_role(context, role_id))\n        logging.debug('TOKEN_REF %s', token_ref)\n        return self._format_authenticate(token_ref, roles_ref, catalog_ref)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, redirect_to):\n        super(HttpResponseRedirect, self).__init__()\n        self['Location'] = iri_to_uri(redirect_to)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _process_wrapping_key_provider_configs(  # noqa: C901\n    raw_keys,  # type: List[RAW_CONFIG]\n    action,  # type: str\n    discovery=None,  # type: bool\n    discovery_filter=None,  # type: Dict[str, Union[str, List[str]]]\n):\n    # type: (...) -> List[MASTER_KEY_PROVIDER_CONFIG]\n    \"\"\"Applied additional processing to prepare the wrapping key provider configuration.\n\n    :param list raw_keys: List of wrapping key provider configurations\n    :param str action: Action defined in CLI input\n    :param bool discovery: Are we in discovery mode?\n    :param Dict[str, Union[str, List[str]]] discovery_filter: Filter options for discovery mode\n    :returns: List of processed wrapping key provider configurations\n    :rtype: list of dicts\n    :raises ParameterParseError: if exactly one provider value is not provided\n    :raises ParameterParseError: if no key values are provided\n    \"\"\"\n    if raw_keys is None:\n        if action == \"decrypt\":\n            # We allow not defining any wrapping key provider configuration if decrypting with aws-kms.\n            _LOGGER.debug(\n                \"No wrapping key provider config provided on decrypt request. Using aws-kms with no wrapping keys.\"\n            )\n            return [{\"provider\": DEFAULT_MASTER_KEY_PROVIDER, \"key\": []}]\n        raise ParameterParseError(\"No wrapping key provider configuration found.\")\n\n    if action == \"decrypt\" and discovery is None:\n        discovery = False\n\n    processed_configs = []  # type: List[MASTER_KEY_PROVIDER_CONFIG]\n    for raw_config in raw_keys:\n        parsed_args = {}  # type: Dict[str, Union[str, List[str], Dict[str, Union[str, List[str]]]]]\n        parsed_args.update(_parse_kwargs(raw_config))\n\n        provider = parsed_args.get(\"provider\", [DEFAULT_MASTER_KEY_PROVIDER])  # If no provider is defined, use aws-kms\n        if len(provider) != 1:\n            raise ParameterParseError(\n                'Exactly one \"provider\" must be provided for each wrapping key provider configuration. '\n                \"{} provided\".format(len(provider))\n            )\n        parsed_args[\"provider\"] = provider[0]  # type: ignore\n\n        aws_kms_on_decrypt = parsed_args[\"provider\"] in (\"aws-kms\", DEFAULT_MASTER_KEY_PROVIDER) and action == \"decrypt\"\n\n        if aws_kms_on_decrypt:\n            if \"key\" in parsed_args and discovery:\n                # Decrypt MUST fail without attempting any decryption if discovery mode is enabled\n                # and at least one key=<Key ARN> parameter value is provided\n                raise ParameterParseError(\n                    \"Exact wrapping keys cannot be specified for aws-kms wrapping key provider on decrypt \"\n                    \"in discovery mode.\"\n                )\n            if \"key\" not in parsed_args and not discovery:\n                # Decrypt MUST fail without attempting any decryption if discovery mode is disabled\n                # and no key=<Key ARN> parameter value is provided\n                raise ParameterParseError(\n                    \"At least one key must be specified for aws-kms wrapping key provider on decrypt \"\n                    \"when discovery mode is disabled.\"\n                )\n            parsed_args[\"key\"] = []\n        elif \"key\" not in parsed_args:\n            raise ParameterParseError(\n                'At least one \"key\" must be provided for each wrapping key provider configuration'\n            )\n        if discovery and discovery_filter:\n            parsed_args[\"discovery_filter\"] = discovery_filter\n        processed_configs.append(parsed_args)  # type: ignore\n    return processed_configs\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_annotation(self, annotation, alias, is_summary=False, select=True):\n        \"\"\"Add a single annotation expression to the Query.\"\"\"\n        annotation = annotation.resolve_expression(\n            self, allow_joins=True, reuse=None, summarize=is_summary\n        )\n        if select:\n            self.append_annotation_mask([alias])\n        else:\n            self.set_annotation_mask(set(self.annotation_select).difference({alias}))\n        self.annotations[alias] = annotation\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_field_options(self, field):\n        options = {}\n        options['label'] = field.label\n        options['help_text'] = field.help_text\n        options['required'] = field.required\n        options['initial'] = field.default_value\n        return options\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def interceptRequest(self, info):\n        \"\"\"Handle the given request.\n\n        Reimplementing this virtual function and setting the interceptor on a\n        profile makes it possible to intercept URL requests. This function is\n        executed on the IO thread, and therefore running long tasks here will\n        block networking.\n\n        info contains the information about the URL request and will track\n        internally whether its members have been altered.\n\n        Args:\n            info: QWebEngineUrlRequestInfo &info\n        \"\"\"\n        if 'log-requests' in self._args.debug_flags:\n            resource_type = debug.qenum_key(QWebEngineUrlRequestInfo,\n                                            info.resourceType())\n            navigation_type = debug.qenum_key(QWebEngineUrlRequestInfo,\n                                              info.navigationType())\n            log.webview.debug(\"{} {}, first-party {}, resource {}, \"\n                              \"navigation {}\".format(\n                                  bytes(info.requestMethod()).decode('ascii'),\n                                  info.requestUrl().toDisplayString(),\n                                  info.firstPartyUrl().toDisplayString(),\n                                  resource_type, navigation_type))\n\n        url = info.requestUrl()\n\n        # FIXME:qtwebengine only block ads for NavigationTypeOther?\n        if self._host_blocker.is_blocked(url):\n            log.webview.info(\"Request to {} blocked by host blocker.\".format(\n                url.host()))\n            info.block(True)\n\n        for header, value in shared.custom_headers(url=url):\n            info.setHttpHeader(header, value)\n\n        user_agent = config.instance.get('content.headers.user_agent', url=url)\n        if user_agent is not None:\n            info.setHttpHeader(b'User-Agent', user_agent.encode('ascii'))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def login():\n    from flask_login import current_user\n\n    default_redirect_url = request.script_root + url_for(\"index\")\n    redirect_url = request.args.get(\"redirect\", default_redirect_url)\n\n    parsed = urlparse(redirect_url)  # check if redirect url is valid\n    if parsed.scheme != \"\" or parsed.netloc != \"\":\n        _logger.warning(\n            f\"Got an invalid redirect URL with the login attempt, misconfiguration or attack attempt: {redirect_url}\"\n        )\n        redirect_url = default_redirect_url\n\n    permissions = sorted(\n        filter(\n            lambda x: x is not None and isinstance(x, OctoPrintPermission),\n            map(\n                lambda x: getattr(Permissions, x.strip()),\n                request.args.get(\"permissions\", \"\").split(\",\"),\n            ),\n        ),\n        key=lambda x: x.get_name(),\n    )\n    if not permissions:\n        permissions = [Permissions.STATUS, Permissions.SETTINGS_READ]\n\n    user_id = request.args.get(\"user_id\", \"\")\n\n    if (not user_id or current_user.get_id() == user_id) and has_permissions(\n        *permissions\n    ):\n        return redirect(redirect_url)\n\n    render_kwargs = {\n        \"theming\": [],\n        \"redirect_url\": redirect_url,\n        \"permission_names\": map(lambda x: x.get_name(), permissions),\n        \"user_id\": user_id,\n        \"logged_in\": not current_user.is_anonymous,\n    }\n\n    try:\n        additional_assets = _add_additional_assets(\"octoprint.theming.login\")\n\n        # backwards compatibility to forcelogin & loginui plugins which were replaced by this built-in dialog\n        additional_assets += _add_additional_assets(\"octoprint.plugin.forcelogin.theming\")\n        additional_assets += _add_additional_assets(\"octoprint.plugin.loginui.theming\")\n\n        render_kwargs.update({\"theming\": additional_assets})\n    except Exception:\n        _logger.exception(\"Error processing theming CSS, ignoring\")\n\n    return render_template(\"login.jinja2\", **render_kwargs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _dump(self, file=None, format=None):\n        import tempfile\n        if not file:\n            file = tempfile.mktemp()\n        self.load()\n        if not format or format == \"PPM\":\n            self.im.save_ppm(file)\n        else:\n            file = file + \".\" + format\n            self.save(file, format)\n        return file\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _parse_returned_data(self, res):\n        try:\n            filtered_output, warnings = _filter_non_json_lines(res.get('stdout', u''))\n            for w in warnings:\n                display.warning(w)\n            data = json.loads(filtered_output)\n            data['_ansible_parsed'] = True\n        except ValueError:\n            # not valid json, lets try to capture error\n            data = dict(failed=True, _ansible_parsed=False)\n            data['msg'] = \"MODULE FAILURE\"\n            data['module_stdout'] = res.get('stdout', u'')\n            if 'stderr' in res:\n                data['module_stderr'] = res['stderr']\n                if res['stderr'].startswith(u'Traceback'):\n                    data['exception'] = res['stderr']\n        return data\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_context_data(self, **kwargs):\n        template = self.kwargs['template']\n        templates = []\n        try:\n            default_engine = Engine.get_default()\n        except ImproperlyConfigured:\n            # Non-trivial TEMPLATES settings aren't supported (#24125).\n            pass\n        else:\n            # This doesn't account for template loaders (#24128).\n            for index, directory in enumerate(default_engine.dirs):\n                template_file = Path(directory) / template\n                if template_file.exists():\n                    template_contents = template_file.read_text()\n                else:\n                    template_contents = ''\n                templates.append({\n                    'file': template_file,\n                    'exists': template_file.exists(),\n                    'contents': template_contents,\n                    'order': index,\n                })\n        return super().get_context_data(**{\n            **kwargs,\n            'name': template,\n            'templates': templates,\n        })\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def _resolve_orders(orders):\n            requester = get_user_or_app_from_context(info.context)\n            if not requester.has_perm(OrderPermissions.MANAGE_ORDERS):\n                orders = list(\n                    filter(lambda order: order.status != OrderStatus.DRAFT, orders)\n                )\n\n            return create_connection_slice(\n                orders, info, kwargs, OrderCountableConnection\n            )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n\n    // TODO: verify performance of not transposing and finding the min max\n    // directly from input_matrix vs the one presented below of transposing and\n    // using the transposed matrix as the transposing operation in itself might\n    // be more costly.\n    // Note that this operation is a calibration step for quantization and will\n    // cease to exist in the final inference graph(will exist as a const node).\n    auto transposed_input = input_matrix.shuffle(shuffling);\n\n    // Find the ranges of each channel in parallel.\n    float out_min_max = std::numeric_limits<float>::min();\n\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  // ENABLE_ONEDNN_OPENMP\n    // TODO: Add eigen parallel_for\n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] =\n          scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\n      if (min_per_channel < 0) is_non_negative = false;\n\n      // Thread-local out_min_max.\n      out_min_max = std::max(out_min_max, ranges[i]);\n    }\n\n    // All local out_min_max gets max-reduced into one global out_min_max at\n    // the end of the loop by specifying reduction(max:out_min_max) along with\n    // omp parallel for.\n\n    // Fixing max to clip_value_max_ (example 6.0 to support relu6)\n    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n\n    Tensor* output_min = nullptr;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\n    output_max->flat<float>()(0) = out_min_max;\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def docker_monitor(\n        self,\n        cidfile: str,\n        tmpdir_prefix: str,\n        cleanup_cidfile: bool,\n        docker_exe: str,\n        process: \"subprocess.Popen[str]\",\n    ) -> None:\n        \"\"\"Record memory usage of the running Docker container.\"\"\"\n        # Todo: consider switching to `docker create` / `docker start`\n        # instead of `docker run` as `docker create` outputs the container ID\n        # to stdout, but the container is frozen, thus allowing us to start the\n        # monitoring process without dealing with the cidfile or too-fast\n        # container execution\n        cid: Optional[str] = None\n        while cid is None:\n            time.sleep(1)\n            if process.returncode is not None:\n                if cleanup_cidfile:\n                    try:\n                        os.remove(cidfile)\n                    except OSError as exc:\n                        _logger.warning(\"Ignored error cleaning up %s cidfile: %s\", docker_exe, exc)\n                    return\n            try:\n                with open(cidfile) as cidhandle:\n                    cid = cidhandle.readline().strip()\n            except OSError:\n                cid = None\n        max_mem = psutil.virtual_memory().total\n        tmp_dir, tmp_prefix = os.path.split(tmpdir_prefix)\n        stats_file = tempfile.NamedTemporaryFile(prefix=tmp_prefix, dir=tmp_dir)\n        stats_file_name = stats_file.name\n        try:\n            with open(stats_file_name, mode=\"w\") as stats_file_handle:\n                cmds = [docker_exe, \"stats\"]\n                if \"podman\" not in docker_exe:\n                    cmds.append(\"--no-trunc\")\n                cmds.extend([\"--format\", \"{{.MemPerc}}\", cid])\n                stats_proc = subprocess.Popen(  # nosec\n                    cmds,\n                    stdout=stats_file_handle,\n                    stderr=subprocess.DEVNULL,\n                )\n                process.wait()\n                stats_proc.kill()\n        except OSError as exc:\n            _logger.warning(\"Ignored error with %s stats: %s\", docker_exe, exc)\n            return\n        max_mem_percent: float = 0.0\n        mem_percent: float = 0.0\n        with open(stats_file_name) as stats:\n            while True:\n                line = stats.readline()\n                if not line:\n                    break\n                try:\n                    mem_percent = float(re.sub(CONTROL_CODE_RE, \"\", line).replace(\"%\", \"\"))\n                    if mem_percent > max_mem_percent:\n                        max_mem_percent = mem_percent\n                except ValueError as exc:\n                    _logger.debug(\"%s stats parsing error in line %s: %s\", docker_exe, line, exc)\n        _logger.info(\n            \"[job %s] Max memory used: %iMiB\",\n            self.name,\n            int((max_mem_percent / 100 * max_mem) / (2**20)),\n        )\n        if cleanup_cidfile:\n            os.remove(cidfile)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_in_or_equal(path_1: str | Path, path_2: str | Path):\n    \"\"\"\n    True if path_1 is a descendant (i.e. located within) path_2 or if the paths are the\n    same, returns False otherwise.\n    Parameters:\n        path_1: str or Path (should be a file)\n        path_2: str or Path (can be a file or directory)\n    \"\"\"\n    path_1, path_2 = abspath(path_1), abspath(path_2)\n    try:\n        if str(path_1.relative_to(path_2)).startswith(\"..\"):  # prevent path traversal\n            return False\n    except ValueError:\n        return False\n    return True\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get(self, id):\n        \"\"\"Get organization\n        ---\n        description: >-\n          Returns the organization specified by the id\\n\n\n          ### Permission Table\\n\n          |Rule name|Scope|Operation|Assigned to node|Assigned to container|\n          Description|\\n\n          |--|--|--|--|--|--|\\n\n          |Organization|Global|View|\u274c|\u274c|View all organizations|\\n\n          |Organization|Collaboration|View|\u2705|\u2705|View a list of organizations\n          within the scope of the collaboration|\\n\n          |Organization|Organization|View|\u2705|\u2705|View a list of organizations\n          that the user is part of|\\n\n\n          Accessible to users.\n\n        parameters:\n          - in: path\n            name: id\n            schema:\n              type: integer\n            description: Organization id\n            required: true\n\n        responses:\n          200:\n            description: Ok\n          404:\n            description: Organization not found\n          401:\n            description: Unauthorized\n\n        security:\n          - bearerAuth: []\n\n        tags: [\"Organization\"]\n        \"\"\"\n\n        # obtain organization of authenticated\n        auth_org = self.obtain_auth_organization()\n\n        # retrieve requested organization\n        req_org = db.Organization.get(id)\n        if not req_org:\n            return {'msg': f'Organization id={id} not found!'}, \\\n                HTTPStatus.NOT_FOUND\n\n        accepted = False\n        # Check if auth has enough permissions\n        if self.r.v_glo.can():\n            accepted = True\n        elif self.r.v_col.can():\n            # check if the organization is whithin a collaboration\n            for col in auth_org.collaborations:\n                if req_org in col.organizations:\n                    accepted = True\n            # or that the organization is auths org\n            if req_org == auth_org:\n                accepted = True\n        elif self.r.v_org.can():\n            accepted = auth_org == req_org\n\n        if accepted:\n            return org_schema.dump(req_org, many=False).data, \\\n                HTTPStatus.OK\n        else:\n            return {'msg': 'You do not have permission to do that!'}, \\\n                HTTPStatus.UNAUTHORIZED\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def read_from_stream(\n        stream: StreamType,\n        pdf: Any,  # PdfReader\n        forced_encoding: Union[None, str, List[str], Dict[int, str]] = None,\n    ) -> \"DictionaryObject\":\n        def get_next_obj_pos(\n            p: int, p1: int, rem_gens: List[int], pdf: Any\n        ) -> int:  # PdfReader\n            l = pdf.xref[rem_gens[0]]\n            for o in l:\n                if p1 > l[o] and p < l[o]:\n                    p1 = l[o]\n            if len(rem_gens) == 1:\n                return p1\n            else:\n                return get_next_obj_pos(p, p1, rem_gens[1:], pdf)\n\n        def read_unsized_from_steam(stream: StreamType, pdf: Any) -> bytes:  # PdfReader\n            # we are just pointing at beginning of the stream\n            eon = get_next_obj_pos(stream.tell(), 2**32, list(pdf.xref), pdf) - 1\n            curr = stream.tell()\n            rw = stream.read(eon - stream.tell())\n            p = rw.find(b\"endstream\")\n            if p < 0:\n                raise PdfReadError(\n                    f\"Unable to find 'endstream' marker for obj starting at {curr}.\"\n                )\n            stream.seek(curr + p + 9)\n            return rw[: p - 1]\n\n        tmp = stream.read(2)\n        if tmp != b\"<<\":\n            raise PdfReadError(\n                f\"Dictionary read error at byte {hex_str(stream.tell())}: \"\n                \"stream must begin with '<<'\"\n            )\n        data: Dict[Any, Any] = {}\n        while True:\n            tok = read_non_whitespace(stream)\n            if tok == b\"\\x00\":\n                continue\n            elif tok == b\"%\":\n                stream.seek(-1, 1)\n                skip_over_comment(stream)\n                continue\n            if not tok:\n                raise PdfStreamError(STREAM_TRUNCATED_PREMATURELY)\n\n            if tok == b\">\":\n                stream.read(1)\n                break\n            stream.seek(-1, 1)\n            key = read_object(stream, pdf)\n            tok = read_non_whitespace(stream)\n            stream.seek(-1, 1)\n            value = read_object(stream, pdf, forced_encoding)\n            if not data.get(key):\n                data[key] = value\n            else:\n                # multiple definitions of key not permitted\n                msg = (\n                    f\"Multiple definitions in dictionary at byte \"\n                    f\"{hex_str(stream.tell())} for key {key}\"\n                )\n                if pdf is not None and pdf.strict:\n                    raise PdfReadError(msg)\n                logger_warning(msg, __name__)\n\n        pos = stream.tell()\n        s = read_non_whitespace(stream)\n        if s == b\"s\" and stream.read(5) == b\"tream\":\n            eol = stream.read(1)\n            # odd PDF file output has spaces after 'stream' keyword but before EOL.\n            # patch provided by Danial Sandler\n            while eol == b\" \":\n                eol = stream.read(1)\n            if eol not in (b\"\\n\", b\"\\r\"):\n                raise PdfStreamError(\"Stream data must be followed by a newline\")\n            if eol == b\"\\r\":\n                # read \\n after\n                if stream.read(1) != b\"\\n\":\n                    stream.seek(-1, 1)\n            # this is a stream object, not a dictionary\n            if SA.LENGTH not in data:\n                raise PdfStreamError(\"Stream length not defined\")\n            length = data[SA.LENGTH]\n            if isinstance(length, IndirectObject):\n                t = stream.tell()\n                length = pdf.get_object(length)\n                stream.seek(t, 0)\n            pstart = stream.tell()\n            data[\"__streamdata__\"] = stream.read(length)\n            e = read_non_whitespace(stream)\n            ndstream = stream.read(8)\n            if (e + ndstream) != b\"endstream\":\n                # (sigh) - the odd PDF file has a length that is too long, so\n                # we need to read backwards to find the \"endstream\" ending.\n                # ReportLab (unknown version) generates files with this bug,\n                # and Python users into PDF files tend to be our audience.\n                # we need to do this to correct the streamdata and chop off\n                # an extra character.\n                pos = stream.tell()\n                stream.seek(-10, 1)\n                end = stream.read(9)\n                if end == b\"endstream\":\n                    # we found it by looking back one character further.\n                    data[\"__streamdata__\"] = data[\"__streamdata__\"][:-1]\n                elif not pdf.strict:\n                    stream.seek(pstart, 0)\n                    data[\"__streamdata__\"] = read_unsized_from_steam(stream, pdf)\n                    pos = stream.tell()\n                else:\n                    stream.seek(pos, 0)\n                    raise PdfReadError(\n                        \"Unable to find 'endstream' marker after stream at byte \"\n                        f\"{hex_str(stream.tell())} (nd='{ndstream!r}', end='{end!r}').\"\n                    )\n        else:\n            stream.seek(pos, 0)\n        if \"__streamdata__\" in data:\n            return StreamObject.initialize_from_dictionary(data)\n        else:\n            retval = DictionaryObject()\n            retval.update(data)\n            return retval\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def call(__self, __context, __obj, *args, **kwargs):\n        \"\"\"Call an object from sandboxed code.\"\"\"\n        # the double prefixes are to avoid double keyword argument\n        # errors when proxying the call.\n        if not __self.is_safe_callable(__obj):\n            raise SecurityError('%r is not safely callable' % (__obj,))\n        return __context.call(__obj, *args, **kwargs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def ask_passwords():\n        ''' prompt for connection and become passwords if needed '''\n\n        op = context.CLIARGS\n        sshpass = None\n        becomepass = None\n        become_prompt = ''\n\n        become_prompt_method = \"BECOME\" if C.AGNOSTIC_BECOME_PROMPT else op['become_method'].upper()\n\n        try:\n            if op['ask_pass']:\n                sshpass = getpass.getpass(prompt=\"SSH password: \")\n                become_prompt = \"%s password[defaults to SSH password]: \" % become_prompt_method\n                if sshpass:\n                    sshpass = to_bytes(sshpass, errors='strict', nonstring='simplerepr')\n            else:\n                become_prompt = \"%s password: \" % become_prompt_method\n\n            if op['become_ask_pass']:\n                becomepass = getpass.getpass(prompt=become_prompt)\n                if op['ask_pass'] and becomepass == '':\n                    becomepass = sshpass\n                if becomepass:\n                    becomepass = to_bytes(becomepass)\n        except EOFError:\n            pass\n\n        return (sshpass, becomepass)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Subgraph::AddNodeWithParameters(\n    const std::vector<int>& inputs, const std::vector<int>& outputs,\n    const std::vector<int>& intermediates, const char* init_data,\n    size_t init_data_size, void* builtin_data,\n    const TfLiteRegistration* registration, int* node_index) {\n  std::unique_ptr<void, decltype(free)*> builtin_data_deleter(builtin_data,\n                                                              free);\n  if (state_ == kStateInvokableAndImmutable) {\n    ReportError(\"AddNodeWithParameters is disallowed when graph is immutable.\");\n    return kTfLiteError;\n  }\n  state_ = kStateUninvokable;\n\n  TF_LITE_ENSURE_OK(&context_, CheckTensorIndices(\"node inputs\", inputs.data(),\n                                                  inputs.size()));\n  TF_LITE_ENSURE_OK(\n      &context_,\n      CheckTensorIndices(\"node outputs\", outputs.data(), outputs.size()));\n\n  int new_node_index = nodes_and_registration_.size();\n  if (node_index) *node_index = new_node_index;\n  nodes_and_registration_.resize(nodes_and_registration_.size() + 1);\n  auto& node_and_reg = nodes_and_registration_.back();\n  TfLiteNode& node = node_and_reg.first;\n  if (node.inputs) TfLiteIntArrayFree(node.inputs);\n  if (node.outputs) TfLiteIntArrayFree(node.outputs);\n  if (node.intermediates) TfLiteIntArrayFree(node.intermediates);\n  if (node.temporaries) TfLiteIntArrayFree(node.temporaries);\n\n  // NOTE, here we are not using move semantics yet, since our internal\n  // representation isn't std::vector, but in the future we would like to avoid\n  // copies, so we want the interface to take r-value references now.\n  node.inputs = ConvertVectorToTfLiteIntArray(inputs);\n  node.outputs = ConvertVectorToTfLiteIntArray(outputs);\n  node.intermediates = ConvertVectorToTfLiteIntArray(intermediates);\n  node.temporaries = TfLiteIntArrayCreate(0);\n  if (init_data) {\n    node.user_data = OpInit(*registration, init_data, init_data_size);\n  } else {\n    node.user_data = OpInit(\n        *registration, static_cast<const char*>(builtin_data_deleter.get()), 0);\n  }\n\n  node.builtin_data = builtin_data_deleter.release();\n  // TODO(ycling): Filling `custom_initial_data` and `custom_initial_data_size`\n  // properly for nodes generated by ReplaceNodeSubsetsWithDelegateKernels.\n\n  if (registration->builtin_code == BuiltinOperator_CUSTOM) {\n    // When it's a CUSTOM op, the `custom_options` field in the Flatbuffer\n    // `Operator` table is passed in.\n    node.custom_initial_data = init_data;\n    node.custom_initial_data_size = init_data_size;\n  } else {\n    node.custom_initial_data = nullptr;\n    node.custom_initial_data_size = 0;\n  }\n\n  node.delegate = nullptr;\n  // Copying of registration is required to support unresolved custom ops.\n  node_and_reg.second = *registration;\n  execution_plan_.push_back(new_node_index);\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input_sizes = context->input(0);\n    const Tensor& filter = context->input(1);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(input_sizes.shape()),\n        errors::InvalidArgument(\n            \"Conv2DBackpropInput: input_sizes input must be 1-dim, not \",\n            input_sizes.dims()));\n    TensorShape input_shape;\n    const int32* in_sizes_data = input_sizes.template flat<int32>().data();\n\n    for (int i = 0; i < input_sizes.NumElements(); ++i) {\n      OP_REQUIRES(context, in_sizes_data[i] >= 0,\n                  errors::InvalidArgument(\"Dimension \", i,\n                                          \" of input_sizes must be >= 0\"));\n      input_shape.AddDim(in_sizes_data[i]);\n    }\n    const TensorShape& filter_shape = filter.shape();\n    EXTRACT_AND_VERIFY_DIMENSIONS(\"DepthwiseConv2DBackpropInput\");\n\n    Tensor* in_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, input_shape, &in_backprop));\n\n    // If there is nothing to compute, return.\n    if (input_shape.num_elements() == 0) {\n      return;\n    }\n\n    // If in_depth==1, this operation is just a standard convolution.\n    // Depthwise convolution is a special case of cuDNN's grouped convolution.\n    bool use_cudnn = std::is_same<Device, GPUDevice>::value &&\n                     (in_depth == 1 ||\n                      (use_cudnn_grouped_conv_ &&\n                       IsCudnnSupportedFilterSize(/*filter_rows=*/filter_rows,\n                                                  /*filter_cols=*/filter_cols,\n                                                  /*in_depth=*/in_depth,\n                                                  /*out_depth=*/out_depth)));\n\n    VLOG(2) << \"DepthwiseConv2dNativeBackpropInput: \"\n            << \" Input: [\" << batch << \", \" << input_rows << \", \" << input_cols\n            << \", \" << in_depth << \"]; Filter: [\" << filter_rows << \", \"\n            << filter_cols << \", \" << in_depth << \", \" << depth_multiplier\n            << \"]; Output: [\" << batch << \", \" << out_rows << \", \" << out_cols\n            << \", \" << out_depth << \"], stride = \" << stride_\n            << \", pad_rows = \" << pad_top << \", pad_cols = \" << pad_left\n            << \", Use cuDNN: \" << use_cudnn;\n\n    if (use_cudnn) {\n      // Reshape from TF depthwise filter to cuDNN grouped convolution filter:\n      //\n      //                  | TensorFlow       | cuDNN\n      // --------------------------------------------------------------------\n      // filter_out_depth | depth_multiplier | depth_multiplier * group_count\n      // filter_in_depth  | in_depth         | in_depth / group_count\n      //\n      // For depthwise convolution, we have group_count == in_depth.\n      int32_t filter_in_depth = 1;\n      TensorShape shape =\n          TensorShape{filter_rows, filter_cols, filter_in_depth, out_depth};\n      Tensor reshaped_filter(/*type=*/dtype_);\n      OP_REQUIRES(\n          context, reshaped_filter.CopyFrom(filter, shape),\n          errors::Internal(\n              \"Failed to reshape filter tensor for grouped convolution.\"));\n      // TODO(yangzihao): Send in arbitrary dilation rates after the dilated\n      // conv is supported.\n      launcher_(context, /*use_cudnn=*/true, cudnn_use_autotune_, out_backprop,\n                reshaped_filter, /*row_dilation=*/1, /*col_dilation=*/1,\n                stride_, stride_, padding_, explicit_paddings_, in_backprop,\n                data_format_);\n      return;\n    }\n\n    auto out_backprop_ptr = out_backprop.template flat<T>().data();\n    auto filter_ptr = filter.template flat<T>().data();\n    auto in_backprop_ptr = in_backprop->template flat<T>().data();\n    LaunchDepthwiseConvBackpropInputOp<Device, T>()(\n        context, args, out_backprop_ptr, filter_ptr, in_backprop_ptr,\n        data_format_);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def safer_getattr(object, name, default=None, getattr=getattr):\n    \"\"\"Getattr implementation which prevents using format on string objects.\n\n    format() is considered harmful:\n    http://lucumr.pocoo.org/2016/12/29/careful-with-str-format/\n\n    \"\"\"\n    if isinstance(object, str) and name == 'format':\n        raise NotImplementedError(\n            'Using format() on a %s is not safe.' % object.__class__.__name__)\n    if name.startswith('_'):\n        raise AttributeError(\n            '\"{name}\" is an invalid attribute name because it '\n            'starts with \"_\"'.format(name=name)\n        )\n    return getattr(object, name, default)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def spider_opened(self, spider):\n        if not hasattr(spider, 'state'):\n            spider.state = {}\n\n        # local fingerprint => key returned by splash\n        spider.state.setdefault(self.remote_keys_key, {})\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _load_post_and_files(self):\n        \"\"\"Populate self._post and self._files if the content-type is a form type\"\"\"\n        if self.method != \"POST\":\n            self._post, self._files = (\n                QueryDict(encoding=self._encoding),\n                MultiValueDict(),\n            )\n            return\n        if self._read_started and not hasattr(self, \"_body\"):\n            self._mark_post_parse_error()\n            return\n\n        if self.content_type == \"multipart/form-data\":\n            if hasattr(self, \"_body\"):\n                # Use already read data\n                data = BytesIO(self._body)\n            else:\n                data = self\n            try:\n                self._post, self._files = self.parse_file_upload(self.META, data)\n            except MultiPartParserError:\n                # An error occurred while parsing POST data. Since when\n                # formatting the error the request handler might access\n                # self.POST, set self._post and self._file to prevent\n                # attempts to parse POST data again.\n                self._mark_post_parse_error()\n                raise\n        elif self.content_type == \"application/x-www-form-urlencoded\":\n            self._post, self._files = (\n                QueryDict(self.body, encoding=self._encoding),\n                MultiValueDict(),\n            )\n        else:\n            self._post, self._files = (\n                QueryDict(encoding=self._encoding),\n                MultiValueDict(),\n            )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n\n    PoolParameters params{context,\n                          ksize_,\n                          stride_,\n                          padding_,\n                          /*explicit_paddings=*/{},\n                          FORMAT_NHWC,\n                          tensor_in.shape()};\n    if (!context->status().ok()) {\n      return;\n    }\n\n    TensorShape out_shape({params.tensor_in_batch, params.out_height,\n                           params.out_width, params.depth});\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n    Tensor* argmax = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, out_shape, &argmax));\n\n    LaunchMaxPoolingWithArgmax<Device, T, Targmax>::launch(\n        context, params, tensor_in, output, argmax, propagate_nans_,\n        include_batch_in_index_);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_image_dimensions(file_or_path, close=False):\n    \"\"\"\n    Returns the (width, height) of an image, given an open file or a path.  Set\n    'close' to True to close the file at the end if it is initially in an open\n    state.\n    \"\"\"\n    # Try to import PIL in either of the two ways it can end up installed.\n    try:\n        from PIL import ImageFile as PILImageFile\n    except ImportError:\n        import ImageFile as PILImageFile\n\n    p = PILImageFile.Parser()\n    if hasattr(file_or_path, 'read'):\n        file = file_or_path\n        file_pos = file.tell()\n        file.seek(0)\n    else:\n        file = open(file_or_path, 'rb')\n        close = True\n    try:\n        while 1:\n            data = file.read(1024)\n            if not data:\n                break\n            p.feed(data)\n            if p.image:\n                return p.image.size\n        return None\n    finally:\n        if close:\n            file.close()\n        else:\n            file.seek(file_pos)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def clean_identity(self):\n        v = self.cleaned_data[\"identity\"]\n        if len(v) > 254:\n            raise forms.ValidationError(\"Address is too long.\")\n\n        if User.objects.filter(email=v).exists():\n            raise forms.ValidationError(\n                \"An account with this email address already exists.\"\n            )\n\n        return v\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def add_cookies(request):\n    global COOKIES\n\n    for cookie in COOKIES:\n        request.set_cookie(**cookie)\n\n    return(request)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, handler):\n        self.handler = handler\n        self.protocol_version = None\n        self.support_legacy_versions = True\n        self.supported_protocols = []\n        self.origin_checker = None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def createRequest(self, op, req, outgoing_data):\n        \"\"\"Return a new QNetworkReply object.\n\n        Args:\n             op: Operation op\n             req: const QNetworkRequest & req\n             outgoing_data: QIODevice * outgoingData\n\n        Return:\n            A QNetworkReply.\n        \"\"\"\n        proxy_factory = objreg.get('proxy-factory', None)\n        if proxy_factory is not None:\n            proxy_error = proxy_factory.get_error()\n            if proxy_error is not None:\n                return networkreply.ErrorNetworkReply(\n                    req, proxy_error, QNetworkReply.UnknownProxyError,\n                    self)\n\n        scheme = req.url().scheme()\n        if scheme in self._scheme_handlers:\n            result = self._scheme_handlers[scheme](req)\n            if result is not None:\n                result.setParent(self)\n                return result\n\n        for header, value in shared.custom_headers(url=req.url()):\n            req.setRawHeader(header, value)\n\n        host_blocker = objreg.get('host-blocker')\n        if host_blocker.is_blocked(req.url()):\n            log.webview.info(\"Request to {} blocked by host blocker.\".format(\n                req.url().host()))\n            return networkreply.ErrorNetworkReply(\n                req, HOSTBLOCK_ERROR_STRING, QNetworkReply.ContentAccessDenied,\n                self)\n\n        # There are some scenarios where we can't figure out current_url:\n        # - There's a generic NetworkManager, e.g. for downloads\n        # - The download was in a tab which is now closed.\n        current_url = QUrl()\n\n        if self._tab_id is not None:\n            assert self._win_id is not None\n            try:\n                tab = objreg.get('tab', scope='tab', window=self._win_id,\n                                 tab=self._tab_id)\n                current_url = tab.url()\n            except (KeyError, RuntimeError):\n                # https://github.com/qutebrowser/qutebrowser/issues/889\n                # Catching RuntimeError because we could be in the middle of\n                # the webpage shutdown here.\n                current_url = QUrl()\n\n        if 'log-requests' in self._args.debug_flags:\n            operation = debug.qenum_key(QNetworkAccessManager, op)\n            operation = operation.replace('Operation', '').upper()\n            log.webview.debug(\"{} {}, first-party {}\".format(\n                operation,\n                req.url().toDisplayString(),\n                current_url.toDisplayString()))\n\n        self.set_referer(req, current_url)\n        return super().createRequest(op, req, outgoing_data)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const float min_input = context->input(1).flat<float>()(0);\n    const float max_input = context->input(2).flat<float>()(0);\n    MaxPoolingOp<Device, T>::Compute(context);\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));\n    output_min->flat<float>()(0) = min_input;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));\n    output_max->flat<float>()(0) = max_input;\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status TrySimplify(NodeDef* consumer, string* simplified_node_name) override {\n    NodeDef* producer;\n    TF_RETURN_IF_ERROR(GetInputNode(consumer->input(0), &producer));\n    const bool producer_is_cast = IsCastLike(*producer);\n    const bool can_optimize =\n        !IsCheckNumerics(*producer) &&\n        ((producer_is_cast && IsValuePreserving(*consumer)) ||\n         (IsValuePreserving(*producer) && IsCastLike(*consumer)));\n    if (!can_optimize || IsControlFlow(*producer) ||\n        IsInPreserveSet(*producer) ||\n        producer->device() != consumer->device()) {\n      return Status::OK();\n    }\n\n    const NodeDef* cast_like_node = producer_is_cast ? producer : consumer;\n    const OpDef* cast_like_op_def = nullptr;\n    TF_RETURN_IF_ERROR(OpRegistry::Global()->LookUpOpDef(cast_like_node->op(),\n                                                         &cast_like_op_def));\n    DataType cast_src_type;\n    TF_RETURN_IF_ERROR(InputTypeForNode(*cast_like_node, *cast_like_op_def, 0,\n                                        &cast_src_type));\n    DataType cast_dst_type;\n    TF_RETURN_IF_ERROR(OutputTypeForNode(*cast_like_node, *cast_like_op_def, 0,\n                                         &cast_dst_type));\n    if (!IsFixedSizeType(cast_src_type) || !IsFixedSizeType(cast_dst_type)) {\n      return Status::OK();\n    } else if (producer_is_cast &&\n               DataTypeSize(cast_dst_type) <= DataTypeSize(cast_src_type)) {\n      return Status::OK();\n    } else if (!producer_is_cast &&\n               DataTypeSize(cast_dst_type) >= DataTypeSize(cast_src_type)) {\n      return Status::OK();\n    }\n\n    // Check that nodes were not already optimized.\n    const string optimized_producer_name = OptimizedNodeName(\n        ParseNodeScopeAndName(producer->name()), DataTypeString(cast_dst_type));\n    const string optimized_consumer_name = OptimizedNodeName(\n        ParseNodeScopeAndName(consumer->name()), DataTypeString(cast_src_type));\n    const bool is_already_optimized =\n        ctx().node_map->NodeExists(optimized_consumer_name) ||\n        ctx().node_map->NodeExists(optimized_producer_name);\n    if (is_already_optimized) {\n      return Status::OK();\n    }\n\n    // Add copies of consumer and producer in reverse order.\n    NodeDef* input;\n    TF_RETURN_IF_ERROR(GetInputNode(producer->input(0), &input));\n    // Create new producer node.\n    NodeDef* new_producer = AddCopyNode(optimized_consumer_name, consumer);\n    new_producer->set_input(0, producer->input(0));\n    ctx().node_map->AddOutput(input->name(), new_producer->name());\n\n    // Create new consumer node.\n    NodeDef* new_consumer = AddCopyNode(optimized_producer_name, producer);\n    new_consumer->set_input(0, new_producer->name());\n\n    NodeDef* new_value_preserving =\n        producer_is_cast ? new_producer : new_consumer;\n    const DataType new_input_type =\n        producer_is_cast ? cast_src_type : cast_dst_type;\n    // Update the input type of the value-preserving node. The input and\n    // output types of the cast-like nodes remain the same.\n    TF_RETURN_IF_ERROR(SetInputType(new_input_type, new_value_preserving));\n    // Make sure there is a kernel registered for the value preserving op\n    // with the new input type.\n    TF_RETURN_IF_ERROR(IsKernelRegisteredForNode(*new_value_preserving));\n    ctx().node_map->AddOutput(new_producer->name(), new_consumer->name());\n\n    AddToOptimizationQueue(new_producer);\n    *simplified_node_name = new_consumer->name();\n\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n        key = self.make_key(key, version=version)\n        return self._cache.add(key, value, self.get_backend_timeout(timeout))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_user_roles(self, context, user_id, tenant_id=None):\n        \"\"\"Get the roles for a user and tenant pair.\n\n        Since we're trying to ignore the idea of user-only roles we're\n        not implementing them in hopes that the idea will die off.\n\n        \"\"\"\n        if tenant_id is None:\n            raise exception.NotImplemented(message='User roles not supported: '\n                                                   'tenant ID required')\n\n        user = self.identity_api.get_user(context, user_id)\n        if user is None:\n            raise exception.UserNotFound(user_id=user_id)\n        tenant = self.identity_api.get_tenant(context, tenant_id)\n        if tenant is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        roles = self.identity_api.get_roles_for_user_and_tenant(\n                context, user_id, tenant_id)\n        return {'roles': [self.identity_api.get_role(context, x)\n                          for x in roles]}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  OpContext op_context(context, node);\n\n    switch (op_context.output->type) {\n      case kTfLiteFloat32:\n        TFLiteOperation<kernel_type, float, OpType>(context, node, op_context);\n        break;\n      case kTfLiteUInt8:\n        TFLiteOperation<kernel_type, uint8_t, OpType>(context, node,\n                                                      op_context);\n        break;\n      case kTfLiteInt8:\n        TFLiteOperation<kernel_type, int8_t, OpType>(context, node, op_context);\n        break;\n      case kTfLiteInt32:\n        TFLiteOperation<kernel_type, int32_t, OpType>(context, node,\n                                                      op_context);\n        break;\n      case kTfLiteInt64:\n        TFLiteOperation<kernel_type, int64_t, OpType>(context, node,\n                                                      op_context);\n        break;\n      case kTfLiteInt16:\n        TFLiteOperation<kernel_type, int16_t, OpType>(context, node,\n                                                      op_context);\n        break;\n      default:\n        context->ReportError(context,\n                             \"Type %d is currently not supported by Maximum.\",\n                             op_context.output->type);\n        return kTfLiteError;\n    }\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def new_control(self, type, name, value, **kwargs):\n        \"\"\"Add a new input element to the form.\n\n        The arguments set the attributes of the new element.\n        \"\"\"\n        # Remove existing input-like elements with the same name\n        for tag in ('input', 'textarea', 'select'):\n            for old in self.form.find_all(tag, {'name': name}):\n                old.decompose()\n        # We don't have access to the original soup object (just the\n        # Tag), so we instantiate a new BeautifulSoup() to call\n        # new_tag(). We're only building the soup object, not parsing\n        # anything, so the parser doesn't matter. Specify the one\n        # included in Python to avoid having dependency issue.\n        control = BeautifulSoup(\"\", \"html.parser\").new_tag('input')\n        control['type'] = type\n        control['name'] = name\n        control['value'] = value\n        for k, v in kwargs.items():\n            control[k] = v\n        self.form.append(control)\n        return control\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status SparseTensorFromContext(OpKernelContext* ctx, const int32_t base_index,\n                               const bool validate_indices,\n                               sparse::SparseTensor* tensor) {\n  // Assume row-major order.\n  TensorShape shape;\n  TF_RETURN_IF_ERROR(TensorShape::BuildTensorShape(\n      ctx->input(base_index + 2).vec<int64_t>(), &shape));\n  CheckRankAtLeast2(ctx, shape);\n  std::vector<int64_t> order(shape.dims());\n  std::iota(order.begin(), order.end(), 0);\n\n  Status status = sparse::SparseTensor::Create(\n      ctx->input(base_index), ctx->input(base_index + 1), shape, order, tensor);\n\n  if (!validate_indices || !status.ok()) return status;\n  return tensor->IndicesValid();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __call__(self, value):\n        if not isinstance(value, str):\n            raise ValidationError(self.message, code=self.code, params={'value': value})\n        # Check if the scheme is valid.\n        scheme = value.split('://')[0].lower()\n        if scheme not in self.schemes:\n            raise ValidationError(self.message, code=self.code, params={'value': value})\n\n        # Then check full URL\n        try:\n            super().__call__(value)\n        except ValidationError as e:\n            # Trivial case failed. Try for possible IDN domain\n            if value:\n                try:\n                    scheme, netloc, path, query, fragment = urlsplit(value)\n                except ValueError:  # for example, \"Invalid IPv6 URL\"\n                    raise ValidationError(self.message, code=self.code, params={'value': value})\n                try:\n                    netloc = punycode(netloc)  # IDN -> ACE\n                except UnicodeError:  # invalid domain part\n                    raise e\n                url = urlunsplit((scheme, netloc, path, query, fragment))\n                super().__call__(url)\n            else:\n                raise\n        else:\n            # Now verify IPv6 in the netloc part\n            host_match = re.search(r'^\\[(.+)\\](?::\\d{2,5})?$', urlsplit(value).netloc)\n            if host_match:\n                potential_ip = host_match[1]\n                try:\n                    validate_ipv6_address(potential_ip)\n                except ValidationError:\n                    raise ValidationError(self.message, code=self.code, params={'value': value})\n\n        # The maximum length of a full host name is 253 characters per RFC 1034\n        # section 3.1. It's defined to be 255 bytes or less, but this includes\n        # one byte for the length of the name and one byte for the trailing dot\n        # that's used to indicate absolute names in DNS.\n        if len(urlsplit(value).hostname) > 253:\n            raise ValidationError(self.message, code=self.code, params={'value': value})\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_json(self, uri):\n        \"\"\"Make a GET request to an endpoint returning JSON and parse result\n\n        :param uri: The URI to make a GET request to.\n        :type uri: unicode\n\n        :return: A deferred containing JSON parsed into a Python object.\n        :rtype: twisted.internet.defer.Deferred[dict[any, any]]\n        \"\"\"\n        logger.debug(\"HTTP GET %s\", uri)\n\n        response = yield self.agent.request(\n            b\"GET\",\n            uri.encode(\"utf8\"),\n        )\n        body = yield readBody(response)\n        try:\n            # json.loads doesn't allow bytes in Python 3.5\n            json_body = json.loads(body.decode(\"UTF-8\"))\n        except Exception as e:\n            logger.exception(\"Error parsing JSON from %s\", uri)\n            raise\n        defer.returnValue(json_body)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status GraphConstructor::MakeEdge(Node* src, int output_index, Node* dst,\n                                  int input_index) {\n  DataType src_out = src->output_type(output_index);\n  DataType dst_in = dst->input_type(input_index);\n  if (!TypesCompatible(dst_in, src_out)) {\n    return errors::InvalidArgument(\n        \"Input \", input_index, \" of node \", dst->name(), \" was passed \",\n        DataTypeString(src_out), \" from \", src->name(), \":\", output_index,\n        \" incompatible with expected \", DataTypeString(dst_in), \".\");\n  }\n  g_->AddEdge(src, output_index, dst, input_index);\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_vault_token(vault_endpoint: str, vault_secret: str) -> str:\n    if not vault_endpoint:\n        raise ValueError('vault_endpoint is required')\n    if not vault_secret:\n        raise ValueError('vault_secret is required')\n\n    headers = {'X-Vault-Token': vault_secret}\n    response = requests.get(vault_endpoint, headers=headers, verify=False)\n\n    if response.status_code != 200:\n        logger.info(response.text)\n        raise ValueError(\n            'Could not get vault token, response code: {}'.format(\n                response.status_code))\n\n    logger.info(\"get oauth token from %s status_code=%s\", vault_endpoint, response.status_code)\n    return response.json()['data']['token']\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def start_jupyter_job(sbatch_options: dict = None, conda_env: str = None, lab: bool = False):\n\n    sbatch_options = sbatch_options if sbatch_options is not None else {}\n    sbatch_options_merged = SETTINGS.SLURM_DEFAULT['sbatch_options']\n    sbatch_options_merged.update(SETTINGS.SBATCH_OPTIONS_TEMPLATES.JUPYTER)\n    sbatch_options_merged.update(sbatch_options)\n    # Construct sbatch options string\n    sbatch_options_str = create_slurm_options_string(sbatch_options_merged)\n\n    template = pkg_resources.resource_string(__name__, \"jupyter_template.sh\").decode(\"utf-8\")\n\n    script = template.format(\n            sbatch_options=sbatch_options_str,\n            use_conda_env=str(conda_env is not None).lower(),\n            conda_env=conda_env,\n            notebook_or_lab=\" notebook\" if not lab else \"-lab\",\n    )\n\n    random_int = np.random.randint(0, 999999)\n    path = f\"/tmp/{random_int}.sh\"\n    while os.path.exists(path):\n        random_int = np.random.randint(0, 999999)\n        path = f\"/tmp/{random_int}.sh\"\n    with open(path, \"w\") as f:\n        f.write(script)\n\n    output = subprocess.run(f'sbatch {path}', shell=True, check=True, capture_output=True).stdout\n    os.remove(path)\n\n    slurm_array_job_id = int(output.split(b' ')[-1])\n    logging.info(f\"Queued Jupyter instance in Slurm job with ID {slurm_array_job_id}.\")\n\n    job_output = subprocess.run(f'scontrol show job {slurm_array_job_id} -o',\n                                shell=True, check=True, capture_output=True).stdout\n    job_output_results = job_output.decode(\"utf-8\").split(\" \")\n    job_info_dict = {x.split('=')[0]: x.split('=')[1] for x in job_output_results if '=' in x}\n    log_file = job_info_dict['StdOut']\n\n    logging.info(f\"The job's log-file is '{log_file}'.\")\n    logging.info(\"Waiting for start-up to fetch the machine and port of the Jupyter instance... \"\n                 \"(ctrl-C to cancel fetching)\")\n\n    while job_info_dict['JobState'] in SlurmStates.PENDING:\n        job_output = subprocess.run(f'scontrol show job {slurm_array_job_id} -o',\n                                    shell=True, check=True, capture_output=True).stdout\n        job_output_results = job_output.decode(\"utf-8\").split(\" \")\n        job_info_dict = {x.split('=')[0]: x.split('=')[1] for x in job_output_results if '=' in x}\n        time.sleep(1)\n    if job_info_dict['JobState'] not in SlurmStates.RUNNING:\n        logging.error(f\"Slurm job failed. See log-file '{log_file}' for more information.\")\n        exit(1)\n\n    logging.info(\"Slurm job is running. Jupyter instance is starting up...\")\n    log_file_contents = \"\"\n    while \" is running at\" not in log_file_contents:\n        if os.path.exists(log_file):\n            with open(log_file, \"r\") as f:\n                log_file_contents = f.read()\n        time.sleep(0.5)\n    log_file_split = log_file_contents.split(\"\\n\")\n    url_lines = [x for x in log_file_split if \"http\" in x]\n    url = url_lines[0].split(\" \")[3]\n    url = url.replace(\"https://\", \"\")\n    url = url.replace(\"http://\", \"\")\n    url = url.rstrip('/')\n    logging.info(f\"Start-up completed. The Jupyter instance is running at '{url}'.\")\n    logging.info(f\"To stop the job, run 'scancel {slurm_array_job_id}'.\")\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    constexpr int tensor_in_and_out_dims = 4;\n\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n      OP_REQUIRES(\n          context, pooling_ratio_[i] <= input_size[i],\n          errors::InvalidArgument(\n              \"Pooling ratio cannot be bigger than input tensor dim size.\"));\n    }\n    // Output size.\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n\n    // Generate pooling sequence.\n    std::vector<int64_t> row_cum_seq;\n    std::vector<int64_t> col_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    row_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                          &generator, pseudo_random_);\n    col_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                          &generator, pseudo_random_);\n\n    // Prepare output.\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_row_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\n                     1, TensorShape({static_cast<int64_t>(row_cum_seq.size())}),\n                     &output_row_seq_tensor));\n    Tensor* output_col_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\n                     2, TensorShape({static_cast<int64_t>(col_cum_seq.size())}),\n                     &output_col_seq_tensor));\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n    // out_count corresponds to number of elements in each pooling cell.\n    Eigen::Matrix<T, Eigen::Dynamic, 1> out_count(out_mat.cols());\n\n    // Initializes the output tensor and out_count with 0.\n    out_mat.setZero();\n    out_count.setZero();\n\n    auto output_row_seq_flat = output_row_seq_tensor->flat<int64_t>();\n    auto output_col_seq_flat = output_col_seq_tensor->flat<int64_t>();\n\n    // Set output tensors.\n    for (int i = 0; i < row_cum_seq.size(); ++i) {\n      output_row_seq_flat(i) = row_cum_seq[i];\n    }\n\n    for (int i = 0; i < col_cum_seq.size(); ++i) {\n      output_col_seq_flat(i) = col_cum_seq[i];\n    }\n\n    // For both input and output,\n    // 0: batch\n    // 1: row / row\n    // 2: col / col\n    // 3: depth / channel\n    const int64_t row_max = input_size[1] - 1;\n    const int64_t col_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // row sequence.\n      for (int64_t hs = 0; hs < row_cum_seq.size() - 1; ++hs) {\n        // row start and end.\n        const int64_t row_start = row_cum_seq[hs];\n        int64_t row_end =\n            overlapping_ ? row_cum_seq[hs + 1] : row_cum_seq[hs + 1] - 1;\n        row_end = std::min(row_end, row_max);\n\n        // col sequence.\n        for (int64_t ws = 0; ws < col_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // col start and end.\n          const int64_t col_start = col_cum_seq[ws];\n          int64_t col_end =\n              overlapping_ ? col_cum_seq[ws + 1] : col_cum_seq[ws + 1] - 1;\n          col_end = std::min(col_end, col_max);\n          for (int64_t h = row_start; h <= row_end; ++h) {\n            for (int64_t w = col_start; w <= col_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) += in_mat.col(in_offset);\n              out_count(out_offset)++;\n            }\n          }\n        }\n      }\n    }\n    DCHECK_GT(out_count.minCoeff(), 0);\n    out_mat.array().rowwise() /= out_count.transpose().array();\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def cookies(self):\n        \"\"\"The session cookies.\"\"\"\n        return self._cookies\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _configure_session(cls, app):\n        tempdir = app.config[\"PYLOAD_API\"].get_cachedir()\n        cache_path = os.path.join(tempdir, \"flask\")\n        os.makedirs(cache_path, exist_ok=True)\n\n        app.config[\"SESSION_FILE_DIR\"] = cache_path\n        app.config[\"SESSION_TYPE\"] = \"filesystem\"\n        app.config[\"SESSION_COOKIE_NAME\"] = \"pyload_session\"\n        app.config[\"SESSION_PERMANENT\"] = False\n\n        session_lifetime = max(app.config[\"PYLOAD_API\"].get_config_value(\"webui\", \"session_lifetime\"), 1) * 60\n        app.config[\"PERMANENT_SESSION_LIFETIME\"] = session_lifetime\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def recovery_parser(xml):\n            parser = XMLParser(recover=True)\n            return parse(BytesIO(xml), parser)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "m3ApiRawFunction(m3_wasi_generic_fd_read)\n{\n    m3ApiReturnType  (uint32_t)\n    m3ApiGetArg      (__wasi_fd_t          , fd)\n    m3ApiGetArgMem   (wasi_iovec_t *       , wasi_iovs)\n    m3ApiGetArg      (__wasi_size_t        , iovs_len)\n    m3ApiGetArgMem   (__wasi_size_t *      , nread)\n\n    m3ApiCheckMem(wasi_iovs,    iovs_len * sizeof(wasi_iovec_t));\n    m3ApiCheckMem(nread,        sizeof(__wasi_size_t));\n\n#if defined(HAS_IOVEC)\n    struct iovec iovs[iovs_len];\n    copy_iov_to_host(_mem, iovs, wasi_iovs, iovs_len);\n\n    ssize_t ret = readv(fd, iovs, iovs_len);\n    if (ret < 0) { m3ApiReturn(errno_to_wasi(errno)); }\n    m3ApiWriteMem32(nread, ret);\n    m3ApiReturn(__WASI_ERRNO_SUCCESS);\n#else\n    ssize_t res = 0;\n    for (__wasi_size_t i = 0; i < iovs_len; i++) {\n        void* addr = m3ApiOffsetToPtr(m3ApiReadMem32(&wasi_iovs[i].buf));\n        size_t len = m3ApiReadMem32(&wasi_iovs[i].buf_len);\n        if (len == 0) continue;\n\n        int ret = read (fd, addr, len);\n        if (ret < 0) m3ApiReturn(errno_to_wasi(errno));\n        res += ret;\n        if ((size_t)ret < len) break;\n    }\n    m3ApiWriteMem32(nread, res);\n    m3ApiReturn(__WASI_ERRNO_SUCCESS);\n#endif\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def do_transform(self, request, response, config):\n        self.request = request\n        self.response = response\n        self.config = config\n        self.response += check_update(config)\n        maltego_misp_event = request.entity\n        self.misp = get_misp_connection(config, request.parameters)\n        event_id = maltego_misp_event.id\n        search_result = self.misp.search(controller='events', eventid=event_id, with_attachments=False)\n        if search_result:\n            self.event_json = search_result.pop()\n        else:\n            return False\n\n        self.response += event_to_entity(self.event_json)\n        return True\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def handler(req):\n\n    \"\"\"\n    Right now, index serves everything.\n\n    Hitting this URL means we've already cleared authn/authz\n    but we still need to use the token for all remote requests.\n    \"\"\"\n\n    my_user = __get_user(req)\n    my_uri = req.uri\n    sess  = __get_session(req)\n\n    if not sess.has_key('cobbler_token'):\n       # using Kerberos instead of Python Auth handler? \n       # We need to get our own token for use with authn_passthru\n       # which should also be configured in /etc/cobbler/modules.conf\n       # if another auth mode is configured in modules.conf this will\n       # most certaintly fail.\n       try:\n           if not os.path.exists(\"/var/lib/cobbler/web.ss\"):\n               apache.log_error(\"cannot load /var/lib/cobbler/web.ss\")\n               return apache.HTTP_UNAUTHORIZED\n           fd = open(\"/var/lib/cobbler/web.ss\")\n           data = fd.read()\n           my_pw = data\n           fd.close()\n           token = xmlrpc_server.login(my_user,my_pw)\n       except Exception, e:\n           apache.log_error(str(e))\n           return apache.HTTP_UNAUTHORIZED\n       sess['cobbler_token'] = token\n    else:\n       token = sess['cobbler_token']\n\n    # needed?\n    # usage later\n    req.add_common_vars()\n \n    # process form and qs data, if any\n    fs = util.FieldStorage(req)\n    form = {}\n    for x in fs.keys():\n        form[x] = str(fs.get(x,'default'))\n\n    fd = open(\"/etc/cobbler/settings\")\n    data = fd.read()\n    fd.close()\n    ydata = yaml.load(data)\n    remote_port = ydata.get(\"xmlrpc_port\", 25151)\n\n    mode = form.get('mode','index')\n\n    # instantiate a CobblerWeb object\n    cw = CobblerWeb.CobblerWeb(\n         apache   = apache,\n         token    = token, \n         base_url = \"/cobbler/web/\",\n         mode     = mode,\n         server   = \"http://127.0.0.1:%s\" % remote_port\n    )\n\n    # check for a valid path/mode\n    # handle invalid paths gracefully\n    if mode in cw.modes():\n        func = getattr( cw, mode )\n        content = func( **form )\n    else:\n        func = getattr( cw, 'error_page' )\n        content = func( \"Invalid Mode: \\\"%s\\\"\" % mode )\n\n    if content.startswith(\"# REDIRECT \"):\n        util.redirect(req, location=content[11:], permanent=False)\n    else:\n        # apache.log_error(\"%s:%s ... %s\" % (my_user, my_uri, str(form)))\n        req.content_type = \"text/html;charset=utf-8\"\n        req.write(unicode(content).encode('utf-8'))\n    \n    if not content.startswith(\"# ERROR\") and content.find(\"<!-- ERROR -->\") == -1:\n       return apache.OK\n    else:\n       # catch Cheetah errors and web errors\n       return apache.HTTP_INTERNAL_SERVER_ERROR\n "}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(\n        self,\n        targets=\"127.0.0.1\",\n        options=\"-sT\",\n        event_callback=None,\n        safe_mode=True,\n        fqp=None,\n    ):\n        \"\"\"\n        Constructor of NmapProcess class.\n\n        :param targets: hosts to be scanned. Could be a string of hosts \\\n        separated with a coma or a python list of hosts/ip.\n        :type targets: string or list\n\n        :param options: list of nmap options to be applied to scan. \\\n        These options are all documented in nmap's man pages.\n\n        :param event_callback: callable function which will be ran \\\n        each time nmap process outputs data. This function will receive \\\n        two parameters:\n\n            1. the nmap process object\n            2. the data produced by nmap process. See readme for examples.\n\n        :param safe_mode: parameter to protect unsafe options like -oN, -oG, \\\n        -iL, -oA,...\n\n        :param fqp: full qualified path, if None, nmap will be searched \\\n        in the PATH\n\n        :return: NmapProcess object\n\n        \"\"\"\n        Thread.__init__(self)\n        unsafe_opts = set(\n            [\n                \"-oG\",\n                \"-oN\",\n                \"-iL\",\n                \"-oA\",\n                \"-oS\",\n                \"-oX\",\n                \"--iflist\",\n                \"--resume\",\n                \"--stylesheet\",\n                \"--datadir\",\n            ]\n        )\n        # more reliable than just using os.name() (cygwin)\n        self.__is_windows = platform.system() == \"Windows\"\n        if fqp:\n            if os.path.isfile(fqp) and os.access(fqp, os.X_OK):\n                self.__nmap_binary = fqp\n            else:\n                raise EnvironmentError(1, \"wrong path or not executable\", fqp)\n        else:\n            nmap_binary_name = \"nmap\"\n            self.__nmap_binary = self._whereis(nmap_binary_name)\n        self.__nmap_fixed_options = \"-oX - -vvv --stats-every 1s\"\n\n        if self.__nmap_binary is None:\n            em = \"nmap is not installed or could not be found in system path\"\n            raise EnvironmentError(1, em)\n\n        if isinstance(targets, str):\n            self.__nmap_targets = targets.replace(\" \", \"\").split(\",\")\n        elif isinstance(targets, list):\n            self.__nmap_targets = targets\n        else:\n            raise Exception(\n                \"Supplied target list should be either a string or a list\"\n            )\n\n        self._nmap_options = set(options.split())\n        if safe_mode and not self._nmap_options.isdisjoint(unsafe_opts):\n            raise Exception(\n                \"unsafe options activated while safe_mode is set True\"\n            )\n        self.__nmap_dynamic_options = options\n        self.__sudo_run = \"\"\n        self.__nmap_command_line = self.get_command_line()\n\n        if event_callback and callable(event_callback):\n            self.__nmap_event_callback = event_callback\n        else:\n            self.__nmap_event_callback = None\n        (\n            self.DONE,\n            self.READY,\n            self.RUNNING,\n            self.CANCELLED,\n            self.FAILED,\n        ) = range(5)\n        self._run_init()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def do_template(self, data, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, disable_lookups=False):\n        if USE_JINJA2_NATIVE and not isinstance(data, string_types):\n            return data\n\n        # For preserving the number of input newlines in the output (used\n        # later in this method)\n        data_newlines = _count_newlines_from_end(data)\n\n        if fail_on_undefined is None:\n            fail_on_undefined = self._fail_on_undefined_errors\n\n        try:\n            # allows template header overrides to change jinja2 options.\n            if overrides is None:\n                myenv = self.environment.overlay()\n            else:\n                myenv = self.environment.overlay(overrides)\n\n            # Get jinja env overrides from template\n            if hasattr(data, 'startswith') and data.startswith(JINJA2_OVERRIDE):\n                eol = data.find('\\n')\n                line = data[len(JINJA2_OVERRIDE):eol]\n                data = data[eol + 1:]\n                for pair in line.split(','):\n                    (key, val) = pair.split(':')\n                    key = key.strip()\n                    setattr(myenv, key, ast.literal_eval(val.strip()))\n\n            # Adds Ansible custom filters and tests\n            myenv.filters.update(self._get_filters())\n            for k in myenv.filters:\n                if not getattr(myenv.filters[k], '__UNROLLED__', False):\n                    myenv.filters[k] = _unroll_iterator(myenv.filters[k])\n            myenv.tests.update(self._get_tests())\n\n            if escape_backslashes:\n                # Allow users to specify backslashes in playbooks as \"\\\\\" instead of as \"\\\\\\\\\".\n                data = _escape_backslashes(data, myenv)\n\n            try:\n                t = myenv.from_string(data)\n            except TemplateSyntaxError as e:\n                raise AnsibleError(\"template error while templating string: %s. String: %s\" % (to_native(e), to_native(data)))\n            except Exception as e:\n                if 'recursion' in to_native(e):\n                    raise AnsibleError(\"recursive loop detected in template string: %s\" % to_native(data))\n                else:\n                    return data\n\n            if disable_lookups:\n                t.globals['query'] = t.globals['q'] = t.globals['lookup'] = self._fail_lookup\n\n            jvars = AnsibleJ2Vars(self, t.globals)\n\n            self.cur_context = new_context = t.new_context(jvars, shared=True)\n            rf = t.root_render_func(new_context)\n\n            try:\n                res = j2_concat(rf)\n                if getattr(new_context, 'unsafe', False):\n                    res = wrap_var(res)\n            except TypeError as te:\n                if 'AnsibleUndefined' in to_native(te):\n                    errmsg = \"Unable to look up a name or access an attribute in template string (%s).\\n\" % to_native(data)\n                    errmsg += \"Make sure your variable name does not contain invalid characters like '-': %s\" % to_native(te)\n                    raise AnsibleUndefinedVariable(errmsg)\n                else:\n                    display.debug(\"failing because of a type error, template data is: %s\" % to_text(data))\n                    raise AnsibleError(\"Unexpected templating type error occurred on (%s): %s\" % (to_native(data), to_native(te)))\n\n            if USE_JINJA2_NATIVE and not isinstance(res, string_types):\n                return res\n\n            if preserve_trailing_newlines:\n                # The low level calls above do not preserve the newline\n                # characters at the end of the input data, so we use the\n                # calculate the difference in newlines and append them\n                # to the resulting output for parity\n                #\n                # jinja2 added a keep_trailing_newline option in 2.7 when\n                # creating an Environment.  That would let us make this code\n                # better (remove a single newline if\n                # preserve_trailing_newlines is False).  Once we can depend on\n                # that version being present, modify our code to set that when\n                # initializing self.environment and remove a single trailing\n                # newline here if preserve_newlines is False.\n                res_newlines = _count_newlines_from_end(res)\n                if data_newlines > res_newlines:\n                    res += self.environment.newline_sequence * (data_newlines - res_newlines)\n            return res\n        except (UndefinedError, AnsibleUndefinedVariable) as e:\n            if fail_on_undefined:\n                raise AnsibleUndefinedVariable(e)\n            else:\n                display.debug(\"Ignoring undefined failure: %s\" % to_text(e))\n                return data\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render(self, name, value, attrs=None):\n        html = super(AdminURLFieldWidget, self).render(name, value, attrs)\n        if value:\n            value = force_text(self._format_value(value))\n            final_attrs = {'href': mark_safe(smart_urlquote(value))}\n            html = format_html(\n                '<p class=\"url\">{0} <a {1}>{2}</a><br />{3} {4}</p>',\n                _('Currently:'), flatatt(final_attrs), value,\n                _('Change:'), html\n            )\n        return html\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def resolve_rpsl_object_journal(rpsl_object, info: GraphQLResolveInfo):\n    \"\"\"\n    Resolve a journal subquery on an RPSL object.\n    \"\"\"\n    database_handler = info.context['request'].app.state.database_handler\n    access_list = f\"sources.{rpsl_object['source']}.nrtm_access_list\"\n    if not is_client_permitted(info.context['request'].client.host, access_list):\n        raise GraphQLError(f\"Access to journal denied for source {rpsl_object['source']}\")\n\n    query = RPSLDatabaseJournalQuery()\n    query.sources([rpsl_object['source']]).rpsl_pk(rpsl_object['rpslPk'])\n    for row in database_handler.execute_query(query, refresh_on_error=True):\n        response = {snake_to_camel_case(k): v for k, v in row.items()}\n        response['operation'] = response['operation'].name\n        if response['origin']:\n            response['origin'] = response['origin'].name\n        yield response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def tx_svcs(self) -> bool:\n        aname = re.sub(\"[^0-9a-zA-Z]+\", \"\", self.args.name) or \"a\"\n        ep = self.host\n        host = ep.split(\":\")[0]\n        hport = ep[ep.find(\":\") :] if \":\" in ep else \"\"\n        rip = (\n            host\n            if self.args.rclone_mdns or not self.args.zm\n            else self.conn.hsrv.nm.map(self.ip) or host\n        )\n        vp = (self.uparam[\"hc\"] or \"\").lstrip(\"/\")\n        html = self.j2s(\n            \"svcs\",\n            args=self.args,\n            accs=bool(self.asrv.acct),\n            s=\"s\" if self.is_https else \"\",\n            rip=rip,\n            ep=ep,\n            vp=vp,\n            rvp=vjoin(self.args.R, vp),\n            host=host,\n            hport=hport,\n            aname=aname,\n            pw=self.pw or \"pw\",\n        )\n        self.reply(html.encode(\"utf-8\"))\n        return True\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor* input_indices;\n    const Tensor* input_values;\n    const Tensor* input_shape;\n    SparseTensorsMap* map;\n\n    OP_REQUIRES_OK(context, context->input(\"sparse_indices\", &input_indices));\n    OP_REQUIRES_OK(context, context->input(\"sparse_values\", &input_values));\n    OP_REQUIRES_OK(context, context->input(\"sparse_shape\", &input_shape));\n    OP_REQUIRES_OK(context, GetMap(context, true /* is_writing */, &map));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_indices->shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be a matrix but received shape \",\n                    input_indices->shape().DebugString()));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_values->shape()),\n                errors::InvalidArgument(\n                    \"Input values should be a vector but received shape \",\n                    input_values->shape().DebugString()));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shape should be a vector but received shape \",\n                    input_shape->shape().DebugString()));\n\n    int rank = input_shape->NumElements();\n\n    OP_REQUIRES(\n        context, rank > 1,\n        errors::InvalidArgument(\n            \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\n\n    TensorShape tensor_input_shape(input_shape->vec<int64>());\n    gtl::InlinedVector<int64, 8> std_order(rank);\n    std::iota(std_order.begin(), std_order.end(), 0);\n    SparseTensor input_st;\n    OP_REQUIRES_OK(context, SparseTensor::Create(*input_indices, *input_values,\n                                                 tensor_input_shape, std_order,\n                                                 &input_st));\n\n    auto input_shape_t = input_shape->vec<int64>();\n    const int64 N = input_shape_t(0);\n\n    Tensor sparse_handles(DT_INT64, TensorShape({N}));\n    auto sparse_handles_t = sparse_handles.vec<int64>();\n\n    OP_REQUIRES_OK(context, input_st.IndicesValid());\n\n    // We can generate the output shape proto string now, for all\n    // minibatch entries.\n    TensorShape output_shape;\n    OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\n                                input_shape_t.data() + 1,\n                                input_shape->NumElements() - 1, &output_shape));\n\n    // Get groups by minibatch dimension\n    std::unordered_set<int64> visited;\n    sparse::GroupIterable minibatch = input_st.group({0});\n    for (const auto& subset : minibatch) {\n      const int64 b = subset.group()[0];\n      visited.insert(b);\n      OP_REQUIRES(\n          context, b > -1 && b < N,\n          errors::InvalidArgument(\n              \"Received unexpected column 0 value in input SparseTensor: \", b,\n              \" < 0 or >= N (= \", N, \")\"));\n\n      const auto indices = subset.indices();\n      const auto values = subset.values<T>();\n      const int64 num_entries = values.size();\n\n      Tensor output_indices = Tensor(DT_INT64, {num_entries, rank - 1});\n      Tensor output_values = Tensor(DataTypeToEnum<T>::value, {num_entries});\n\n      auto output_indices_t = output_indices.matrix<int64>();\n      auto output_values_t = output_values.vec<T>();\n\n      for (int i = 0; i < num_entries; ++i) {\n        for (int d = 1; d < rank; ++d) {\n          output_indices_t(i, d - 1) = indices(i, d);\n        }\n        output_values_t(i) = values(i);\n      }\n\n      SparseTensor st_i;\n      OP_REQUIRES_OK(context,\n                     SparseTensor::Create(output_indices, output_values,\n                                          output_shape, &st_i));\n      int64 handle;\n      OP_REQUIRES_OK(context, map->AddSparseTensor(context, st_i, &handle));\n      sparse_handles_t(b) = handle;\n    }\n\n    // Fill in any gaps; we must provide an empty ST for batch entries\n    // the grouper didn't find.\n    if (visited.size() < N) {\n      Tensor empty_indices(DT_INT64, {0, rank - 1});\n      Tensor empty_values(DataTypeToEnum<T>::value, {0});\n      SparseTensor empty_st;\n      OP_REQUIRES_OK(context, SparseTensor::Create(empty_indices, empty_values,\n                                                   output_shape, &empty_st));\n\n      for (int64 b = 0; b < N; ++b) {\n        // We skipped this batch entry.\n        if (visited.find(b) == visited.end()) {\n          int64 handle;\n          OP_REQUIRES_OK(context,\n                         map->AddSparseTensor(context, empty_st, &handle));\n          sparse_handles_t(b) = handle;\n        }\n      }\n    }\n\n    context->set_output(0, sparse_handles);\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    // Read ragged_splits inputs.\n    OpInputList ragged_nested_splits_in;\n    OP_REQUIRES_OK(context, context->input_list(\"rt_nested_splits\",\n                                                &ragged_nested_splits_in));\n    const int ragged_nested_splits_len = ragged_nested_splits_in.size();\n    RaggedTensorVariant batched_ragged_input;\n    // Read ragged_values input.\n    batched_ragged_input.set_values(context->input(ragged_nested_splits_len));\n    batched_ragged_input.mutable_nested_splits()->reserve(\n        ragged_nested_splits_len);\n    for (int i = 0; i < ragged_nested_splits_len; i++) {\n      batched_ragged_input.append_splits(ragged_nested_splits_in[i]);\n    }\n\n    if (!batched_input_) {\n      // Encode as a Scalar Variant Tensor.\n      Tensor* encoded_scalar;\n      OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}),\n                                                       &encoded_scalar));\n      encoded_scalar->scalar<Variant>()() = std::move(batched_ragged_input);\n      return;\n    }\n\n    // Unbatch the Ragged Tensor and encode the components.\n    std::vector<RaggedTensorVariant> unbatched_ragged_input;\n    auto batched_splits_top_vec =\n        batched_ragged_input.splits(0).vec<SPLIT_TYPE>();\n    int num_components = batched_splits_top_vec.size() - 1;\n    OP_REQUIRES(context, num_components >= 0,\n                errors::Internal(\"Invalid split argument.\"));\n    OP_REQUIRES_OK(context, UnbatchRaggedZerothDim<VALUE_TYPE, SPLIT_TYPE>(\n                                batched_ragged_input, &unbatched_ragged_input));\n\n    // Bundle the encoded scalar Variant Tensors into a rank-1 Variant Tensor.\n    Tensor* encoded_vector;\n    int output_size = unbatched_ragged_input.size();\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({output_size}),\n                                            &encoded_vector));\n    auto encoded_vector_t = encoded_vector->vec<Variant>();\n    for (int i = 0; i < output_size; i++) {\n      encoded_vector_t(i) = unbatched_ragged_input[i];\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def error_page(self, **kwargs):\n        \"\"\"\n        Default error page shown to the user when an unexpected error occur.\n        \"\"\"\n        # Log exception.\n        logger.error(\n            'error page: %s %s\\n%s' % (kwargs.get('status', ''), kwargs.get('message', ''), kwargs.get('traceback', ''))\n        )\n\n        # Check expected response type.\n        mtype = cherrypy.tools.accept.callable(['text/html', 'text/plain'])  # @UndefinedVariable\n        if mtype == 'text/plain':\n            return kwargs.get('message')\n\n        # Try to build a nice error page.\n        try:\n            page = Controller()\n            return page._compile_template('error_page_default.html', **kwargs)\n        except Exception:\n            # If failing, send the raw error message.\n            return kwargs.get('message')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def explain_query_prefix(self, format=None, **options):\n        prefix = super().explain_query_prefix(format)\n        extra = {}\n        if format:\n            extra[\"FORMAT\"] = format\n        if options:\n            extra.update(\n                {\n                    name.upper(): \"true\" if value else \"false\"\n                    for name, value in options.items()\n                }\n            )\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_oauth2_authorisation_code(permission_url, redirect_uri, redirect_listen_address, username, connection_info):\n        \"\"\"Submit an authorisation request to the parent app and block until it is provided (or the request fails)\"\"\"\n        token_request = {'connection': connection_info, 'permission_url': permission_url,\n                         'redirect_uri': redirect_uri, 'redirect_listen_address': redirect_listen_address,\n                         'username': username, 'expired': False}\n        REQUEST_QUEUE.put(token_request)\n        wait_time = 0\n        while True:\n            try:\n                data = RESPONSE_QUEUE.get(block=True, timeout=1)\n            except queue.Empty:\n                wait_time += 1\n                if wait_time < AUTHENTICATION_TIMEOUT:\n                    continue\n                else:\n                    token_request['expired'] = True\n                    if 'local_server_auth_wsgi' in token_request:\n                        token_request['local_server_auth_wsgi'].server_close()\n                    REQUEST_QUEUE.put(token_request)  # re-insert the request as expired so the parent app can remove it\n                    return False, None\n\n            if data is QUEUE_SENTINEL:  # app is closing\n                RESPONSE_QUEUE.put(QUEUE_SENTINEL)  # make sure all watchers exit\n                return False, None\n\n            elif data['connection'] == connection_info:  # found an authentication response meant for us\n                # to improve no-GUI mode we also support the use of a local server to receive the OAuth redirection\n                # (note: not enabled by default because no-GUI mode is typically unattended, but useful in some cases)\n                if 'local_server_auth' in data:\n                    threading.Thread(target=OAuth2Helper.start_redirection_receiver_server, args=(data,),\n                                     name='EmailOAuth2Proxy-auth-%s' % data['username'], daemon=True).start()\n\n                else:\n                    if 'response_url' in data and 'code=' in data['response_url']:\n                        authorisation_code = OAuth2Helper.oauth2_url_unescape(\n                            data['response_url'].split('code=')[1].split('&')[0])\n                        if authorisation_code:\n                            return True, authorisation_code\n                    return False, None\n\n            else:  # not for this thread - put back into queue\n                RESPONSE_QUEUE.put(data)\n                time.sleep(1)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus GatherNd(const TfLiteTensor* params, const TfLiteTensor* indices,\n                      TfLiteTensor* output) {\n  reference_ops::GatherNd(\n      GetTensorShape(params), GetTensorData<ParamsT>(params),\n      GetTensorShape(indices), GetTensorData<IndicesT>(indices),\n      GetTensorShape(output), GetTensorData<ParamsT>(output));\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def urlize_quoted_links(text, trim_url_limit=None, nofollow=True, autoescape=True):\n    \"\"\"\n    Converts any URLs in text into clickable links.\n\n    Works on http://, https://, www. links, and also on links ending in one of\n    the original seven gTLDs (.com, .edu, .gov, .int, .mil, .net, and .org).\n    Links can have trailing punctuation (periods, commas, close-parens) and\n    leading punctuation (opening parens) and it'll still do the right thing.\n\n    If trim_url_limit is not None, the URLs in link text longer than this limit\n    will truncated to trim_url_limit-3 characters and appended with an ellipsis.\n\n    If nofollow is True, the URLs in link text will get a rel=\"nofollow\"\n    attribute.\n\n    If autoescape is True, the link text and URLs will get autoescaped.\n    \"\"\"\n    def trim_url(x, limit=trim_url_limit):\n        return limit is not None and (len(x) > limit and ('%s...' % x[:max(0, limit - 3)])) or x\n\n    safe_input = isinstance(text, SafeData)\n    words = word_split_re.split(force_text(text))\n    for i, word in enumerate(words):\n        if '.' in word or '@' in word or ':' in word:\n            # Deal with punctuation.\n            lead, middle, trail = '', word, ''\n            for punctuation in TRAILING_PUNCTUATION:\n                if middle.endswith(punctuation):\n                    middle = middle[:-len(punctuation)]\n                    trail = punctuation + trail\n            for opening, closing in WRAPPING_PUNCTUATION:\n                if middle.startswith(opening):\n                    middle = middle[len(opening):]\n                    lead = lead + opening\n                # Keep parentheses at the end only if they're balanced.\n                if (\n                    middle.endswith(closing) and\n                    middle.count(closing) == middle.count(opening) + 1\n                ):\n                    middle = middle[:-len(closing)]\n                    trail = closing + trail\n\n            # Make URL we want to point to.\n            url = None\n            nofollow_attr = ' rel=\"nofollow\"' if nofollow else ''\n            if simple_url_re.match(middle):\n                url = smart_urlquote_wrapper(middle)\n            elif simple_url_2_re.match(middle):\n                url = smart_urlquote_wrapper('http://%s' % middle)\n            elif ':' not in middle and simple_email_re.match(middle):\n                local, domain = middle.rsplit('@', 1)\n                try:\n                    domain = domain.encode('idna').decode('ascii')\n                except UnicodeError:\n                    continue\n                url = 'mailto:%s@%s' % (local, domain)\n                nofollow_attr = ''\n\n            # Make link.\n            if url:\n                trimmed = trim_url(middle)\n                if autoescape and not safe_input:\n                    lead, trail = escape(lead), escape(trail)\n                    url, trimmed = escape(url), escape(trimmed)\n                middle = '<a href=\"%s\"%s>%s</a>' % (url, nofollow_attr, trimmed)\n                words[i] = mark_safe('%s%s%s' % (lead, middle, trail))\n            else:\n                if safe_input:\n                    words[i] = mark_safe(word)\n                elif autoescape:\n                    words[i] = escape(word)\n        elif safe_input:\n            words[i] = mark_safe(word)\n        elif autoescape:\n            words[i] = escape(word)\n    return ''.join(words)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  function uploadFiles (form, fileInput, name) {\n    var url = fileInput.getAttribute('data-url')\n    fileInput.loaded = 0\n    fileInput.total = 0\n    var promises = Array.from(fileInput.files).map(function (file) {\n      form.total += file.size\n      fileInput.total += file.size\n      var s3Form = new window.FormData()\n      Array.from(fileInput.attributes).forEach(function (attr) {\n        var name = attr.name\n\n        if (name.startsWith('data-fields')) {\n          name = name.replace('data-fields-', '')\n          s3Form.append(name, attr.value)\n        }\n      })\n      s3Form.append('success_action_status', '201')\n      s3Form.append('Content-Type', file.type)\n      s3Form.append('file', file)\n      return request('POST', url, s3Form, fileInput, file, form)\n    })\n    Promise.all(promises).then(function (results) {\n      results.forEach(function (result) {\n        var hiddenFileInput = document.createElement('input')\n        hiddenFileInput.type = 'hidden'\n        hiddenFileInput.name = name\n        hiddenFileInput.value = parseURL(result)\n        form.appendChild(hiddenFileInput)\n      })\n      fileInput.name = ''\n      window.uploading -= 1\n    }, function (err) {\n      console.log(err)\n      fileInput.setCustomValidity(err)\n      fileInput.reportValidity()\n    })\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def make_byte_array_copier(dst, src):\n    assert isinstance(src.typ, _BytestringT)\n    assert isinstance(dst.typ, _BytestringT)\n\n    _check_assign_bytes(dst, src)\n\n    # TODO: remove this branch, copy_bytes and get_bytearray_length should handle\n    if src.value == \"~empty\":\n        # set length word to 0.\n        return STORE(dst, 0)\n\n    with src.cache_when_complex(\"src\") as (b1, src):\n        with get_bytearray_length(src).cache_when_complex(\"len\") as (b2, len_):\n            max_bytes = src.typ.maxlen\n\n            ret = [\"seq\"]\n            # store length\n            ret.append(STORE(dst, len_))\n\n            dst = bytes_data_ptr(dst)\n            src = bytes_data_ptr(src)\n\n            ret.append(copy_bytes(dst, src, len_, max_bytes))\n            return b1.resolve(b2.resolve(ret))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _reverse_with_prefix(self, lookup_view, _prefix, *args, **kwargs):\n        if args and kwargs:\n            raise ValueError(\"Don't mix *args and **kwargs in call to reverse()!\")\n        text_args = [force_text(v) for v in args]\n        text_kwargs = dict((k, force_text(v)) for (k, v) in kwargs.items())\n\n        if not self._populated:\n            self._populate()\n\n        try:\n            if lookup_view in self._callback_strs:\n                lookup_view = get_callable(lookup_view, True)\n        except (ImportError, AttributeError) as e:\n            raise NoReverseMatch(\"Error importing '%s': %s.\" % (lookup_view, e))\n        possibilities = self.reverse_dict.getlist(lookup_view)\n\n        prefix_norm, prefix_args = normalize(urlquote(_prefix))[0]\n        for possibility, pattern, defaults in possibilities:\n            for result, params in possibility:\n                if args:\n                    if len(args) != len(params) + len(prefix_args):\n                        continue\n                    candidate_subs = dict(zip(prefix_args + params, text_args))\n                else:\n                    if set(kwargs.keys()) | set(defaults.keys()) != set(params) | set(defaults.keys()) | set(prefix_args):\n                        continue\n                    matches = True\n                    for k, v in defaults.items():\n                        if kwargs.get(k, v) != v:\n                            matches = False\n                            break\n                    if not matches:\n                        continue\n                    candidate_subs = text_kwargs\n                # WSGI provides decoded URLs, without %xx escapes, and the URL\n                # resolver operates on such URLs. First substitute arguments\n                # without quoting to build a decoded URL and look for a match.\n                # Then, if we have a match, redo the substitution with quoted\n                # arguments in order to return a properly encoded URL.\n                candidate_pat = prefix_norm.replace('%', '%%') + result\n                if re.search('^%s%s' % (prefix_norm, pattern), candidate_pat % candidate_subs, re.UNICODE):\n                    candidate_subs = dict((k, urlquote(v)) for (k, v) in candidate_subs.items())\n                    return candidate_pat % candidate_subs\n        # lookup_view can be URL label, or dotted path, or callable, Any of\n        # these can be passed in at the top, but callables are not friendly in\n        # error messages.\n        m = getattr(lookup_view, '__module__', None)\n        n = getattr(lookup_view, '__name__', None)\n        if m is not None and n is not None:\n            lookup_view_s = \"%s.%s\" % (m, n)\n        else:\n            lookup_view_s = lookup_view\n\n        patterns = [pattern for (possibility, pattern, defaults) in possibilities]\n        raise NoReverseMatch(\"Reverse for '%s' with arguments '%s' and keyword \"\n                \"arguments '%s' not found. %d pattern(s) tried: %s\" %\n                             (lookup_view_s, args, kwargs, len(patterns), patterns))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    OP_REQUIRES_OK_ASYNC(c,\n                         FillCollectiveParams(col_params, REDUCTION_COLLECTIVE,\n                                              /*group_size*/ c->input(1),\n                                              /*group_key*/ c->input(2),\n                                              /*instance_key*/ c->input(3)),\n                         done);\n    col_params->instance.shape = c->input(0).shape();\n    col_params->merge_op = merge_op_.get();\n    col_params->final_op = final_op_.get();\n    VLOG(1) << \"CollectiveReduceV2 group_size \" << col_params->group.group_size\n            << \" group_key \" << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    // Allocate the output tensor, trying to reuse the input.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(c,\n                         c->forward_input_or_allocate_output(\n                             {0}, 0, col_params->instance.shape, &output),\n                         done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def idp_initiated_logout(self):\n        \"\"\"\n        Logout all SP sessions when the logout comes from the IdP.\n\n        For the current user only.\n\n        Only use HTTP-Redirect to start the logout. This is guaranteed\n        to be supported in SAML 2.\n        \"\"\"\n        self.debug(\"IdP-initiated SAML2 logout\")\n        us = UserSession()\n        user = us.get_user()\n\n        saml_sessions = self.sessionfactory\n        # pylint: disable=unused-variable\n        (mech, session) = saml_sessions.get_next_logout(\n            logout_mechs=[lasso.SAML2_METADATA_BINDING_REDIRECT])\n        if session is None:\n            return\n\n        logout = self.idp.get_logout_handler()\n        logout.setSessionFromDump(session.login_session)\n        try:\n            logout.initRequest(session.provider_id)\n        except lasso.ServerProviderNotFoundError:\n            self.error(\n                'Service Provider %s not found. Trying next session' %\n                session.provider_id\n            )\n            return self.idp_initiated_logout()\n        except lasso.ProfileUnsupportedProfileError:\n            self.error(\n                'Service Provider %s does not support Single Logout. '\n                'Trying next session' %\n                session.provider_id\n            )\n            return self.idp_initiated_logout()\n        try:\n            logout.buildRequestMsg()\n        except lasso.Error, e:\n            self.error('failure to build logout request msg: %s' % e)\n            raise cherrypy.HTTPRedirect(400, 'Failed to log out user: %s '\n                                        % e)\n\n        # Add a fake session to indicate where the user should\n        # be redirected to when all SP's are logged out.\n        idpurl = self._root.instance_base_url()\n        session_id = \"_\" + uuid.uuid4().hex.upper()\n        saml_sessions.add_session(session_id, idpurl, user.name, \"\", \"\",\n                                  [lasso.SAML2_METADATA_BINDING_REDIRECT])\n        init_session = saml_sessions.get_session_by_id(session_id)\n        saml_sessions.start_logout(init_session, relaystate=idpurl)\n\n        # Add the logout request id we just created to the session to be\n        # logged out so that when it responds we can find the right\n        # session.\n        session.set_logoutstate(request_id=logout.request.id)\n        saml_sessions.start_logout(session, initial=False)\n\n        self.debug('Sending initial logout request to %s' % logout.msgUrl)\n        raise cherrypy.HTTPRedirect(logout.msgUrl)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def response_for_exception(request, exc):\n    if isinstance(exc, Http404):\n        if settings.DEBUG:\n            response = debug.technical_404_response(request, exc)\n        else:\n            response = get_exception_response(request, get_resolver(get_urlconf()), 404, exc)\n\n    elif isinstance(exc, PermissionDenied):\n        response = get_exception_response(request, get_resolver(get_urlconf()), 403, exc)\n        log_response(\n            'Forbidden (Permission denied): %s', request.path,\n            response=response,\n            request=request,\n            exc_info=sys.exc_info(),\n        )\n\n    elif isinstance(exc, MultiPartParserError):\n        response = get_exception_response(request, get_resolver(get_urlconf()), 400, exc)\n        log_response(\n            'Bad request (Unable to parse request body): %s', request.path,\n            response=response,\n            request=request,\n            exc_info=sys.exc_info(),\n        )\n\n    elif isinstance(exc, BadRequest):\n        if settings.DEBUG:\n            response = debug.technical_500_response(request, *sys.exc_info(), status_code=400)\n        else:\n            response = get_exception_response(request, get_resolver(get_urlconf()), 400, exc)\n        log_response(\n            '%s: %s', str(exc), request.path,\n            response=response,\n            request=request,\n            exc_info=sys.exc_info(),\n        )\n    elif isinstance(exc, SuspiciousOperation):\n        if isinstance(exc, (RequestDataTooBig, TooManyFieldsSent)):\n            # POST data can't be accessed again, otherwise the original\n            # exception would be raised.\n            request._mark_post_parse_error()\n\n        # The request logger receives events for any problematic request\n        # The security logger receives events for all SuspiciousOperations\n        security_logger = logging.getLogger('django.security.%s' % exc.__class__.__name__)\n        security_logger.error(\n            str(exc),\n            extra={'status_code': 400, 'request': request},\n        )\n        if settings.DEBUG:\n            response = debug.technical_500_response(request, *sys.exc_info(), status_code=400)\n        else:\n            response = get_exception_response(request, get_resolver(get_urlconf()), 400, exc)\n\n    elif isinstance(exc, SystemExit):\n        # Allow sys.exit() to actually exit. See tickets #1023 and #4701\n        raise\n\n    else:\n        signals.got_request_exception.send(sender=None, request=request)\n        response = handle_uncaught_exception(request, get_resolver(get_urlconf()), sys.exc_info())\n        log_response(\n            '%s: %s', response.reason_phrase, request.path,\n            response=response,\n            request=request,\n            exc_info=sys.exc_info(),\n        )\n\n    # Force a TemplateResponse to be rendered.\n    if not getattr(response, 'is_rendered', True) and callable(getattr(response, 'render', None)):\n        response = response.render()\n\n    return response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void DecodeGifV2(OpKernelContext* context, StringPiece input) {\n    // GIF has 3 channels.\n    OP_REQUIRES(context, channels_ == 0 || channels_ == 3,\n                errors::InvalidArgument(\"channels must be 0 or 3 for GIF, got \",\n                                        channels_));\n\n    if (op_type_ == \"DecodeBmp\") {\n      // TODO(b/171060723): Only DecodeBmp as op_type_ is not acceptable here\n      // because currently `decode_(jpeg|png|gif)` ops can decode any one of\n      // jpeg, png or gif but not bmp. Similarly, `decode_bmp` cannot decode\n      // anything but bmp formats. This behavior needs to be revisited. For more\n      // details, please refer to the bug.\n      OP_REQUIRES(context, false,\n                  errors::InvalidArgument(\n                      \"Trying to decode GIF format using DecodeBmp op. Use \"\n                      \"`decode_gif` or `decode_image` instead.\"));\n    } else if (op_type_ == \"DecodeAndCropJpeg\") {\n      OP_REQUIRES(context, false,\n                  errors::InvalidArgument(\n                      \"DecodeAndCropJpeg operation can run on JPEG only, but \"\n                      \"detected GIF.\"));\n    }\n\n    // Decode GIF, allocating tensor if dtype is uint8, otherwise defer tensor\n    // allocation til after dtype conversion is done. `gif`::Decode` supports\n    // uint8 only.\n    Tensor* output = nullptr;\n    int buffer_size = 0;\n    string error_string;\n    uint8* buffer = gif::Decode(\n        input.data(), input.size(),\n        [&](int num_frames, int width, int height, int channels) -> uint8* {\n          buffer_size = num_frames * height * width * channels;\n\n          Status status;\n          // By the existing API, we support decoding GIF with `decode_jpeg` or\n          // with `decode_png` if the GIF is a single-frame GIF (non-animated).\n          // We need to make sure to return 3-D shapes when using in this case.\n          if (op_type_ == \"DecodePng\" || op_type_ == \"DecodeJpeg\") {\n            if (num_frames == 1) {\n              status = context->allocate_output(\n                  0, TensorShape({height, width, channels}), &output);\n            } else {\n              status = errors::InvalidArgument(\n                  \"Got \", num_frames, \" frames, but animated gifs \",\n                  \"can only be decoded by tf.io.decode_gif or \",\n                  \"tf.io.decode_image\");\n            }\n          } else if (op_type_ == \"DecodeGif\" ||\n                     (op_type_ == \"DecodeImage\" && expand_animations_)) {\n            status = context->allocate_output(\n                0, TensorShape({num_frames, height, width, channels}), &output);\n          } else if (op_type_ == \"DecodeImage\" && !expand_animations_) {\n            status = context->allocate_output(\n                0, TensorShape({height, width, channels}), &output);\n          } else {\n            status = errors::InvalidArgument(\"Bad op type \", op_type_);\n          }\n          if (!status.ok()) {\n            VLOG(1) << status;\n            context->SetStatus(status);\n            return nullptr;\n          }\n\n          if (data_type_ == DataType::DT_UINT8) {\n            return output->flat<uint8>().data();\n          } else {\n            return new uint8[buffer_size];\n          }\n        },\n        &error_string, expand_animations_);\n\n    OP_REQUIRES(context, buffer,\n                errors::InvalidArgument(\"Invalid GIF data (size \", input.size(),\n                                        \"), \", error_string));\n\n    // For when desired data type is uint8, the output buffer is already\n    // allocated during the `gif::Decode` call above; return.\n    if (data_type_ == DataType::DT_UINT8) {\n      return;\n    }\n    // Make sure we don't forget to deallocate `buffer`.\n    std::unique_ptr<uint8[]> buffer_unique_ptr(buffer);\n\n    // Convert the raw uint8 buffer to desired dtype.\n    // Use eigen threadpooling to speed up the copy operation.\n    TTypes<uint8>::UnalignedConstFlat buffer_view(buffer, buffer_size);\n    const auto& device = context->eigen_device<Eigen::ThreadPoolDevice>();\n    if (data_type_ == DataType::DT_UINT16) {\n      uint16 scale = floor((std::numeric_limits<uint16>::max() + 1) /\n                           (std::numeric_limits<uint8>::max() + 1));\n      // Fill output tensor with desired dtype.\n      output->flat<uint16>().device(device) =\n          buffer_view.cast<uint16>() * scale;\n    } else if (data_type_ == DataType::DT_FLOAT) {\n      float scale = 1. / std::numeric_limits<uint8>::max();\n      // Fill output tensor with desired dtype.\n      output->flat<float>().device(device) = buffer_view.cast<float>() * scale;\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _changeform_view(self, request, object_id, form_url, extra_context):\n        to_field = request.POST.get(TO_FIELD_VAR, request.GET.get(TO_FIELD_VAR))\n        if to_field and not self.to_field_allowed(request, to_field):\n            raise DisallowedModelAdminToField(\"The field %s cannot be referenced.\" % to_field)\n\n        model = self.model\n        opts = model._meta\n\n        if request.method == 'POST' and '_saveasnew' in request.POST:\n            object_id = None\n\n        add = object_id is None\n\n        if add:\n            if not self.has_add_permission(request):\n                raise PermissionDenied\n            obj = None\n\n        else:\n            obj = self.get_object(request, unquote(object_id), to_field)\n\n            if not self.has_view_or_change_permission(request, obj):\n                raise PermissionDenied\n\n            if obj is None:\n                return self._get_obj_does_not_exist_redirect(request, opts, object_id)\n\n        ModelForm = self.get_form(request, obj, change=not add)\n        if request.method == 'POST':\n            form = ModelForm(request.POST, request.FILES, instance=obj)\n            form_validated = form.is_valid()\n            if form_validated:\n                new_object = self.save_form(request, form, change=not add)\n            else:\n                new_object = form.instance\n            formsets, inline_instances = self._create_formsets(request, new_object, change=not add)\n            if all_valid(formsets) and form_validated:\n                self.save_model(request, new_object, form, not add)\n                self.save_related(request, form, formsets, not add)\n                change_message = self.construct_change_message(request, form, formsets, add)\n                if add:\n                    self.log_addition(request, new_object, change_message)\n                    return self.response_add(request, new_object)\n                else:\n                    self.log_change(request, new_object, change_message)\n                    return self.response_change(request, new_object)\n            else:\n                form_validated = False\n        else:\n            if add:\n                initial = self.get_changeform_initial_data(request)\n                form = ModelForm(initial=initial)\n                formsets, inline_instances = self._create_formsets(request, form.instance, change=False)\n            else:\n                form = ModelForm(instance=obj)\n                formsets, inline_instances = self._create_formsets(request, obj, change=True)\n\n        if not add and not self.has_change_permission(request, obj):\n            readonly_fields = flatten_fieldsets(self.get_fieldsets(request, obj))\n        else:\n            readonly_fields = self.get_readonly_fields(request, obj)\n        adminForm = helpers.AdminForm(\n            form,\n            list(self.get_fieldsets(request, obj)),\n            # Clear prepopulated fields on a view-only form to avoid a crash.\n            self.get_prepopulated_fields(request, obj) if add or self.has_change_permission(request, obj) else {},\n            readonly_fields,\n            model_admin=self)\n        media = self.media + adminForm.media\n\n        inline_formsets = self.get_inline_formsets(request, formsets, inline_instances, obj)\n        for inline_formset in inline_formsets:\n            media = media + inline_formset.media\n\n        if add:\n            title = _('Add %s')\n        elif self.has_change_permission(request, obj):\n            title = _('Change %s')\n        else:\n            title = _('View %s')\n        context = {\n            **self.admin_site.each_context(request),\n            'title': title % opts.verbose_name,\n            'adminform': adminForm,\n            'object_id': object_id,\n            'original': obj,\n            'is_popup': IS_POPUP_VAR in request.POST or IS_POPUP_VAR in request.GET,\n            'to_field': to_field,\n            'media': media,\n            'inline_admin_formsets': inline_formsets,\n            'errors': helpers.AdminErrorList(form, formsets),\n            'preserved_filters': self.get_preserved_filters(request),\n        }\n\n        # Hide the \"Save\" and \"Save and continue\" buttons if \"Save as New\" was\n        # previously chosen to prevent the interface from getting confusing.\n        if request.method == 'POST' and not form_validated and \"_saveasnew\" in request.POST:\n            context['show_save'] = False\n            context['show_save_and_continue'] = False\n            # Use the change template instead of the add template.\n            add = False\n\n        context.update(extra_context or {})\n\n        return self.render_change_form(request, context, add=add, change=not add, obj=obj, form_url=form_url)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def load_tasks(request, project):\n    \"\"\" Load tasks from different types of request.data / request.files\n    \"\"\"\n    file_upload_ids, found_formats, data_keys = [], [], set()\n    could_be_tasks_lists = False\n\n    # take tasks from request FILES\n    if len(request.FILES):\n        check_file_sizes_and_number(request.FILES)\n        for filename, file in request.FILES.items():\n            file_upload = create_file_upload(request, project, file)\n            if file_upload.format_could_be_tasks_list:\n                could_be_tasks_lists = True\n            file_upload_ids.append(file_upload.id)\n        tasks, found_formats, data_keys = FileUpload.load_tasks_from_uploaded_files(project, file_upload_ids)\n\n    # take tasks from url address\n    elif 'application/x-www-form-urlencoded' in request.content_type:\n        # empty url\n        url = request.data.get('url')\n        if not url:\n            raise ValidationError('\"url\" is not found in request data')\n\n        # try to load json with task or tasks from url as string\n        json_data = str_to_json(url)\n        if json_data:\n            file_upload = create_file_upload(request, project, SimpleUploadedFile('inplace.json', url.encode()))\n            file_upload_ids.append(file_upload.id)\n            tasks, found_formats, data_keys = FileUpload.load_tasks_from_uploaded_files(project, file_upload_ids)\n            \n        # download file using url and read tasks from it\n        else:\n            if settings.SSRF_PROTECTION_ENABLED and url_is_local(url):\n                raise ImportFromLocalIPError\n            data_keys, found_formats, tasks, file_upload_ids = tasks_from_url(\n                file_upload_ids, project, request, url\n            )\n\n    # take one task from request DATA\n    elif 'application/json' in request.content_type and isinstance(request.data, dict):\n        tasks = [request.data]\n\n    # take many tasks from request DATA\n    elif 'application/json' in request.content_type and isinstance(request.data, list):\n        tasks = request.data\n\n    # incorrect data source\n    else:\n        raise ValidationError('load_tasks: No data found in DATA or in FILES')\n\n    # check is data root is list\n    if not isinstance(tasks, list):\n        raise ValidationError('load_tasks: Data root must be list')\n\n    # empty tasks error\n    if not tasks:\n        raise ValidationError('load_tasks: No tasks added')\n\n    check_max_task_number(tasks)\n    return tasks, file_upload_ids, could_be_tasks_lists, found_formats, list(data_keys)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, *, openapi: GeneratorData) -> None:\n        self.openapi: GeneratorData = openapi\n        self.env: Environment = Environment(loader=PackageLoader(__package__), trim_blocks=True, lstrip_blocks=True)\n\n        self.project_name: str = self.project_name_override or f\"{utils.kebab_case(openapi.title).lower()}-client\"\n        self.project_dir: Path = Path.cwd() / self.project_name\n\n        self.package_name: str = self.package_name_override or self.project_name.replace(\"-\", \"_\")\n        self.package_dir: Path = self.project_dir / self.package_name\n        self.package_description: str = f\"A client library for accessing {self.openapi.title}\"\n        self.version: str = openapi.version\n\n        self.env.filters.update(self.TEMPLATE_FILTERS)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    run_id = request_dict.get(\"run_id\") or request_dict.get(\"run_uuid\")\n    run = _get_tracking_store().get_run(run_id)\n\n    if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=run.info.artifact_uri,\n            relative_path=request_dict[\"path\"],\n        )\n    else:\n        artifact_repo = _get_artifact_repo(run)\n        artifact_path = request_dict[\"path\"]\n\n    return _send_artifact(artifact_repo, artifact_path)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _list_artifacts():\n    request_message = _get_request_message(\n        ListArtifacts(),\n        schema={\n            \"run_id\": [_assert_string, _assert_required],\n            \"path\": [_assert_string],\n            \"page_token\": [_assert_string],\n        },\n    )\n    response_message = ListArtifacts.Response()\n    if request_message.HasField(\"path\"):\n        path = request_message.path\n    else:\n        path = None\n    run_id = request_message.run_id or request_message.run_uuid\n    run = _get_tracking_store().get_run(run_id)\n\n    if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n        artifact_entities = _list_artifacts_for_proxied_run_artifact_root(\n            proxied_artifact_root=run.info.artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_entities = _get_artifact_repo(run).list_artifacts(path)\n\n    response_message.files.extend([a.to_proto() for a in artifact_entities])\n    response_message.root_uri = run.info.artifact_uri\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def config():\n    config_disabled = (\n            app.config['CONFIG_DISABLE'] or\n            not valid_user_session(session))\n    if request.method == 'GET':\n        return json.dumps(g.user_config.__dict__)\n    elif request.method == 'PUT' and not config_disabled:\n        if 'name' in request.args:\n            config_pkl = os.path.join(\n                app.config['CONFIG_PATH'],\n                request.args.get('name'))\n            session['config'] = (pickle.load(open(config_pkl, 'rb'))\n                                 if os.path.exists(config_pkl)\n                                 else session['config'])\n            return json.dumps(session['config'])\n        else:\n            return json.dumps({})\n    elif not config_disabled:\n        config_data = request.form.to_dict()\n        if 'url' not in config_data or not config_data['url']:\n            config_data['url'] = g.user_config.url\n\n        # Save config by name to allow a user to easily load later\n        if 'name' in request.args:\n            pickle.dump(\n                config_data,\n                open(os.path.join(\n                    app.config['CONFIG_PATH'],\n                    request.args.get('name')), 'wb'))\n\n        session['config'] = config_data\n        return redirect(config_data['url'])\n    else:\n        return redirect(url_for('.index'), code=403)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def set_storage_slots(vyper_module: vy_ast.Module) -> StorageLayout:\n    \"\"\"\n    Parse module-level Vyper AST to calculate the layout of storage variables.\n    Returns the layout as a dict of variable name -> variable info\n    \"\"\"\n    # Allocate storage slots from 0\n    # note storage is word-addressable, not byte-addressable\n    storage_slot = 0\n\n    ret: Dict[str, Dict] = {}\n\n    for node in vyper_module.get_children(vy_ast.FunctionDef):\n        type_ = node._metadata[\"type\"]\n        if type_.nonreentrant is None:\n            continue\n\n        variable_name = f\"nonreentrant.{type_.nonreentrant}\"\n\n        # a nonreentrant key can appear many times in a module but it\n        # only takes one slot. after the first time we see it, do not\n        # increment the storage slot.\n        if variable_name in ret:\n            _slot = ret[variable_name][\"slot\"]\n            type_.set_reentrancy_key_position(StorageSlot(_slot))\n            continue\n\n        type_.set_reentrancy_key_position(StorageSlot(storage_slot))\n\n        # TODO this could have better typing but leave it untyped until\n        # we nail down the format better\n        ret[variable_name] = {\"type\": \"nonreentrant lock\", \"slot\": storage_slot}\n\n        # TODO use one byte - or bit - per reentrancy key\n        # requires either an extra SLOAD or caching the value of the\n        # location in memory at entrance\n        storage_slot += 1\n\n    for node in vyper_module.get_children(vy_ast.VariableDecl):\n        # skip non-storage variables\n        if node.is_constant or node.is_immutable:\n            continue\n\n        varinfo = node.target._metadata[\"varinfo\"]\n        varinfo.set_position(StorageSlot(storage_slot))\n\n        type_ = varinfo.typ\n\n        # this could have better typing but leave it untyped until\n        # we understand the use case better\n        ret[node.target.id] = {\"type\": str(type_), \"slot\": storage_slot}\n\n        # CMC 2021-07-23 note that HashMaps get assigned a slot here.\n        # I'm not sure if it's safe to avoid allocating that slot\n        # for HashMaps because downstream code might use the slot\n        # ID as a salt.\n        storage_slot += math.ceil(type_.size_in_bytes / 32)\n\n    return ret\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_mediastatic_content(url):\n    if url.startswith(settings.STATIC_URL):\n        local_path = settings.STATIC_ROOT / url[len(settings.STATIC_URL) :]\n    elif url.startswith(settings.MEDIA_URL):\n        local_path = settings.MEDIA_ROOT / url[len(settings.MEDIA_URL) :]\n    else:\n        raise FileNotFoundError()\n\n    with open(local_path, \"rb\") as f:\n        return f.read()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    const findHeadingElement = (cell: Cell): void => {\n      model.getCellHeadings(cell).forEach(async heading => {\n        const elementId = await getIdForHeading(heading, this.parser!);\n\n        const selector = elementId\n          ? `h${heading.level}[id=\"${elementId}\"]`\n          : `h${heading.level}`;\n\n        if (heading.outputIndex !== undefined) {\n          // Code cell\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              (heading.cellRef as CodeCell).outputArea.widgets[\n                heading.outputIndex\n              ].node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        } else {\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              heading.cellRef.node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        }\n      });\n    };\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(\n        self,\n        reactor: IReactorCore,\n        tls_client_options_factory: Optional[FederationPolicyForHTTPS],\n        user_agent: bytes,\n        _srv_resolver: Optional[SrvResolver] = None,\n        _well_known_resolver: Optional[WellKnownResolver] = None,\n    ):\n        self._reactor = reactor\n        self._clock = Clock(reactor)\n        self._pool = HTTPConnectionPool(reactor)\n        self._pool.retryAutomatically = False\n        self._pool.maxPersistentPerHost = 5\n        self._pool.cachedConnectionTimeout = 2 * 60\n\n        self._agent = Agent.usingEndpointFactory(\n            self._reactor,\n            MatrixHostnameEndpointFactory(\n                reactor, tls_client_options_factory, _srv_resolver\n            ),\n            pool=self._pool,\n        )\n        self.user_agent = user_agent\n\n        if _well_known_resolver is None:\n            _well_known_resolver = WellKnownResolver(\n                self._reactor,\n                agent=Agent(\n                    self._reactor,\n                    pool=self._pool,\n                    contextFactory=tls_client_options_factory,\n                ),\n                user_agent=self.user_agent,\n            )\n\n        self._well_known_resolver = _well_known_resolver\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    const Tensor& tag = c->input(0);\n    const Tensor& tensor = c->input(1);\n    OP_REQUIRES(c, TensorShapeUtils::IsScalar(tag.shape()),\n                errors::InvalidArgument(\"Tag must be a scalar\"));\n    OP_REQUIRES(c, tensor.dims() >= 2 && tensor.dims() <= 3,\n                errors::InvalidArgument(\"Tensor must be 3-D or 2-D, got: \",\n                                        tensor.shape().DebugString()));\n    const string& base_tag = tag.scalar<tstring>()();\n\n    float sample_rate = sample_rate_attr_;\n    if (!has_sample_rate_attr_) {\n      const Tensor& sample_rate_tensor = c->input(2);\n      sample_rate = sample_rate_tensor.scalar<float>()();\n    }\n    OP_REQUIRES(c, sample_rate > 0.0f,\n                errors::InvalidArgument(\"sample_rate must be > 0\"));\n\n    const int batch_size = tensor.dim_size(0);\n    const int64_t length_frames = tensor.dim_size(1);\n    const int64_t num_channels =\n        tensor.dims() == 2 ? 1 : tensor.dim_size(tensor.dims() - 1);\n\n    Summary s;\n    const int N = std::min<int>(max_outputs_, batch_size);\n    for (int i = 0; i < N; ++i) {\n      Summary::Value* v = s.add_value();\n      if (max_outputs_ > 1) {\n        v->set_tag(strings::StrCat(base_tag, \"/audio/\", i));\n      } else {\n        v->set_tag(strings::StrCat(base_tag, \"/audio\"));\n      }\n\n      Summary::Audio* sa = v->mutable_audio();\n      sa->set_sample_rate(sample_rate);\n      sa->set_num_channels(num_channels);\n      sa->set_length_frames(length_frames);\n      sa->set_content_type(\"audio/wav\");\n\n      auto values =\n          tensor.shaped<float, 3>({batch_size, length_frames, num_channels});\n      const float* data =\n          tensor.NumElements() == 0 ? nullptr : &values(i, 0, 0);\n\n      size_t sample_rate_truncated = lrintf(sample_rate);\n      if (sample_rate_truncated == 0) {\n        sample_rate_truncated = 1;\n      }\n      OP_REQUIRES_OK(c, wav::EncodeAudioAsS16LEWav(\n                            data, sample_rate_truncated, num_channels,\n                            length_frames, sa->mutable_encoded_audio_string()));\n    }\n\n    Tensor* summary_tensor = nullptr;\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape({}), &summary_tensor));\n    CHECK(SerializeToTString(s, &summary_tensor->scalar<tstring>()()));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        function set_up(container) {\n            // Find prev/next links\n            var prev = container.find('.pagination .prev');\n            var next = container.find('.pagination .next');\n\n            var prev_url = null, next_url = null;\n\n            if (prev.length)\n                prev_url = prev.attr('href');\n            if (next.length)\n                next_url = next.attr('href');\n\n            function ajax(url, handler) {\n                // URL should start with a slash, but cannot start with two slashes.\n                // (Otherwise we have an XSS vulnerability.)\n                if (url[0] != '/' || url[1] == '/')\n                    url = (''+location).replace( /[#\\?].*/, '') + url;\n\n                // Append 'xhr' to make sure all content is loaded.\n                if (url.indexOf('&xhr') < 0 && url.indexOf('?xhr') < 0)\n                    if (url.indexOf('?') < 0)\n                        url += '?xhr';\n                    else\n                        url += '&xhr';\n\n                $.ajax({\n                        type: 'GET',\n                        url: url,\n                        datatype: \"html\",\n                        cache: true,\n                        success: handler,\n                        error: function(xhr, ajaxOptions, thrownError) {\n                            handler($('<div/>').append(\n                                    $('<strong>').append(''+xhr.status + ' ' + thrownError)).html());\n                        }\n                });\n            }\n\n            function showLoader() {\n                var loader = $('.paginate-loading').eq(0);\n\n                for (var i in containers)\n                {\n                    // Keep containers height\n                    var height = containers[i].height();\n                    if (height < 50) height = 50;\n                    containers[i].css('height', height + 'px');\n\n                    // Show loader\n                    containers[i].empty();\n                    containers[i].append(loader.html());\n                }\n            }\n\n            // Replace page AJAX handler\n            function replacePage(url, receivedHtml) {\n\t\t\t\t// Set location hash\n\t\t\t\tif (setHash)\n\t\t\t\t\tlocation.hash = 'page:' + url;\n\n\n                for (var i in containers) {\n\t\t\t\t\t// Empty the paginate nodes\n                    containers[i].empty();\n                    containers[i].css('height', '');\n\n                    // Fill the paginate containers with the new content\n                    receivedHtml = $.trim(receivedHtml);\n                    $(receivedHtml).find('.paginate-container').eq(i).each(function() {\n                        containers[i].append($(this).html());\n                        set_up(containers[i]);\n                    });\n                }\n\n                // Trigger event (Only add trigger to $(document) )\n                $(document).trigger('paginatorPageReplaced', [ containers ]);\n\n                // When the URL contains a hash, scroll to that element\n                if (location.hash.substr(1).indexOf('#') > 0) {\n                    var hash = (''+location.hash).replace( /.*#/, '');\n                    scroll(hash);\n                }\n                // Otherwise, make sure that the top of the clicked paginator is visible\n                else {\n                    var paginatorTop = container.offset().top;\n\n                    if (paginatorTop < $(window).scrollTop() || paginatorTop > $(window).scrollTop() + $(window).height()) {\n                        $('html,body').animate({scrollTop: paginatorTop}, 350);\n                    }\n                }\n            }\n\n            // Process hash (link with # opened from bookmark)\n            if (processHash && location.hash.match( /^#page/)) {\n                var url = location.hash.replace( /^#page:/, '');\n                container.empty();\n                ajax(url, function(html) { replacePage(url,html); });\n            }\n            processHash = false; // Never process twice. (set_up is called recursively)\n\n            // Preload previous page\n            var previousPageHtml = null;\n            var nextPageHtml = null;\n            var clickedPrevious = false;\n            var clickedNext = false;\n\n            var preload = $('.pagination_preload').size() > 0;\n\n            if (preload) {\n\t\t\t\tif (prev_url)\n                    ajax(prev_url, function(html) {\n                        previousPageHtml = html;\n                        if (clickedPrevious) replacePage(prev_url, html);\n                    });\n                if (next_url)\n                    ajax(next_url, function(html) {\n                        nextPageHtml = html;\n                        if (clickedNext) replacePage(next_url, html);\n                    });\n\n                // Handle clicks on the prev/next buttons.\n                prev.click(function() {\n                    showLoader();\n                    if (previousPageHtml)\n                        replacePage(prev_url, previousPageHtml);\n                    else\n                        clickedPrevious = true;\n                    return false;\n                });\n\n                next.click(function() {\n                    showLoader();\n                    if (nextPageHtml)\n                        replacePage(next_url, nextPageHtml);\n                    else\n                        clickedNext = true;\n                    return false;\n                });\n            }\n\n            // The digg-paginator also contains direct links to other pages\n            // then only the prev and next page.\n            container.find('.pagination a, .pagination-helper a, a.pagination-helper').each(function() {\n\n                $(this).click(function (){\n                \tvar url = $(this).attr('href');\n                    showLoader();\n\n                    if (url[0] == '#') {\n                        var hash = url.replace( /#/, '');\n                        scroll(hash);\n                    }\n                    else\n                        ajax(url, function(html) {\n\t\t\t\t\t\t\treplacePage(url, html);\n                        });\n                    return false;\n                });\n            });\n\n            // Pagination forms are forms for which the result is displayed in the paginator\n            // They always have method=\"get\" and action=\"?\"\n            container.find('form.pagination-form').each(function() {\n                var form = $(this);\n                if (form.attr('method') == 'get' && form.attr('action').match(/.*?/)) {\n                    // Submit handler\n                    form.submit(function() {\n                        // Build submit query string\n                        var url = form.attr('action') + form.serialize();\n\n                        // AJAX request\n                        ajax(url, function(html) {\n                            replacePage(url, html);\n                        });\n                        return false;\n                    });\n                }\n            });\n        }\n    };"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "inline int ComputeOutSize(TfLitePadding padding, int image_size,\n                          int filter_size, int stride, int dilation_rate = 1) {\n  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;\n  switch (padding) {\n    case kTfLitePaddingSame:\n      return (image_size + stride - 1) / stride;\n    case kTfLitePaddingValid:\n      return (image_size + stride - effective_filter_size) / stride;\n    default:\n      return 0;\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def create_promptlab_run_handler():\n    def assert_arg_exists(arg_name, arg):\n        if not arg:\n            raise MlflowException(\n                message=f\"CreatePromptlabRun request must specify {arg_name}.\",\n                error_code=INVALID_PARAMETER_VALUE,\n            )\n\n    args = request.json\n    experiment_id = args.get(\"experiment_id\")\n    assert_arg_exists(\"experiment_id\", experiment_id)\n    run_name = args.get(\"run_name\", None)\n    tags = args.get(\"tags\", [])\n    prompt_template = args.get(\"prompt_template\")\n    assert_arg_exists(\"prompt_template\", prompt_template)\n    raw_prompt_parameters = args.get(\"prompt_parameters\")\n    assert_arg_exists(\"prompt_parameters\", raw_prompt_parameters)\n    prompt_parameters = [\n        Param(param.get(\"key\"), param.get(\"value\")) for param in args.get(\"prompt_parameters\")\n    ]\n    model_route = args.get(\"model_route\")\n    assert_arg_exists(\"model_route\", model_route)\n    raw_model_parameters = args.get(\"model_parameters\", [])\n    model_parameters = [\n        Param(param.get(\"key\"), param.get(\"value\")) for param in raw_model_parameters\n    ]\n    model_input = args.get(\"model_input\")\n    assert_arg_exists(\"model_input\", model_input)\n    model_output = args.get(\"model_output\", None)\n    raw_model_output_parameters = args.get(\"model_output_parameters\", [])\n    model_output_parameters = [\n        Param(param.get(\"key\"), param.get(\"value\")) for param in raw_model_output_parameters\n    ]\n    mlflow_version = args.get(\"mlflow_version\")\n    assert_arg_exists(\"mlflow_version\", mlflow_version)\n    user_id = args.get(\"user_id\", \"unknown\")\n\n    # use current time if not provided\n    start_time = args.get(\"start_time\", int(time.time() * 1000))\n\n    store = _get_tracking_store()\n\n    run = _create_promptlab_run_impl(\n        store,\n        experiment_id=experiment_id,\n        run_name=run_name,\n        tags=tags,\n        prompt_template=prompt_template,\n        prompt_parameters=prompt_parameters,\n        model_route=model_route,\n        model_parameters=model_parameters,\n        model_input=model_input,\n        model_output=model_output,\n        model_output_parameters=model_output_parameters,\n        mlflow_version=mlflow_version,\n        user_id=user_id,\n        start_time=start_time,\n    )\n    response_message = CreateRun.Response()\n    response_message.run.MergeFrom(run.to_proto())\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ReadBlock(RandomAccessFile* file, const BlockHandle& handle,\n                 BlockContents* result) {\n  result->data = StringPiece();\n  result->cachable = false;\n  result->heap_allocated = false;\n\n  // Read the block contents as well as the type/crc footer.\n  // See table_builder.cc for the code that built this structure.\n  size_t n = static_cast<size_t>(handle.size());\n  char* buf = new char[n + kBlockTrailerSize];\n  StringPiece contents;\n  Status s = file->Read(handle.offset(), n + kBlockTrailerSize, &contents, buf);\n  if (!s.ok()) {\n    delete[] buf;\n    return s;\n  }\n  if (contents.size() != n + kBlockTrailerSize) {\n    delete[] buf;\n    return errors::DataLoss(\"truncated block read\");\n  }\n\n  // Check the crc of the type and the block contents\n  const char* data = contents.data();  // Pointer to where Read put the data\n  // This checksum verification is optional.  We leave it on for now\n  const bool verify_checksum = true;\n  if (verify_checksum) {\n    const uint32 crc = crc32c::Unmask(core::DecodeFixed32(data + n + 1));\n    const uint32 actual = crc32c::Value(data, n + 1);\n    if (actual != crc) {\n      delete[] buf;\n      s = errors::DataLoss(\"block checksum mismatch\");\n      return s;\n    }\n  }\n\n  switch (data[n]) {\n    case kNoCompression:\n      if (data != buf) {\n        // File implementation gave us pointer to some other data.\n        // Use it directly under the assumption that it will be live\n        // while the file is open.\n        delete[] buf;\n        result->data = StringPiece(data, n);\n        result->heap_allocated = false;\n        result->cachable = false;  // Do not double-cache\n      } else {\n        result->data = StringPiece(buf, n);\n        result->heap_allocated = true;\n        result->cachable = true;\n      }\n\n      // Ok\n      break;\n    case kSnappyCompression: {\n      size_t ulength = 0;\n      if (!port::Snappy_GetUncompressedLength(data, n, &ulength)) {\n        delete[] buf;\n        return errors::DataLoss(\"corrupted compressed block contents\");\n      }\n      char* ubuf = new char[ulength];\n      if (!port::Snappy_Uncompress(data, n, ubuf)) {\n        delete[] buf;\n        delete[] ubuf;\n        return errors::DataLoss(\"corrupted compressed block contents\");\n      }\n      delete[] buf;\n      result->data = StringPiece(ubuf, ulength);\n      result->heap_allocated = true;\n      result->cachable = true;\n      break;\n    }\n    default:\n      delete[] buf;\n      return errors::DataLoss(\"bad block type\");\n  }\n\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  public Response render(ServeEvent serveEvent) {\n    ResponseDefinition responseDefinition = serveEvent.getResponseDefinition();\n    if (targetAddressProhibited(responseDefinition.getProxyUrl())) {\n      return response()\n          .status(500)\n          .headers(new HttpHeaders(new HttpHeader(\"Content-Type\", \"text/plain\")))\n          .body(\"The target proxy address is denied in WireMock's configuration.\")\n          .build();\n    }\n\n    HttpUriRequest httpRequest = getHttpRequestFor(responseDefinition);\n    addRequestHeaders(httpRequest, responseDefinition);\n\n    GlobalSettings settings = settingsStore.get();\n\n    Request originalRequest = responseDefinition.getOriginalRequest();\n    if ((originalRequest.getBody() != null && originalRequest.getBody().length > 0)\n        || originalRequest.containsHeader(CONTENT_LENGTH)) {\n      httpRequest.setEntity(buildEntityFrom(originalRequest));\n    }\n\n    CloseableHttpClient client = chooseClient(serveEvent.getRequest().isBrowserProxyRequest());\n\n    try {\n      return client.execute(\n          httpRequest,\n          httpResponse ->\n              response()\n                  .status(httpResponse.getCode())\n                  .headers(headersFrom(httpResponse, responseDefinition))\n                  .body(getEntityAsByteArray(httpResponse))\n                  .fromProxy(true)\n                  .configureDelay(\n                      settings.getFixedDelay(),\n                      settings.getDelayDistribution(),\n                      responseDefinition.getFixedDelayMilliseconds(),\n                      responseDefinition.getDelayDistribution())\n                  .chunkedDribbleDelay(responseDefinition.getChunkedDribbleDelay())\n                  .build());\n    } catch (SSLException e) {\n      return proxyResponseError(\"SSL\", httpRequest, e);\n    } catch (IOException e) {\n      return proxyResponseError(\"Network\", httpRequest, e);\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void CalculateOutputIndexValueRowID(\n      const RowPartitionTensor& value_rowids,\n      const vector<INDEX_TYPE>& parent_output_index,\n      INDEX_TYPE output_index_multiplier, INDEX_TYPE output_size,\n      vector<INDEX_TYPE>* result) {\n    const INDEX_TYPE index_size = value_rowids.size();\n    result->reserve(index_size);\n    if (index_size == 0) {\n      return;\n    }\n\n    INDEX_TYPE current_output_column = 0;\n    INDEX_TYPE current_value_rowid = value_rowids(0);\n    DCHECK_LT(current_value_rowid, parent_output_index.size());\n    INDEX_TYPE current_output_index = parent_output_index[current_value_rowid];\n    result->push_back(current_output_index);\n    for (INDEX_TYPE i = 1; i < index_size; ++i) {\n      INDEX_TYPE next_value_rowid = value_rowids(i);\n      if (next_value_rowid == current_value_rowid) {\n        if (current_output_index >= 0) {\n          ++current_output_column;\n          if (current_output_column < output_size) {\n            current_output_index += output_index_multiplier;\n          } else {\n            current_output_index = -1;\n          }\n        }\n      } else {\n        current_output_column = 0;\n        current_value_rowid = next_value_rowid;\n        DCHECK_LT(next_value_rowid, parent_output_index.size());\n        current_output_index = parent_output_index[next_value_rowid];\n      }\n      result->push_back(current_output_index);\n    }\n    DCHECK_EQ(result->size(), value_rowids.size());\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& a = ctx->input(0);\n    const Tensor& b = ctx->input(1);\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(a.shape()),\n                errors::InvalidArgument(\"a is not a matrix\"));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(b.shape()),\n                errors::InvalidArgument(\"b is not a matrix\"));\n\n    const int m = transpose_a_ ? a.dim_size(1) : a.dim_size(0);\n    const int k = transpose_a_ ? a.dim_size(0) : a.dim_size(1);\n    const int n = transpose_b_ ? b.dim_size(0) : b.dim_size(1);\n    const int k2 = transpose_b_ ? b.dim_size(1) : b.dim_size(0);\n\n    OP_REQUIRES(ctx, k == k2,\n                errors::InvalidArgument(\n                    \"Matrix size incompatible: a: \", a.shape().DebugString(),\n                    \", b: \", b.shape().DebugString()));\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({m, n}), &output));\n\n    if (k == 0) {\n      // If the inner dimension k in the matrix multiplication is zero, we fill\n      // the output with zeros.\n      functor::SetZeroFunctor<CPUDevice, float> f;\n      f(ctx->eigen_device<CPUDevice>(), output->flat<float>());\n      return;\n    }\n\n    auto out = output->matrix<float>();\n\n    std::unique_ptr<Tensor> a_float;\n    std::unique_ptr<Tensor> b_float;\n    if (!a_is_sparse_ && !b_is_sparse_) {\n      auto left = &a;\n      auto right = &b;\n      // TODO(agarwal): multi-thread the conversions from bfloat16 to float.\n      if (std::is_same<TL, bfloat16>::value) {\n        a_float.reset(new Tensor(DT_FLOAT, a.shape()));\n        BFloat16ToFloat(a.flat<bfloat16>().data(),\n                        a_float->flat<float>().data(), a.NumElements());\n        left = a_float.get();\n      }\n      if (std::is_same<TR, bfloat16>::value) {\n        b_float.reset(new Tensor(DT_FLOAT, b.shape()));\n        BFloat16ToFloat(b.flat<bfloat16>().data(),\n                        b_float->flat<float>().data(), b.NumElements());\n        right = b_float.get();\n      }\n      Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n      dim_pair[0].first = transpose_a_ ? 0 : 1;\n      dim_pair[0].second = transpose_b_ ? 1 : 0;\n\n      out.device(ctx->template eigen_device<CPUDevice>()) =\n          left->matrix<float>().contract(right->matrix<float>(), dim_pair);\n      return;\n    }\n\n    auto left = &a;\n    auto right = &b;\n    bool transpose_output = false;\n    bool transpose_a = transpose_a_;\n    bool transpose_b = transpose_b_;\n    if (!a_is_sparse_) {\n      // Swap the order of multiplications using the identity:\n      // A * B = (B' *  A')'.\n      std::swap(left, right);\n      std::swap(transpose_a, transpose_b);\n      transpose_a = !transpose_a;\n      transpose_b = !transpose_b;\n      transpose_output = !transpose_output;\n    }\n\n    std::unique_ptr<Tensor> right_tr;\n    if (transpose_b) {\n      // TODO(agarwal): avoid transposing the matrix here and directly handle\n      // transpose in CreateDenseSlices.\n      OP_REQUIRES(ctx, right->dim_size(0) != 0,\n                  errors::InvalidArgument(\"b has an entry 0 in it's shape.\"));\n      OP_REQUIRES(ctx, right->dim_size(1) != 0,\n                  errors::InvalidArgument(\"b has an entry 0 in it's shape.\"));\n      right_tr.reset(\n          new Tensor(right->dtype(),\n                     TensorShape({right->dim_size(1), right->dim_size(0)})));\n\n      const auto perm = dsizes_10();\n      if (transpose_output) {\n        right_tr->matrix<TL>().device(ctx->template eigen_device<CPUDevice>()) =\n            right->matrix<TL>().shuffle(perm);\n      } else {\n        right_tr->matrix<TR>().device(ctx->template eigen_device<CPUDevice>()) =\n            right->matrix<TR>().shuffle(perm);\n      }\n      right = right_tr.get();\n    }\n\n    if (transpose_output) {\n      DoMatMul<TR, TL>::Compute(&this->cache_tr_, left->matrix<TR>(),\n                                right->matrix<TL>(), transpose_a,\n                                ctx->device()->tensorflow_cpu_worker_threads(),\n                                transpose_output, &out);\n    } else {\n      DoMatMul<TL, TR>::Compute(&this->cache_nt_, left->matrix<TL>(),\n                                right->matrix<TR>(), transpose_a,\n                                ctx->device()->tensorflow_cpu_worker_threads(),\n                                transpose_output, &out);\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def verify_mode(self, value):\n        self._ctx.set_verify(value, _verify_callback)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _grab_image_cache_params(args):\n    if args.cache_from_image or args.cache_from_version:\n        logger.debug(\"Image caching is requested - create build image cache params obj\")\n        vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)\n        vault_endpoint = _resolve_vault_endpoint(args)\n        return bigflow.build.operate.BuildImageCacheParams(\n            auth_method=args.auth_method,\n            vault_endpoint=vault_endpoint,\n            vault_secret=vault_secret,\n            cache_from_version=args.cache_from_version,\n            cache_from_image=args.cache_from_image,\n        )\n    else:\n        logger.debug(\"No caching is requested - so just disable it completly\")\n        return None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def guard(dest, ev_args):\n    next(dest)\n    depth = 1\n    while True:\n        ev = yield\n        if ev[0] == \"start\":\n            depth += 1\n        elif ev[0] == \"end\":\n            depth -= 1\n        try:\n            dest.send(ev)\n        except StopIteration as exc:\n            return exc.value\n        except Exception as exc:\n            error = exc\n            break\n    while depth > 0:\n        ev_type, *_ = yield\n        if ev_type == \"end\":\n            depth -= 1\n    raise error\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __new__(cls, obj, *args, **kwargs):\n        # In our usage we should only receive unicode strings.\n        # This conditional and conversion exists to sanity check the values\n        # we're given but we may want to take it out for testing and sanitize\n        # our input instead.\n        if isinstance(obj, string_types):\n            obj = to_text(obj, errors='surrogate_or_strict')\n            return AnsibleUnsafeText(obj)\n        return obj\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, filename):\n        \"\"\"\n        Validate the filename by calling get_valid_name() and return a filename\n        to be passed to the save() method.\n        \"\"\"\n        # `filename` may include a path as returned by FileField.upload_to.\n        dirname, filename = os.path.split(filename)\n        if '..' in pathlib.PurePath(dirname).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dirname)\n        return os.path.normpath(os.path.join(dirname, self.get_valid_name(filename)))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def exit():\n    # delete the session\n    session.clear()\n    return redirect(url_for(\".home\"))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_config(\n    extra_config_path: Optional[str] = None,\n    ignore_local_config: bool = False,\n    **kwargs,\n) -> FluffConfig:\n    \"\"\"Get a config object from kwargs.\"\"\"\n    plain_output = OutputStreamFormatter.should_produce_plain_output(kwargs[\"nocolor\"])\n    if kwargs.get(\"dialect\"):\n        try:\n            # We're just making sure it exists at this stage.\n            # It will be fetched properly in the linter.\n            dialect_selector(kwargs[\"dialect\"])\n        except SQLFluffUserError as err:\n            click.echo(\n                OutputStreamFormatter.colorize_helper(\n                    plain_output,\n                    f\"Error loading dialect '{kwargs['dialect']}': {str(err)}\",\n                    color=Color.red,\n                )\n            )\n            sys.exit(EXIT_ERROR)\n        except KeyError:\n            click.echo(\n                OutputStreamFormatter.colorize_helper(\n                    plain_output,\n                    f\"Error: Unknown dialect '{kwargs['dialect']}'\",\n                    color=Color.red,\n                )\n            )\n            sys.exit(EXIT_ERROR)\n    from_root_kwargs = {}\n    if \"require_dialect\" in kwargs:\n        from_root_kwargs[\"require_dialect\"] = kwargs.pop(\"require_dialect\")\n    # Instantiate a config object (filtering out the nulls)\n    overrides = {k: kwargs[k] for k in kwargs if kwargs[k] is not None}\n    try:\n        return FluffConfig.from_root(\n            extra_config_path=extra_config_path,\n            ignore_local_config=ignore_local_config,\n            overrides=overrides,\n            **from_root_kwargs,\n        )\n    except SQLFluffUserError as err:  # pragma: no cover\n        click.echo(\n            OutputStreamFormatter.colorize_helper(\n                plain_output,\n                f\"Error loading config: {str(err)}\",\n                color=Color.red,\n            )\n        )\n        sys.exit(EXIT_ERROR)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "            self.updatePassword = function (username, password) {\n                return OctoPrint.access.users.changePassword(username, password);\n            };\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create_table(\n        self,\n        name: str,\n        obj: pd.DataFrame | pa.Table | ir.Table | None = None,\n        *,\n        schema: ibis.Schema | None = None,\n        database: str | None = None,\n        temp: bool | None = None,\n        overwrite: bool = False,\n        default_collate: str | None = None,\n        partition_by: str | None = None,\n        cluster_by: Iterable[str] | None = None,\n        options: Mapping[str, Any] | None = None,\n    ) -> ir.Table:\n        \"\"\"Create a table in BigQuery.\n\n        Parameters\n        ----------\n        name\n            Name of the table to create\n        obj\n            The data with which to populate the table; optional, but one of `obj`\n            or `schema` must be specified\n        schema\n            The schema of the table to create; optional, but one of `obj` or\n            `schema` must be specified\n        database\n            The BigQuery *dataset* in which to create the table; optional\n        temp\n            This parameter is not yet supported in the BigQuery backend\n        overwrite\n            If `True`, replace the table if it already exists, otherwise fail if\n            the table exists\n        default_collate\n            Default collation for string columns. See BigQuery's documentation\n            for more details: https://cloud.google.com/bigquery/docs/reference/standard-sql/collation-concepts\n        partition_by\n            Partition the table by the given expression. See BigQuery's documentation\n            for more details: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#partition_expression\n        cluster_by\n            List of columns to cluster the table by. See BigQuery's documentation\n            for more details: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#clustering_column_list\n        options\n            BigQuery-specific table options; see the BigQuery documentation for\n            details: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#table_option_list\n\n        Returns\n        -------\n        Table\n            The table that was just created\n        \"\"\"\n        if obj is None and schema is None:\n            raise com.IbisError(\"One of the `schema` or `obj` parameter is required\")\n\n        if temp:\n            # TODO: these require a BQ session; figure out how to handle that\n            raise NotImplementedError(\n                \"Temporary tables in the BigQuery backend are not yet supported\"\n            )\n\n        create_stmt = \"CREATE\"\n\n        if overwrite:\n            create_stmt += \" OR REPLACE\"\n\n        table_ref = self._fully_qualified_name(name, database)\n\n        create_stmt += f\" TABLE `{table_ref}`\"\n\n        if isinstance(obj, ir.Table) and schema is not None:\n            if not schema.equals(obj.schema()):\n                raise com.IbisTypeError(\n                    \"Provided schema and Ibis table schema are incompatible. Please \"\n                    \"align the two schemas, or provide only one of the two arguments.\"\n                )\n\n        if schema is not None:\n            schema_str = \", \".join(\n                (\n                    f\"{name} {BigQueryType.from_ibis(typ)}\"\n                    + \" NOT NULL\" * (not typ.nullable)\n                )\n                for name, typ in schema.items()\n            )\n            create_stmt += f\" ({schema_str})\"\n\n        if default_collate is not None:\n            create_stmt += f\" DEFAULT COLLATE {default_collate!r}\"\n\n        if partition_by is not None:\n            create_stmt += f\" PARTITION BY {partition_by}\"\n\n        if cluster_by is not None:\n            create_stmt += f\" CLUSTER BY {', '.join(cluster_by)}\"\n\n        if options:\n            pairs = \", \".join(f\"{k}={v!r}\" for k, v in options.items())\n            create_stmt += f\" OPTIONS({pairs})\"\n\n        if obj is not None:\n            import pyarrow as pa\n\n            if isinstance(obj, (pd.DataFrame, pa.Table)):\n                table = ibis.memtable(obj, schema=schema)\n            else:\n                table = obj\n\n            create_stmt += f\" AS ({self.compile(table)})\"\n\n        self.raw_sql(create_stmt)\n\n        return self.table(table_ref)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def build(self):\n        g = Grammar(self.tokens)\n\n        for level, (assoc, terms) in enumerate(self.precedence, 1):\n            for term in terms:\n                g.set_precedence(term, assoc, level)\n\n        for prod_name, syms, func, precedence in self.productions:\n            g.add_production(prod_name, syms, func, precedence)\n\n        g.set_start()\n\n        for unused_term in g.unused_terminals():\n            warnings.warn(\n                \"Token %r is unused\" % unused_term,\n                ParserGeneratorWarning,\n                stacklevel=2\n            )\n        for unused_prod in g.unused_productions():\n            warnings.warn(\n                \"Production %r is not reachable\" % unused_prod,\n                ParserGeneratorWarning,\n                stacklevel=2\n            )\n\n        g.build_lritems()\n        g.compute_first()\n        g.compute_follow()\n\n        # win32 temp directories are already per-user\n        if os.name == \"nt\":\n            cache_file = os.path.join(\n                tempfile.gettempdir(),\n                \"rply-%s-%s-%s.json\" % (\n                    self.VERSION, self.cache_id, self.compute_grammar_hash(g)\n                )\n            )\n        else:\n            cache_file = os.path.join(\n                tempfile.gettempdir(),\n                \"rply-%s-%s-%s-%s.json\" % (\n                    self.VERSION,\n                    os.getuid(),\n                    self.cache_id,\n                    self.compute_grammar_hash(g)\n                )\n            )\n        table = None\n        if os.path.exists(cache_file):\n            with open(cache_file) as f:\n                data = json.load(f)\n                stat_result = os.fstat(f.fileno())\n            if (\n                os.name == \"nt\" or (\n                    stat_result.st_uid == os.getuid() and\n                    stat.S_IMODE(stat_result.st_mode) == 0o0600\n                )\n            ):\n                if self.data_is_valid(g, data):\n                    table = LRTable.from_cache(g, data)\n        if table is None:\n            table = LRTable.from_grammar(g)\n            fd = os.open(\n                cache_file, os.O_RDWR | os.O_CREAT | os.O_EXCL, 0o0600\n            )\n            with os.fdopen(fd, \"w\") as f:\n                json.dump(self.serialize_table(table), f)\n        if table.sr_conflicts:\n            warnings.warn(\n                \"%d shift/reduce conflict%s\" % (\n                    len(table.sr_conflicts),\n                    \"s\" if len(table.sr_conflicts) > 1 else \"\"\n                ),\n                ParserGeneratorWarning,\n                stacklevel=2,\n            )\n        if table.rr_conflicts:\n            warnings.warn(\n                \"%d reduce/reduce conflict%s\" % (\n                    len(table.rr_conflicts),\n                    \"s\" if len(table.rr_conflicts) > 1 else \"\"\n                ),\n                ParserGeneratorWarning,\n                stacklevel=2,\n            )\n        return LRParser(table, self.error_handler)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def kick(bot, trigger):\n    \"\"\"Kick a user from the channel.\"\"\"\n    chanops = get_chanops(str(trigger.sender), bot.memory['channelmgnt']['jdcache'])\n    dodeop = False\n    if chanops:\n        if bot.channels[trigger.sender].privileges[bot.nick] < OP and trigger.account in chanops:\n            bot.say('Please wait...')\n            bot.say('op ' + trigger.sender, 'ChanServ')\n            time.sleep(1)\n            dodeop = True\n        text = trigger.group().split()\n        argc = len(text)\n        if argc < 2:\n            return\n        nick = Identifier(text[1])\n        reason = ' '.join(text[2:])\n        if nick != bot.config.core.nick and trigger.account in chanops:\n            bot.write(['KICK', trigger.sender, nick, ':' + reason])\n            if dodeop:\n                deopbot(trigger.sender, bot)\n        else:\n            bot.reply('Access Denied. If in error, please contact the channel founder.')\n    else:\n        bot.reply(f'No ChanOps Found. Please ask for assistance in {bot.settings.channelmgnt.support_channel}')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "      void context.ready.then(async () => {\n        const file = context.model;\n        const workspace = file.toJSON() as unknown as Workspace.IWorkspace;\n        const path = context.path;\n        const id = workspace.metadata.id;\n\n        // Save the file contents as a workspace.\n        await this._workspaces.save(id, workspace);\n\n        // Save last save location for the save command.\n        await this._state.save(LAST_SAVE_ID, path);\n\n        // Navigate to new workspace.\n        const url = URLExt.join(this._application, 'workspaces', id);\n        if (this._router) {\n          this._router.navigate(url, { hard: true });\n        } else {\n          document.location.href = url;\n        }\n      });\n      return dummyWidget(context);"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def fetch_file(self, in_path, out_path):\n        ''' fetch a file from zone to local '''\n\n        in_path = self._normalize_path(in_path, self.get_zone_path())\n        vvv(\"FETCH %s TO %s\" % (in_path, out_path), host=self.zone)\n\n        self._copy_file(in_path, out_path)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    run_id = request_dict.get(\"run_id\") or request_dict.get(\"run_uuid\")\n    path = request_dict[\"path\"]\n    validate_path_is_safe(path)\n    run = _get_tracking_store().get_run(run_id)\n\n    if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=run.info.artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_repo = _get_artifact_repo(run)\n        artifact_path = path\n\n    return _send_artifact(artifact_repo, artifact_path)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus GatherStrings(TfLiteContext* context, const TfLiteTensor* input,\n                           const TfLiteTensor* positions,\n                           TfLiteTensor* output) {\n  DynamicBuffer buffer;\n  const PositionT* indexes = GetTensorData<PositionT>(positions);\n  const PositionT num_strings = GetStringCount(input);\n  const int num_indexes = NumElements(positions);\n\n  for (int i = 0; i < num_indexes; ++i) {\n    const PositionT pos = indexes[i];\n    TF_LITE_ENSURE(context, pos < num_strings);\n    const auto string_ref = GetString(input, pos);\n    buffer.AddString(string_ref.str, string_ref.len);\n  }\n  buffer.WriteToTensor(output, /*new_shape=*/nullptr);\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void ValidateInputTensors(OpKernelContext* ctx, const Tensor& in0,\n                            const Tensor& in1) override {\n    OP_REQUIRES(\n        ctx, in0.dims() >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0.dims()));\n\n    OP_REQUIRES(\n        ctx, in1.dims() >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in1.dims()));\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, fobj):\n        \"\"\"\n        fobj is a file-like object as an icns resource\n        \"\"\"\n        # signature : (start, length)\n        self.dct = dct = {}\n        self.fobj = fobj\n        sig, filesize = nextheader(fobj)\n        if sig != b'icns':\n            raise SyntaxError('not an icns file')\n        i = HEADERSIZE\n        while i < filesize:\n            sig, blocksize = nextheader(fobj)\n            i += HEADERSIZE\n            blocksize -= HEADERSIZE\n            dct[sig] = (i, blocksize)\n            fobj.seek(blocksize, 1)\n            i += blocksize\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def process_jwt(jwt):\n    \"\"\"\n    Process a JSON Web Token without verifying it.\n\n    Call this before :func:`verify_jwt` if you need access to the header or claims in the token before verifying it. For example, the claims might identify the issuer such that you can retrieve the appropriate public key.\n\n    :param jwt: The JSON Web Token to verify.\n    :type jwt: str or unicode\n\n    :rtype: tuple\n    :returns: ``(header, claims)``\n    \"\"\"\n    header, claims, _ = jwt.split('.')\n    parsed_header = json_decode(base64url_decode(header))\n    parsed_claims = json_decode(base64url_decode(claims))\n    return parsed_header, parsed_claims\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def schedule(self, request_id, scheduler, coro):\n        method, okey, seq = request_id\n        loop = asyncio.get_running_loop()\n        self._futures[request_id] = loop.create_future()\n        ev = asyncio.Event()\n        heapq.heappush(self._pending[okey], _SeqItem(method, seq, ev))\n\n        while True:\n            assert len(self._pending[okey]) > 0\n            s = self._pending[okey][0]\n            if s.seq == seq:\n                break\n            # Wait until the head item finishes.\n            await s.ev.wait()\n\n        job = await scheduler.spawn(coro)\n        job._explicit = True\n        self._jobs[request_id] = job\n\n        def cb(s, rqst_id, task):\n            _, okey, _ = rqst_id\n            s.ev.set()\n            fut = self.get_fut(rqst_id)\n            if task.cancelled():\n                result = CANCELLED\n                # already removed from the pending heap\n                _resolve_future(rqst_id, fut, result, self._log)\n            else:\n                heapq.heappop(self._pending[okey])\n                if task.exception() is not None:\n                    _resolve_future(rqst_id, fut, task.exception(), self._log)\n                else:\n                    _resolve_future(rqst_id, fut, task.result(), self._log)\n\n        job._task.add_done_callback(functools.partial(cb, s, request_id))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus ResizeOutput(TfLiteContext* context, const TfLiteTensor* input,\n                          const TfLiteTensor* axis, TfLiteTensor* output) {\n  int axis_value;\n  // Retrive all 8 bytes when axis type is kTfLiteInt64 to avoid data loss.\n  if (axis->type == kTfLiteInt64) {\n    axis_value = static_cast<int>(*GetTensorData<int64_t>(axis));\n  } else {\n    axis_value = *GetTensorData<int>(axis);\n  }\n  if (axis_value < 0) {\n    axis_value += NumDimensions(input);\n  }\n\n  // Copy the input dimensions to output except the axis dimension.\n  TfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\n  int j = 0;\n  for (int i = 0; i < NumDimensions(input); ++i) {\n    if (i != axis_value) {\n      output_dims->data[j] = SizeOfDimension(input, i);\n      ++j;\n    }\n  }\n  return context->ResizeTensor(context, output, output_dims);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    function dismissChangeRelatedObjectPopup(win, objId, newRepr, newId) {\n        objId = html_unescape(objId);\n        newRepr = html_unescape(newRepr);\n        var id = windowname_to_id(win.name).replace(/^edit_/, '');\n        var selectsSelector = interpolate('#%s, #%s_from, #%s_to', [id, id, id]);\n        var selects = django.jQuery(selectsSelector);\n        selects.find('option').each(function() {\n            if (this.value === objId) {\n                this.innerHTML = newRepr;\n                this.value = newId;\n            }\n        });\n        win.close();\n    }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "inline void GatherNd(const RuntimeShape& params_shape,\n                     const ParamsT* params_data,\n                     const RuntimeShape& indices_shape,\n                     const IndicesT* indices_data,\n                     const RuntimeShape& output_shape, ParamsT* output_data) {\n  ruy::profiler::ScopeLabel label(\"GatherNd\");\n\n  const GatherNdHelperResult res = GatherNdHelper(params_shape, indices_shape);\n  for (int i = 0; i < res.n_slices; ++i) {\n    int from_pos = 0;\n    for (int j = 0; j < res.indices_nd; ++j) {\n      from_pos += indices_data[i * res.indices_nd + j] * res.dims_to_count[j];\n    }\n    std::memcpy(output_data + i * res.slice_size, params_data + from_pos,\n                sizeof(ParamsT) * res.slice_size);\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def edit(request, bookmark_id: int):\n    try:\n        bookmark = Bookmark.objects.get(pk=bookmark_id, owner=request.user)\n    except Bookmark.DoesNotExist:\n        raise Http404('Bookmark does not exist')\n\n    if request.method == 'POST':\n        form = BookmarkForm(request.POST, instance=bookmark)\n        return_url = form.data['return_url']\n        if form.is_valid():\n            tag_string = convert_tag_string(form.data['tag_string'])\n            update_bookmark(form.save(commit=False), tag_string, request.user)\n            return HttpResponseRedirect(return_url)\n    else:\n        return_url = request.GET.get('return_url')\n        form = BookmarkForm(instance=bookmark)\n\n    return_url = return_url if return_url else reverse('bookmarks:index')\n\n    form.initial['tag_string'] = build_tag_string(bookmark.tag_names, ' ')\n    form.initial['return_url'] = return_url\n\n    context = {\n        'form': form,\n        'bookmark_id': bookmark_id,\n        'return_url': return_url\n    }\n\n    return render(request, 'bookmarks/edit.html', context)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_ordering(self, *ordering):\n        \"\"\"\n        Add items from the 'ordering' sequence to the query's \"order by\"\n        clause. These items are either field names (not column names) --\n        possibly with a direction prefix ('-' or '?') -- or OrderBy\n        expressions.\n\n        If 'ordering' is empty, clear all ordering from the query.\n        \"\"\"\n        errors = []\n        for item in ordering:\n            if isinstance(item, str):\n                if '.' in item:\n                    warnings.warn(\n                        'Passing column raw column aliases to order_by() is '\n                        'deprecated. Wrap %r in a RawSQL expression before '\n                        'passing it to order_by().' % item,\n                        category=RemovedInDjango40Warning,\n                        stacklevel=3,\n                    )\n                    continue\n                if item == '?':\n                    continue\n                if item.startswith('-'):\n                    item = item[1:]\n                if item in self.annotations:\n                    continue\n                if self.extra and item in self.extra:\n                    continue\n                # names_to_path() validates the lookup. A descriptive\n                # FieldError will be raise if it's not.\n                self.names_to_path(item.split(LOOKUP_SEP), self.model._meta)\n            elif not hasattr(item, 'resolve_expression'):\n                errors.append(item)\n            if getattr(item, 'contains_aggregate', False):\n                raise FieldError(\n                    'Using an aggregate in order_by() without also including '\n                    'it in annotate() is not allowed: %s' % item\n                )\n        if errors:\n            raise FieldError('Invalid order_by arguments: %s' % errors)\n        if ordering:\n            self.order_by += ordering\n        else:\n            self.default_ordering = False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compile(XlaOpKernelContext* ctx) override {\n    if (ctx->num_inputs() != 3) {\n      OP_REQUIRES(\n          ctx, ctx->num_inputs() == 5,\n          errors::InvalidArgument(\"Must supply ksize and stride arguments.\"));\n      const TensorShape ksize_shape = ctx->InputShape(3);\n      // Validate input sizes.\n      OP_REQUIRES(ctx, TensorShapeUtils::IsVector(ksize_shape),\n                  errors::InvalidArgument(\"ksize must be a vector, not shape \",\n                                          ksize_shape.DebugString()));\n      OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntVector(3, &ksize_));\n\n      const TensorShape stride_shape = ctx->InputShape(4);\n      // Validate input sizes.\n      OP_REQUIRES(ctx, TensorShapeUtils::IsVector(stride_shape),\n                  errors::InvalidArgument(\"stride must be a vector, not shape \",\n                                          stride_shape.DebugString()));\n      OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntVector(4, &stride_));\n    }\n\n    OP_REQUIRES(ctx, ksize_.size() == num_dims(),\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify \",\n                                        num_dims(), \" dimensions\"));\n    OP_REQUIRES(ctx, stride_.size() == num_dims(),\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify \",\n                                        num_dims(), \" dimensions\"));\n\n    const TensorShape tensor_in_shape = ctx->InputShape(0);\n    const TensorShape tensor_out_shape = ctx->InputShape(1);\n    const TensorShape out_backprop_shape = ctx->InputShape(2);\n\n    // For maxpooling, tensor_in should have num_dims() dimensions.\n    OP_REQUIRES(ctx, tensor_in_shape.dims() == num_dims(),\n                errors::InvalidArgument(\"tensor_in must be \", num_dims(),\n                                        \"-dimensional\"));\n    OP_REQUIRES(ctx, tensor_out_shape.dims() == num_dims(),\n                errors::InvalidArgument(\"tensor_out must be \", num_dims(),\n                                        \"-dimensional\"));\n    // For maxpooling, out_backprop should have num_dims() dimensions.\n    OP_REQUIRES(ctx, out_backprop_shape.dims() == num_dims(),\n                errors::InvalidArgument(\"out_backprop must be \", num_dims(),\n                                        \"-dimensional\"));\n\n    // TODO(phawkins): The XLA version doesn't need tensor_out. Investigate\n    // whether this is a good time/space tradeoff.\n    auto input = ctx->Input(0);\n    auto out_backprop = ctx->Input(2);\n    // We ensured padding_ is not EXPLICIT in the constructor.\n    xla::Padding xla_padding =\n        (padding_ == VALID) ? xla::Padding::kValid : xla::Padding::kSame;\n\n    // Create a MaxPool operation to check the expected resulting shape, and\n    // then throw away the operation because we don't actually need it here.\n    TensorShape expected_out_shape;\n    auto pooling =\n        xla::MaxPool(ctx->Input(0), ksize_, stride_, xla_padding,\n                     XlaTensorFormat(data_format_, tensor_in_shape.dims() - 2));\n    auto status_or_shape = pooling.builder()->GetShape(pooling);\n    OP_REQUIRES_OK(ctx, status_or_shape.status());\n    OP_REQUIRES_OK(ctx, XLAShapeToTensorShape(status_or_shape.value(),\n                                              &expected_out_shape));\n    OP_REQUIRES(ctx, expected_out_shape == out_backprop_shape,\n                errors::Unimplemented(\"The output dimensions do not match the \"\n                                      \"other input values.\"));\n\n    xla::PrimitiveType element_type;\n    OP_REQUIRES_OK(ctx, DataTypeToPrimitiveType(input_type(2), &element_type));\n    xla::XlaOp init_value = XlaHelpers::Zero(ctx->builder(), input_type(2));\n    auto select = CreateScalarGeComputation(element_type, ctx->builder());\n    auto scatter = CreateScalarAddComputation(element_type, ctx->builder());\n    xla::XlaOp gradients =\n        xla::SelectAndScatter(input, select, ksize_, stride_, xla_padding,\n                              out_backprop, init_value, scatter);\n\n    ctx->SetOutput(0, gradients);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def unban(bot, trigger):\n    \"\"\"Unban a user from the channel. The bot must be a channel operator for this command to work.\"\"\"\n    makemodechange(bot, trigger, '-b', isbqmode=True)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _ensureSnapshotsFolder(self, subdir=None):\n        \"\"\" Ensure that the appropriate snapshot folder exists.\n        \"\"\"\n        path = ['snapshots', self._snapshot_id]\n\n        if subdir is not None:\n            path.extend(subdir.split('/'))\n\n        current = self._tool\n\n        for element in path:\n\n            if element not in current.objectIds():\n                # No Unicode IDs!\n                current._setObject(str(element), Folder(element))\n\n            current = current._getOb(element)\n\n        return current\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static const char* ConvertOneFloat(PyObject* v, T* out) {\n  if (PyErr_Occurred()) {\n    return nullptr;\n  }\n  if (TF_PREDICT_TRUE(PyFloat_Check(v))) {\n    const double as_double = PyFloat_AS_DOUBLE(v);\n    *out = static_cast<T>(as_double);\n    // Check for overflow\n    if (TF_PREDICT_FALSE(sizeof(T) < sizeof(double) && std::isinf(*out) &&\n                         std::isfinite(as_double))) {\n      return ErrorOutOfRangeDouble;\n    }\n    return nullptr;\n  }\n#if PY_MAJOR_VERSION < 3\n  if (PyInt_Check(v)) {\n    *out = PyInt_AS_LONG(v);\n    return nullptr;\n  }\n#endif\n  if (PyLong_Check(v)) {\n    *out = PyLong_AsDouble(v);\n    if (PyErr_Occurred()) return ErrorOutOfRangeDouble;\n    return nullptr;\n  }\n  if (PyIsInstance(v, &PyFloatingArrType_Type)) {  // NumPy float types\n    Safe_PyObjectPtr as_float = make_safe(PyNumber_Float(v));\n    if (PyErr_Occurred()) {\n      return nullptr;\n    }\n    return ConvertOneFloat<T>(as_float.get(), out);\n  }\n  if (PyIsInstance(v, &PyIntegerArrType_Type)) {  // NumPy integers\n#if PY_MAJOR_VERSION < 3\n    Safe_PyObjectPtr as_int = make_safe(PyNumber_Int(v));\n#else\n    Safe_PyObjectPtr as_int = make_safe(PyNumber_Long(v));\n#endif\n    if (PyErr_Occurred()) {\n      return nullptr;\n    }\n    return ConvertOneFloat<T>(as_int.get(), out);\n  }\n  return ErrorMixedTypes;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(\n        self,\n        privacy_request: PrivacyRequest,\n        dsr_data: Dict[str, Any],\n    ):\n        \"\"\"\n        Manages populating HTML templates from the given data and adding the generated\n        pages to a zip file in a way that the pages can be navigated between.\n        \"\"\"\n\n        # zip file variables\n        self.baos = BytesIO()\n\n        # we close this in the finally block of generate()\n        # pylint: disable=consider-using-with\n        self.out = zipfile.ZipFile(self.baos, \"w\")\n\n        # Jinja template environment initialization\n        def pretty_print(value: str, indent: int = 4) -> str:\n            return json.dumps(value, indent=indent, default=storage_json_encoder)\n\n        jinja2.filters.FILTERS[\"pretty_print\"] = pretty_print\n        self.template_loader = Environment(loader=FileSystemLoader(DSR_DIRECTORY))\n\n        # to pass in custom colors in the future\n        self.template_data: Dict[str, Any] = {\n            \"text_color\": TEXT_COLOR,\n            \"header_color\": HEADER_COLOR,\n            \"border_color\": BORDER_COLOR,\n        }\n        self.main_links: Dict[str, Any] = {}  # used to track the generated pages\n\n        # report data to populate the templates\n        self.request_data = _map_privacy_request(privacy_request)\n        self.dsr_data = dsr_data\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __call__(self, args: argparse.Namespace) -> None:\n        \"\"\"Perform the update.\"\"\"\n        manifest, path = dfetch.manifest.manifest.get_manifest()\n\n        exceptions: List[str] = []\n        with in_directory(os.path.dirname(path)):\n            for project in manifest.selected_projects(args.projects):\n                with catch_runtime_exceptions(exceptions) as exceptions:\n                    dfetch.project.make(project).update(force=args.force)\n\n                if not args.no_recommendations and os.path.isdir(project.destination):\n                    with in_directory(project.destination):\n                        check_child_manifests(manifest, project, path)\n\n        if exceptions:\n            raise RuntimeError(\"\\n\".join(exceptions))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const Tensor& filter_sizes = context->input(1);\n    const Tensor& out_backprop = context->input(2);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(filter_sizes.shape()),\n        errors::InvalidArgument(\n            \"Conv2DCustomBackpropFilter: filter_sizes input must be 1-dim, \"\n            \"not \",\n            filter_sizes.dims()));\n    TensorShape filter_shape;\n    OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\n                                filter_sizes.vec<int32>(), &filter_shape));\n\n    ConvBackpropDimensions dims;\n    OP_REQUIRES_OK(\n        context,\n        ConvBackpropComputeDimensionsV2(\n            \"Conv2DCustomBackpropFilter\", /*num_spatial_dims=*/2, input.shape(),\n            filter_shape, out_backprop.shape(), dilations_, strides_, padding_,\n            explicit_paddings_, data_format_, &dims));\n\n    Tensor* filter_backprop;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, filter_shape, &filter_backprop));\n\n    // If there is nothing to compute, return.\n    if (filter_shape.num_elements() == 0) {\n      return;\n    }\n\n    int64 pad_top, pad_bottom;\n    int64 pad_left, pad_right;\n    if (padding_ == Padding::EXPLICIT) {\n      pad_top = explicit_paddings_[2];\n      pad_bottom = explicit_paddings_[3];\n      pad_left = explicit_paddings_[4];\n      pad_right = explicit_paddings_[5];\n    }\n    OP_REQUIRES_OK(\n        context,\n        GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\n            dims.spatial_dims[0].stride, padding_,\n            &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom));\n    OP_REQUIRES_OK(\n        context,\n        GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\n            dims.spatial_dims[1].stride, padding_,\n            &dims.spatial_dims[1].output_size, &pad_left, &pad_right));\n#if defined TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS && \\\n    defined TENSORFLOW_USE_LIBXSMM_BACKWARD_CONVOLUTIONS\n    if (pad_left == pad_right && pad_top == pad_bottom) {\n      if (LaunchXsmmBackwardFilter<Device, T>()(\n              context, context->eigen_device<Device>(), input.tensor<T, 4>(),\n              filter_backprop->tensor<T, 4>(), out_backprop.tensor<T, 4>(),\n              dims.spatial_dims[0].input_size, dims.spatial_dims[1].input_size,\n              static_cast<int>(dims.spatial_dims[0].stride),\n              static_cast<int>(dims.spatial_dims[1].stride),\n              static_cast<int>(pad_top), static_cast<int>(pad_left),\n              data_format_)) {\n        return;\n      }\n    }\n#endif\n\n    // The total dimension size of each kernel.\n    const int filter_total_size = dims.spatial_dims[0].filter_size *\n                                  dims.spatial_dims[1].filter_size *\n                                  dims.in_depth;\n    // The output image size is the spatial size of the output.\n    const int output_image_size =\n        dims.spatial_dims[0].output_size * dims.spatial_dims[1].output_size;\n\n    // Shard 'batch' images into 'shard_size' groups of images to be fed\n    // into the parallel matmul. Calculate 'shard_size' by dividing the L3 cache\n    // size ('target_working_set_size') by the matmul size of an individual\n    // image ('work_unit_size').\n\n    // TODO(andydavis)\n    // *) Get L3 cache size from device at runtime (30MB is from ivybridge).\n    // *) Consider reducing 'target_working_set_size' if L3 is shared by\n    //    other concurrently running tensorflow ops.\n    const size_t target_working_set_size = (30LL << 20) / sizeof(T);\n\n    const size_t size_A = output_image_size * filter_total_size;\n\n    const size_t size_B = output_image_size * dims.out_depth;\n\n    const size_t size_C = filter_total_size * dims.out_depth;\n\n    const size_t work_unit_size = size_A + size_B + size_C;\n\n    const size_t shard_size =\n        (target_working_set_size + work_unit_size - 1) / work_unit_size;\n\n    Tensor col_buffer;\n    OP_REQUIRES_OK(context,\n                   context->allocate_temp(\n                       DataTypeToEnum<T>::value,\n                       TensorShape({static_cast<int64>(shard_size),\n                                    static_cast<int64>(output_image_size),\n                                    static_cast<int64>(filter_total_size)}),\n                       &col_buffer));\n\n    // The input offset corresponding to a single input image.\n    const int input_offset = dims.spatial_dims[0].input_size *\n                             dims.spatial_dims[1].input_size * dims.in_depth;\n    // The output offset corresponding to a single output image.\n    const int output_offset = dims.spatial_dims[0].output_size *\n                              dims.spatial_dims[1].output_size * dims.out_depth;\n\n    const T* input_data = input.template flat<T>().data();\n    T* col_buffer_data = col_buffer.template flat<T>().data();\n    const T* out_backprop_data = out_backprop.template flat<T>().data();\n    T* filter_backprop_data = filter_backprop->template flat<T>().data();\n\n    typedef Eigen::TensorMap<Eigen::Tensor<T, 2, Eigen::RowMajor>,\n                             Eigen::Unaligned>\n        TensorMap;\n    typedef Eigen::TensorMap<Eigen::Tensor<const T, 2, Eigen::RowMajor>,\n                             Eigen::Unaligned>\n        ConstTensorMap;\n\n    TensorMap C(filter_backprop_data, filter_total_size, dims.out_depth);\n    C.setZero();\n\n    // Initialize contraction dims (we need to transpose 'A' below).\n    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> contract_dims;\n    contract_dims[0].first = 0;\n    contract_dims[0].second = 0;\n\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n\n    for (int image_id = 0; image_id < dims.batch_size; image_id += shard_size) {\n      const int shard_limit =\n          std::min(static_cast<int>(shard_size),\n                   static_cast<int>(dims.batch_size) - image_id);\n\n      auto shard = [&input_data, &col_buffer_data, &dims, &pad_top, &pad_left,\n                    &pad_bottom, &pad_right, &input_offset,\n                    &size_A](int64 start, int64 limit) {\n        for (int shard_id = start; shard_id < limit; ++shard_id) {\n          const T* input_data_shard = input_data + shard_id * input_offset;\n          T* col_data_shard = col_buffer_data + shard_id * size_A;\n\n          // When we compute the gradient with respect to the filters, we need\n          // to do im2col to allow gemm-type computation.\n          Im2col<T>(\n              input_data_shard, dims.in_depth, dims.spatial_dims[0].input_size,\n              dims.spatial_dims[1].input_size, dims.spatial_dims[0].filter_size,\n              dims.spatial_dims[1].filter_size, pad_top, pad_left, pad_bottom,\n              pad_right, dims.spatial_dims[0].stride,\n              dims.spatial_dims[1].stride, col_data_shard);\n        }\n      };\n      Shard(worker_threads.num_threads, worker_threads.workers, shard_limit,\n            size_A, shard);\n\n      ConstTensorMap A(col_buffer_data, output_image_size * shard_limit,\n                       filter_total_size);\n      ConstTensorMap B(out_backprop_data, output_image_size * shard_limit,\n                       dims.out_depth);\n\n      // Gradient with respect to filter.\n      C.device(context->eigen_cpu_device()) += A.contract(B, contract_dims);\n\n      input_data += input_offset * shard_limit;\n      out_backprop_data += output_offset * shard_limit;\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def qute_settings(url):\n    \"\"\"Handler for qute://settings. View/change qute configuration.\"\"\"\n    if url.path() == '/set':\n        return _qute_settings_set(url)\n\n    src = jinja.render('settings.html', title='settings',\n                       configdata=configdata,\n                       confget=config.instance.get_str)\n    return 'text/html', src\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _clean_code(self, code: str) -> str:\n        \"\"\"\n        A method to clean the code to prevent malicious code execution\n\n        Args:\n            code(str): A python code\n\n        Returns (str): Returns a Clean Code String\n\n        \"\"\"\n\n        tree = ast.parse(code)\n\n        new_body = []\n\n        # clear recent optional dependencies\n        self._additional_dependencies = []\n\n        for node in tree.body:\n            if isinstance(node, (ast.Import, ast.ImportFrom)):\n                self._check_imports(node)\n                continue\n            if self._is_df_overwrite(node):\n                continue\n            new_body.append(node)\n\n        new_tree = ast.Module(body=new_body)\n        return astor.to_source(new_tree, pretty_source=lambda x: \"\".join(x)).strip()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render_POST(self, request):\n        \"\"\"\n        Register with the Identity Server\n        \"\"\"\n        send_cors(request)\n\n        args = get_args(request, ('matrix_server_name', 'access_token'))\n\n        result = yield self.client.get_json(\n            \"matrix://%s/_matrix/federation/v1/openid/userinfo?access_token=%s\" % (\n                args['matrix_server_name'], urllib.parse.quote(args['access_token']),\n            ),\n            1024 * 5,\n        )\n        if 'sub' not in result:\n            raise Exception(\"Invalid response from homeserver\")\n\n        user_id = result['sub']\n        tok = yield issueToken(self.sydent, user_id)\n\n        # XXX: `token` is correct for the spec, but we released with `access_token`\n        # for a substantial amount of time. Serve both to make spec-compliant clients\n        # happy.\n        defer.returnValue({\n            \"access_token\": tok,\n            \"token\": tok,\n        })\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def _wait_for_free_memory(self, alloc_size: int, timeout: Optional[float]):\n        start_time = time.perf_counter()\n        loop = asyncio.get_event_loop()\n\n        self.enqueued_size_bytes += alloc_size\n        allocated = False\n        try:\n            context_manager = async_timeout.timeout(timeout) if timeout != 0 else contextlib.AsyncExitStack()\n            # contextlib.AsyncExitStack() is used as a null context here\n            async with context_manager:\n                if timeout == 0 and self.current_size_bytes + self.enqueued_size_bytes > self.max_size_bytes:\n                    raise AllocationFailed(f\"Could not allocate {alloc_size} bytes immediately: out of memory\")\n                async with enter_asynchronously(self._lock_acquire_memory):\n                    if self.current_size_bytes + alloc_size > self.max_size_bytes:\n                        if timeout == 0:\n                            raise AllocationFailed(f\"Could not allocate {alloc_size} bytes immediately: out of memory\")\n                        elapsed_time = time.perf_counter() - start_time\n                        remaining_timeout = max(0.0, timeout - elapsed_time) if timeout is not None else None\n                        await loop.run_in_executor(None, self._wait_until_available, alloc_size, remaining_timeout)\n\n                allocated = True\n                self.enqueued_size_bytes -= alloc_size\n                yield\n        except asyncio.TimeoutError:\n            raise AllocationFailed(f\"Could not allocate {alloc_size} within {timeout} seconds\")\n        finally:\n            if not allocated:\n                self.enqueued_size_bytes -= alloc_size\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static int generate_key(DH *dh)\n{\n    int ok = 0;\n    int generate_new_key = 0;\n#ifndef FIPS_MODULE\n    unsigned l;\n#endif\n    BN_CTX *ctx = NULL;\n    BIGNUM *pub_key = NULL, *priv_key = NULL;\n\n    if (BN_num_bits(dh->params.p) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n        return 0;\n    }\n\n    if (BN_num_bits(dh->params.p) < DH_MIN_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_SMALL);\n        return 0;\n    }\n\n    ctx = BN_CTX_new_ex(dh->libctx);\n    if (ctx == NULL)\n        goto err;\n\n    if (dh->priv_key == NULL) {\n        priv_key = BN_secure_new();\n        if (priv_key == NULL)\n            goto err;\n        generate_new_key = 1;\n    } else {\n        priv_key = dh->priv_key;\n    }\n\n    if (dh->pub_key == NULL) {\n        pub_key = BN_new();\n        if (pub_key == NULL)\n            goto err;\n    } else {\n        pub_key = dh->pub_key;\n    }\n    if (generate_new_key) {\n        /* Is it an approved safe prime ?*/\n        if (DH_get_nid(dh) != NID_undef) {\n            int max_strength =\n                    ossl_ifc_ffc_compute_security_bits(BN_num_bits(dh->params.p));\n\n            if (dh->params.q == NULL\n                || dh->length > BN_num_bits(dh->params.q))\n                goto err;\n            /* dh->length = maximum bit length of generated private key */\n            if (!ossl_ffc_generate_private_key(ctx, &dh->params, dh->length,\n                                               max_strength, priv_key))\n                goto err;\n        } else {\n#ifdef FIPS_MODULE\n            if (dh->params.q == NULL)\n                goto err;\n#else\n            if (dh->params.q == NULL) {\n                /* secret exponent length, must satisfy 2^(l-1) <= p */\n                if (dh->length != 0\n                    && dh->length >= BN_num_bits(dh->params.p))\n                    goto err;\n                l = dh->length ? dh->length : BN_num_bits(dh->params.p) - 1;\n                if (!BN_priv_rand_ex(priv_key, l, BN_RAND_TOP_ONE,\n                                     BN_RAND_BOTTOM_ANY, 0, ctx))\n                    goto err;\n                /*\n                 * We handle just one known case where g is a quadratic non-residue:\n                 * for g = 2: p % 8 == 3\n                 */\n                if (BN_is_word(dh->params.g, DH_GENERATOR_2)\n                    && !BN_is_bit_set(dh->params.p, 2)) {\n                    /* clear bit 0, since it won't be a secret anyway */\n                    if (!BN_clear_bit(priv_key, 0))\n                        goto err;\n                }\n            } else\n#endif\n            {\n                /* Do a partial check for invalid p, q, g */\n                if (!ossl_ffc_params_simple_validate(dh->libctx, &dh->params,\n                                                     FFC_PARAM_TYPE_DH, NULL))\n                    goto err;\n                /*\n                 * For FFC FIPS 186-4 keygen\n                 * security strength s = 112,\n                 * Max Private key size N = len(q)\n                 */\n                if (!ossl_ffc_generate_private_key(ctx, &dh->params,\n                                                   BN_num_bits(dh->params.q),\n                                                   MIN_STRENGTH,\n                                                   priv_key))\n                    goto err;\n            }\n        }\n    }\n\n    if (!ossl_dh_generate_public_key(ctx, dh, priv_key, pub_key))\n        goto err;\n\n    dh->pub_key = pub_key;\n    dh->priv_key = priv_key;\n    dh->dirty_cnt++;\n    ok = 1;\n err:\n    if (ok != 1)\n        ERR_raise(ERR_LIB_DH, ERR_R_BN_LIB);\n\n    if (pub_key != dh->pub_key)\n        BN_free(pub_key);\n    if (priv_key != dh->priv_key)\n        BN_free(priv_key);\n    BN_CTX_free(ctx);\n    return ok;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def delete(self):\n        from airflow.api.common.experimental import delete_dag\n        from airflow.exceptions import DagFileExists, DagNotFound\n\n        dag_id = request.values.get('dag_id')\n        origin = request.values.get('origin') or url_for('Airflow.index')\n\n        try:\n            delete_dag.delete_dag(dag_id)\n        except DagNotFound:\n            flash(\"DAG with id {} not found. Cannot delete\".format(dag_id), 'error')\n            return redirect(request.referrer)\n        except DagFileExists:\n            flash(\"Dag id {} is still in DagBag. \"\n                  \"Remove the DAG file first.\".format(dag_id),\n                  'error')\n            return redirect(request.referrer)\n\n        flash(\"Deleting DAG with id {}. May take a couple minutes to fully\"\n              \" disappear.\".format(dag_id))\n\n        # Upon success return to origin.\n        return redirect(origin)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def download_url(url, proxies=None):\n    \"\"\"\n    Downloads a given url in a temporary file. This function is not safe to use in multiple processes. Its only use is\n    for deprecated behavior allowing to download config/models with a single url instead of using the Hub.\n\n    Args:\n        url (`str`): The url of the file to download.\n        proxies (`Dict[str, str]`, *optional*):\n            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n            'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n\n    Returns:\n        `str`: The location of the temporary file where the url was downloaded.\n    \"\"\"\n    warnings.warn(\n        f\"Using `from_pretrained` with the url of a file (here {url}) is deprecated and won't be possible anymore in\"\n        \" v5 of Transformers. You should host your file on the Hub (hf.co) instead and use the repository ID. Note\"\n        \" that this is not compatible with the caching system (your file will be downloaded at each execution) or\"\n        \" multiple processes (each process will download the file in a different temporary file).\"\n    )\n    tmp_file = tempfile.mktemp()\n    with open(tmp_file, \"wb\") as f:\n        http_get(url, f, proxies=proxies)\n    return tmp_file\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def put(self, name: str):\n        \"\"\"add new file\n        params in FormData:\n            - file\n            - original_file_name [optional]\n        \"\"\"\n\n        data = {}\n        mindsdb_file_name = name\n\n        existing_file_names = ca.file_controller.get_files_names()\n\n        def on_field(field):\n            name = field.field_name.decode()\n            value = field.value.decode()\n            data[name] = value\n\n        file_object = None\n\n        def on_file(file):\n            nonlocal file_object\n            data[\"file\"] = file.file_name.decode()\n            file_object = file.file_object\n\n        temp_dir_path = tempfile.mkdtemp(prefix=\"mindsdb_file_\")\n\n        if request.headers[\"Content-Type\"].startswith(\"multipart/form-data\"):\n            parser = multipart.create_form_parser(\n                headers=request.headers,\n                on_field=on_field,\n                on_file=on_file,\n                config={\n                    \"UPLOAD_DIR\": temp_dir_path.encode(),  # bytes required\n                    \"UPLOAD_KEEP_FILENAME\": True,\n                    \"UPLOAD_KEEP_EXTENSIONS\": True,\n                    \"MAX_MEMORY_FILE_SIZE\": 0,\n                },\n            )\n\n            while True:\n                chunk = request.stream.read(8192)\n                if not chunk:\n                    break\n                parser.write(chunk)\n            parser.finalize()\n            parser.close()\n\n            if file_object is not None and not file_object.closed:\n                file_object.close()\n        else:\n            data = request.json\n\n        if mindsdb_file_name in existing_file_names:\n            return http_error(\n                400,\n                \"File already exists\",\n                f\"File with name '{data['file']}' already exists\",\n            )\n\n        if data.get(\"source_type\") == \"url\":\n            url = data[\"source\"]\n            data[\"file\"] = data[\"name\"]\n\n            config = Config()\n            is_cloud = config.get(\"cloud\", False)\n            if is_cloud is True and ctx.user_class != 1:\n                info = requests.head(url)\n                file_size = info.headers.get(\"Content-Length\")\n                try:\n                    file_size = int(file_size)\n                except Exception:\n                    pass\n\n                if file_size is None:\n                    return http_error(\n                        400,\n                        \"Error getting file info\",\n                        \"\u0421an't determine remote file size\",\n                    )\n                if file_size > 1024 * 1024 * 100:\n                    return http_error(\n                        400, \"File is too big\", \"Upload limit for file is 100Mb\"\n                    )\n            with requests.get(url, stream=True) as r:\n                if r.status_code != 200:\n                    return http_error(\n                        400, \"Error getting file\", f\"Got status code: {r.status_code}\"\n                    )\n                file_path = os.path.join(temp_dir_path, data[\"file\"])\n                with open(file_path, \"wb\") as f:\n                    for chunk in r.iter_content(chunk_size=8192):\n                        f.write(chunk)\n\n        original_file_name = data.get(\"original_file_name\")\n\n        file_path = os.path.join(temp_dir_path, data[\"file\"])\n        lp = file_path.lower()\n        if lp.endswith((\".zip\", \".tar.gz\")):\n            if lp.endswith(\".zip\"):\n                with zipfile.ZipFile(file_path) as f:\n                    f.extractall(temp_dir_path)\n            elif lp.endswith(\".tar.gz\"):\n                with tarfile.open(file_path) as f:\n                    safe_extract(f, temp_dir_path)\n            os.remove(file_path)\n            files = os.listdir(temp_dir_path)\n            if len(files) != 1:\n                os.rmdir(temp_dir_path)\n                return http_error(\n                    400, \"Wrong content.\", \"Archive must contain only one data file.\"\n                )\n            file_path = os.path.join(temp_dir_path, files[0])\n            mindsdb_file_name = files[0]\n            if not os.path.isfile(file_path):\n                os.rmdir(temp_dir_path)\n                return http_error(\n                    400, \"Wrong content.\", \"Archive must contain data file in root.\"\n                )\n\n        ca.file_controller.save_file(\n            mindsdb_file_name, file_path, file_name=original_file_name\n        )\n\n        os.rmdir(temp_dir_path)\n\n        return \"\", 200\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "   public static File writeStreamToTempFile(InputStream inputStream, String tempFileSuffix) throws IOException\n   {\n      FileOutputStream outputStream = null;\n\n      try\n      {\n         File file = File.createTempFile(\"mpxj\", tempFileSuffix);\n         outputStream = new FileOutputStream(file);\n         byte[] buffer = new byte[1024];\n         while (true)\n         {\n            int bytesRead = inputStream.read(buffer);\n            if (bytesRead == -1)\n            {\n               break;\n            }\n            outputStream.write(buffer, 0, bytesRead);\n         }\n         return file;\n      }\n\n      finally\n      {\n         if (outputStream != null)\n         {\n            outputStream.close();\n         }\n      }\n   }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_services(self, context):\n        service_list = self.catalog_api.list_services(context)\n        service_refs = [self.catalog_api.get_service(context, x)\n                        for x in service_list]\n        return {'OS-KSADM:services': service_refs}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& spectrogram = context->input(0);\n    OP_REQUIRES(context, spectrogram.dims() == 3,\n                errors::InvalidArgument(\"spectrogram must be 3-dimensional\",\n                                        spectrogram.shape().DebugString()));\n    const Tensor& sample_rate_tensor = context->input(1);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(sample_rate_tensor.shape()),\n                errors::InvalidArgument(\n                    \"Input sample_rate should be a scalar tensor, got \",\n                    sample_rate_tensor.shape().DebugString(), \" instead.\"));\n    const int32_t sample_rate = sample_rate_tensor.scalar<int32>()();\n\n    const int spectrogram_channels = spectrogram.dim_size(2);\n    const int spectrogram_samples = spectrogram.dim_size(1);\n    const int audio_channels = spectrogram.dim_size(0);\n\n    Mfcc mfcc;\n    mfcc.set_upper_frequency_limit(upper_frequency_limit_);\n    mfcc.set_lower_frequency_limit(lower_frequency_limit_);\n    mfcc.set_filterbank_channel_count(filterbank_channel_count_);\n    mfcc.set_dct_coefficient_count(dct_coefficient_count_);\n    OP_REQUIRES(context, mfcc.Initialize(spectrogram_channels, sample_rate),\n                errors::InvalidArgument(\n                    \"Mfcc initialization failed for channel count \",\n                    spectrogram_channels, \" and sample rate \", sample_rate));\n\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\n                       0,\n                       TensorShape({audio_channels, spectrogram_samples,\n                                    dct_coefficient_count_}),\n                       &output_tensor));\n\n    const float* spectrogram_flat = spectrogram.flat<float>().data();\n    float* output_flat = output_tensor->flat<float>().data();\n\n    for (int audio_channel = 0; audio_channel < audio_channels;\n         ++audio_channel) {\n      for (int spectrogram_sample = 0; spectrogram_sample < spectrogram_samples;\n           ++spectrogram_sample) {\n        const float* sample_data =\n            spectrogram_flat +\n            (audio_channel * spectrogram_samples * spectrogram_channels) +\n            (spectrogram_sample * spectrogram_channels);\n        std::vector<double> mfcc_input(sample_data,\n                                       sample_data + spectrogram_channels);\n        std::vector<double> mfcc_output;\n        mfcc.Compute(mfcc_input, &mfcc_output);\n        DCHECK_EQ(dct_coefficient_count_, mfcc_output.size());\n        float* output_data =\n            output_flat +\n            (audio_channel * spectrogram_samples * dct_coefficient_count_) +\n            (spectrogram_sample * dct_coefficient_count_);\n        for (int i = 0; i < dct_coefficient_count_; ++i) {\n          output_data[i] = mfcc_output[i];\n        }\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def render_markdown(value):\n    \"\"\"\n    Render text as Markdown\n\n    Example:\n        {{ text | render_markdown }}\n    \"\"\"\n    # Strip HTML tags\n    value = strip_tags(value)\n\n    # Sanitize Markdown links\n    schemes = \"|\".join(settings.ALLOWED_URL_SCHEMES)\n    pattern = rf\"\\[(.+)\\]\\((?!({schemes})).*:(.+)\\)\"\n    value = re.sub(pattern, \"[\\\\1](\\\\3)\", value, flags=re.IGNORECASE)\n\n    # Render Markdown\n    html = markdown(value, extensions=[\"fenced_code\", \"tables\"])\n\n    return mark_safe(html)  # noqa: S308\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def ban(bot, trigger):\n    \"\"\"Ban a user from the channel. The bot must be a channel operator for this command to work.\"\"\"\n    makemodechange(bot, trigger, '+b', isbqmode=True)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def authorize(request, configured_username, configured_password):\n    \"\"\"\n    Match authorization header present in the request against\n    configured username and password.\n    \"\"\"\n    # Use request.META instead of request.headers to make it\n    # compatible with Django versions below 2.2.\n    if \"HTTP_AUTHORIZATION\" not in request.META:\n        raise Unauthorized('\"HTTP_AUTHORIZATION\" is not present in the request object.')\n\n    authentication = request.META[\"HTTP_AUTHORIZATION\"]\n\n    disable_consumption = getattr(\n        settings, \"BASIC_AUTH_DISABLE_CONSUMING_AUTHORIZATION_HEADER\", False,\n    )\n    if not disable_consumption:\n        # Delete \"Authorization\" header so other authentication\n        # mechanisms do not try to use it.\n        request.META.pop(\"HTTP_AUTHORIZATION\")\n\n    authentication_tuple = authentication.split(\" \", 1)\n    if len(authentication_tuple) != 2:\n        raise Unauthorized(\"Invalid format of the authorization header.\")\n    auth_method = authentication_tuple[0]\n    auth = authentication_tuple[1]\n    if \"basic\" != auth_method.lower():\n        raise Unauthorized('\"Basic\" is not an authorization method.')\n    auth = base64.b64decode(auth.strip()).decode(\"utf-8\")\n    username, password = auth.split(\":\", 1)\n    if username == configured_username and password == configured_password:\n        return True\n    raise Unauthorized(\"Basic authentication credentials are invalid.\")\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __setitem__(self, key, value):\n        self.dict[_hkey(key)] = [value if isinstance(value, unicode) else\n                                 str(value)]\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _process_discovery_args(parsed_args):\n    \"\"\"\n    Process rules for discovery filters on Account ID and Partition ID\n    :param parsed_args:\n    :return: Dict[str, Union[str, List[str]]]\n    :raises: ParameterParseError\n    \"\"\"\n    if parsed_args.discovery is False:\n        if parsed_args.discovery_account:\n            raise ParameterParseError(\"Error: --discovery-account is only available if discovery is enabled\")\n        if parsed_args.discovery_partition:\n            raise ParameterParseError(\"Error: --discovery-partition is only available if discovery is enabled\")\n        return None\n    if not parsed_args.discovery_partition and not parsed_args.discovery_account:\n        return None\n    if parsed_args.discovery_partition and not parsed_args.discovery_account:\n        raise ParameterParseError(\"Error: --discovery-partition requires at least one --discovery-account\")\n    if parsed_args.discovery_account and not parsed_args.discovery_partition:\n        raise ParameterParseError(\"Error: --discovery-account requires at least one --discovery-partition\")\n\n    return {\"account_ids\": parsed_args.discovery_account, \"partition\": parsed_args.discovery_partition}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        dir_name, file_name = os.path.split(name)\n        if '..' in pathlib.PurePath(dir_name).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n        validate_file_name(file_name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, add an underscore and a random 7\n        # character alphanumeric string (before the file extension, if one\n        # exists) to the filename until the generated filename doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n        return name\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool VerifyTensors(const Model& model, ErrorReporter* error_reporter) {\n  if (!model.subgraphs()) {\n    return true;\n  }\n  for (const auto& subgraph : *model.subgraphs()) {\n    if (!subgraph->tensors()) {\n      continue;\n    }\n    for (const auto& tensor : *subgraph->tensors()) {\n      if (!tensor->buffer()) {\n        continue;\n      }\n      if (tensor->buffer() >= model.buffers()->size()) {\n        ReportError(error_reporter, \"Invalid tensor buffer index: %d\",\n                    tensor->buffer());\n        return false;\n      }\n      auto* buffer = model.buffers()->Get(tensor->buffer());\n      if (!buffer || !buffer->data()) {\n        ReportError(error_reporter, \"Tensor buffer %d not set\",\n                    tensor->buffer());\n        return false;\n      }\n\n      if (tensor->type() == TensorType_STRING) {\n        if (!VerifyStringTensorBuffer(*buffer, error_reporter)) {\n          return false;\n        }\n      } else {\n        if (!VerifyNumericTensorBuffer(*tensor, *buffer, error_reporter)) {\n          return false;\n        }\n      }\n    }\n  }\n  return true;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _login(server, creds_file=None, api_key=None, silent=False):\n    \"\"\"Login main function.\"\"\"\n    # fetch_api_key and check_version expect server to end in /\n    if server[-1] != \"/\":\n        server = server + \"/\"\n\n    # creds file path setup\n    if creds_file is None:\n        creds_file = os.path.expanduser(\"~/.onecodex\")\n\n    # check if the creds file exists and is readable\n    if not os.path.exists(creds_file):\n        if silent:\n            return None\n\n        creds = {}\n    elif not os.access(creds_file, os.R_OK):\n        click.echo(\"Please check the permissions on {}\".format(collapse_user(creds_file)), err=True)\n        sys.exit(1)\n    else:\n        # it is, so let's read it!\n        with open(creds_file, \"r\") as fp:\n            try:\n                creds = json.load(fp)\n            except ValueError:\n                click.echo(\n                    \"Your ~/.onecodex credentials file appears to be corrupted. \"  # noqa\n                    \"Please delete it and re-authorize.\",\n                    err=True,\n                )\n                sys.exit(1)\n\n        # check for updates if logged in more than one day ago\n        last_update = creds.get(\"updated_at\") or creds.get(\"saved_at\")\n        last_update = last_update if last_update else datetime.datetime.now().strftime(DATE_FORMAT)\n        diff = datetime.datetime.now() - datetime.datetime.strptime(last_update, DATE_FORMAT)\n\n        if diff.days >= 1:\n            # if creds_file is old, check for updates\n            upgrade_required, msg = check_version(__version__, server)\n            creds[\"updated_at\"] = datetime.datetime.now().strftime(DATE_FORMAT)\n\n            try:\n                json.dump(creds, open(creds_file, \"w\"))\n            except Exception as e:\n                if e.errno == errno.EACCES:\n                    click.echo(\n                        \"Please check the permissions on {}\".format(collapse_user(creds_file)),\n                        err=True,\n                    )\n                    sys.exit(1)\n                else:\n                    raise\n\n            if upgrade_required:\n                click.echo(\"\\nWARNING: {}\\n\".format(msg), err=True)\n\n        # finally, give the user back what they want (whether silent or not)\n        if silent:\n            return creds.get(\"api_key\", None)\n\n        click.echo(\n            \"Credentials file already exists ({}). Logout first.\".format(collapse_user(creds_file)),\n            err=True,\n        )\n        return creds.get(\"email\", None)\n\n    # creds_file was not found and we're not silent, so prompt user to login\n    email, api_key = login_uname_pwd(server, api_key=api_key)\n\n    if api_key is None:\n        click.echo(\n            \"We could not verify your credentials. Please check your email and password again. \"\n            \"If you have single sign-on (SSO) enabled, you must login with `onecodex --api-key $API_KEY login`. \"\n            \"Please contact help@onecodex.com if you continue to experience problems.\"\n        )\n        sys.exit(1)\n\n    creds.update(\n        {\n            \"api_key\": api_key,\n            \"saved_at\": datetime.datetime.now().strftime(DATE_FORMAT),\n            \"updated_at\": None,\n            \"email\": email,\n        }\n    )\n\n    try:\n        json.dump(creds, open(creds_file, \"w\"))\n    except Exception as e:\n        if e.errno == errno.EACCES:\n            click.echo(\"Please check the permissions on {}\".format(creds_file), err=True)\n            sys.exit(1)\n        else:\n            raise\n\n    click.echo(\"Your ~/.onecodex credentials file was successfully created.\", err=True)\n\n    return email\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& indices = context->input(0);\n    const Tensor& values = context->input(1);\n    const Tensor& shape = context->input(2);\n    const Tensor& weights = context->input(3);\n    bool use_weights = weights.NumElements() > 0;\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(indices.shape()),\n                errors::InvalidArgument(\n                    \"Input indices must be a 2-dimensional tensor. Got: \",\n                    indices.shape().DebugString()));\n\n    if (use_weights) {\n      OP_REQUIRES(\n          context, weights.shape() == values.shape(),\n          errors::InvalidArgument(\n              \"Weights and values must have the same shape. Weight shape: \",\n              weights.shape().DebugString(),\n              \"; values shape: \", values.shape().DebugString()));\n    }\n\n    bool is_1d = shape.NumElements() == 1;\n    int num_batches = is_1d ? 1 : shape.flat<int64>()(0);\n    int num_values = values.NumElements();\n\n    OP_REQUIRES(context, num_values == indices.shape().dim_size(0),\n                errors::InvalidArgument(\n                    \"Number of values must match first dimension of indices.\",\n                    \"Got \", num_values,\n                    \" values, indices shape: \", indices.shape().DebugString()));\n\n    const auto indices_values = indices.matrix<int64>();\n    const auto values_values = values.flat<T>();\n    const auto weight_values = weights.flat<W>();\n\n    auto per_batch_counts = BatchedMap<W>(num_batches);\n\n    T max_value = 0;\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      int batch = is_1d ? 0 : indices_values(idx, 0);\n      const auto& value = values_values(idx);\n      if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {\n        if (binary_output_) {\n          per_batch_counts[batch][value] = 1;\n        } else if (use_weights) {\n          per_batch_counts[batch][value] += weight_values(idx);\n        } else {\n          per_batch_counts[batch][value]++;\n        }\n        if (value > max_value) {\n          max_value = value;\n        }\n      }\n    }\n\n    int num_output_values = GetOutputSize(max_value, maxlength_, minlength_);\n    OP_REQUIRES_OK(context, OutputSparse<W>(per_batch_counts, num_output_values,\n                                            is_1d, context));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, verbose_name=None, name=None, verify_exists=True, **kwargs):\n        kwargs['max_length'] = kwargs.get('max_length', 200)\n        CharField.__init__(self, verbose_name, name, **kwargs)\n        self.validators.append(validators.URLValidator(verify_exists=verify_exists))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def from_unix_seconds(seconds: Union[int, float]) -> datetime:\n    while abs(seconds) > MS_WATERSHED:\n        seconds /= 1000\n    dt = EPOCH + timedelta(seconds=seconds)\n    return dt.replace(tzinfo=timezone.utc)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def savepoint(self, name: t.Optional[str] = None) -> Savepoint:\n        name = name or f\"savepoint_{self.get_savepoint_id()}\"\n        await self.connection.execute(f\"SAVEPOINT {name}\")\n        return Savepoint(name=name, transaction=self)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const TensorShape& input_shape = input.shape();\n    const int32_t input_dims = input_shape.dims();\n\n    const Tensor& segment_id = context->input(1);\n    const TensorShape& segment_id_shape = segment_id.shape();\n    const int32_t segment_dims = segment_id_shape.dims();\n\n    const Tensor& num_segments_tensor = context->input(2);\n    OP_REQUIRES(context, num_segments_tensor.NumElements() != 0,\n                errors::InvalidArgument(\"Number of segments cannot be empty.\"));\n    auto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();\n\n    OP_REQUIRES(\n        context, num_segments >= 0,\n        errors::InvalidArgument(\n            \"Number of segments must be non-negative but got \", num_segments));\n    OP_REQUIRES(context, segment_dims != 0,\n                errors::InvalidArgument(\"Segment_id cannot have rank 0\"));\n\n    OP_REQUIRES(\n        context, segment_dims <= input_dims,\n        errors::OutOfRange(\"Invalid segment_id rank \", segment_dims,\n                           \" for input with \", input_dims, \" dimension(s)\"));\n    for (auto i = 0; i < segment_dims; i++) {\n      OP_REQUIRES(\n          context, segment_id_shape.dim_size(i) == input_shape.dim_size(i),\n          errors::InvalidArgument(\n              \"Segment dimension is \", segment_id_shape.dim_size(i),\n              \" while input dimension is \", input_dims, \" in rank \", i));\n    }\n\n    // Making output tensor.\n    Tensor* output_tensor = nullptr;\n    TensorShape output_shape =\n        GetOutputShape(input_shape, segment_id_shape, num_segments);\n    OP_REQUIRES_OK(context, context->allocate_output(\"output\", output_shape,\n                                                     &output_tensor));\n\n    // Preparating flat tensors.\n    auto output_flat = output_tensor->flat<tstring>();\n    auto flat_segment_id = segment_id.flat<INDICES_TYPE>();\n    auto flat_input = input.flat<tstring>();\n\n    for (int i = 0; i < flat_segment_id.size(); i++) {\n      OP_REQUIRES(\n          context,\n          ((flat_segment_id(i) < num_segments) && (flat_segment_id(i) >= 0)),\n          errors::InvalidArgument(\n              \"segment_ids are not allowed to exceed num_segments or\"\n              \" to have negative values.\"));\n    }\n\n    int64_t big_stride;\n    int64_t small_stride;\n    std::tie(big_stride, small_stride) =\n        GetStrides<INDICES_TYPE>(input_shape, segment_id_shape);\n    auto relative_offset_set =\n        GetFlattenedRelativeOffsets<INDICES_TYPE>(small_stride, big_stride);\n    for (auto start_offset = 0; start_offset < big_stride; start_offset++) {\n      for (auto i = 0; i < relative_offset_set.size(); i++) {\n        auto output_index = start_offset + flat_segment_id(i) * big_stride;\n        auto offset = start_offset + relative_offset_set[i];\n        if (output_flat(output_index).length() != 0)\n          output_flat(output_index).append(separator_.c_str());\n        output_flat(output_index).append(flat_input(offset));\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ModelWeights::Initialize(OpKernelContext* const context) {\n  OpInputList sparse_indices_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"sparse_indices\", &sparse_indices_inputs));\n  OpInputList sparse_weights_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"sparse_weights\", &sparse_weights_inputs));\n  OpInputList dense_weights_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"dense_weights\", &dense_weights_inputs));\n\n  OpOutputList sparse_weights_outputs;\n  TF_RETURN_IF_ERROR(context->output_list(\"out_delta_sparse_weights\",\n                                          &sparse_weights_outputs));\n\n  OpOutputList dense_weights_outputs;\n  TF_RETURN_IF_ERROR(\n      context->output_list(\"out_delta_dense_weights\", &dense_weights_outputs));\n\n  for (int i = 0; i < sparse_weights_inputs.size(); ++i) {\n    Tensor* delta_t;\n    TF_RETURN_IF_ERROR(sparse_weights_outputs.allocate(\n        i, sparse_weights_inputs[i].shape(), &delta_t));\n    // Convert the input vector to a row matrix in internal representation.\n    auto deltas = delta_t->shaped<float, 2>({1, delta_t->NumElements()});\n    deltas.setZero();\n    sparse_weights_.emplace_back(FeatureWeightsSparseStorage{\n        sparse_indices_inputs[i].flat<int64>(),\n        sparse_weights_inputs[i].shaped<float, 2>(\n            {1, sparse_weights_inputs[i].NumElements()}),\n        deltas});\n  }\n\n  // Reads in the weights, and allocates and initializes the delta weights.\n  const auto initialize_weights =\n      [&](const OpInputList& weight_inputs, OpOutputList* const weight_outputs,\n          std::vector<FeatureWeightsDenseStorage>* const feature_weights) {\n        for (int i = 0; i < weight_inputs.size(); ++i) {\n          Tensor* delta_t;\n          TF_RETURN_IF_ERROR(\n              weight_outputs->allocate(i, weight_inputs[i].shape(), &delta_t));\n          // Convert the input vector to a row matrix in internal\n          // representation.\n          auto deltas = delta_t->shaped<float, 2>({1, delta_t->NumElements()});\n          deltas.setZero();\n          feature_weights->emplace_back(FeatureWeightsDenseStorage{\n              weight_inputs[i].shaped<float, 2>(\n                  {1, weight_inputs[i].NumElements()}),\n              deltas});\n        }\n        return Status::OK();\n      };\n\n  return initialize_weights(dense_weights_inputs, &dense_weights_outputs,\n                            &dense_weights_);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _test_setup_modules(authn=\"authn_testing\",authz=\"authz_allowall\",pxe_once=1):\n\n    # rewrite modules.conf so we know we can use the testing module\n    # for xmlrpc rw testing (Makefile will put the user value back)\n    \n    import yaml\n    import Cheetah.Template as Template\n\n    MODULES_TEMPLATE = \"installer_templates/modules.conf.template\"\n    DEFAULTS = \"installer_templates/defaults\"\n    fh = open(DEFAULTS)\n    data = yaml.safe_load(fh.read())\n    fh.close()\n    data[\"authn_module\"] = authn\n    data[\"authz_module\"] = authz\n    data[\"pxe_once\"] = pxe_once\n    \n    t = Template.Template(file=MODULES_TEMPLATE, searchList=[data])\n    open(\"/etc/cobbler/modules.conf\",\"w+\").write(t.respond())\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_users(self, email):\n        \"\"\"Given an email, return matching user(s) who should receive a reset.\n\n        This allows subclasses to more easily customize the default policies\n        that prevent inactive users and users with unusable passwords from\n        resetting their password.\n        \"\"\"\n        email_field_name = UserModel.get_email_field_name()\n        active_users = UserModel._default_manager.filter(**{\n            '%s__iexact' % email_field_name: email,\n            'is_active': True,\n        })\n        return (\n            u for u in active_users\n            if u.has_usable_password() and\n            _unicode_ci_compare(email, getattr(u, email_field_name))\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def reset_metric_variables(self) -> None:\n    \"\"\"Reset metric variable values to their initial values.\"\"\"\n    with self._lock:\n      self._reset_metric_variables()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // inputs must be at least a matrix\n    OP_REQUIRES(\n        ctx, sorted_inputs_t.shape().dims() >= 2,\n        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n    OP_REQUIRES_OK(\n        ctx, functor::UpperBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def repercent_broken_unicode(path):\n    \"\"\"\n    As per section 3.2 of RFC 3987, step three of converting a URI into an IRI,\n    repercent-encode any octet produced that is not part of a strictly legal\n    UTF-8 octet sequence.\n    \"\"\"\n    changed_parts = []\n    while True:\n        try:\n            path.decode()\n        except UnicodeDecodeError as e:\n            # CVE-2019-14235: A recursion shouldn't be used since the exception\n            # handling uses massive amounts of memory\n            repercent = quote(path[e.start : e.end], safe=b\"/#%[]=:;$&()+,!?*@'~\")\n            changed_parts.append(path[: e.start] + repercent.encode())\n            path = path[e.end :]\n        else:\n            return b\"\".join(changed_parts) + path\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "export function getSessionUrl(baseUrl: string, id: string): string {\n  const servicesBase = URLExt.join(baseUrl, SESSION_SERVICE_URL);\n  const result = URLExt.join(servicesBase, id);\n  if (!result.startsWith(servicesBase)) {\n    throw new Error('Can only be used for services requests');\n  }\n  return result;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_model_version_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    name = request_dict.get(\"name\")\n    version = request_dict.get(\"version\")\n    path = request_dict[\"path\"]\n    validate_path_is_safe(path)\n    artifact_uri = _get_model_registry_store().get_model_version_download_uri(name, version)\n    if _is_servable_proxied_run_artifact_root(artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_repo = get_artifact_repository(artifact_uri)\n        artifact_path = path\n\n    return _send_artifact(artifact_repo, artifact_path)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def delete_service(self, context, service_id):\n        self.assert_admin(context)\n        service_ref = self.catalog_api.get_service(context, service_id)\n        if not service_ref:\n            raise exception.ServiceNotFound(service_id=service_id)\n        self.catalog_api.delete_service(context, service_id)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def wrapper(self, *args, **kwargs):\n        try:\n            result = method(self, *args, **kwargs)\n        except web.HTTPError as e:\n            status = e.status_code\n            message = e.log_message\n            self.log.warn(message)\n            self.set_status(e.status_code)\n            self.set_header('Content-Type', 'application/json')\n            self.finish(json.dumps(dict(message=message)))\n        except Exception:\n            self.log.error(\"Unhandled error in API request\", exc_info=True)\n            status = 500\n            message = \"Unknown server error\"\n            t, value, tb = sys.exc_info()\n            self.set_status(status)\n            tb_text = ''.join(traceback.format_exception(t, value, tb))\n            reply = dict(message=message, traceback=tb_text)\n            self.set_header('Content-Type', 'application/json')\n            self.finish(json.dumps(reply))\n        else:\n            return result\n    return wrapper"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create(self, request, *args, **kwargs):\n        \"\"\"Create a new StockItem object via the API.\n\n        We override the default 'create' implementation.\n\n        If a location is *not* specified, but the linked *part* has a default location,\n        we can pre-fill the location automatically.\n        \"\"\"\n        user = request.user\n\n        # Copy the request data, to side-step \"mutability\" issues\n        data = OrderedDict()\n        # Update with cleaned input data\n        data.update(self.clean_data(request.data))\n\n        quantity = data.get('quantity', None)\n\n        if quantity is None:\n            raise ValidationError({\n                'quantity': _('Quantity is required'),\n            })\n\n        try:\n            part = Part.objects.get(pk=data.get('part', None))\n        except (ValueError, Part.DoesNotExist):\n            raise ValidationError({\n                'part': _('Valid part must be supplied'),\n            })\n\n        # Set default location (if not provided)\n        if 'location' not in data:\n            location = part.get_default_location()\n\n            if location:\n                data['location'] = location.pk\n\n        # An expiry date was *not* specified - try to infer it!\n        if 'expiry_date' not in data and part.default_expiry > 0:\n            data['expiry_date'] = datetime.now().date() + timedelta(days=part.default_expiry)\n\n        # Attempt to extract serial numbers from submitted data\n        serials = None\n\n        # Check if a set of serial numbers was provided\n        serial_numbers = data.get('serial_numbers', '')\n\n        # Assign serial numbers for a trackable part\n        if serial_numbers:\n\n            if not part.trackable:\n                raise ValidationError({\n                    'serial_numbers': [_(\"Serial numbers cannot be supplied for a non-trackable part\")]\n                })\n\n            # If serial numbers are specified, check that they match!\n            try:\n                serials = extract_serial_numbers(serial_numbers, quantity, part.getLatestSerialNumberInt())\n\n                # Determine if any of the specified serial numbers already exist!\n                existing = []\n\n                for serial in serials:\n                    if part.checkIfSerialNumberExists(serial):\n                        existing.append(serial)\n\n                if len(existing) > 0:\n\n                    msg = _(\"The following serial numbers already exist\")\n                    msg += \" : \"\n                    msg += \",\".join([str(e) for e in existing])\n\n                    raise ValidationError({\n                        'serial_numbers': [msg],\n                    })\n\n            except DjangoValidationError as e:\n                raise ValidationError({\n                    'quantity': e.messages,\n                    'serial_numbers': e.messages,\n                })\n\n        if serials is not None:\n            \"\"\"If the stock item is going to be serialized, set the quantity to 1.\"\"\"\n            data['quantity'] = 1\n\n        # De-serialize the provided data\n        serializer = self.get_serializer(data=data)\n        serializer.is_valid(raise_exception=True)\n\n        with transaction.atomic():\n\n            # Create an initial StockItem object\n            item = serializer.save()\n\n            if serials:\n                # Assign the first serial number to the \"master\" item\n                item.serial = serials[0]\n\n            # Save the item (with user information)\n            item.save(user=user)\n\n            if serials:\n                for serial in serials[1:]:\n\n                    # Create a duplicate stock item with the next serial number\n                    item.pk = None\n                    item.serial = serial\n\n                    item.save(user=user)\n\n                response_data = {\n                    'quantity': quantity,\n                    'serial_numbers': serials,\n                }\n\n            else:\n                response_data = serializer.data\n\n            return Response(response_data, status=status.HTTP_201_CREATED, headers=self.get_success_headers(serializer.data))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def respond_error(self, context, exception):\n        context.respond_server_error()\n        stack = traceback.format_exc()\n        return \"\"\"\n        <html>\n            <body>\n\n                <style>\n                    body {\n                        font-family: sans-serif;\n                        color: #888;\n                        text-align: center;\n                    }\n\n                    body pre {\n                        width: 600px;\n                        text-align: left;\n                        margin: auto;\n                        font-family: monospace;\n                    }\n                </style>\n\n                <img src=\"/ajenti:static/main/error.jpeg\" />\n                <br/>\n                <p>\n                    Server error\n                </p>\n                <pre>\n%s\n                </pre>\n            </body>\n        </html>\n        \"\"\" % cgi.escape(stack)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, database_url):\n        self._ss = SAML2SessionStore(database_url=database_url)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def cpu_profile(\n        self, pid: int, format=\"flamegraph\", duration: float = 5, native: bool = False\n    ) -> (bool, str):\n        pyspy = shutil.which(\"py-spy\")\n        if pyspy is None:\n            return False, \"py-spy is not installed\"\n\n        if format not in (\"flamegraph\", \"raw\", \"speedscope\"):\n            return (\n                False,\n                f\"Invalid format {format}, \" + \"must be [flamegraph, raw, speedscope]\",\n            )\n\n        if format == \"flamegraph\":\n            extension = \"svg\"\n        else:\n            extension = \"txt\"\n        profile_file_path = (\n            self.profile_dir_path / f\"{format}_{pid}_cpu_profiling.{extension}\"\n        )\n        cmd = [\n            pyspy,\n            \"record\",\n            \"-o\",\n            profile_file_path,\n            \"-p\",\n            str(pid),\n            \"-d\",\n            str(duration),\n            \"-f\",\n            format,\n        ]\n        if sys.platform == \"linux\" and native:\n            cmd.append(\"--native\")\n        if await _can_passwordless_sudo():\n            cmd = [\"sudo\", \"-n\"] + cmd\n        process = await asyncio.create_subprocess_exec(\n            *cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        stdout, stderr = await process.communicate()\n        if process.returncode != 0:\n            return False, _format_failed_pyspy_command(cmd, stdout, stderr)\n        else:\n            return True, open(profile_file_path, \"rb\").read()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_context(self, name, value, attrs):\n        context = super().get_context(name, value, attrs)\n        rel_to = self.rel.model\n        if rel_to in self.admin_site._registry:\n            # The related object is registered with the same AdminSite\n            related_url = reverse(\n                'admin:%s_%s_changelist' % (\n                    rel_to._meta.app_label,\n                    rel_to._meta.model_name,\n                ),\n                current_app=self.admin_site.name,\n            )\n\n            params = self.url_parameters()\n            if params:\n                related_url += '?' + urlencode(params)\n            context['related_url'] = related_url\n            context['link_title'] = _('Lookup')\n            # The JavaScript code looks for this class.\n            context['widget']['attrs'].setdefault('class', 'vForeignKeyRawIdAdminField')\n        else:\n            context['related_url'] = None\n        if context['widget']['value']:\n            context['link_label'], context['link_url'] = self.label_and_url_for_value(value)\n        else:\n            context['link_label'] = None\n        return context\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_user_roles(self, context, user_id, tenant_id=None):\n        \"\"\"Get the roles for a user and tenant pair.\n\n        Since we're trying to ignore the idea of user-only roles we're\n        not implementing them in hopes that the idea will die off.\n\n        \"\"\"\n        self.assert_admin(context)\n        if tenant_id is None:\n            raise exception.NotImplemented(message='User roles not supported: '\n                                                   'tenant ID required')\n\n        user = self.identity_api.get_user(context, user_id)\n        if user is None:\n            raise exception.UserNotFound(user_id=user_id)\n        tenant = self.identity_api.get_tenant(context, tenant_id)\n        if tenant is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        roles = self.identity_api.get_roles_for_user_and_tenant(\n                context, user_id, tenant_id)\n        return {'roles': [self.identity_api.get_role(context, x)\n                          for x in roles]}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& indices = context->input(0);\n    const Tensor& values = context->input(1);\n    const Tensor& shape = context->input(2);\n    const Tensor& weights = context->input(3);\n    bool use_weights = weights.NumElements() > 0;\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(indices.shape()),\n                errors::InvalidArgument(\n                    \"Input indices must be a 2-dimensional tensor. Got: \",\n                    indices.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(values.shape()),\n                errors::InvalidArgument(\"Input values must be a vector. Got: \",\n                                        values.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(shape.shape()),\n                errors::InvalidArgument(\"Input shape must be a vector. Got: \",\n                                        shape.shape().DebugString()));\n    OP_REQUIRES(context,\n                values.shape().dim_size(0) == indices.shape().dim_size(0),\n                errors::InvalidArgument(\n                    \"Number of values must match first dimension of indices.\",\n                    \"Got \", values.shape().dim_size(0),\n                    \" values, indices shape: \", indices.shape().DebugString()));\n    OP_REQUIRES(\n        context, shape.shape().dim_size(0) == indices.shape().dim_size(1),\n        errors::InvalidArgument(\n            \"Number of dimensions must match second dimension of indices.\",\n            \"Got \", shape.shape().dim_size(0),\n            \" dimensions, indices shape: \", indices.shape().DebugString()));\n    OP_REQUIRES(context, shape.NumElements() > 0,\n                errors::InvalidArgument(\n                    \"The shape argument requires at least one element.\"));\n\n    if (use_weights) {\n      OP_REQUIRES(\n          context, weights.shape() == values.shape(),\n          errors::InvalidArgument(\n              \"Weights and values must have the same shape. Weight shape: \",\n              weights.shape().DebugString(),\n              \"; values shape: \", values.shape().DebugString()));\n    }\n\n    bool is_1d = shape.NumElements() == 1;\n    auto shape_vector = shape.flat<int64_t>();\n    int num_batches = is_1d ? 1 : shape_vector(0);\n    int num_values = values.NumElements();\n\n    const auto indices_values = indices.matrix<int64_t>();\n    const auto values_values = values.flat<T>();\n    const auto weight_values = weights.flat<W>();\n\n    auto per_batch_counts = BatchedMap<W>(num_batches);\n\n    T max_value = 0;\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      int batch = is_1d ? 0 : indices_values(idx, 0);\n      if (batch >= num_batches) {\n        OP_REQUIRES(context, batch < num_batches,\n                    errors::InvalidArgument(\n                        \"Indices value along the first dimension must be \",\n                        \"lower than the first index of the shape.\", \"Got \",\n                        batch, \" as batch and \", num_batches,\n                        \" as the first dimension of the shape.\"));\n      }\n      const auto& value = values_values(idx);\n      if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {\n        if (binary_output_) {\n          per_batch_counts[batch][value] = 1;\n        } else if (use_weights) {\n          per_batch_counts[batch][value] += weight_values(idx);\n        } else {\n          per_batch_counts[batch][value]++;\n        }\n        if (value > max_value) {\n          max_value = value;\n        }\n      }\n    }\n\n    int num_output_values = GetOutputSize(max_value, maxlength_, minlength_);\n    OP_REQUIRES_OK(context, OutputSparse<W>(per_batch_counts, num_output_values,\n                                            is_1d, context));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_valid_filename(name):\n    \"\"\"\n    Return the given string converted to a string that can be used for a clean\n    filename. Remove leading and trailing spaces; convert other spaces to\n    underscores; and remove anything that is not an alphanumeric, dash,\n    underscore, or dot.\n    >>> get_valid_filename(\"john's portrait in 2004.jpg\")\n    'johns_portrait_in_2004.jpg'\n    \"\"\"\n    s = str(name).strip().replace(' ', '_')\n    s = re.sub(r'(?u)[^-\\w.]', '', s)\n    if s in {'', '.', '..'}:\n        raise SuspiciousFileOperation(\"Could not derive file name from '%s'\" % name)\n    return s\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def main():\n    global logger\n    result = 0\n\n    parser = argparse.ArgumentParser(description='Keycloak REST client',\n                    prog=prog_name,\n                    epilog=verbose_help.format(prog_name=prog_name),\n                    formatter_class=argparse.RawDescriptionHelpFormatter)\n\n    parser.add_argument('-v', '--verbose', action='store_true',\n                        help='be chatty')\n\n    parser.add_argument('-d', '--debug', action='store_true',\n                        help='turn on debug info')\n\n    parser.add_argument('--show-traceback', action='store_true',\n                        help='exceptions print traceback in addition to '\n                             'error message')\n\n    parser.add_argument('--log-file',\n                        default='/tmp/{prog_name}.log'.format(\n                            prog_name=prog_name),\n                        help='log file pathname')\n\n    parser.add_argument('--permit-insecure-transport',  action='store_true',\n                        help='Normally secure transport such as TLS '\n                        'is required, defeat this check')\n\n    parser.add_argument('--tls-verify', action=TlsVerifyAction,\n                        default=True,\n                        help='TLS certificate verification for requests to'\n                        ' the server. May be one of case insenstive '\n                        '[true, yes, on] to enable,'\n                        '[false, no, off] to disable.'\n                        'Or the pathname to a OpenSSL CA bundle to use.'\n                        ' Default is True.')\n\n    group = parser.add_argument_group('Server')\n\n    group.add_argument('-s', '--server',\n                       required=True,\n                       help='DNS name or IP address of Keycloak server')\n\n    group.add_argument('-a', '--auth-role',\n                       choices=AUTH_ROLES,\n                       default='root-admin',\n                       help='authenticating as what type of user (default: root-admin)')\n\n    group.add_argument('-u', '--admin-username',\n                       default='admin',\n                       help='admin user name (default: admin)')\n\n    group.add_argument('-P', '--admin-password-file',\n                       type=argparse.FileType('rb'),\n                       help=('file containing admin password '\n                             '(or use a hyphen \"-\" to read the password '\n                             'from stdin)'))\n\n    group.add_argument('--admin-realm',\n                       default='master',\n                       help='realm admin belongs to')\n\n    cmd_parsers = parser.add_subparsers(help='available commands')\n\n    # --- realm commands ---\n    realm_parser = cmd_parsers.add_parser('realm',\n                                          help='realm operations')\n\n    sub_parser = realm_parser.add_subparsers(help='realm commands')\n\n    cmd_parser = sub_parser.add_parser('server_info',\n                                       help='dump server info')\n    cmd_parser.set_defaults(func=do_server_info)\n\n    cmd_parser = sub_parser.add_parser('list',\n                                       help='list realm names')\n    cmd_parser.set_defaults(func=do_list_realms)\n\n    cmd_parser = sub_parser.add_parser('create',\n                                       help='create new realm')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_create_realm)\n\n    cmd_parser = sub_parser.add_parser('delete',\n                                       help='delete existing realm')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_delete_realm)\n\n    cmd_parser = sub_parser.add_parser('metadata',\n                                       help='retrieve realm metadata')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_get_realm_metadata)\n\n    # --- client commands ---\n    client_parser = cmd_parsers.add_parser('client',\n                                           help='client operations')\n\n    sub_parser = client_parser.add_subparsers(help='client commands')\n\n    cmd_parser = sub_parser.add_parser('list',\n                                       help='list client names')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n\n    cmd_parser.set_defaults(func=do_list_clients)\n\n    cmd_parser = sub_parser.add_parser('create',\n                                       help='create new client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-m', '--metadata', type=argparse.FileType('rb'),\n                            required=True,\n                            help='SP metadata file or stdin')\n    cmd_parser.set_defaults(func=do_create_client)\n\n    cmd_parser = sub_parser.add_parser('register',\n                                       help='register new client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-m', '--metadata', type=argparse.FileType('rb'),\n                            required=True,\n                            help='SP metadata file or stdin')\n    cmd_parser.add_argument('--initial-access-token', required=True,\n                            help='realm initial access token for '\n                            'client registeration')\n    cmd_parser.set_defaults(func=do_register_client)\n\n    cmd_parser = sub_parser.add_parser('delete',\n                                       help='delete existing client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-c', '--client-name', required=True,\n                            help='client name')\n    cmd_parser.set_defaults(func=do_delete_client)\n\n    cmd_parser = sub_parser.add_parser('test',\n                                       help='experimental test used during '\n                                       'development')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-c', '--client-name', required=True,\n                            help='client name')\n    cmd_parser.set_defaults(func=do_client_test)\n\n    # Process command line arguments\n    options = parser.parse_args()\n    configure_logging(options)\n\n    if options.permit_insecure_transport:\n        os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'\n\n    # Get admin password\n    options.admin_password = None\n\n    # 1. Try password file\n    if options.admin_password_file is not None:\n        options.admin_password = options.keycloak_admin_password_file.readline().strip()\n        options.keycloak_admin_password_file.close()\n\n    # 2. Try KEYCLOAK_ADMIN_PASSWORD environment variable\n    if options.admin_password is None:\n        if (('KEYCLOAK_ADMIN_PASSWORD' in os.environ) and\n            (os.environ['KEYCLOAK_ADMIN_PASSWORD'])):\n            options.admin_password = os.environ['KEYCLOAK_ADMIN_PASSWORD']\n\n    try:\n        anonymous_conn = KeycloakAnonymousConnection(options.server,\n                                                     options.tls_verify)\n\n        admin_conn = KeycloakAdminConnection(options.server,\n                                             options.auth_role,\n                                             options.admin_realm,\n                                             ADMIN_CLIENT_ID,\n                                             options.admin_username,\n                                             options.admin_password,\n                                             options.tls_verify)\n    except Exception as e:\n        if options.show_traceback:\n            traceback.print_exc()\n        print(six.text_type(e), file=sys.stderr)\n        result = 1\n        return result\n\n    try:\n        if options.func == do_register_client:\n            conn = admin_conn\n        else:\n            conn = admin_conn\n        result = options.func(options, conn)\n    except Exception as e:\n        if options.show_traceback:\n            traceback.print_exc()\n        print(six.text_type(e), file=sys.stderr)\n        result = 2\n        return result\n\n    return result\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def read_config(self, config, **kwargs):\n        # FIXME: federation_domain_whitelist needs sytests\n        self.federation_domain_whitelist = None  # type: Optional[dict]\n        federation_domain_whitelist = config.get(\"federation_domain_whitelist\", None)\n\n        if federation_domain_whitelist is not None:\n            # turn the whitelist into a hash for speed of lookup\n            self.federation_domain_whitelist = {}\n\n            for domain in federation_domain_whitelist:\n                self.federation_domain_whitelist[domain] = True\n\n        ip_range_blacklist = config.get(\"ip_range_blacklist\", [])\n\n        # Attempt to create an IPSet from the given ranges\n        try:\n            self.ip_range_blacklist = IPSet(ip_range_blacklist)\n        except Exception as e:\n            raise ConfigError(\"Invalid range(s) provided in ip_range_blacklist: %s\" % e)\n        # Always blacklist 0.0.0.0, ::\n        self.ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n\n        # The federation_ip_range_blacklist is used for backwards-compatibility\n        # and only applies to federation and identity servers. If it is not given,\n        # default to ip_range_blacklist.\n        federation_ip_range_blacklist = config.get(\n            \"federation_ip_range_blacklist\", ip_range_blacklist\n        )\n        try:\n            self.federation_ip_range_blacklist = IPSet(federation_ip_range_blacklist)\n        except Exception as e:\n            raise ConfigError(\n                \"Invalid range(s) provided in federation_ip_range_blacklist: %s\" % e\n            )\n        # Always blacklist 0.0.0.0, ::\n        self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n\n        federation_metrics_domains = config.get(\"federation_metrics_domains\") or []\n        validate_config(\n            _METRICS_FOR_DOMAINS_SCHEMA,\n            federation_metrics_domains,\n            (\"federation_metrics_domains\",),\n        )\n        self.federation_metrics_domains = set(federation_metrics_domains)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def readNextEndLine(self, stream, limit_offset=0):\n        debug = False\n        if debug: print(\">>readNextEndLine\")\n        line_parts = []\n        while True:\n            # Prevent infinite loops in malformed PDFs\n            if stream.tell() == 0 or stream.tell() == limit_offset:\n                raise PdfReadError(\"Could not read malformed PDF file\")\n            x = stream.read(1)\n            if debug: print((\"  x:\", x, \"%x\"%ord(x)))\n            if stream.tell() < 2:\n                raise PdfReadError(\"EOL marker not found\")\n            stream.seek(-2, 1)\n            if x == b_('\\n') or x == b_('\\r'): ## \\n = LF; \\r = CR\n                crlf = False\n                while x == b_('\\n') or x == b_('\\r'):\n                    if debug:\n                        if ord(x) == 0x0D: print(\"  x is CR 0D\")\n                        elif ord(x) == 0x0A: print(\"  x is LF 0A\")\n                    x = stream.read(1)\n                    if x == b_('\\n') or x == b_('\\r'): # account for CR+LF\n                        stream.seek(-1, 1)\n                        crlf = True\n                    if stream.tell() < 2:\n                        raise PdfReadError(\"EOL marker not found\")\n                    stream.seek(-2, 1)\n                stream.seek(2 if crlf else 1, 1) # if using CR+LF, go back 2 bytes, else 1\n                break\n            else:\n                if debug: print(\"  x is neither\")\n                line_parts.append(x)\n        if debug: print(\"leaving RNEL\")\n        line_parts.reverse()\n        return b\"\".join(line_parts)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse_index_server(cls, line):\n        \"\"\"\n\n        :param line:\n        :return:\n        \"\"\"\n        groups = re.split(pattern=\"[=\\s]+\", string=line.strip(), maxsplit=100)\n\n        if len(groups) >= 2:\n            return groups[1] if groups[1].endswith(\"/\") else groups[1] + \"/\"\n\n        return None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool IsHostMemoryArg(const EagerOperation& op, const NodeDef* node_def,\n                     const Device* op_device, const KernelDef* kernel_def,\n                     const int port_id) {\n  if (op.is_function()) return false;\n  if (node_def == nullptr) return false;\n  if (kernel_def == nullptr || op_device == nullptr) return false;\n  const auto& host_memory_args = kernel_def->host_memory_arg();\n  const OpDef& op_def = OpRegistry::Global()->LookUp(op.Name())->op_def;\n  const int arg_id = OpPortIdToArgId(*node_def, op_def.input_arg(), port_id);\n  // Fail if argument ID not found.\n  if (arg_id < 0) {\n    return false;\n  }\n  return std::find(host_memory_args.begin(), host_memory_args.end(),\n                   op_def.input_arg(arg_id).name()) != host_memory_args.end();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def deferred_init(self) -> None:\n        all_vols = self.asrv.vfs.all_vols\n        have_e2d = self.init_indexes(all_vols, [], False)\n\n        if self.stop:\n            # up-mt consistency not guaranteed if init is interrupted;\n            # drop caches for a full scan on next boot\n            with self.mutex:\n                self._drop_caches()\n\n            if self.pp:\n                self.pp.end = True\n                self.pp = None\n\n            return\n\n        if not self.pp and self.args.exit == \"idx\":\n            return self.hub.sigterm()\n\n        Daemon(self._snapshot, \"up2k-snapshot\")\n        if have_e2d:\n            Daemon(self._hasher, \"up2k-hasher\")\n            Daemon(self._sched_rescan, \"up2k-rescan\")\n            if self.mtag:\n                for n in range(max(1, self.args.mtag_mt)):\n                    Daemon(self._tagger, \"tagger-{}\".format(n))\n\n                Daemon(self._run_all_mtp, \"up2k-mtp-init\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _is_javascript_scheme(s):\n    is_image_url = False\n    for image_type in _find_image_dataurls(s):\n        is_image_url = True\n        if _is_unsafe_image_type(image_type):\n            return True\n    if is_image_url:\n        return False\n    return bool(_is_possibly_malicious_scheme(s))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def decrypt_data(key, data):\n    \"\"\" Decrypt the data with the given secret key. \"\"\"\n    iv = data[:16]\n    cipher = AES.new(key, AES.MODE_CBC, iv)\n    try:\n        result = cipher.decrypt(data[16:])\n    except Exception:\n        raise DecryptError('Encrypted data appears to be corrupted.')\n\n    # Strip the last n padding bytes where n is the last value in\n    # the plaintext\n    padding = ord(result[-1])\n    return result[:-1 * padding]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _create_database(self, last_upgrade_to_run):\n        \"\"\"\n        Make sure that the database is created and has appropriate file permissions.\n        This should be done before storing any sensitive data in it.\n        \"\"\"\n        # Prepare a file\n        fd = os.open(\n            self.filename,\n            flags=os.O_RDWR | os.O_CREAT,\n            mode=stat.S_IRUSR | stat.S_IWUSR,\n        )\n        os.close(fd)\n        # Create the tables in the database\n        conn = self._connect()\n        try:\n            with conn:\n                self._create_tables(conn, last_upgrade_to_run)\n        finally:\n            conn.close()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _on_load_finished(self, ok):\n        if sip.isdeleted(self._widget):\n            # https://github.com/qutebrowser/qutebrowser/issues/3498\n            return\n\n        sess_manager = objreg.get('session-manager')\n        sess_manager.save_autosave()\n\n        if ok:\n            if self.url().scheme() == 'https':\n                if self.url().host() in self._insecure_hosts:\n                    self._set_load_status(usertypes.LoadStatus.warn)\n                else:\n                    self._set_load_status(usertypes.LoadStatus.success_https)\n            else:\n                self._set_load_status(usertypes.LoadStatus.success)\n        elif ok:\n            self._set_load_status(usertypes.LoadStatus.warn)\n        else:\n            self._set_load_status(usertypes.LoadStatus.error)\n\n        self.load_finished.emit(ok)\n\n        if not self.title():\n            self.title_changed.emit(self.url().toDisplayString())\n\n        self.zoom.set_current()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def feed_data(\n        self,\n        data: bytes,\n        SEP: bytes = b\"\\r\\n\",\n        EMPTY: bytes = b\"\",\n        CONTENT_LENGTH: istr = hdrs.CONTENT_LENGTH,\n        METH_CONNECT: str = hdrs.METH_CONNECT,\n        SEC_WEBSOCKET_KEY1: istr = hdrs.SEC_WEBSOCKET_KEY1,\n    ) -> Tuple[List[Tuple[_MsgT, StreamReader]], bool, bytes]:\n\n        messages = []\n\n        if self._tail:\n            data, self._tail = self._tail + data, b\"\"\n\n        data_len = len(data)\n        start_pos = 0\n        loop = self.loop\n\n        while start_pos < data_len:\n\n            # read HTTP message (request/response line + headers), \\r\\n\\r\\n\n            # and split by lines\n            if self._payload_parser is None and not self._upgraded:\n                pos = data.find(SEP, start_pos)\n                # consume \\r\\n\n                if pos == start_pos and not self._lines:\n                    start_pos = pos + 2\n                    continue\n\n                if pos >= start_pos:\n                    # line found\n                    self._lines.append(data[start_pos:pos])\n                    start_pos = pos + 2\n\n                    # \\r\\n\\r\\n found\n                    if self._lines[-1] == EMPTY:\n                        try:\n                            msg: _MsgT = self.parse_message(self._lines)\n                        finally:\n                            self._lines.clear()\n\n                        def get_content_length() -> Optional[int]:\n                            # payload length\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\n                            if length_hdr is None:\n                                return None\n\n                            # Shouldn't allow +/- or other number formats.\n                            # https://www.rfc-editor.org/rfc/rfc9110#section-8.6-2\n                            if not length_hdr.strip(\" \\t\").isdigit():\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            return int(length_hdr)\n\n                        length = get_content_length()\n                        # do not support old websocket spec\n                        if SEC_WEBSOCKET_KEY1 in msg.headers:\n                            raise InvalidHeader(SEC_WEBSOCKET_KEY1)\n\n                        self._upgraded = msg.upgrade\n\n                        method = getattr(msg, \"method\", self.method)\n\n                        assert self.protocol is not None\n                        # calculate payload\n                        if (\n                            (length is not None and length > 0)\n                            or msg.chunked\n                            and not msg.upgrade\n                        ):\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            payload_parser = HttpPayloadParser(\n                                payload,\n                                length=length,\n                                chunked=msg.chunked,\n                                method=method,\n                                compression=msg.compression,\n                                code=self.code,\n                                readall=self.readall,\n                                response_with_body=self.response_with_body,\n                                auto_decompress=self._auto_decompress,\n                            )\n                            if not payload_parser.done:\n                                self._payload_parser = payload_parser\n                        elif method == METH_CONNECT:\n                            assert isinstance(msg, RawRequestMessage)\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            self._upgraded = True\n                            self._payload_parser = HttpPayloadParser(\n                                payload,\n                                method=msg.method,\n                                compression=msg.compression,\n                                readall=True,\n                                auto_decompress=self._auto_decompress,\n                            )\n                        else:\n                            if (\n                                getattr(msg, \"code\", 100) >= 199\n                                and length is None\n                                and self.read_until_eof\n                            ):\n                                payload = StreamReader(\n                                    self.protocol,\n                                    timer=self.timer,\n                                    loop=loop,\n                                    limit=self._limit,\n                                )\n                                payload_parser = HttpPayloadParser(\n                                    payload,\n                                    length=length,\n                                    chunked=msg.chunked,\n                                    method=method,\n                                    compression=msg.compression,\n                                    code=self.code,\n                                    readall=True,\n                                    response_with_body=self.response_with_body,\n                                    auto_decompress=self._auto_decompress,\n                                )\n                                if not payload_parser.done:\n                                    self._payload_parser = payload_parser\n                            else:\n                                payload = EMPTY_PAYLOAD\n\n                        messages.append((msg, payload))\n                else:\n                    self._tail = data[start_pos:]\n                    data = EMPTY\n                    break\n\n            # no parser, just store\n            elif self._payload_parser is None and self._upgraded:\n                assert not self._lines\n                break\n\n            # feed payload\n            elif data and start_pos < data_len:\n                assert not self._lines\n                assert self._payload_parser is not None\n                try:\n                    eof, data = self._payload_parser.feed_data(data[start_pos:])\n                except BaseException as exc:\n                    if self.payload_exception is not None:\n                        self._payload_parser.payload.set_exception(\n                            self.payload_exception(str(exc))\n                        )\n                    else:\n                        self._payload_parser.payload.set_exception(exc)\n\n                    eof = True\n                    data = b\"\"\n\n                if eof:\n                    start_pos = 0\n                    data_len = len(data)\n                    self._payload_parser = None\n                    continue\n            else:\n                break\n\n        if data and start_pos < data_len:\n            data = data[start_pos:]\n        else:\n            data = EMPTY\n\n        return messages, self._upgraded, data\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _parse_qsl(qs):\n    r = []\n    for pair in qs.split('&'):\n        if not pair: continue\n        nv = pair.split('=', 1)\n        if len(nv) != 2: nv.append('')\n        key = urlunquote(nv[0].replace('+', ' '))\n        value = urlunquote(nv[1].replace('+', ' '))\n        r.append((key, value))\n    return r\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def authenticate(self, context, auth=None):\n        \"\"\"Authenticate credentials and return a token.\n\n        Accept auth as a dict that looks like::\n\n            {\n                \"auth\":{\n                    \"passwordCredentials\":{\n                        \"username\":\"test_user\",\n                        \"password\":\"mypass\"\n                    },\n                    \"tenantName\":\"customer-x\"\n                }\n            }\n\n        In this case, tenant is optional, if not provided the token will be\n        considered \"unscoped\" and can later be used to get a scoped token.\n\n        Alternatively, this call accepts auth with only a token and tenant\n        that will return a token that is scoped to that tenant.\n        \"\"\"\n\n        token_id = uuid.uuid4().hex\n        if 'passwordCredentials' in auth:\n            username = auth['passwordCredentials'].get('username', '')\n            password = auth['passwordCredentials'].get('password', '')\n            tenant_name = auth.get('tenantName', None)\n\n            user_id = auth['passwordCredentials'].get('userId', None)\n            if username:\n                user_ref = self.identity_api.get_user_by_name(\n                        context=context, user_name=username)\n                if user_ref:\n                    user_id = user_ref['id']\n\n            # more compat\n            tenant_id = auth.get('tenantId', None)\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                if tenant_ref:\n                    tenant_id = tenant_ref['id']\n\n            try:\n                auth_info = self.identity_api.authenticate(context=context,\n                                                           user_id=user_id,\n                                                           password=password,\n                                                           tenant_id=tenant_id)\n                (user_ref, tenant_ref, metadata_ref) = auth_info\n\n                # If the user is disabled don't allow them to authenticate\n                if not user_ref.get('enabled', True):\n                    LOG.warning('User %s is disabled' % user_id)\n                    raise exception.Unauthorized()\n\n                # If the tenant is disabled don't allow them to authenticate\n                if tenant_ref and not tenant_ref.get('enabled', True):\n                    LOG.warning('Tenant %s is disabled' % tenant_id)\n                    raise exception.Unauthorized()\n            except AssertionError as e:\n                raise exception.Unauthorized(e.message)\n\n            token_ref = self.token_api.create_token(\n                    context, token_id, dict(id=token_id,\n                                            user=user_ref,\n                                            tenant=tenant_ref,\n                                            metadata=metadata_ref))\n            if tenant_ref:\n                catalog_ref = self.catalog_api.get_catalog(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'],\n                        metadata=metadata_ref)\n            else:\n                catalog_ref = {}\n\n        elif 'token' in auth:\n            token = auth['token'].get('id', None)\n\n            tenant_name = auth.get('tenantName')\n\n            # more compat\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                tenant_id = tenant_ref['id']\n            else:\n                tenant_id = auth.get('tenantId', None)\n\n            try:\n                old_token_ref = self.token_api.get_token(context=context,\n                                                         token_id=token)\n            except exception.NotFound:\n                raise exception.Unauthorized()\n\n            user_ref = old_token_ref['user']\n\n            # If the user is disabled don't allow them to authenticate\n            current_user_ref = self.identity_api.get_user(\n                                                    context=context,\n                                                    user_id=user_ref['id'])\n            if not current_user_ref.get('enabled', True):\n                LOG.warning('User %s is disabled' % user_ref['id'])\n                raise exception.Unauthorized()\n\n            tenants = self.identity_api.get_tenants_for_user(context,\n                                                             user_ref['id'])\n            if tenant_id:\n                assert tenant_id in tenants\n\n            tenant_ref = self.identity_api.get_tenant(context=context,\n                                                      tenant_id=tenant_id)\n\n            # If the tenant is disabled don't allow them to authenticate\n            if tenant_ref and not tenant_ref.get('enabled', True):\n                LOG.warning('Tenant %s is disabled' % tenant_id)\n                raise exception.Unauthorized()\n\n            if tenant_ref:\n                metadata_ref = self.identity_api.get_metadata(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'])\n                catalog_ref = self.catalog_api.get_catalog(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'],\n                        metadata=metadata_ref)\n            else:\n                metadata_ref = {}\n                catalog_ref = {}\n\n            token_ref = self.token_api.create_token(\n                    context, token_id, dict(id=token_id,\n                                            user=user_ref,\n                                            tenant=tenant_ref,\n                                            metadata=metadata_ref,\n                                            expires=old_token_ref['expires']))\n\n        # TODO(termie): optimize this call at some point and put it into the\n        #               the return for metadata\n        # fill out the roles in the metadata\n        roles_ref = []\n        for role_id in metadata_ref.get('roles', []):\n            roles_ref.append(self.identity_api.get_role(context, role_id))\n        logging.debug('TOKEN_REF %s', token_ref)\n        return self._format_authenticate(token_ref, roles_ref, catalog_ref)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def values_from_list(values: List[str]) -> Dict[str, str]:\n        \"\"\" Convert a list of values into dict of {name: value} \"\"\"\n        output: Dict[str, str] = {}\n\n        for i, value in enumerate(values):\n            if value[0].isalpha():\n                key = value.upper()\n            else:\n                key = f\"VALUE_{i}\"\n            if key in output:\n                raise ValueError(f\"Duplicate key {key} in Enum\")\n            sanitized_key = utils.fix_keywords(utils.sanitize(key))\n            output[sanitized_key] = utils.remove_string_escapes(value)\n\n        return output\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, expression, delimiter, **extra):\n        delimiter_expr = Value(str(delimiter))\n        super().__init__(expression, delimiter_expr, **extra)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def urlize(text, trim_url_limit=None, nofollow=False, autoescape=False):\n    \"\"\"\n    Converts any URLs in text into clickable links.\n\n    Works on http://, https://, www. links, and also on links ending in one of\n    the original seven gTLDs (.com, .edu, .gov, .int, .mil, .net, and .org).\n    Links can have trailing punctuation (periods, commas, close-parens) and\n    leading punctuation (opening parens) and it'll still do the right thing.\n\n    If trim_url_limit is not None, the URLs in the link text longer than this\n    limit will be truncated to trim_url_limit-3 characters and appended with\n    an ellipsis.\n\n    If nofollow is True, the links will get a rel=\"nofollow\" attribute.\n\n    If autoescape is True, the link text and URLs will be autoescaped.\n    \"\"\"\n    safe_input = isinstance(text, SafeData)\n\n    def trim_url(x, limit=trim_url_limit):\n        if limit is None or len(x) <= limit:\n            return x\n        return '%s...' % x[:max(0, limit - 3)]\n\n    def unescape(text, trail):\n        \"\"\"\n        If input URL is HTML-escaped, unescape it so as we can safely feed it to\n        smart_urlquote. For example:\n        http://example.com?x=1&amp;y=&lt;2&gt; => http://example.com?x=1&y=<2>\n        \"\"\"\n        unescaped = (text + trail).replace(\n            '&amp;', '&').replace('&lt;', '<').replace(\n            '&gt;', '>').replace('&quot;', '\"').replace('&#39;', \"'\")\n        if trail and unescaped.endswith(trail):\n            # Remove trail for unescaped if it was not consumed by unescape\n            unescaped = unescaped[:-len(trail)]\n        elif trail == ';':\n            # Trail was consumed by unescape (as end-of-entity marker), move it to text\n            text += trail\n            trail = ''\n        return text, unescaped, trail\n\n    def is_email_simple(value):\n        \"\"\"Return True if value looks like an email address.\"\"\"\n        # An @ must be in the middle of the value.\n        if '@' not in value or value.startswith('@') or value.endswith('@'):\n            return False\n        try:\n            p1, p2 = value.split('@')\n        except ValueError:\n            # value contains more than one @.\n            return False\n        # Dot must be in p2 (e.g. example.com)\n        if '.' not in p2 or p2.startswith('.'):\n            return False\n        return True\n\n    words = word_split_re.split(force_text(text))\n    for i, word in enumerate(words):\n        if '.' in word or '@' in word or ':' in word:\n            # Deal with punctuation.\n            lead, middle, trail = '', word, ''\n            for punctuation in TRAILING_PUNCTUATION:\n                if middle.endswith(punctuation):\n                    middle = middle[:-len(punctuation)]\n                    trail = punctuation + trail\n            for opening, closing in WRAPPING_PUNCTUATION:\n                if middle.startswith(opening):\n                    middle = middle[len(opening):]\n                    lead = lead + opening\n                # Keep parentheses at the end only if they're balanced.\n                if (middle.endswith(closing)\n                        and middle.count(closing) == middle.count(opening) + 1):\n                    middle = middle[:-len(closing)]\n                    trail = closing + trail\n\n            # Make URL we want to point to.\n            url = None\n            nofollow_attr = ' rel=\"nofollow\"' if nofollow else ''\n            if simple_url_re.match(middle):\n                middle, middle_unescaped, trail = unescape(middle, trail)\n                url = smart_urlquote(middle_unescaped)\n            elif simple_url_2_re.match(middle):\n                middle, middle_unescaped, trail = unescape(middle, trail)\n                url = smart_urlquote('http://%s' % middle_unescaped)\n            elif ':' not in middle and is_email_simple(middle):\n                local, domain = middle.rsplit('@', 1)\n                try:\n                    domain = domain.encode('idna').decode('ascii')\n                except UnicodeError:\n                    continue\n                url = 'mailto:%s@%s' % (local, domain)\n                nofollow_attr = ''\n\n            # Make link.\n            if url:\n                trimmed = trim_url(middle)\n                if autoescape and not safe_input:\n                    lead, trail = escape(lead), escape(trail)\n                    trimmed = escape(trimmed)\n                middle = '<a href=\"%s\"%s>%s</a>' % (escape(url), nofollow_attr, trimmed)\n                words[i] = mark_safe('%s%s%s' % (lead, middle, trail))\n            else:\n                if safe_input:\n                    words[i] = mark_safe(word)\n                elif autoescape:\n                    words[i] = escape(word)\n        elif safe_input:\n            words[i] = mark_safe(word)\n        elif autoescape:\n            words[i] = escape(word)\n    return ''.join(words)\nurlize = allow_lazy(urlize, six.text_type)"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(0).shape()),\n                errors::InvalidArgument(\"dim must be a scalar\"));\n    int32_t dim = context->input(0).scalar<int32_t>()();\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(0).shape()),\n                errors::InvalidArgument(\"num_results must be a scalar\"));\n    int32_t num_results = context->input(1).scalar<int32_t>()();\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(0).shape()),\n                errors::InvalidArgument(\"skip must be a scalar\"));\n    int32_t skip = context->input(2).scalar<int32_t>()();\n\n    OP_REQUIRES(context, dim >= 1,\n                errors::InvalidArgument(\"dim must be at least one\"));\n    OP_REQUIRES(context, dim <= sobol_data::kMaxSobolDim,\n                errors::InvalidArgument(\"dim must be at most \",\n                                        sobol_data::kMaxSobolDim));\n    OP_REQUIRES(context, num_results >= 1,\n                errors::InvalidArgument(\"num_results must be at least one\"));\n    OP_REQUIRES(context, skip >= 0,\n                errors::InvalidArgument(\"skip must be non-negative\"));\n    OP_REQUIRES(context,\n                num_results < std::numeric_limits<int32_t>::max() - skip,\n                errors::InvalidArgument(\"num_results+skip must be less than \",\n                                        std::numeric_limits<int32_t>::max()));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\n                       0, TensorShape({num_results, dim}), &output));\n    auto output_flat = output->flat<T>();\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n    int num_threads = worker_threads.num_threads;\n    int block_size = std::max(\n        kMinBlockSize, static_cast<int>(std::ceil(\n                           static_cast<float>(num_results) / num_threads)));\n    worker_threads.workers->TransformRangeConcurrently(\n        block_size, num_results /* total */,\n        [&dim, &skip, &output_flat](const int start, const int end) {\n          CalculateSobolSample<T>(dim, end - start /* num_results */, skip,\n                                  start, output_flat);\n        });\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def save(request, name):\n  name = str_param('name', name)\n\n  if 'change' not in getPermissions(request.user):\n    return json_response( dict(error=\"Must be logged in with appropriate permissions to save\") )\n  # Deserialize and reserialize as a validation step\n  state = str( json.dumps( json.loads( request.POST['state'] ) ) )\n\n  try:\n    dashboard = Dashboard.objects.get(name=name)\n  except Dashboard.DoesNotExist:\n    dashboard = Dashboard.objects.create(name=name, state=state)\n  else:\n    dashboard.state = state\n    dashboard.save()\n\n  return json_response( dict(success=True) )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "std::vector<int64_t> CastPyArg2Longs(PyObject* obj,\n                                     const std::string& op_type,\n                                     ssize_t arg_pos) {\n  std::vector<int64_t> value;\n  if (PyList_Check(obj)) {\n    Py_ssize_t len = PyList_Size(obj);\n    PyObject* item = nullptr;\n    for (Py_ssize_t i = 0; i < len; i++) {\n      item = PyList_GetItem(obj, i);\n      if (PyObject_CheckLongOrToLong(&item)) {\n        value.emplace_back((int64_t)PyLong_AsLongLong(item));\n      } else {\n        PADDLE_THROW(platform::errors::InvalidArgument(\n            \"%s(): argument (position %d) must be \"\n            \"list of int, but got %s at pos %d\",\n            op_type,\n            arg_pos + 1,\n            ((PyTypeObject*)item->ob_type)->tp_name,  // NOLINT\n            i));\n      }\n    }\n  } else if (PyTuple_Check(obj)) {\n    Py_ssize_t len = PyTuple_Size(obj);\n    PyObject* item = nullptr;\n    for (Py_ssize_t i = 0; i < len; i++) {\n      item = PyTuple_GetItem(obj, i);\n      if (PyObject_CheckLongOrToLong(&item)) {\n        value.emplace_back((int64_t)PyLong_AsLongLong(item));\n      } else {\n        PADDLE_THROW(platform::errors::InvalidArgument(\n            \"%s(): argument (position %d) must be \"\n            \"list of int, but got %s at pos %d\",\n            op_type,\n            arg_pos + 1,\n            ((PyTypeObject*)item->ob_type)->tp_name,  // NOLINT\n            i));\n      }\n    }\n  } else if (PySequence_Check(obj) && !PyObject_TypeCheck(obj, p_tensor_type)) {\n    Py_ssize_t len = PySequence_Size(obj);\n    PyObject* item = nullptr;\n    for (Py_ssize_t i = 0; i < len; i++) {\n      item = PySequence_GetItem(obj, i);\n      if (PyObject_CheckLongOrToLong(&item)) {\n        value.emplace_back((int64_t)PyLong_AsLongLong(item));\n      } else {\n        PADDLE_THROW(platform::errors::InvalidArgument(\n            \"%s(): argument (position %d) must be \"\n            \"list of int, but got %s at pos %d\",\n            op_type,\n            arg_pos + 1,\n            ((PyTypeObject*)item->ob_type)->tp_name,  // NOLINT\n            i));\n      }\n    }\n  } else if (obj == Py_None) {\n    return {};\n  } else if (PyObject_CheckLongOrToLong(&obj)) {\n    return {(int64_t)PyLong_AsLongLong(obj)};\n  } else {\n    PADDLE_THROW(platform::errors::InvalidArgument(\n        \"%s(): argument (position %d) must be \"\n        \"list or tuple, but got %s\",\n        op_type,\n        arg_pos + 1,\n        ((PyTypeObject*)obj->ob_type)->tp_name));  // NOLINT\n  }\n\n  return value;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void pointZZ_pMul(PointZZ_p * rop, const PointZZ_p * point, const mpz_t scalar, const CurveZZ_p * curve) {\n    // handle the identity element\n    if(pointZZ_pIsIdentityElement(point)) {\n        return pointZZ_pSetToIdentityElement(rop);\n    }\n\n    PointZZ_p R0, R1, tmp;\n    mpz_inits(R1.x, R1.y, tmp.x, tmp.y, NULL);\n    mpz_init_set(R0.x, point->x);\n    mpz_init_set(R0.y, point->y);\n    pointZZ_pDouble(&R1, point, curve);\n\n    int dbits = mpz_sizeinbase(scalar, 2), i;\n\n    for(i = dbits - 2; i >= 0; i--) {\n        if(mpz_tstbit(scalar, i)) {\n            mpz_set(tmp.x, R0.x);\n            mpz_set(tmp.y, R0.y);\n            pointZZ_pAdd(&R0, &R1, &tmp, curve);\n            mpz_set(tmp.x, R1.x);\n            mpz_set(tmp.y, R1.y);\n            pointZZ_pDouble(&R1, &tmp, curve);\n        }\n        else {\n            mpz_set(tmp.x, R1.x);\n            mpz_set(tmp.y, R1.y);\n            pointZZ_pAdd(&R1, &R0, &tmp, curve);\n            mpz_set(tmp.x, R0.x);\n            mpz_set(tmp.y, R0.y);\n            pointZZ_pDouble(&R0, &tmp, curve);\n        }\n    }\n\n    mpz_init_set(rop->x, R0.x);\n    mpz_init_set(rop->y, R0.y);\n    mpz_clears(R0.x, R0.y, R1.x, R1.y, tmp.x, tmp.y, NULL);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  explicit FractionalMaxPoolOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pooling_ratio\", &pooling_ratio_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"pseudo_random\", &pseudo_random_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n\n    OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                errors::InvalidArgument(\"pooling_ratio field must \"\n                                        \"specify 4 dimensions\"));\n    for (std::size_t i = 0; i < pooling_ratio_.size(); ++i) {\n      OP_REQUIRES(context, pooling_ratio_[i] >= 1,\n                  errors::InvalidArgument(\n                      \"pooling_ratio cannot be smaller than 1, got: \",\n                      pooling_ratio_[i]));\n    }\n\n    OP_REQUIRES(\n        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n        errors::Unimplemented(\"Fractional max pooling is not yet \"\n                              \"supported on the batch nor channel dimension.\"));\n\n    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed2\", &seed2_));\n    if (deterministic_) {\n      // If both seeds are not set when deterministic_ is true, force set seeds.\n      if ((seed_ == 0) && (seed2_ == 0)) {\n        seed_ = random::New64();\n        seed2_ = random::New64();\n      }\n    } else {\n      OP_REQUIRES(\n          context, (seed_ == 0) && (seed2_ == 0),\n          errors::InvalidArgument(\n              \"Both seed and seed2 should be 0 if deterministic is false.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def authenticate_request(self, request, content):\n        \"\"\"Authenticates a Matrix federation request based on the X-Matrix header\n        XXX: Copied largely from synapse\n\n        :param request: The request object to authenticate\n        :type request: twisted.web.server.Request\n        :param content: The content of the request, if any\n        :type content: bytes or None\n\n        :return: The origin of the server whose signature was validated\n        :rtype: twisted.internet.defer.Deferred[unicode]\n        \"\"\"\n        json_request = {\n            \"method\": request.method,\n            \"uri\": request.uri,\n            \"destination_is\": self.sydent.server_name,\n            \"signatures\": {},\n        }\n\n        if content is not None:\n            json_request[\"content\"] = content\n\n        origin = None\n\n        def parse_auth_header(header_str):\n            \"\"\"\n            Extracts a server name, signing key and payload signature from an\n            authentication header.\n\n            :param header_str: The content of the header\n            :type header_str: unicode\n\n            :return: The server name, the signing key, and the payload signature.\n            :rtype: tuple[unicode]\n            \"\"\"\n            try:\n                params = header_str.split(u\" \")[1].split(u\",\")\n                param_dict = dict(kv.split(u\"=\") for kv in params)\n\n                def strip_quotes(value):\n                    if value.startswith(u\"\\\"\"):\n                        return value[1:-1]\n                    else:\n                        return value\n\n                origin = strip_quotes(param_dict[\"origin\"])\n                key = strip_quotes(param_dict[\"key\"])\n                sig = strip_quotes(param_dict[\"sig\"])\n                return origin, key, sig\n            except Exception:\n                raise SignatureVerifyException(\"Malformed Authorization header\")\n\n        auth_headers = request.requestHeaders.getRawHeaders(u\"Authorization\")\n\n        if not auth_headers:\n            raise NoAuthenticationError(\"Missing Authorization headers\")\n\n        for auth in auth_headers:\n            if auth.startswith(u\"X-Matrix\"):\n                (origin, key, sig) = parse_auth_header(auth)\n                json_request[\"origin\"] = origin\n                json_request[\"signatures\"].setdefault(origin, {})[key] = sig\n\n        if not json_request[\"signatures\"]:\n            raise NoAuthenticationError(\"Missing X-Matrix Authorization header\")\n\n        if not is_valid_matrix_server_name(json_request[\"origin\"]):\n            raise InvalidServerName(\"X-Matrix header's origin parameter must be a valid Matrix server name\")\n\n        yield self.verifyServerSignedJson(json_request, [origin])\n\n        logger.info(\"Verified request from HS %s\", origin)\n\n        defer.returnValue(origin)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def __setstate__(self, state):\n    \"\"\"Restore from pickled state.\"\"\"\n    self.__dict__ = state\n    self._lock = threading.RLock()\n    self._descriptor_cache = weakref.WeakKeyDictionary()\n    self._key_for_call_stats = self._get_key_for_call_stats()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def update(self, request, *args, **kwargs):\n        \"\"\"Perform 'update' function and mark this part as 'starred' (or not)\"\"\"\n        # Clean up input data\n        data = self.clean_data(request.data)\n\n        if 'starred' in data:\n            starred = str2bool(data.get('starred', False))\n\n            self.get_object().set_starred(request.user, starred)\n\n        response = super().update(request, *args, **kwargs)\n\n        return response\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status CalculateOutputIndexValueRowID(\n      const RowPartitionTensor& value_rowids,\n      const vector<INDEX_TYPE>& parent_output_index,\n      INDEX_TYPE output_index_multiplier, INDEX_TYPE output_size,\n      vector<INDEX_TYPE>* result) {\n    const INDEX_TYPE index_size = value_rowids.size();\n    result->reserve(index_size);\n    if (index_size == 0) {\n      return Status::OK();\n    }\n\n    INDEX_TYPE current_output_column = 0;\n    INDEX_TYPE current_value_rowid = value_rowids(0);\n\n    if (current_value_rowid >= parent_output_index.size()) {\n      return errors::InvalidArgument(\n          \"Got current_value_rowid=\", current_value_rowid,\n          \" which is not less than \", parent_output_index.size());\n    }\n\n    INDEX_TYPE current_output_index = parent_output_index[current_value_rowid];\n    result->push_back(current_output_index);\n    for (INDEX_TYPE i = 1; i < index_size; ++i) {\n      INDEX_TYPE next_value_rowid = value_rowids(i);\n      if (next_value_rowid == current_value_rowid) {\n        if (current_output_index >= 0) {\n          ++current_output_column;\n          if (current_output_column < output_size) {\n            current_output_index += output_index_multiplier;\n          } else {\n            current_output_index = -1;\n          }\n        }\n      } else {\n        current_output_column = 0;\n        current_value_rowid = next_value_rowid;\n\n        if (next_value_rowid >= parent_output_index.size()) {\n          return errors::InvalidArgument(\n              \"Got next_value_rowid=\", next_value_rowid,\n              \" which is not less than \", parent_output_index.size());\n        }\n\n        current_output_index = parent_output_index[next_value_rowid];\n      }\n      result->push_back(current_output_index);\n    }\n\n    if (result->size() != value_rowids.size()) {\n      return errors::InvalidArgument(\"Invalid row ids.\");\n    }\n\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def build_IR(self, expr, args, kwargs, context):\n        input_buf = context.new_internal_variable(get_type_for_exact_size(128))\n        output_buf = MemoryPositions.FREE_VAR_SPACE\n        return IRnode.from_list(\n            [\n                \"seq\",\n                # clear output memory first, ecrecover can return 0 bytes\n                [\"mstore\", output_buf, 0],\n                [\"mstore\", input_buf, args[0]],\n                [\"mstore\", input_buf + 32, args[1]],\n                [\"mstore\", input_buf + 64, args[2]],\n                [\"mstore\", input_buf + 96, args[3]],\n                [\"staticcall\", \"gas\", 1, input_buf, 128, output_buf, 32],\n                [\"mload\", output_buf],\n            ],\n            typ=AddressT(),\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_annotation(self, annotation, alias, is_summary=False, select=True):\n        \"\"\"Add a single annotation expression to the Query.\"\"\"\n        self.check_alias(alias)\n        annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,\n                                                   summarize=is_summary)\n        if select:\n            self.append_annotation_mask([alias])\n        else:\n            self.set_annotation_mask(set(self.annotation_select).difference({alias}))\n        self.annotations[alias] = annotation\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _rpsl_db_query_to_graphql_out(query: RPSLDatabaseQuery, info: GraphQLResolveInfo):\n    \"\"\"\n    Given an RPSL database query, execute it and clean up the output\n    to be suitable to return to GraphQL.\n\n    Main changes are:\n    - Enum handling\n    - Adding the asn and prefix fields if applicable\n    - Ensuring the right fields are returned as a list of strings or a string\n    \"\"\"\n    database_handler = info.context['request'].app.state.database_handler\n    if info.context.get('sql_trace'):\n        if 'sql_queries' not in info.context:\n            info.context['sql_queries'] = [repr(query)]\n        else:\n            info.context['sql_queries'].append(repr(query))\n\n    for row in database_handler.execute_query(query, refresh_on_error=True):\n        graphql_result = {snake_to_camel_case(k): v for k, v in row.items() if k != 'parsed_data'}\n        if 'object_text' in row:\n            graphql_result['objectText'] = remove_auth_hashes(row['object_text'])\n        if 'rpki_status' in row:\n            graphql_result['rpkiStatus'] = row['rpki_status']\n        if row.get('ip_first') is not None and row.get('prefix_length'):\n            graphql_result['prefix'] = row['ip_first'] + '/' + str(row['prefix_length'])\n        if row.get('asn_first') is not None and row.get('asn_first') == row.get('asn_last'):\n            graphql_result['asn'] = row['asn_first']\n\n        object_type = resolve_rpsl_object_type(row)\n        for key, value in row.get('parsed_data', dict()).items():\n            if key == 'auth':\n                value = [remove_auth_hashes(v) for v in value]\n            graphql_type = schema.graphql_types[object_type][key]\n            if graphql_type == 'String' and isinstance(value, list):\n                value = '\\n'.join(value)\n            graphql_result[snake_to_camel_case(key)] = value\n        yield graphql_result\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_safe_url(url, host=None):\n    \"\"\"\n    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to\n    a different host and uses a safe scheme).\n\n    Always returns ``False`` on an empty url.\n    \"\"\"\n    if url is not None:\n        url = url.strip()\n    if not url:\n        return False\n    # Chrome treats \\ completely as / in paths but it could be part of some\n    # basic auth credentials so we need to check both URLs.\n    return _is_safe_url(url, host) and _is_safe_url(url.replace('\\\\', '/'), host)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def apt_sync(self, repo):\n\n        \"\"\"\n        Handle copying of http:// and ftp:// debian repos.\n        \"\"\"\n\n        # warn about not having mirror program.\n\n        mirror_program = \"/usr/bin/debmirror\"\n        if not os.path.exists(mirror_program):\n            utils.die(self.logger, \"no %s found, please install it\" % (mirror_program))\n\n        cmd = \"\"                  # command to run\n\n        # detect cases that require special handling\n\n        if repo.rpm_list != \"\" and repo.rpm_list != []:\n            utils.die(self.logger, \"has_rpm_list not yet supported on apt repos\")\n\n        if not repo.arch:\n            utils.die(self.logger, \"Architecture is required for apt repositories\")\n\n        # built destination path for the repo\n        dest_path = os.path.join(\"/var/www/cobbler/repo_mirror\", repo.name)\n\n        if repo.mirror_locally:\n            # NOTE: Dropping @@suite@@ replace as it is also dropped from\n            # from manage_import_debian_ubuntu.py due that repo has no os_version\n            # attribute. If it is added again it will break the Web UI!\n            # mirror = repo.mirror.replace(\"@@suite@@\",repo.os_version)\n            mirror = repo.mirror\n\n            idx = mirror.find(\"://\")\n            method = mirror[:idx]\n            mirror = mirror[idx + 3:]\n\n            idx = mirror.find(\"/\")\n            host = mirror[:idx]\n            mirror = mirror[idx:]\n\n            dists = \",\".join(repo.apt_dists)\n            components = \",\".join(repo.apt_components)\n\n            mirror_data = \"--method=%s --host=%s --root=%s --dist=%s --section=%s\" % (pipes.quote(method), pipes.quote(host), pipes.quote(mirror), pipes.quote(dists), pipes.quote(components))\n\n            rflags = \"--nocleanup\"\n            for x in repo.yumopts:\n                if repo.yumopts[x]:\n                    rflags += \" %s %s\" % (x, repo.yumopts[x])\n                else:\n                    rflags += \" %s\" % x\n            cmd = \"%s %s %s %s\" % (mirror_program, rflags, mirror_data, dest_path)\n            cmd = \"%s %s %s %s\" % (mirror_program, rflags, mirror_data, pipes.quote(dest_path))\n            if repo.arch == \"src\":\n                cmd = \"%s --source\" % cmd\n            else:\n                arch = repo.arch\n                if arch == \"x86\":\n                    arch = \"i386\"       # FIX potential arch errors\n                if arch == \"x86_64\":\n                    arch = \"amd64\"      # FIX potential arch errors\n                cmd = \"%s --nosource -a %s\" % (cmd, arch)\n\n            # Set's an environment variable for subprocess, otherwise debmirror will fail\n            # as it needs this variable to exist.\n            # FIXME: might this break anything? So far it doesn't\n            os.putenv(\"HOME\", \"/var/lib/cobbler\")\n\n            rc = utils.subprocess_call(self.logger, cmd)\n            if rc != 0:\n                utils.die(self.logger, \"cobbler reposync failed\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def rollback_to(self):\n        validate_savepoint_name(self.name)\n        await self.transaction.connection.execute(\n            f\"ROLLBACK TO SAVEPOINT {self.name}\"\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def build_attrs(self, *args, **kwargs):\n        attrs = super().build_attrs(*args, **kwargs)\n\n        accept = attrs.get(\"accept\")\n        response = self.client.generate_presigned_post(\n            self.bucket_name,\n            str(pathlib.PurePosixPath(self.upload_folder, \"${filename}\")),\n            Conditions=self.get_conditions(accept),\n            ExpiresIn=self.expires,\n        )\n\n        defaults = {\n            \"data-fields-%s\" % key: value for key, value in response[\"fields\"].items()\n        }\n        defaults[\"data-url\"] = response[\"url\"]\n        signer = signing.Signer(\n            salt=f\"{S3FileInputMixin.__module__}.{S3FileInputMixin.__name__}\"\n        )\n        print(self.upload_folder)\n        defaults[\"data-s3f-signature\"] = signer.signature(self.upload_folder)\n        defaults.update(attrs)\n\n        try:\n            defaults[\"class\"] += \" s3file\"\n        except KeyError:\n            defaults[\"class\"] = \"s3file\"\n        return defaults\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _group_or_org_create(context, data_dict, is_org=False):\n    model = context['model']\n    user = context['user']\n    session = context['session']\n    data_dict['is_organization'] = is_org\n\n    upload = uploader.get_uploader('group')\n    upload.update_data_dict(data_dict, 'image_url',\n                            'image_upload', 'clear_upload')\n    # get the schema\n    group_type = data_dict.get('type', 'organization' if is_org else 'group')\n    group_plugin = lib_plugins.lookup_group_plugin(group_type)\n    try:\n        schema = group_plugin.form_to_db_schema_options({\n            'type': 'create', 'api': 'api_version' in context,\n            'context': context})\n    except AttributeError:\n        schema = group_plugin.form_to_db_schema()\n\n    if 'api_version' not in context:\n        # old plugins do not support passing the schema so we need\n        # to ensure they still work\n        try:\n            group_plugin.check_data_dict(data_dict, schema)\n        except TypeError:\n            group_plugin.check_data_dict(data_dict)\n\n    data, errors = lib_plugins.plugin_validate(\n        group_plugin, context, data_dict, schema,\n        'organization_create' if is_org else 'group_create')\n    log.debug('group_create validate_errs=%r user=%s group=%s data_dict=%r',\n              errors, context.get('user'), data_dict.get('name'), data_dict)\n\n    if errors:\n        session.rollback()\n        raise ValidationError(errors)\n\n    group = model_save.group_dict_save(data, context)\n\n    # Needed to let extensions know the group id\n    session.flush()\n\n    if is_org:\n        plugin_type = plugins.IOrganizationController\n    else:\n        plugin_type = plugins.IGroupController\n\n    for item in plugins.PluginImplementations(plugin_type):\n        item.create(group)\n\n    if is_org:\n        activity_type = 'new organization'\n    else:\n        activity_type = 'new group'\n\n    user_id = model.User.by_name(six.ensure_text(user)).id\n\n    activity_dict = {\n        'user_id': user_id,\n        'object_id': group.id,\n        'activity_type': activity_type,\n    }\n    activity_dict['data'] = {\n        'group': ckan.lib.dictization.table_dictize(group, context)\n    }\n    activity_create_context = {\n        'model': model,\n        'user': user,\n        'defer_commit': True,\n        'ignore_auth': True,\n        'session': session\n    }\n    logic.get_action('activity_create')(activity_create_context, activity_dict)\n\n    if hasattr(upload, \"verify_type\"):\n        upload.verify_type()\n\n    upload.upload(uploader.get_max_image_size())\n\n    if not context.get('defer_commit'):\n        model.repo.commit()\n    context[\"group\"] = group\n    context[\"id\"] = group.id\n\n    # creator of group/org becomes an admin\n    # this needs to be after the repo.commit or else revisions break\n    member_dict = {\n        'id': group.id,\n        'object': user_id,\n        'object_type': 'user',\n        'capacity': 'admin',\n    }\n    member_create_context = {\n        'model': model,\n        'user': user,\n        'ignore_auth': True,  # we are not a member of the group at this point\n        'session': session\n    }\n    logic.get_action('member_create')(member_create_context, member_dict)\n\n    log.debug('Created object %s' % group.name)\n\n    return_id_only = context.get('return_id_only', False)\n    action = 'organization_show' if is_org else 'group_show'\n\n    output = context['id'] if return_id_only \\\n        else _get_action(action)(context, {'id': group.id})\n\n    return output\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def explain(self, using, format=None, **options):\n        q = self.clone()\n        for option_name in options:\n            if (\n                not EXPLAIN_OPTIONS_PATTERN.fullmatch(option_name)\n                or \"--\" in option_name\n            ):\n                raise ValueError(f\"Invalid option name: {option_name!r}.\")\n        q.explain_info = ExplainInfo(format, options)\n        compiler = q.get_compiler(using=using)\n        return \"\\n\".join(compiler.explain_query())\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _execute_module(self, conn, tmp, module_name, args,\n        async_jid=None, async_module=None, async_limit=None, inject=None, persist_files=False, complex_args=None, delete_remote_tmp=True):\n\n        ''' transfer and run a module along with its arguments on the remote side'''\n\n        # hack to support fireball mode\n        if module_name == 'fireball':\n            args = \"%s password=%s\" % (args, base64.b64encode(str(utils.key_for_hostname(conn.host))))\n            if 'port' not in args:\n                args += \" port=%s\" % C.ZEROMQ_PORT\n\n        (\n        module_style,\n        shebang,\n        module_data\n        ) = self._configure_module(conn, module_name, args, inject, complex_args)\n\n        # a remote tmp path may be necessary and not already created\n        if self._late_needs_tmp_path(conn, tmp, module_style):\n            tmp = self._make_tmp_path(conn)\n\n        remote_module_path = conn.shell.join_path(tmp, module_name)\n\n        if (module_style != 'new'\n           or async_jid is not None\n           or not conn.has_pipelining\n           or not C.ANSIBLE_SSH_PIPELINING\n           or C.DEFAULT_KEEP_REMOTE_FILES\n           or self.su):\n            self._transfer_str(conn, tmp, module_name, module_data)\n\n        environment_string = self._compute_environment_string(conn, inject)\n\n        if \"tmp\" in tmp and ((self.sudo and self.sudo_user != 'root') or (self.su and self.su_user != 'root')):\n            # deal with possible umask issues once sudo'ed to other user\n            self._remote_chmod(conn, 'a+r', remote_module_path, tmp)\n\n        cmd = \"\"\n        in_data = None\n        if module_style != 'new':\n            if 'CHECKMODE=True' in args:\n                # if module isn't using AnsibleModuleCommon infrastructure we can't be certain it knows how to\n                # do --check mode, so to be safe we will not run it.\n                return ReturnData(conn=conn, result=dict(skipped=True, msg=\"cannot yet run check mode against old-style modules\"))\n            elif 'NO_LOG' in args:\n                return ReturnData(conn=conn, result=dict(skipped=True, msg=\"cannot use no_log: with old-style modules\"))\n\n            args = template.template(self.basedir, args, inject)\n\n            # decide whether we need to transfer JSON or key=value\n            argsfile = None\n            if module_style == 'non_native_want_json':\n                if complex_args:\n                    complex_args.update(utils.parse_kv(args))\n                    argsfile = self._transfer_str(conn, tmp, 'arguments', utils.jsonify(complex_args))\n                else:\n                    argsfile = self._transfer_str(conn, tmp, 'arguments', utils.jsonify(utils.parse_kv(args)))\n\n            else:\n                argsfile = self._transfer_str(conn, tmp, 'arguments', args)\n\n            if (self.sudo and self.sudo_user != 'root') or (self.su and self.su_user != 'root'):\n                # deal with possible umask issues once sudo'ed to other user\n                self._remote_chmod(conn, 'a+r', argsfile, tmp)\n\n            if async_jid is None:\n                cmd = \"%s %s\" % (remote_module_path, argsfile)\n            else:\n                cmd = \" \".join([str(x) for x in [remote_module_path, async_jid, async_limit, async_module, argsfile]])\n        else:\n            if async_jid is None:\n                if conn.has_pipelining and C.ANSIBLE_SSH_PIPELINING and not C.DEFAULT_KEEP_REMOTE_FILES and not self.su:\n                    in_data = module_data\n                else:\n                    cmd = \"%s\" % (remote_module_path)\n            else:\n                cmd = \" \".join([str(x) for x in [remote_module_path, async_jid, async_limit, async_module]])\n\n        if not shebang:\n            raise errors.AnsibleError(\"module is missing interpreter line\")\n\n        rm_tmp = None\n        if \"tmp\" in tmp and not C.DEFAULT_KEEP_REMOTE_FILES and not persist_files and delete_remote_tmp:\n            if not self.sudo or self.su or self.sudo_user == 'root' or self.su_user == 'root':\n                # not sudoing or sudoing to root, so can cleanup files in the same step\n                rm_tmp = tmp\n\n        cmd = conn.shell.build_module_command(environment_string, shebang, cmd, rm_tmp)\n        cmd = cmd.strip()\n\n        sudoable = True\n        if module_name == \"accelerate\":\n            # always run the accelerate module as the user\n            # specified in the play, not the sudo_user\n            sudoable = False\n\n        if self.su:\n            res = self._low_level_exec_command(conn, cmd, tmp, su=True, in_data=in_data)\n        else:\n            res = self._low_level_exec_command(conn, cmd, tmp, sudoable=sudoable, in_data=in_data)\n\n        if \"tmp\" in tmp and not C.DEFAULT_KEEP_REMOTE_FILES and not persist_files and delete_remote_tmp:\n            if (self.sudo and self.sudo_user != 'root') or (self.su and self.su_user != 'root'):\n            # not sudoing to root, so maybe can't delete files as that other user\n            # have to clean up temp files as original user in a second step\n                cmd2 = conn.shell.remove(tmp, recurse=True)\n                self._low_level_exec_command(conn, cmd2, tmp, sudoable=False)\n\n        data = utils.parse_json(res['stdout'], from_remote=True)\n        if 'parsed' in data and data['parsed'] == False:\n            data['msg'] += res['stderr']\n        return ReturnData(conn=conn, result=data)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static int ReadHuffmanCodeLengths(\n    VP8LDecoder* const dec, const int* const code_length_code_lengths,\n    int num_symbols, int* const code_lengths) {\n  int ok = 0;\n  VP8LBitReader* const br = &dec->br_;\n  int symbol;\n  int max_symbol;\n  int prev_code_len = DEFAULT_CODE_LENGTH;\n  HuffmanTables tables;\n\n  if (!VP8LHuffmanTablesAllocate(1 << LENGTHS_TABLE_BITS, &tables) ||\n      !VP8LBuildHuffmanTable(&tables, LENGTHS_TABLE_BITS,\n                             code_length_code_lengths, NUM_CODE_LENGTH_CODES)) {\n    goto End;\n  }\n\n  if (VP8LReadBits(br, 1)) {    // use length\n    const int length_nbits = 2 + 2 * VP8LReadBits(br, 3);\n    max_symbol = 2 + VP8LReadBits(br, length_nbits);\n    if (max_symbol > num_symbols) {\n      goto End;\n    }\n  } else {\n    max_symbol = num_symbols;\n  }\n\n  symbol = 0;\n  while (symbol < num_symbols) {\n    const HuffmanCode* p;\n    int code_len;\n    if (max_symbol-- == 0) break;\n    VP8LFillBitWindow(br);\n    p = &tables.curr_segment->start[VP8LPrefetchBits(br) & LENGTHS_TABLE_MASK];\n    VP8LSetBitPos(br, br->bit_pos_ + p->bits);\n    code_len = p->value;\n    if (code_len < kCodeLengthLiterals) {\n      code_lengths[symbol++] = code_len;\n      if (code_len != 0) prev_code_len = code_len;\n    } else {\n      const int use_prev = (code_len == kCodeLengthRepeatCode);\n      const int slot = code_len - kCodeLengthLiterals;\n      const int extra_bits = kCodeLengthExtraBits[slot];\n      const int repeat_offset = kCodeLengthRepeatOffsets[slot];\n      int repeat = VP8LReadBits(br, extra_bits) + repeat_offset;\n      if (symbol + repeat > num_symbols) {\n        goto End;\n      } else {\n        const int length = use_prev ? prev_code_len : 0;\n        while (repeat-- > 0) code_lengths[symbol++] = length;\n      }\n    }\n  }\n  ok = 1;\n\n End:\n  VP8LHuffmanTablesDeallocate(&tables);\n  if (!ok) return VP8LSetError(dec, VP8_STATUS_BITSTREAM_ERROR);\n  return ok;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def start(\n        self,\n        measurement_info: MeasurementInfo,\n        service_info: ServiceInfo,\n        configuration_parameter_list: List[ParameterMetadata],\n        output_parameter_list: List[ParameterMetadata],\n        measure_function: Callable,\n        owner: object = None,\n    ) -> str:\n        \"\"\"Start the gRPC server and register it with the discovery service.\n\n        Returns:\n            The insecure port.\n        \"\"\"\n        interceptors: List[grpc.ServerInterceptor] = []\n        if ServerLogger.is_enabled():\n            interceptors.append(ServerLogger())\n        self._server = grpc.server(\n            logging_pool.pool(max_workers=10),\n            interceptors=interceptors,\n            options=[\n                (\"grpc.max_receive_message_length\", -1),\n                (\"grpc.max_send_message_length\", -1),\n            ],\n        )\n        for interface in service_info.provided_interfaces:\n            if interface == _V1_INTERFACE:\n                servicer_v1 = MeasurementServiceServicerV1(\n                    measurement_info,\n                    configuration_parameter_list,\n                    output_parameter_list,\n                    measure_function,\n                    owner,\n                )\n                v1_measurement_service_pb2_grpc.add_MeasurementServiceServicer_to_server(\n                    servicer_v1, self._server\n                )\n            elif interface == _V2_INTERFACE:\n                servicer_v2 = MeasurementServiceServicerV2(\n                    measurement_info,\n                    configuration_parameter_list,\n                    output_parameter_list,\n                    measure_function,\n                    owner,\n                )\n                v2_measurement_service_pb2_grpc.add_MeasurementServiceServicer_to_server(\n                    servicer_v2, self._server\n                )\n            else:\n                raise ValueError(\n                    f\"Unknown interface was provided in the .serviceconfig file: {interface}\"\n                )\n        host = \"[::1]\"\n        port = str(self._server.add_insecure_port(f\"{host}:0\"))\n        address = f\"http://{host}:{port}\"\n        self._server.start()\n        _logger.info(\"Measurement service listening on: %s\", address)\n\n        self._service_location = ServiceLocation(\"localhost\", port, \"\")\n        self._registration_id = self._discovery_client.register_service(\n            service_info, self.service_location\n        )\n        return port\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def rsync_sync(self, repo):\n\n        \"\"\"\n        Handle copying of rsync:// and rsync-over-ssh repos.\n        \"\"\"\n\n        if not repo.mirror_locally:\n            utils.die(self.logger, \"rsync:// urls must be mirrored locally, yum cannot access them directly\")\n\n        if repo.rpm_list != \"\" and repo.rpm_list != []:\n            self.logger.warning(\"--rpm-list is not supported for rsync'd repositories\")\n\n        # FIXME: don't hardcode\n        dest_path = os.path.join(self.settings.webdir + \"/repo_mirror\", repo.name)\n\n        spacer = \"\"\n        if not repo.mirror.startswith(\"rsync://\") and not repo.mirror.startswith(\"/\"):\n            spacer = \"-e ssh\"\n        if not repo.mirror.strip().endswith(\"/\"):\n            repo.mirror = \"%s/\" % repo.mirror\n\n        # FIXME: wrapper for subprocess that logs to logger\n        cmd = \"rsync -rltDv --copy-unsafe-links --delete-after %s --delete --exclude-from=/etc/cobbler/rsync.exclude %s %s\" % (spacer, pipes.quote(repo.mirror), pipes.quote(dest_path))\n        rc = utils.subprocess_call(self.logger, cmd)\n\n        if rc != 0:\n            utils.die(self.logger, \"cobbler reposync failed\")\n        os.path.walk(dest_path, self.createrepo_walker, repo)\n        self.create_local_file(dest_path, repo)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status SparseTensorToCSRSparseMatrixCPUFunctor::operator()(\n    const int64 batch_size, const int num_rows,\n    TTypes<int64>::ConstMatrix indices, TTypes<int32>::Vec batch_ptr,\n    TTypes<int32>::Vec csr_row_ptr, TTypes<int32>::Vec csr_col_ind) {\n  // Validate inputs.\n  if (batch_ptr.size() != batch_size + 1) {\n    return errors::InvalidArgument(\n        \"Expected batch_ptr.size() == batch_size + 1. Got: \", batch_ptr.size(),\n        \" vs. \", batch_size + 1);\n  }\n  if (csr_row_ptr.size() != batch_size * (num_rows + 1)) {\n    return errors::InvalidArgument(\n        \"Expected csr_row_ptr.size() == batch_size * (num_rows + 1). Got: \",\n        csr_row_ptr.size(), \" vs. \", batch_size * (num_rows + 1));\n  }\n\n  const int64 total_nnz = indices.dimension(0);\n  const int rank = indices.dimension(1);\n  if (rank == 2 && batch_size != 1) {\n    return errors::InvalidArgument(\n        \"Expected batch_size == 1 when rank is 2. Got batch_size: \",\n        batch_size);\n  }\n  if (csr_col_ind.size() != total_nnz) {\n    return errors::InvalidArgument(\n        \"Expected csr_col_ind.size() == total_nnz. Got: \", csr_col_ind.size(),\n        \" vs. \", total_nnz);\n  }\n\n  int prev_batch = -1;\n  if (rank == 2) {\n    // For a single batch, the batch_ptrs are {0, total_nnz}.\n    batch_ptr(0) = 0;\n    ++prev_batch;\n\n    for (int64 i = 0; i < total_nnz; ++i) {\n      // For now, the rows pointers store the corresponding row counts.\n      int64 ix = indices(i, 0) + 1;\n      if (ix >= csr_row_ptr.size()) {\n        return errors::InvalidArgument(\"Got an index \", ix,\n                                       \" that is outside of csr_row_ptr\");\n      }\n      csr_row_ptr(indices(i, 0) + 1) += 1;\n      csr_col_ind(i) = indices(i, 1);\n    }\n  } else {  // rank == 3\n    for (int64 i = 0; i < total_nnz; ++i) {\n      const int cur_batch = indices(i, 0);\n      // For now, the rows pointers store the corresponding row counts.\n      csr_row_ptr(cur_batch * (num_rows + 1) + indices(i, 1) + 1) += 1;\n      csr_col_ind(i) = indices(i, 2);\n\n      // We're at a new batch and might have skipped over empty batches.\n      while (prev_batch < cur_batch) {\n        // The previous batch ends at position i.\n        batch_ptr(prev_batch + 1) = i;\n        ++prev_batch;\n      }\n    }\n  }\n  // Set the last element of batch_ptr and account for trailing empty batches.\n  while (prev_batch < batch_size) {\n    batch_ptr(prev_batch + 1) = total_nnz;\n    ++prev_batch;\n  }\n\n  // Compute the cumulative row counts for each batch.\n  for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n    auto* row_ptr_batch = csr_row_ptr.data() + batch_idx * (num_rows + 1);\n    std::partial_sum(row_ptr_batch, row_ptr_batch + num_rows + 1,\n                     row_ptr_batch);\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def login():\n    user = flask.request.form[\"username\"]\n    password = flask.request.form[\"password\"]\n\n    api = flask.current_app.config[\"PYLOAD_API\"]\n    user_info = api.check_auth(user, password)\n\n    sanitized_user = user.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\\\\r\")\n    if not user_info:\n        log.error(f\"Login failed for user '{sanitized_user}'\")\n        return jsonify(False)\n\n    s = set_session(user_info)\n    log.info(f\"User '{sanitized_user}' successfully logged in\")\n    flask.flash(\"Logged in successfully\")\n\n    return jsonify(s)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _ttl_action(self, version, action):\n        # type: (str) -> TtlActions\n        \"\"\"Determine the correct action to take based on the local resources and TTL.\n\n        :param action: The action being taken (encrypt or decrypt)\n\n        :returns: decision\n        :rtype: TtlActions\n        \"\"\"\n        try:\n            if action == _ENCRYPT_ACTION:\n                # On encrypt, always check the class-level variable indicating when we last checked for updates.\n                # The cache timestamps will be updated by calls to decrypt, so we don't want frequent decrypts to\n                # prevent us from re-checking for a newer encryption version\n                if self._last_updated is None:\n                    return TtlActions.EXPIRED\n                timestamp = self._last_updated\n            else:\n                timestamp, _ = self._cache.get(version)\n            time_since_updated = time.time() - timestamp\n\n            if time_since_updated < self._version_ttl:\n                return TtlActions.LIVE\n\n            if time_since_updated < self._version_ttl + self._grace_period:\n                return TtlActions.GRACE_PERIOD\n\n            _LOGGER.debug(\"TTL Expired because known version has expired\")\n            return TtlActions.EXPIRED\n        except KeyError:\n            _LOGGER.debug(\"TTL Expired because the requested version doesn't exist in the cache\")\n            return TtlActions.EXPIRED\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool ConstantFolding::IsFoldableUncached(\n    const NodeDef& node, const GraphProperties* properties) const {\n  // Folding not applicable to ops with no inputs.\n  if (node.input().empty()) {\n    return false;\n  }\n  // We can only fold nodes if all their inputs are known statically, except in\n  // the case of a merge node that propagate the first inputs that becomes\n  // available, and therefore only requires a single constant input to be\n  // foldable.\n  bool merge_has_constant_input = false;\n  const bool is_merge = IsMerge(node);\n  for (const auto& input : node.input()) {\n    if (IsControlInput(input)) {\n      continue;\n    }\n    const NodeDef* input_node = node_map_->GetNode(input);\n    if (!input_node) {\n      return false;\n    }\n    bool is_const = IsReallyConstant(*input_node);\n    if (is_const) {\n      // Don't fold strings constants for now since this causes problems with\n      // checkpointing.\n      if (input_node->attr().count(\"dtype\") == 0 ||\n          input_node->attr().at(\"dtype\").type() == DT_STRING) {\n        return false;\n      }\n      // Special case: If a Merge node has at least one constant input that\n      // does not depend on a control input, we can fold it.\n      merge_has_constant_input |= !HasControlInputs(*input_node);\n    } else if (!is_merge) {\n      return false;\n    }\n  }\n  if (is_merge && !merge_has_constant_input) return false;\n  if (disable_compressed_tensor_optimization_ &&\n      (IsFill(node) || IsZerosLike(node) || IsOnesLike(node)))\n    return false;\n\n  // If we know the output shapes, make sure that the outputs are small enough\n  // to materialize.\n  if (properties != nullptr && properties->HasOutputProperties(node.name())) {\n    const std::vector<OpInfo::TensorProperties>& input_props =\n        properties->GetInputProperties(node.name());\n    const std::vector<OpInfo::TensorProperties>& output_props =\n        properties->GetOutputProperties(node.name());\n    // Compute total size of inputs.\n    int64_t input_size_bytes = 0;\n    for (const auto& input_prop : input_props) {\n      const PartialTensorShape input_shape(input_prop.shape());\n      if (input_shape.IsFullyDefined()) {\n        input_size_bytes +=\n            input_shape.num_elements() * DataTypeSize(input_prop.dtype());\n      }\n    }\n    for (const auto& output_prop : output_props) {\n      PartialTensorShape output_shape;\n      if (!PartialTensorShape::BuildPartialTensorShape(output_prop.shape(),\n                                                       &output_shape)\n               .ok()) {\n        return false;\n      }\n      if (output_shape.IsFullyDefined()) {\n        const int64_t num_bytes =\n            output_shape.num_elements() * DataTypeSize(output_prop.dtype());\n        if (num_bytes > input_size_bytes && num_bytes > kMaxConstantSize) {\n          // Do not fold nodes if the in-memory size of output is too large.\n          // Notice that this is not exactly the same check used in\n          // CreateNodeDef() where the actual encoded size is checked.\n          return false;\n        }\n      }\n    }\n  }\n\n  return true;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render_prefs_panel(self, panelid, action=None, **kwargs):  # @UnusedVariable\n\n        # Handle action\n        form = SshForm()\n        delete_form = DeleteSshForm()\n        if action == \"add\" and form.is_submitted():\n            self._add_key(action, form)\n        elif action == 'delete' and delete_form.is_submitted():\n            self._delete_key(action, DeleteSshForm())\n\n        # Get SSH keys if file exists.\n        params = {'form': form}\n        try:\n            params[\"sshkeys\"] = [\n                {'title': key.comment or (key.keytype + ' ' + key.key[:18]), 'fingerprint': key.fingerprint}\n                for key in self.app.currentuser.authorizedkeys\n            ]\n        except IOError:\n            params[\"sshkeys\"] = []\n            flash(_(\"Failed to get SSH keys\"), level='error')\n            _logger.warning(\"error reading SSH keys\", exc_info=1)\n\n        return \"prefs_sshkeys.html\", params\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def delete_shelf(shelf_id):\n    cur_shelf = ub.session.query(ub.Shelf).filter(ub.Shelf.id == shelf_id).first()\n    try:\n        delete_shelf_helper(cur_shelf)\n        flash(_(\"Shelf successfully deleted\"), category=\"success\")\n    except InvalidRequestError:\n        ub.session.rollback()\n        log.error(\"Settings DB is not Writeable\")\n        flash(_(\"Settings DB is not Writeable\"), category=\"error\")\n    return redirect(url_for('web.index'))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_headers_finished(self) -> None:\n        disposition, options = parse_options_header(\n            self._current_part.content_disposition\n        )\n        try:\n            self._current_part.field_name = _user_safe_decode(\n                options[b\"name\"], self._charset\n            )\n        except KeyError:\n            raise MultiPartException(\n                'The Content-Disposition header field \"name\" must be ' \"provided.\"\n            )\n        if b\"filename\" in options:\n            self._current_files += 1\n            if self._current_files > self.max_files:\n                raise MultiPartException(\n                    f\"Too many files. Maximum number of files is {self.max_files}.\"\n                )\n            filename = _user_safe_decode(options[b\"filename\"], self._charset)\n            tempfile = SpooledTemporaryFile(max_size=self.max_file_size)\n            self._current_part.file = UploadFile(\n                file=tempfile,  # type: ignore[arg-type]\n                size=0,\n                filename=filename,\n                headers=Headers(raw=self._current_part.item_headers),\n            )\n        else:\n            self._current_fields += 1\n            if self._current_fields > self.max_fields:\n                raise MultiPartException(\n                    f\"Too many fields. Maximum number of fields is {self.max_fields}.\"\n                )\n            self._current_part.file = None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  inline Eigen::IndexList<Eigen::DenseIndex, Eigen::type2index<1>> NByOne(\n      Eigen::DenseIndex n) {\n    Eigen::IndexList<Eigen::DenseIndex, Eigen::type2index<1>> ret;\n    ret.set(0, n);\n    return ret;\n  }\n  inline Eigen::IndexList<Eigen::type2index<1>, Eigen::DenseIndex> OneByM("}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def feed_data(\n        self,\n        data: bytes,\n        SEP: _SEP = b\"\\r\\n\",\n        EMPTY: bytes = b\"\",\n        CONTENT_LENGTH: istr = hdrs.CONTENT_LENGTH,\n        METH_CONNECT: str = hdrs.METH_CONNECT,\n        SEC_WEBSOCKET_KEY1: istr = hdrs.SEC_WEBSOCKET_KEY1,\n    ) -> Tuple[List[Tuple[_MsgT, StreamReader]], bool, bytes]:\n        messages = []\n\n        if self._tail:\n            data, self._tail = self._tail + data, b\"\"\n\n        data_len = len(data)\n        start_pos = 0\n        loop = self.loop\n\n        while start_pos < data_len:\n            # read HTTP message (request/response line + headers), \\r\\n\\r\\n\n            # and split by lines\n            if self._payload_parser is None and not self._upgraded:\n                pos = data.find(SEP, start_pos)\n                # consume \\r\\n\n                if pos == start_pos and not self._lines:\n                    start_pos = pos + len(SEP)\n                    continue\n\n                if pos >= start_pos:\n                    # line found\n                    line = data[start_pos:pos]\n                    if SEP == b\"\\n\":  # For lax response parsing\n                        line = line.rstrip(b\"\\r\")\n                    self._lines.append(line)\n                    start_pos = pos + len(SEP)\n\n                    # \\r\\n\\r\\n found\n                    if self._lines[-1] == EMPTY:\n                        try:\n                            msg: _MsgT = self.parse_message(self._lines)\n                        finally:\n                            self._lines.clear()\n\n                        def get_content_length() -> Optional[int]:\n                            # payload length\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\n                            if length_hdr is None:\n                                return None\n\n                            # Shouldn't allow +/- or other number formats.\n                            # https://www.rfc-editor.org/rfc/rfc9110#section-8.6-2\n                            # msg.headers is already stripped of leading/trailing wsp\n                            if not DIGITS.fullmatch(length_hdr):\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            return int(length_hdr)\n\n                        length = get_content_length()\n                        # do not support old websocket spec\n                        if SEC_WEBSOCKET_KEY1 in msg.headers:\n                            raise InvalidHeader(SEC_WEBSOCKET_KEY1)\n\n                        self._upgraded = msg.upgrade\n\n                        method = getattr(msg, \"method\", self.method)\n                        # code is only present on responses\n                        code = getattr(msg, \"code\", 0)\n\n                        assert self.protocol is not None\n                        # calculate payload\n                        empty_body = status_code_must_be_empty_body(code) or bool(\n                            method and method_must_be_empty_body(method)\n                        )\n                        if not empty_body and (\n                            (length is not None and length > 0)\n                            or msg.chunked\n                            and not msg.upgrade\n                        ):\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            payload_parser = HttpPayloadParser(\n                                payload,\n                                length=length,\n                                chunked=msg.chunked,\n                                method=method,\n                                compression=msg.compression,\n                                code=self.code,\n                                readall=self.readall,\n                                response_with_body=self.response_with_body,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                            if not payload_parser.done:\n                                self._payload_parser = payload_parser\n                        elif method == METH_CONNECT:\n                            assert isinstance(msg, RawRequestMessage)\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            self._upgraded = True\n                            self._payload_parser = HttpPayloadParser(\n                                payload,\n                                method=msg.method,\n                                compression=msg.compression,\n                                readall=True,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                        elif not empty_body and length is None and self.read_until_eof:\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            payload_parser = HttpPayloadParser(\n                                payload,\n                                length=length,\n                                chunked=msg.chunked,\n                                method=method,\n                                compression=msg.compression,\n                                code=self.code,\n                                readall=True,\n                                response_with_body=self.response_with_body,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                            if not payload_parser.done:\n                                self._payload_parser = payload_parser\n                        else:\n                            payload = EMPTY_PAYLOAD\n\n                        messages.append((msg, payload))\n                else:\n                    self._tail = data[start_pos:]\n                    data = EMPTY\n                    break\n\n            # no parser, just store\n            elif self._payload_parser is None and self._upgraded:\n                assert not self._lines\n                break\n\n            # feed payload\n            elif data and start_pos < data_len:\n                assert not self._lines\n                assert self._payload_parser is not None\n                try:\n                    eof, data = self._payload_parser.feed_data(data[start_pos:], SEP)\n                except BaseException as exc:\n                    if self.payload_exception is not None:\n                        self._payload_parser.payload.set_exception(\n                            self.payload_exception(str(exc))\n                        )\n                    else:\n                        self._payload_parser.payload.set_exception(exc)\n\n                    eof = True\n                    data = b\"\"\n\n                if eof:\n                    start_pos = 0\n                    data_len = len(data)\n                    self._payload_parser = None\n                    continue\n            else:\n                break\n\n        if data and start_pos < data_len:\n            data = data[start_pos:]\n        else:\n            data = EMPTY\n\n        return messages, self._upgraded, data\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_extra(self, select, select_params, where, params, tables, order_by):\n        \"\"\"\n        Add data to the various extra_* attributes for user-created additions\n        to the query.\n        \"\"\"\n        if select:\n            # We need to pair any placeholder markers in the 'select'\n            # dictionary with their parameters in 'select_params' so that\n            # subsequent updates to the select dictionary also adjust the\n            # parameters appropriately.\n            select_pairs = {}\n            if select_params:\n                param_iter = iter(select_params)\n            else:\n                param_iter = iter([])\n            for name, entry in select.items():\n                self.check_alias(name)\n                entry = str(entry)\n                entry_params = []\n                pos = entry.find(\"%s\")\n                while pos != -1:\n                    if pos == 0 or entry[pos - 1] != \"%\":\n                        entry_params.append(next(param_iter))\n                    pos = entry.find(\"%s\", pos + 2)\n                select_pairs[name] = (entry, entry_params)\n            self.extra.update(select_pairs)\n        if where or params:\n            self.where.add(ExtraWhere(where, params), AND)\n        if tables:\n            self.extra_tables += tuple(tables)\n        if order_by:\n            self.extra_order_by = order_by\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def post(self, request, *args, **kwargs):\n        content = request.POST.get(\"content\", \"\")\n        lexer = request.POST.get(\"lexer\", highlight.LEXER_DEFAULT).strip()\n        filename = request.POST.get(\"filename\", \"\").strip()\n        expires = request.POST.get(\"expires\", \"\").strip()\n        response_format = request.POST.get(\"format\", \"default\").strip()\n\n        if not content.strip():\n            return HttpResponseBadRequest(\"No content given\")\n\n        # We need at least a lexer or a filename\n        if not lexer and not filename:\n            return HttpResponseBadRequest(\n                \"No lexer or filename given. Unable to \"\n                \"determine a highlight. Valid lexers are: %s\"\n                % \", \".join(highlight.LEXER_KEYS)\n            )\n\n        # A lexer is given, check if its valid at all\n        if lexer and lexer not in highlight.LEXER_KEYS:\n            return HttpResponseBadRequest(\n                f'Invalid lexer choice \"{escape(lexer)}\" given. Valid lexer values are: {\", \".join(highlight.LEXER_KEYS)}'\n            )\n\n        # No lexer is given, but we have a filename, try to get the lexer\n        #  out of it. In case Pygments cannot determine the lexer of the\n        # filename, we fallback to 'plain' code.\n        if not lexer and filename:\n            try:\n                lexer_cls = get_lexer_for_filename(filename)\n                lexer = lexer_cls.aliases[0]\n            except (ClassNotFound, IndexError):\n                lexer = config.PLAIN_CODE_SYMBOL\n\n        if expires:\n            expire_options = [str(i) for i in dict(config.EXPIRE_CHOICES)]\n            if expires not in expire_options:\n                return HttpResponseBadRequest(\n                    f'Invalid expire choice \"{escape(expires)}\" given. Valid expire values are: {\", \".join(expire_options)}'\n                )\n            expires, expire_type = get_expire_values(expires)\n        else:\n            expires = datetime.datetime.now() + datetime.timedelta(seconds=60 * 60 * 24)\n            expire_type = Snippet.EXPIRE_TIME\n\n        snippet = Snippet.objects.create(\n            content=content,\n            lexer=lexer,\n            expires=expires,\n            expire_type=expire_type,\n        )\n\n        # Custom formatter for the API response\n        formatter = getattr(self, f\"_format_{response_format}\", None)\n        if callable(formatter):\n            return HttpResponse(formatter(snippet))\n\n        # Otherwise use the default one.\n        return HttpResponse(self._format_default(snippet))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& data = context->input(0);\n    const Tensor& segment_ids = context->input(1);\n    const Tensor& num_segments = context->input(2);\n    if (!UnsortedSegmentReductionDoValidation(this, context, data, segment_ids,\n                                              num_segments)) {\n      return;\n    }\n    const auto segment_flat = segment_ids.flat<Index>();\n    const int64 output_rows = internal::SubtleMustCopy(static_cast<int64>(\n        num_segments.dtype() == DT_INT32 ? num_segments.scalar<int32>()()\n                                         : num_segments.scalar<int64>()()));\n    OP_REQUIRES(context, output_rows >= 0,\n                errors::InvalidArgument(\"Input num_segments == \", output_rows,\n                                        \" must not be negative.\"));\n    TensorShape output_shape;\n    output_shape.AddDim(output_rows);\n    for (int i = segment_ids.dims(); i < data.dims(); i++) {\n      output_shape.AddDim(data.dim_size(i));\n    }\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &output));\n    auto output_flat = output->flat_outer_dims<T>();\n    auto data_flat = data.flat_inner_outer_dims<T, 2>(segment_ids.dims() - 1);\n    reduction_functor_(context, segment_ids.shape(), segment_flat, data_flat,\n                       output_flat);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def _received_remote_receipt(self, origin: str, content: JsonDict) -> None:\n        \"\"\"Called when we receive an EDU of type m.receipt from a remote HS.\"\"\"\n        receipts = []\n        for room_id, room_values in content.items():\n            # If we're not in the room just ditch the event entirely. This is\n            # probably an old server that has come back and thinks we're still in\n            # the room (or we've been rejoined to the room by a state reset).\n            is_in_room = await self.event_auth_handler.is_host_in_room(\n                room_id, self.server_name\n            )\n            if not is_in_room:\n                logger.info(\n                    \"Ignoring receipt for room %r from server %s as we're not in the room\",\n                    room_id,\n                    origin,\n                )\n                continue\n\n            # Let's check that the origin server is in the room before accepting the receipt.\n            # We don't want to block waiting on a partial state so take an\n            # approximation if needed.\n            domains = await self._storage_controllers.state.get_current_hosts_in_room_or_partial_state_approximation(\n                room_id\n            )\n            if origin not in domains:\n                logger.info(\n                    \"Ignoring receipt for room %r from server %s as they're not in the room\",\n                    room_id,\n                    origin,\n                )\n                continue\n\n            for receipt_type, users in room_values.items():\n                for user_id, user_values in users.items():\n                    if get_domain_from_id(user_id) != origin:\n                        logger.info(\n                            \"Received receipt for user %r from server %s, ignoring\",\n                            user_id,\n                            origin,\n                        )\n                        continue\n\n                    # Check if these receipts apply to a thread.\n                    data = user_values.get(\"data\", {})\n                    thread_id = data.get(\"thread_id\")\n                    # If the thread ID is invalid, consider it missing.\n                    if not isinstance(thread_id, str):\n                        thread_id = None\n\n                    receipts.append(\n                        ReadReceipt(\n                            room_id=room_id,\n                            receipt_type=receipt_type,\n                            user_id=user_id,\n                            event_ids=user_values[\"event_ids\"],\n                            thread_id=thread_id,\n                            data=data,\n                        )\n                    )\n\n        await self._handle_new_receipts(receipts)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render(self, request):\n        redir = self.base_url + self.basepath\n        request.redirect(protect_redirect_url(redir))\n        return redir\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def parse_type_line(type_line, rcb, loc):\n    \"\"\"Parses a type annotation specified as a comment.\n\n    Example inputs:\n        # type: (Tensor, torch.Tensor) -> Tuple[Tensor]\n        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tensor\n    \"\"\"\n    arg_ann_str, ret_ann_str = split_type_line(type_line)\n\n    try:\n        arg_ann = _eval_no_call(arg_ann_str, {}, EvalEnv(rcb))\n    except (NameError, SyntaxError) as e:\n        raise RuntimeError(\"Failed to parse the argument list of a type annotation\") from e\n\n    if not isinstance(arg_ann, tuple):\n        arg_ann = (arg_ann,)\n\n    try:\n        ret_ann = _eval_no_call(ret_ann_str, {}, EvalEnv(rcb))\n    except (NameError, SyntaxError) as e:\n        raise RuntimeError(\"Failed to parse the return type of a type annotation\") from e\n\n    arg_types = [ann_to_type(ann, loc) for ann in arg_ann]\n    return arg_types, ann_to_type(ret_ann, loc)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _handle_forwarded_room_key_event(\n        self,\n        sender,  # type: str\n        sender_key,  # type: str\n        payload,  # type: Dict[Any, Any]\n    ):\n        # type: (...) -> Union[ForwardedRoomKeyEvent, BadEventType, None]\n        event = ForwardedRoomKeyEvent.from_dict(payload, sender, sender_key)\n\n        if isinstance(event, (BadEvent, UnknownBadEvent)):\n            return event\n\n        if not self._should_accept_forward(sender, sender_key, event):\n            return None\n\n        content = payload[\"content\"]\n\n        session_sender_key = content[\"sender_key\"]\n        signing_key = content[\"sender_claimed_ed25519_key\"]\n        chain = content[\"forwarding_curve25519_key_chain\"]\n        chain.append(session_sender_key)\n\n        session = Olm._import_group_session(\n            content[\"session_key\"],\n            signing_key,\n            session_sender_key,\n            event.room_id,\n            chain,\n        )\n\n        if not session:\n            return None\n\n        if self.inbound_group_store.add(session):\n            self.save_inbound_group_session(session)\n\n        key_request = self.outgoing_key_requests.pop(event.session_id)\n        self.store.remove_outgoing_key_request(key_request)\n        self.outgoing_to_device_messages.append(\n            key_request.as_cancellation(self.user_id, self.device_id)\n        )\n\n        return event\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def to_yaml(self, **kwargs):\n    \"\"\"Returns a yaml string containing the network configuration.\n\n    Note: Since TF 2.6, this method is no longer supported and will raise a\n    RuntimeError.\n\n    To load a network from a yaml save file, use\n    `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n\n    `custom_objects` should be a dictionary mapping\n    the names of custom losses / layers / etc to the corresponding\n    functions / classes.\n\n    Arguments:\n        **kwargs: Additional keyword arguments\n            to be passed to `yaml.dump()`.\n\n    Returns:\n        A YAML string.\n\n    Raises:\n        RuntimeError: announces that the method poses a security risk\n    \"\"\"\n    raise RuntimeError(\n        'Method `model.to_yaml()` has been removed due to security risk of '\n        'arbitrary code execution. Please use `model.to_json()` instead.'\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor* hypothesis_indices;\n    const Tensor* hypothesis_values;\n    const Tensor* hypothesis_shape;\n    const Tensor* truth_indices;\n    const Tensor* truth_values;\n    const Tensor* truth_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_indices\", &hypothesis_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_values\", &hypothesis_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_shape\", &hypothesis_shape));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_indices\", &truth_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_values\", &truth_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_shape\", &truth_shape));\n\n    OP_REQUIRES_OK(\n        ctx, ValidateShapes(ctx, *hypothesis_indices, *hypothesis_values,\n                            *hypothesis_shape, *truth_indices, *truth_values,\n                            *truth_shape));\n\n    TensorShape hypothesis_st_shape;\n    OP_REQUIRES_OK(\n        ctx, TensorShapeUtils::MakeShape(hypothesis_shape->vec<int64>().data(),\n                                         hypothesis_shape->NumElements(),\n                                         &hypothesis_st_shape));\n    TensorShape truth_st_shape;\n    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n                            truth_shape->vec<int64>().data(),\n                            truth_shape->NumElements(), &truth_st_shape));\n\n    // Assume indices are sorted in row-major order.\n    std::vector<int64> sorted_order(truth_st_shape.dims());\n    std::iota(sorted_order.begin(), sorted_order.end(), 0);\n\n    sparse::SparseTensor hypothesis;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *hypothesis_indices, *hypothesis_values,\n                            hypothesis_st_shape, sorted_order, &hypothesis));\n\n    sparse::SparseTensor truth;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *truth_indices, *truth_values, truth_st_shape,\n                            sorted_order, &truth));\n\n    // Group dims 0, 1, ..., RANK - 1.  The very last dim is assumed\n    // to store the variable length sequences.\n    std::vector<int64> group_dims(truth_st_shape.dims() - 1);\n    std::iota(group_dims.begin(), group_dims.end(), 0);\n\n    TensorShape output_shape;\n    for (int d = 0; d < static_cast<int>(group_dims.size()); ++d) {\n      output_shape.AddDim(std::max(hypothesis_st_shape.dim_size(d),\n                                   truth_st_shape.dim_size(d)));\n    }\n    const auto output_elements = output_shape.num_elements();\n    OP_REQUIRES(\n        ctx, output_elements > 0,\n        errors::InvalidArgument(\"Got output shape \", output_shape.DebugString(),\n                                \" which has 0 elements\"));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"output\", output_shape, &output));\n    auto output_t = output->flat<float>();\n    output_t.setZero();\n\n    std::vector<int64> output_strides(output_shape.dims());\n    output_strides[output_shape.dims() - 1] = 1;\n    for (int d = output_shape.dims() - 2; d >= 0; --d) {\n      output_strides[d] = output_strides[d + 1] * output_shape.dim_size(d + 1);\n    }\n\n    auto hypothesis_grouper = hypothesis.group(group_dims);\n    auto truth_grouper = truth.group(group_dims);\n\n    auto hypothesis_iter = hypothesis_grouper.begin();\n    auto truth_iter = truth_grouper.begin();\n\n    auto cmp = std::equal_to<T>();\n\n    while (hypothesis_iter != hypothesis_grouper.end() &&\n           truth_iter != truth_grouper.end()) {\n      sparse::Group truth_i = *truth_iter;\n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64> g_truth = truth_i.group();\n      std::vector<int64> g_hypothesis = hypothesis_j.group();\n      auto truth_seq = truth_i.values<T>();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n\n      if (g_truth == g_hypothesis) {\n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) =\n            gtl::LevenshteinDistance<T>(truth_seq, hypothesis_seq, cmp);\n        if (normalize_) output_t(loc) /= truth_seq.size();\n\n        ++hypothesis_iter;\n        ++truth_iter;\n      } else if (g_truth > g_hypothesis) {  // zero-length truth\n        auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                      output_strides.begin(), int64{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = hypothesis_seq.size();\n        if (normalize_ && output_t(loc) != 0.0f) {\n          output_t(loc) = std::numeric_limits<float>::infinity();\n        }\n        ++hypothesis_iter;\n      } else {  // zero-length hypothesis\n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n        ++truth_iter;\n      }\n    }\n    while (hypothesis_iter != hypothesis_grouper.end()) {  // zero-length truths\n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64> g_hypothesis = hypothesis_j.group();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                    output_strides.begin(), int64{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = hypothesis_seq.size();\n      if (normalize_ && output_t(loc) != 0.0f) {\n        output_t(loc) = std::numeric_limits<float>::infinity();\n      }\n      ++hypothesis_iter;\n    }\n    while (truth_iter != truth_grouper.end()) {  // missing hypotheses\n      sparse::Group truth_i = *truth_iter;\n      std::vector<int64> g_truth = truth_i.group();\n      auto truth_seq = truth_i.values<T>();\n      auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                    output_strides.begin(), int64{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n      ++truth_iter;\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def process_pdus_for_room(room_id):\n            logger.debug(\"Processing PDUs for %s\", room_id)\n            try:\n                yield self.check_server_matches_acl(origin_host, room_id)\n            except AuthError as e:\n                logger.warn(\n                    \"Ignoring PDUs for room %s from banned server\", room_id,\n                )\n                for pdu in pdus_by_room[room_id]:\n                    event_id = pdu.event_id\n                    pdu_results[event_id] = e.error_dict()\n                return\n\n            for pdu in pdus_by_room[room_id]:\n                event_id = pdu.event_id\n                try:\n                    yield self._handle_received_pdu(\n                        origin, pdu\n                    )\n                    pdu_results[event_id] = {}\n                except FederationError as e:\n                    logger.warn(\"Error handling PDU %s: %s\", event_id, e)\n                    pdu_results[event_id] = {\"error\": str(e)}\n                except Exception as e:\n                    f = failure.Failure()\n                    pdu_results[event_id] = {\"error\": str(e)}\n                    logger.error(\n                        \"Failed to handle PDU %s: %s\",\n                        event_id, f.getTraceback().rstrip(),\n                    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _redirect_safe(self, url, default=None):\n        \"\"\"Redirect if url is on our PATH\n\n        Full-domain redirects are allowed if they pass our CORS origin checks.\n\n        Otherwise use default (self.base_url if unspecified).\n        \"\"\"\n        if default is None:\n            default = self.base_url\n        parsed = urlparse(url)\n        if parsed.netloc or not (parsed.path + '/').startswith(self.base_url):\n            # require that next_url be absolute path within our path\n            allow = False\n            # OR pass our cross-origin check\n            if parsed.netloc:\n                # if full URL, run our cross-origin check:\n                origin = '%s://%s' % (parsed.scheme, parsed.netloc)\n                origin = origin.lower()\n                if self.allow_origin:\n                    allow = self.allow_origin == origin\n                elif self.allow_origin_pat:\n                    allow = bool(self.allow_origin_pat.match(origin))\n            if not allow:\n                # not allowed, use default\n                self.log.warning(\"Not allowing login redirect to %r\" % url)\n                url = default\n        self.redirect(url)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _add_auth_parsers_arguments(parser):\n    class VaultEndpointVerifyAction(argparse.Action):\n        def __call__(self, parser, args, values, option_string=None):\n            if values in ['true', 'false']:\n                setattr(args, self.dest, values == 'true')\n            else:\n                setattr(args, self.dest, str(values))\n\n    parser.add_argument('-a', '--auth-method',\n                        type=bigflow.deploy.AuthorizationType,\n                        default='local_account',\n                        help='One of two authentication method: '\n                             'local_account -- you are using credentials of your local user authenticated in gcloud; '\n                             'vault -- credentials for service account are obtained from Vault. '\n                             'Default: local_account',\n                        choices=list(bigflow.deploy.AuthorizationType),\n    )\n    parser.add_argument('-ve', '--vault-endpoint',\n                        type=str,\n                        help='URL of a Vault endpoint to get OAuth token for service account. '\n                             'Required if auth-method is vault. '\n                             'If not set, will be read from deployment_config.py.'\n                        )\n    parser.add_argument('-vev', '--vault-endpoint-verify',\n                        type=str,\n                        action=VaultEndpointVerifyAction,\n                        help='Can be \"true\", \"false\", a path to certificate PEM file or a path to directory with PEM files. '\n                             'Enables/disables vault endpoint TLS certificate verification. Enabled by default. '\n                             'Disabling makes execution vulnerable for MITM attacks - do it only when justified and in trusted environments. '\n                             'For details see: https://requests.readthedocs.io/en/latest/user/advanced/#ssl-cert-verification',\n                        dest='vault_endpoint_verify',\n                        default=True\n                        )\n    parser.add_argument('-vs', '--vault-secret',\n                        type=str,\n                        help='Vault secret token. '\n                             'Required if auth-method is vault.'\n                        )\n    parser.add_argument('-dc', '--deployment-config-path',\n                        type=str,\n                        help='Path to the deployment_config.py file. '\n                             'If not set, {current_dir}/deployment_config.py will be used.')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "export const redirectAnyRequest = response => {\n  const {headers: {location}} = Response.message (response);\n  const original = Response.request (response);\n  const oldUrl = Request.url (original);\n  const newUrl = mergeUrls (oldUrl, location);\n  const request = Request (Request.options (original))\n                          (newUrl)\n                          (Request.body (original));\n\n  return sameHost (oldUrl, newUrl) ? request : overHeaders (request, xs => (\n    xs.filter (([name]) => !confidentialHeaders.includes (name.toLowerCase ()))\n  ));\n};\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def user_create(context, data_dict):\n    '''Create a new user.\n\n    You must be authorized to create users.\n\n    :param name: the name of the new user, a string between 2 and 100\n        characters in length, containing only lowercase alphanumeric\n        characters, ``-`` and ``_``\n    :type name: string\n    :param email: the email address for the new user\n    :type email: string\n    :param password: the password of the new user, a string of at least 4\n        characters\n    :type password: string\n    :param id: the id of the new user (optional)\n    :type id: string\n    :param fullname: the full name of the new user (optional)\n    :type fullname: string\n    :param about: a description of the new user (optional)\n    :type about: string\n    :param image_url: the URL to an image to be displayed on the group's page\n        (optional)\n    :type image_url: string\n    :param plugin_extras: private extra user data belonging to plugins.\n        Only sysadmin users may set this value. It should be a dict that can\n        be dumped into JSON, and plugins should namespace their extras with\n        the plugin name to avoid collisions with other plugins, eg::\n\n            {\n                \"name\": \"test_user\",\n                \"email\": \"test@example.com\",\n                \"plugin_extras\": {\n                    \"my_plugin\": {\n                        \"private_extra\": 1\n                    },\n                    \"another_plugin\": {\n                        \"another_extra\": True\n                    }\n                }\n            }\n    :type plugin_extras: dict\n\n\n    :returns: the newly created user\n    :rtype: dictionary\n\n    '''\n    model = context['model']\n    schema = context.get('schema') or ckan.logic.schema.default_user_schema()\n    session = context['session']\n\n    _check_access('user_create', context, data_dict)\n\n    upload = uploader.get_uploader('user')\n    upload.update_data_dict(data_dict, 'image_url',\n                            'image_upload', 'clear_upload')\n    data, errors = _validate(data_dict, schema, context)\n\n    if errors:\n        session.rollback()\n        raise ValidationError(errors)\n\n    # user schema prevents non-sysadmins from providing password_hash\n    if 'password_hash' in data:\n        data['_password'] = data.pop('password_hash')\n\n    user = model_save.user_dict_save(data, context)\n    plugins.toolkit.signals.user_created.send(user.name, user=user)\n    # Flush the session to cause user.id to be initialised, because\n    # activity_create() (below) needs it.\n    session.flush()\n\n    activity_create_context = {\n        'model': model,\n        'user': context['user'],\n        'defer_commit': True,\n        'ignore_auth': True,\n        'session': session\n    }\n    activity_dict = {\n        'user_id': user.id,\n        'object_id': user.id,\n        'activity_type': 'new user',\n    }\n    logic.get_action('activity_create')(activity_create_context, activity_dict)\n\n    if hasattr(upload, \"verify_type\"):\n        upload.verify_type()\n\n    upload.upload(uploader.get_max_image_size())\n\n    if not context.get('defer_commit'):\n        model.repo.commit()\n\n    # A new context is required for dictizing the newly constructed user in\n    # order that all the new user's data is returned, in particular, the\n    # api_key.\n    #\n    # The context is copied so as not to clobber the caller's context dict.\n    user_dictize_context = context.copy()\n    user_dictize_context['keep_apikey'] = True\n    user_dictize_context['keep_email'] = True\n\n    author_obj = model.User.get(context.get('user'))\n    include_plugin_extras = False\n    if author_obj:\n        include_plugin_extras = author_obj.sysadmin and 'plugin_extras' in data\n    user_dict = model_dictize.user_dictize(\n        user, user_dictize_context,\n        include_plugin_extras=include_plugin_extras\n    )\n\n    context['user_obj'] = user\n    context['id'] = user.id\n\n    model.Dashboard.get(user.id)  # Create dashboard for user.\n\n    log.debug('Created user {name}'.format(name=user.name))\n    return user_dict\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteSpaceToDepthParams*>(node->builtin_data);\n\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n\n  auto data_type = output->type;\n  TF_LITE_ENSURE(context,\n                 data_type == kTfLiteFloat32 || data_type == kTfLiteUInt8 ||\n                     data_type == kTfLiteInt8 || data_type == kTfLiteInt32 ||\n                     data_type == kTfLiteInt64);\n  TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n\n  const int block_size = params->block_size;\n  TF_LITE_ENSURE(context, block_size > 0);\n  const int input_height = input->dims->data[1];\n  const int input_width = input->dims->data[2];\n  int output_height = input_height / block_size;\n  int output_width = input_width / block_size;\n\n  TF_LITE_ENSURE_EQ(context, input_height, output_height * block_size);\n  TF_LITE_ENSURE_EQ(context, input_width, output_width * block_size);\n\n  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);\n  output_size->data[0] = input->dims->data[0];\n  output_size->data[1] = output_height;\n  output_size->data[2] = output_width;\n  output_size->data[3] = input->dims->data[3] * block_size * block_size;\n\n  return context->ResizeTensor(context, output, output_size);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def pascal_case(value: str) -> str:\n    return fix_keywords(stringcase.pascalcase(sanitize(value)))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _is_unsafe(self, val):\n        '''\n        Our helper function, which will also recursively check dict and\n        list entries due to the fact that they may be repr'd and contain\n        a key or value which contains jinja2 syntax and would otherwise\n        lose the AnsibleUnsafe value.\n        '''\n        if isinstance(val, dict):\n            for key in val.keys():\n                if self._is_unsafe(val[key]):\n                    return True\n        elif isinstance(val, list):\n            for item in val:\n                if self._is_unsafe(item):\n                    return True\n        elif hasattr(val, '__UNSAFE__'):\n            return True\n        return False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def download(self, debug_file_id, project):\n        rate_limited = ratelimits.is_limited(\n            project=project,\n            key=f\"rl:DSymFilesEndpoint:download:{debug_file_id}:{project.id}\",\n            limit=10,\n        )\n        if rate_limited:\n            logger.info(\n                \"notification.rate_limited\",\n                extra={\"project_id\": project.id, \"project_debug_file_id\": debug_file_id},\n            )\n            return HttpResponse({\"Too many download requests\"}, status=429)\n\n        debug_file = ProjectDebugFile.objects.filter(\n            id=debug_file_id, project_id=project.id\n        ).first()\n\n        if debug_file is None:\n            raise Http404\n\n        try:\n            fp = debug_file.file.getfile()\n            response = StreamingHttpResponse(\n                iter(lambda: fp.read(4096), b\"\"), content_type=\"application/octet-stream\"\n            )\n            response[\"Content-Length\"] = debug_file.file.size\n            response[\"Content-Disposition\"] = 'attachment; filename=\"{}{}\"'.format(\n                posixpath.basename(debug_file.debug_id),\n                debug_file.file_extension,\n            )\n            return response\n        except OSError:\n            raise Http404\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def finalize_auth(token, identity_type, cookie_dict_extra=None):\n    \"\"\"\n    Finalises login. Validates provided token, sets cookies\n    and redirects to the final page.\n    :param token: token string\n    :param identity_type:  identity_type e.g. x509, userpass, oidc, saml\n    :param cookie_dict_extra: extra cookies to set, dictionary expected\n    :returns: redirects to the final page or renders a page with an error message.\n    \"\"\"\n    cookie = []\n    valid_token_dict = validate_webui_token(from_cookie=False, session_token=token)\n    if not valid_token_dict:\n        return render_template(\"problem.html\", msg=\"It was not possible to validate and finalize your login with the provided token.\")\n    try:\n        attribs = list_account_attributes(valid_token_dict['account'], valid_token_dict['vo'])\n        accounts = identity.list_accounts_for_identity(valid_token_dict['identity'], identity_type)\n        accvalues = \"\"\n        for acc in accounts:\n            accvalues += acc + \" \"\n        accounts = accvalues[:-1]\n\n        cookie.extend([{'key': 'x-rucio-auth-token', 'value': quote(token)},\n                       {'key': 'x-rucio-auth-type', 'value': quote(identity_type)},\n                       {'key': 'rucio-auth-token-created-at', 'value': str(long(time()))},\n                       {'key': 'rucio-available-accounts', 'value': quote(accounts)},\n                       {'key': 'rucio-account-attr', 'value': quote(dumps(attribs))},\n                       {'key': 'rucio-selected-account', 'value': quote(valid_token_dict['account'])},\n                       {'key': 'rucio-selected-vo', 'value': quote(valid_token_dict['vo'])}])\n\n        if cookie_dict_extra:\n            for key, value in cookie_dict_extra.items():\n                cookie.append({'key': key, 'value': value})\n        return redirect_to_last_known_url(cookie)\n    except Exception:\n        return render_template(\"problem.html\", msg=\"It was not possible to validate and finalize your login with the provided token.\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def link_dialog(request):\n    # list of wiki pages\n    name = request.values.get(\"pagename\", \"\")\n    name_escaped = wikiutil.escape(name)\n    if name:\n        from MoinMoin import search\n        # XXX error handling!\n        searchresult = search.searchPages(request, 't:\"%s\"' % name)\n\n        pages = [p.page_name for p in searchresult.hits]\n        pages.sort()\n        pages[0:0] = [name]\n        page_list = '''\n         <tr>\n          <td colspan=2>\n           <select id=\"sctPagename\" size=\"1\" onchange=\"OnChangePagename(this.value);\">\n           %s\n           </select>\n          <td>\n         </tr>\n''' % \"\\n\".join(['<option value=\"%s\">%s</option>' % (wikiutil.escape(page), wikiutil.escape(page))\n                 for page in pages])\n    else:\n        page_list = \"\"\n\n    # list of interwiki names\n    interwiki_list = wikiutil.load_wikimap(request)\n    interwiki = interwiki_list.keys()\n    interwiki.sort()\n    iwpreferred = request.cfg.interwiki_preferred[:]\n    if not iwpreferred or iwpreferred and iwpreferred[-1] is not None:\n        resultlist = iwpreferred\n        for iw in interwiki:\n            if not iw in iwpreferred:\n                resultlist.append(iw)\n    else:\n        resultlist = iwpreferred[:-1]\n    interwiki = \"\\n\".join(\n        ['<option value=\"%s\">%s</option>' % (wikiutil.escape(key), wikiutil.escape(key))\n         for key in resultlist])\n\n    # wiki url\n    url_prefix_static = request.cfg.url_prefix_static\n    scriptname = request.script_root + '/'\n    action = scriptname\n    basepage = wikiutil.escape(request.page.page_name)\n    request.write(u'''\n<!--\n * FCKeditor - The text editor for internet\n * Copyright (C) 2003-2004 Frederico Caldeira Knabben\n *\n * Licensed under the terms of the GNU Lesser General Public License:\n *   http://www.opensource.org/licenses/lgpl-license.php\n *\n * For further information visit:\n *   http://www.fckeditor.net/\n *\n * File Name: fck_link.html\n *  Link dialog window.\n *\n * Version:  2.0 FC (Preview)\n * Modified: 2005-02-18 23:55:22\n *\n * File Authors:\n *   Frederico Caldeira Knabben (fredck@fckeditor.net)\n-->\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\">\n<meta name=\"robots\" content=\"index,nofollow\">\n<html>\n <head>\n  <title>Link Properties</title>\n  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n  <meta name=\"robots\" content=\"noindex,nofollow\" />\n  <script src=\"%(url_prefix_static)s/applets/FCKeditor/editor/dialog/common/fck_dialog_common.js\" type=\"text/javascript\"></script>\n  <script src=\"%(url_prefix_static)s/applets/moinFCKplugins/moinlink/fck_link.js\" type=\"text/javascript\"></script>\n  <script src=\"%(url_prefix_static)s/applets/moinFCKplugins/moinurllib.js\" type=\"text/javascript\"></script>\n </head>\n <body scroll=\"no\" style=\"OVERFLOW: hidden\">\n  <div id=\"divInfo\" style=\"DISPLAY: none\">\n   <span fckLang=\"DlgLnkType\">Link Type</span><br />\n   <select id=\"cmbLinkType\" onchange=\"SetLinkType(this.value);\">\n    <option value=\"wiki\" selected=\"selected\">WikiPage</option>\n    <option value=\"interwiki\">Interwiki</option>\n    <option value=\"url\" fckLang=\"DlgLnkTypeURL\">URL</option>\n   </select>\n   <br />\n   <br />\n   <div id=\"divLinkTypeWiki\">\n    <table height=\"100%%\" cellSpacing=\"0\" cellPadding=\"0\" width=\"100%%\" border=\"0\">\n     <tr>\n      <td>\n       <form action=%(action)s method=\"GET\">\n       <input type=\"hidden\" name=\"action\" value=\"fckdialog\">\n       <input type=\"hidden\" name=\"dialog\" value=\"link\">\n       <input type=\"hidden\" id=\"basepage\" name=\"basepage\" value=\"%(basepage)s\">\n       <table cellSpacing=\"0\" cellPadding=\"0\" align=\"center\" border=\"0\">\n        <tr>\n         <td>\n          <span fckLang=\"PageDlgName\">Page Name</span><br>\n          <input id=\"txtPagename\" name=\"pagename\" size=\"30\" value=\"%(name_escaped)s\">\n         </td>\n         <td valign=\"bottom\">\n           <input id=btnSearchpage type=\"submit\" value=\"Search\">\n         </td>\n        </tr>\n        %(page_list)s\n       </table>\n       </form>\n      </td>\n     </tr>\n    </table>\n   </div>\n   <div id=\"divLinkTypeInterwiki\">\n    <table height=\"100%%\" cellSpacing=\"0\" cellPadding=\"0\" width=\"100%%\" border=\"0\">\n     <tr>\n      <td>\n       <table cellSpacing=\"0\" cellPadding=\"0\" align=\"center\" border=\"0\">\n        <tr>\n         <td>\n          <span fckLang=\"WikiDlgName\">Wiki:PageName</span><br>\n          <select id=\"sctInterwiki\" size=\"1\">\n          %(interwiki)s\n          </select>:\n          <input id=\"txtInterwikipagename\"></input>\n         </td>\n        </tr>\n       </table>\n      </td>\n     </tr>\n    </table>\n   </div>\n   <div id=\"divLinkTypeUrl\">\n    <table cellspacing=\"0\" cellpadding=\"0\" width=\"100%%\" border=\"0\">\n     <tr>\n      <td nowrap=\"nowrap\">\n       <span fckLang=\"DlgLnkProto\">Protocol</span><br />\n       <select id=\"cmbLinkProtocol\">\n        <option value=\"http://\" selected=\"selected\">http://</option>\n        <option value=\"https://\">https://</option>\n        <option value=\"ftp://\">ftp://</option>\n        <option value=\"file://\">file://</option>\n        <option value=\"news://\">news://</option>\n        <option value=\"mailto:\">mailto:</option>\n        <option value=\"\" fckLang=\"DlgLnkProtoOther\">&lt;other&gt;</option>\n       </select>\n      </td>\n      <td nowrap=\"nowrap\">&nbsp;</td>\n      <td nowrap=\"nowrap\" width=\"100%%\">\n       <span fckLang=\"DlgLnkURL\">URL</span><br />\n       <input id=\"txtUrl\" style=\"WIDTH: 100%%\" type=\"text\" onkeyup=\"OnUrlChange();\" onchange=\"OnUrlChange();\" />\n      </td>\n     </tr>\n    </table>\n    <br />\n   </div>\n  </div>\n </body>\n</html>\n''' % locals())\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def logout(request):\n    \"\"\" Clears the session and logs the current user out. \"\"\"\n    request.user_logout()\n    # FIXME(gabriel): we don't ship a view named splash\n    return shortcuts.redirect('splash')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def modify_setting(self, setting_name, value, token):\n        self._log(\"modify_setting(%s)\" % setting_name, token=token)\n        self.check_access(token, \"modify_setting\")\n        try:\n            self.api.settings().set(setting_name, value)\n            return 0\n        except:\n            return 1\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, hs):\n        self.server_name = hs.hostname\n        self.client = hs.get_federation_http_client()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def renderCell(self, item):\n        value = get_value(item, self.attribute, default=())\n\n        if not isinstance(value, (list, tuple)):\n            value = (value,)\n\n        gtool = getToolByName(plone.api.portal.get(), 'portal_groups')\n        mtool = getToolByName(plone.api.portal.get(), 'portal_membership')\n        principals = []\n        for principal_id in value:\n            user = mtool.getMemberById(principal_id)\n            if user is not None:\n                principals.append(escape(user.getProperty('fullname', None)) or user.getId())\n            else:\n                group = gtool.getGroupById(principal_id)\n                if group is not None:\n                    principals.append(escape(group.getProperty('title', None)) or group.getId())\n\n        return ', '.join(principals).decode('utf-8')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_paths(root: str, sub_path: str) \\\n        -> typing.Tuple[pathlib.Path, pathlib.Path]:\n    base_path = flask.safe_join(root, sub_path)\n    data_file = pathlib.Path(base_path + \".data\")\n    metadata_file = pathlib.Path(base_path + \".meta\")\n\n    return data_file, metadata_file\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus ScatterNd(const TfLiteTensor* indices, const TfLiteTensor* updates,\n                       TfLiteTensor* output) {\n  return reference_ops::ScatterNd(\n      GetTensorShape(indices), GetTensorData<IndicesT>(indices),\n      GetTensorShape(updates), GetTensorData<UpdatesT>(updates),\n      GetTensorShape(output), GetTensorData<UpdatesT>(output));\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& gradient = ctx->input(0);\n    const Tensor& input = ctx->input(1);\n    Tensor* input_backprop = nullptr;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, input.shape(), &input_backprop));\n    OP_REQUIRES(\n        ctx, axis_ >= -1,\n        errors::InvalidArgument(\"Axis must be at least -1. Found \", axis_));\n    OP_REQUIRES(ctx, (axis_ == -1 || axis_ < input.shape().dims()),\n                errors::InvalidArgument(\n                    \"Axis should be -1 or 0 or a positive value less than \",\n                    input.shape().dims(), \"but given axis value was \", axis_));\n\n    OP_REQUIRES(\n        ctx, input.IsSameSize(gradient),\n        errors::InvalidArgument(\"gradient and input must be the same size\"));\n    const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n    const Tensor& input_min_tensor = ctx->input(2);\n    OP_REQUIRES(ctx,\n                input_min_tensor.dims() == 0 || input_min_tensor.dims() == 1,\n                errors::InvalidArgument(\n                    \"Input min tensor must have dimension 1. Recieved \",\n                    input_min_tensor.dims(), \".\"));\n    const Tensor& input_max_tensor = ctx->input(3);\n    OP_REQUIRES(ctx,\n                input_max_tensor.dims() == 0 || input_max_tensor.dims() == 1,\n                errors::InvalidArgument(\n                    \"Input max tensor must have dimension 1. Recieved \",\n                    input_max_tensor.dims(), \".\"));\n    if (axis_ != -1) {\n      OP_REQUIRES(\n          ctx, input_min_tensor.dim_size(0) == depth,\n          errors::InvalidArgument(\"min has incorrect size, expected \", depth,\n                                  \" was \", input_min_tensor.dim_size(0)));\n      OP_REQUIRES(\n          ctx, input_max_tensor.dim_size(0) == depth,\n          errors::InvalidArgument(\"max has incorrect size, expected \", depth,\n                                  \" was \", input_max_tensor.dim_size(0)));\n    }\n\n    TensorShape min_max_shape(input_min_tensor.shape());\n    Tensor* input_min_backprop;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(1, min_max_shape, &input_min_backprop));\n\n    Tensor* input_max_backprop;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(2, min_max_shape, &input_max_backprop));\n\n    if (axis_ == -1) {\n      functor::QuantizeAndDequantizeOneScaleGradientFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(), gradient.template flat<T>(),\n        input.template flat<T>(), input_min_tensor.scalar<T>(),\n        input_max_tensor.scalar<T>(), input_backprop->template flat<T>(),\n        input_min_backprop->template scalar<T>(),\n        input_max_backprop->template scalar<T>());\n    } else {\n      functor::QuantizeAndDequantizePerChannelGradientFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(),\n        gradient.template flat_inner_outer_dims<T, 3>(axis_ - 1),\n        input.template flat_inner_outer_dims<T, 3>(axis_ - 1),\n        &input_min_tensor, &input_max_tensor,\n        input_backprop->template flat_inner_outer_dims<T, 3>(axis_ - 1),\n        input_min_backprop->template flat<T>(),\n        input_max_backprop->template flat<T>());\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def class_instances_from_soap_enveloped_saml_thingies(text, modules):\n    \"\"\"Parses a SOAP enveloped header and body SAML thing and returns the\n    thing as a dictionary class instance.\n\n    :param text: The SOAP object as XML\n    :param modules: modules representing xsd schemas\n    :return: The body and headers as class instances\n    \"\"\"\n    try:\n        envelope = defusedxml.ElementTree.fromstring(text)\n    except Exception as exc:\n        raise XmlParseError(\"%s\" % exc)\n\n    assert envelope.tag == '{%s}Envelope' % soapenv.NAMESPACE\n    assert len(envelope) >= 1\n    env = {\"header\": [], \"body\": None}\n\n    for part in envelope:\n        if part.tag == '{%s}Body' % soapenv.NAMESPACE:\n            assert len(part) == 1\n            env[\"body\"] = instanciate_class(part[0], modules)\n        elif part.tag == \"{%s}Header\" % soapenv.NAMESPACE:\n            for item in part:\n                env[\"header\"].append(instanciate_class(item, modules))\n\n    return env\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, filename):\n        \"\"\"\n        Validate the filename by calling get_valid_name() and return a filename\n        to be passed to the save() method.\n        \"\"\"\n        # `filename` may include a path as returned by FileField.upload_to.\n        dirname, filename = os.path.split(filename)\n        if '..' in pathlib.PurePath(dirname).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dirname)\n        return os.path.normpath(os.path.join(dirname, self.get_valid_name(filename)))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const Tensor& filter_sizes = context->input(1);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(filter_sizes.shape()),\n        errors::InvalidArgument(\n            \"Conv2DBackpropFilter: filter_sizes input must be 1-dim, not \",\n            filter_sizes.dims()));\n    TensorShape filter_shape;\n    const int32* filter_sizes_data = filter_sizes.template flat<int32>().data();\n    for (int i = 0; i < filter_sizes.NumElements(); ++i) {\n      OP_REQUIRES(context, filter_sizes_data[i] >= 0,\n                  errors::InvalidArgument(\"Dimension \", i,\n                                          \" of filter_sizes must be >= 0\"));\n      OP_REQUIRES_OK(context,\n                     filter_shape.AddDimWithStatus(filter_sizes_data[i]));\n    }\n    const TensorShape& input_shape = input.shape();\n\n    EXTRACT_AND_VERIFY_DIMENSIONS(\"DepthwiseConv2DBackpropFilter\");\n    Tensor* filter_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {1}, 0, filter_shape, &filter_backprop));\n\n    // If there is nothing to compute, return.\n    if (out_backprop.shape().num_elements() == 0) {\n      return;\n    }\n\n    // If in_depth==1, this operation is just a standard convolution.\n    // Depthwise convolution is a special case of cuDNN's grouped convolution.\n    bool use_cudnn = std::is_same<Device, GPUDevice>::value &&\n                     (in_depth == 1 ||\n                      (use_cudnn_grouped_conv_ &&\n                       (IsCudnnSupportedFilterSize(/*filter_rows=*/filter_rows,\n                                                   /*filter_cols=*/filter_cols,\n                                                   /*in_depth=*/in_depth,\n                                                   /*out_depth=*/out_depth) ||\n                        OpDeterminismRequired())));\n\n    VLOG(2) << \"DepthwiseConv2dNativeBackpropFilter: \"\n            << \" Input: [\" << batch << \", \" << input_rows << \", \" << input_cols\n            << \", \" << in_depth << \"]; Filter: [\" << filter_rows << \", \"\n            << filter_cols << \", \" << in_depth << \", \" << depth_multiplier\n            << \"]; Output: [\" << batch << \", \" << out_rows << \", \" << out_cols\n            << \", \" << out_depth << \"], stride = \" << stride_\n            << \", pad_rows = \" << pad_top << \", pad_cols = \" << pad_left\n            << \", Use cuDNN: \" << use_cudnn;\n\n    if (use_cudnn) {\n      // Reshape from TF depthwise filter to cuDNN grouped convolution filter:\n      //\n      //                  | TensorFlow       | cuDNN\n      // --------------------------------------------------------------------\n      // filter_out_depth | depth_multiplier | depth_multiplier * group_count\n      // filter_in_depth  | in_depth         | in_depth / group_count\n      //\n      // For depthwise convolution, we have group_count == in_depth.\n      int32_t filter_in_depth = 1;\n      TensorShape shape =\n          TensorShape{filter_rows, filter_cols, filter_in_depth, out_depth};\n      Tensor reshaped_filter(/*type=*/dtype_);\n      OP_REQUIRES(\n          context, reshaped_filter.CopyFrom(*filter_backprop, shape),\n          errors::Internal(\n              \"Failed to reshape filter tensor for grouped convolution.\"));\n\n      // TODO(yangzihao): Send in arbitrary dilation rates after the dilated\n      // conv is supported.\n      launcher_(context, /*use_cudnn=*/true, cudnn_use_autotune_, out_backprop,\n                input,\n                /*row_dilation=*/1, /*col_dilation=*/1, stride_, stride_,\n                padding_, explicit_paddings_, &reshaped_filter, data_format_);\n      return;\n    }\n\n    // For GPU inputs with type half, we cast inputs to float and outputs back\n    // to half, as half implementation is slow and does not use full precision\n    // accumulation in some cases.\n    constexpr bool cast_to_float = std::is_same<T, Eigen::half>::value &&\n                                   std::is_same<Device, GPUDevice>::value;\n    using U = typename std::conditional<cast_to_float, float, T>::type;\n    Tensor casted_out_backprop = out_backprop;\n    Tensor casted_input = input;\n    Tensor casted_filter_backprop = *filter_backprop;\n    const Device& device = context->template eigen_device<Device>();\n    if (cast_to_float) {\n      functor::CastFunctor<Device, float, Eigen::half> cast;\n      OP_REQUIRES_OK(context,\n                     context->allocate_temp(DT_FLOAT, out_backprop.shape(),\n                                            &casted_out_backprop));\n      cast(device, casted_out_backprop.template flat<float>(),\n           out_backprop.template flat<Eigen::half>());\n      OP_REQUIRES_OK(context, context->allocate_temp(DT_FLOAT, input.shape(),\n                                                     &casted_input));\n      cast(device, casted_input.template flat<float>(),\n           input.template flat<Eigen::half>());\n      OP_REQUIRES_OK(context,\n                     context->allocate_temp(DT_FLOAT, filter_backprop->shape(),\n                                            &casted_filter_backprop));\n    }\n\n    auto out_backprop_ptr = casted_out_backprop.template flat<U>().data();\n    auto input_ptr = casted_input.template flat<U>().data();\n    auto filter_backprop_ptr = casted_filter_backprop.template flat<U>().data();\n    LaunchDepthwiseConvBackpropFilterOp<Device, U>()(\n        context, args, out_backprop_ptr, input_ptr, filter_backprop_ptr,\n        data_format_);\n\n    if (cast_to_float) {\n      functor::CastFunctor<Device, Eigen::half, float> cast;\n      const Tensor& casted_filter_backprop_const = casted_filter_backprop;\n      cast(device, filter_backprop->template flat<Eigen::half>(),\n           casted_filter_backprop_const.template flat<float>());\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status GetFirstDimensionSize(OpKernelContext* context, INDEX_TYPE* result) {\n    const Tensor first_partition_tensor =\n        context->input(kFirstPartitionInputIndex);\n    if (row_partition_types_.empty()) {\n      return errors::InvalidArgument(\"No row_partition_types given.\");\n    }\n    const RowPartitionType first_partition_type = row_partition_types_[0];\n    switch (first_partition_type) {\n      case RowPartitionType::FIRST_DIM_SIZE:\n        *result = first_partition_tensor.scalar<INDEX_TYPE>()();\n        return Status::OK();\n      case RowPartitionType::VALUE_ROWIDS:\n        return errors::InvalidArgument(\n            \"Cannot handle VALUE_ROWIDS in first dimension.\");\n      case RowPartitionType::ROW_SPLITS:\n        *result = first_partition_tensor.shape().dim_size(0) - 1;\n        return Status::OK();\n      default:\n        return errors::InvalidArgument(\n            \"Cannot handle type \",\n            RowPartitionTypeToString(first_partition_type));\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static cache_accel_t *read_cache_accel(RBuffer *cache_buf, cache_hdr_t *hdr, cache_map_t *maps, int n_maps) {\n\tif (!cache_buf || !hdr || !hdr->accelerateInfoSize || !hdr->accelerateInfoAddr) {\n\t\treturn NULL;\n\t}\n\tsize_t mc = R_MIN (hdr->mappingCount, n_maps);\n\tut64 offset = va2pa (hdr->accelerateInfoAddr, mc, maps, cache_buf, 0, NULL, NULL);\n\tif (!offset) {\n\t\treturn NULL;\n\t}\n\n\tut64 size = sizeof (cache_accel_t);\n\tcache_accel_t *accel = R_NEW0 (cache_accel_t);\n\tif (!accel) {\n\t\treturn NULL;\n\t}\n\n\tif (r_buf_fread_at (cache_buf, offset, (ut8*) accel, \"16il\", 1) != size) {\n\t\tR_FREE (accel);\n\t\treturn NULL;\n\t}\n\n\taccel->imagesExtrasOffset += offset;\n\taccel->bottomUpListOffset += offset;\n\taccel->dylibTrieOffset += offset;\n\taccel->initializersOffset += offset;\n\taccel->dofSectionsOffset += offset;\n\taccel->reExportListOffset += offset;\n\taccel->depListOffset += offset;\n\taccel->rangeTableOffset += offset;\n\n\treturn accel;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def on_claim_client_keys(\n        self, query: List[Tuple[str, str, str, int]], always_include_fallback_keys: bool\n    ) -> Dict[str, Any]:\n        if any(\n            not self.hs.is_mine(UserID.from_string(user_id))\n            for user_id, _, _, _ in query\n        ):\n            raise SynapseError(400, \"User is not hosted on this homeserver\")\n\n        log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})\n        results = await self._e2e_keys_handler.claim_local_one_time_keys(\n            query, always_include_fallback_keys=always_include_fallback_keys\n        )\n\n        json_result: Dict[str, Dict[str, Dict[str, JsonDict]]] = {}\n        for result in results:\n            for user_id, device_keys in result.items():\n                for device_id, keys in device_keys.items():\n                    for key_id, key in keys.items():\n                        json_result.setdefault(user_id, {}).setdefault(device_id, {})[\n                            key_id\n                        ] = key\n\n        logger.info(\n            \"Claimed one-time-keys: %s\",\n            \",\".join(\n                (\n                    \"%s for %s:%s\" % (key_id, user_id, device_id)\n                    for user_id, user_keys in json_result.items()\n                    for device_id, device_keys in user_keys.items()\n                    for key_id, _ in device_keys.items()\n                )\n            ),\n        )\n\n        return {\"one_time_keys\": json_result}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def title(self):\n        if base_hasattr(self.context, 'get_full_title'):\n            title = self.context.get_full_title()\n        else:\n            title = self.context.Title()\n        title = title and safe_unicode(title) or u\"\"\n        return escape(title, quote=True)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def save_cover_from_url(url, book_path):\n    try:\n        if not cli.allow_localhost:\n            # 127.0.x.x, localhost, [::1], [::ffff:7f00:1]\n            ip = socket.getaddrinfo(urlparse(url).hostname, 0)[0][4][0]\n            if ip.startswith(\"127.\") or ip.startswith('::ffff:7f') or ip == \"::1\" or ip == \"0.0.0.0\" or ip == \"::\":\n                log.error(\"Localhost was accessed for cover upload\")\n                return False, _(\"You are not allowed to access localhost for cover uploads\")\n        img = requests.get(url, timeout=(10, 200), allow_redirects=False)      # ToDo: Error Handling\n        img.raise_for_status()\n        return save_cover(img, book_path)\n    except (socket.gaierror,\n            requests.exceptions.HTTPError,\n            requests.exceptions.ConnectionError,\n            requests.exceptions.Timeout) as ex:\n        log.info(u'Cover Download Error %s', ex)\n        return False, _(\"Error Downloading Cover\")\n    except MissingDelegateError as ex:\n        log.info(u'File Format Error %s', ex)\n        return False, _(\"Cover Format Error\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    // This call processes inputs 1 and 2 to write output 0.\n    ReshapeOp::Compute(ctx);\n    if (!ctx->status().ok()) {\n      return;\n    }\n\n    const auto& input_min_float_tensor = ctx->input(2);\n    const auto& input_min_float_shape = input_min_float_tensor.shape();\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsScalar(input_min_float_shape) ||\n                    (TensorShapeUtils::IsVector(input_min_float_shape) &&\n                     (input_min_float_shape.dim_size(0) == 1)),\n                errors::InvalidArgument(\n                    \"input_min must be a scalar or a vector of 1 element\"));\n    const float input_min_float = input_min_float_tensor.flat<float>()(0);\n    const auto& input_max_float_tensor = ctx->input(3);\n    const auto& input_max_float_shape = input_max_float_tensor.shape();\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsScalar(input_max_float_shape) ||\n                    (TensorShapeUtils::IsVector(input_max_float_shape) &&\n                     (input_max_float_shape.dim_size(0) == 1)),\n                errors::InvalidArgument(\n                    \"input_max must be a scalar or a vector of 1 element\"));\n    const float input_max_float = input_max_float_tensor.flat<float>()(0);\n\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\n    output_min->flat<float>()(0) = input_min_float;\n\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(2, TensorShape({}), &output_max));\n    output_max->flat<float>()(0) = input_max_float;\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "const void* copy_iov_to_host(IM3Runtime runtime, void* _mem, struct iovec* host_iov, wasi_iovec_t* wasi_iov, int32_t iovs_len)\n{\n    // Convert wasi memory offsets to host addresses\n    for (int i = 0; i < iovs_len; i++) {\n        host_iov[i].iov_base = m3ApiOffsetToPtr(m3ApiReadMem32(&wasi_iov[i].buf));\n        host_iov[i].iov_len  = m3ApiReadMem32(&wasi_iov[i].buf_len);\n        m3ApiCheckMem(host_iov[i].iov_base,     host_iov[i].iov_len);\n    }\n    m3ApiSuccess();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_extra(self, select, select_params, where, params, tables, order_by):\n        \"\"\"\n        Add data to the various extra_* attributes for user-created additions\n        to the query.\n        \"\"\"\n        if select:\n            # We need to pair any placeholder markers in the 'select'\n            # dictionary with their parameters in 'select_params' so that\n            # subsequent updates to the select dictionary also adjust the\n            # parameters appropriately.\n            select_pairs = {}\n            if select_params:\n                param_iter = iter(select_params)\n            else:\n                param_iter = iter([])\n            for name, entry in select.items():\n                self.check_alias(name)\n                entry = str(entry)\n                entry_params = []\n                pos = entry.find(\"%s\")\n                while pos != -1:\n                    if pos == 0 or entry[pos - 1] != \"%\":\n                        entry_params.append(next(param_iter))\n                    pos = entry.find(\"%s\", pos + 2)\n                select_pairs[name] = (entry, entry_params)\n            self.extra.update(select_pairs)\n        if where or params:\n            self.where.add(ExtraWhere(where, params), AND)\n        if tables:\n            self.extra_tables += tuple(tables)\n        if order_by:\n            self.extra_order_by = order_by\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def encode(self, types: Iterable[TypeStr], args: Iterable[Any]) -> bytes:\n        \"\"\"\n        Encodes the python values in ``args`` as a sequence of binary values of\n        the ABI types in ``types`` via the head-tail mechanism.\n\n        :param types: A list or tuple of string representations of the ABI types\n            that will be used for encoding e.g.  ``('uint256', 'bytes[]',\n            '(int,int)')``\n        :param args: A list or tuple of python values to be encoded.\n\n        :returns: The head-tail encoded binary representation of the python\n            values in ``args`` as values of the ABI types in ``types``.\n        \"\"\"\n        # validate encode types\n        validate_codec_types(types)\n\n        # validate encode args\n        validate_list_like_param(args, \"args\")\n\n        encoders = [self._registry.get_encoder(type_str) for type_str in types]\n\n        encoder = TupleEncoder(encoders=encoders)\n\n        return encoder(args)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_header_end(self) -> None:\n        field = self._current_partial_header_name.lower()\n        if field == b\"content-disposition\":\n            self._current_part.content_disposition = self._current_partial_header_value\n        self._current_part.item_headers.append(\n            (field, self._current_partial_header_value)\n        )\n        self._current_partial_header_name = b\"\"\n        self._current_partial_header_value = b\"\"\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def write_data(self, data, thefile, shred=True, mode=0o600):\n        # TODO: add docstrings for arg types since this code is picky about that\n        \"\"\"Write the data bytes to given path\n\n        This is used to write a byte string to a file or stdout. It is used for\n        writing the results of vault encryption or decryption. It is used for\n        saving the ciphertext after encryption and it is also used for saving the\n        plaintext after decrypting a vault. The type of the 'data' arg should be bytes,\n        since in the plaintext case, the original contents can be of any text encoding\n        or arbitrary binary data.\n\n        When used to write the result of vault encryption, the val of the 'data' arg\n        should be a utf-8 encoded byte string and not a text typ and not a text type..\n\n        When used to write the result of vault decryption, the val of the 'data' arg\n        should be a byte string and not a text type.\n\n        :arg data: the byte string (bytes) data\n        :arg thefile: file descriptor or filename to save 'data' to.\n        :arg shred: if shred==True, make sure that the original data is first shredded so that is cannot be recovered.\n        :returns: None\n        \"\"\"\n        # FIXME: do we need this now? data_bytes should always be a utf-8 byte string\n        b_file_data = to_bytes(data, errors='strict')\n\n        # check if we have a file descriptor instead of a path\n        is_fd = False\n        try:\n            is_fd = (isinstance(thefile, int) and fcntl.fcntl(thefile, fcntl.F_GETFD) != -1)\n        except Exception:\n            pass\n\n        if is_fd:\n            # if passed descriptor, use that to ensure secure access, otherwise it is a string.\n            # assumes the fd is securely opened by caller (mkstemp)\n            os.ftruncate(thefile, 0)\n            os.write(thefile, b_file_data)\n        elif thefile == '-':\n            # get a ref to either sys.stdout.buffer for py3 or plain old sys.stdout for py2\n            # We need sys.stdout.buffer on py3 so we can write bytes to it since the plaintext\n            # of the vaulted object could be anything/binary/etc\n            output = getattr(sys.stdout, 'buffer', sys.stdout)\n            output.write(b_file_data)\n        else:\n            # file names are insecure and prone to race conditions, so remove and create securely\n            if os.path.isfile(thefile):\n                if shred:\n                    self._shred_file(thefile)\n                else:\n                    os.remove(thefile)\n\n            # when setting new umask, we get previous as return\n            current_umask = os.umask(0o077)\n            try:\n                try:\n                    # create file with secure permissions\n                    fd = os.open(thefile, os.O_CREAT | os.O_EXCL | os.O_RDWR | os.O_TRUNC, mode)\n                except OSError as ose:\n                    # Want to catch FileExistsError, which doesn't exist in Python 2, so catch OSError\n                    # and compare the error number to get equivalent behavior in Python 2/3\n                    if ose.errno == errno.EEXIST:\n                        raise AnsibleError('Vault file got recreated while we were operating on it: %s' % to_native(ose))\n\n                    raise AnsibleError('Problem creating temporary vault file: %s' % to_native(ose))\n\n                try:\n                    # now write to the file and ensure ours is only data in it\n                    os.ftruncate(fd, 0)\n                    os.write(fd, b_file_data)\n                except OSError as e:\n                    raise AnsibleError('Unable to write to temporary vault file: %s' % to_native(e))\n                finally:\n                    # Make sure the file descriptor is always closed and reset umask\n                    os.close(fd)\n            finally:\n                os.umask(current_umask)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __parse_content_stream(self, stream: StreamType) -> None:\n        # 7.8.2 Content Streams\n        stream.seek(0, 0)\n        operands: List[Union[int, str, PdfObject]] = []\n        while True:\n            peek = read_non_whitespace(stream)\n            if peek == b\"\" or peek == 0:\n                break\n            stream.seek(-1, 1)\n            if peek.isalpha() or peek in (b\"'\", b'\"'):\n                operator = read_until_regex(stream, NameObject.delimiter_pattern)\n                if operator == b\"BI\":\n                    # begin inline image - a completely different parsing\n                    # mechanism is required, of course... thanks buddy...\n                    assert operands == []\n                    ii = self._read_inline_image(stream)\n                    self.operations.append((ii, b\"INLINE IMAGE\"))\n                else:\n                    self.operations.append((operands, operator))\n                    operands = []\n            elif peek == b\"%\":\n                # If we encounter a comment in the content stream, we have to\n                # handle it here.  Typically, read_object will handle\n                # encountering a comment -- but read_object assumes that\n                # following the comment must be the object we're trying to\n                # read.  In this case, it could be an operator instead.\n                while peek not in (b\"\\r\", b\"\\n\", b\"\"):\n                    peek = stream.read(1)\n            else:\n                operands.append(read_object(stream, None, self.forced_encoding))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "int ossl_dh_compute_key(unsigned char *key, const BIGNUM *pub_key, DH *dh)\n{\n    BN_CTX *ctx = NULL;\n    BN_MONT_CTX *mont = NULL;\n    BIGNUM *z = NULL, *pminus1;\n    int ret = -1;\n\n    if (BN_num_bits(dh->params.p) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n        goto err;\n    }\n\n    if (dh->params.q != NULL\n        && BN_num_bits(dh->params.q) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_Q_TOO_LARGE);\n        goto err;\n    }\n\n    if (BN_num_bits(dh->params.p) < DH_MIN_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_SMALL);\n        return 0;\n    }\n\n    ctx = BN_CTX_new_ex(dh->libctx);\n    if (ctx == NULL)\n        goto err;\n    BN_CTX_start(ctx);\n    pminus1 = BN_CTX_get(ctx);\n    z = BN_CTX_get(ctx);\n    if (z == NULL)\n        goto err;\n\n    if (dh->priv_key == NULL) {\n        ERR_raise(ERR_LIB_DH, DH_R_NO_PRIVATE_VALUE);\n        goto err;\n    }\n\n    if (dh->flags & DH_FLAG_CACHE_MONT_P) {\n        mont = BN_MONT_CTX_set_locked(&dh->method_mont_p,\n                                      dh->lock, dh->params.p, ctx);\n        BN_set_flags(dh->priv_key, BN_FLG_CONSTTIME);\n        if (!mont)\n            goto err;\n    }\n\n    /* (Step 1) Z = pub_key^priv_key mod p */\n    if (!dh->meth->bn_mod_exp(dh, z, pub_key, dh->priv_key, dh->params.p, ctx,\n                              mont)) {\n        ERR_raise(ERR_LIB_DH, ERR_R_BN_LIB);\n        goto err;\n    }\n\n    /* (Step 2) Error if z <= 1 or z = p - 1 */\n    if (BN_copy(pminus1, dh->params.p) == NULL\n        || !BN_sub_word(pminus1, 1)\n        || BN_cmp(z, BN_value_one()) <= 0\n        || BN_cmp(z, pminus1) == 0) {\n        ERR_raise(ERR_LIB_DH, DH_R_INVALID_SECRET);\n        goto err;\n    }\n\n    /* return the padded key, i.e. same number of bytes as the modulus */\n    ret = BN_bn2binpad(z, key, BN_num_bytes(dh->params.p));\n err:\n    BN_clear(z); /* (Step 2) destroy intermediate values */\n    BN_CTX_end(ctx);\n    BN_CTX_free(ctx);\n    return ret;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_python_instance_state(self, instance, state, unsafe=False):\n        if hasattr(instance, '__setstate__'):\n            instance.__setstate__(state)\n        else:\n            slotstate = {}\n            if isinstance(state, tuple) and len(state) == 2:\n                state, slotstate = state\n            if hasattr(instance, '__dict__'):\n                if not unsafe and state:\n                    for key in state.keys():\n                        self.check_state_key(key)\n                instance.__dict__.update(state)\n            elif state:\n                slotstate.update(state)\n            for key, value in slotstate.items():\n                if not unsafe:\n                    self.check_state_key(key)\n                setattr(instance, key, value)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _init_file(self):\n        \"\"\"Init the password file, set the password for it.\n        \"\"\"\n\n        password = None\n        while password is None:\n            password = self._getpass(\"Please set a password for your new keyring\")\n            password2 = self._getpass('Password (again): ')\n            if password != password2:\n                sys.stderr.write(\"Error: Your passwords didn't match\\n\")\n                password = None\n                continue\n            if '' == password.strip():\n                # forbid the blank password\n                sys.stderr.write(\"Error: blank passwords aren't allowed.\\n\")\n                password = None\n\n        # write down the initialization\n        config = ConfigParser.RawConfigParser()\n        self._write_config(config, password)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _download_artifact(artifact_path):\n    \"\"\"\n    A request handler for `GET /mlflow-artifacts/artifacts/<artifact_path>` to download an artifact\n    from `artifact_path` (a relative path from the root artifact directory).\n    \"\"\"\n    artifact_path = validate_path_is_safe(artifact_path)\n    tmp_dir = tempfile.TemporaryDirectory()\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    dst = artifact_repo.download_artifacts(artifact_path, tmp_dir.name)\n\n    # Ref: https://stackoverflow.com/a/24613980/6943581\n    file_handle = open(dst, \"rb\")  # noqa: SIM115\n\n    def stream_and_remove_file():\n        yield from file_handle\n        file_handle.close()\n        tmp_dir.cleanup()\n\n    file_sender_response = current_app.response_class(stream_and_remove_file())\n\n    return _response_with_file_attachment_headers(artifact_path, file_sender_response)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void DoCompute(const ComputeOptions& options, OpKernelContext* const context) {\n  ModelWeights model_weights;\n  OP_REQUIRES_OK(context, model_weights.Initialize(context));\n\n  Examples examples;\n  OP_REQUIRES_OK(\n      context,\n      examples.Initialize(context, model_weights, options.num_sparse_features,\n                          options.num_sparse_features_with_values,\n                          options.num_dense_features));\n\n  const Tensor* example_state_data_t;\n  OP_REQUIRES_OK(context,\n                 context->input(\"example_state_data\", &example_state_data_t));\n  OP_REQUIRES(\n      context, TensorShapeUtils::IsMatrix(example_state_data_t->shape()),\n      errors::InvalidArgument(\"example_state_data must be rank 2 but is rank \",\n                              example_state_data_t->dims()));\n  TensorShape expected_example_state_shape({examples.num_examples(), 4});\n  OP_REQUIRES(context,\n              example_state_data_t->shape() == expected_example_state_shape,\n              errors::InvalidArgument(\n                  \"Expected shape \", expected_example_state_shape.DebugString(),\n                  \" for example_state_data, got \",\n                  example_state_data_t->shape().DebugString()));\n\n  Tensor mutable_example_state_data_t(*example_state_data_t);\n  auto example_state_data = mutable_example_state_data_t.matrix<float>();\n  OP_REQUIRES_OK(context, context->set_output(\"out_example_state_data\",\n                                              mutable_example_state_data_t));\n\n  if (options.adaptive) {\n    OP_REQUIRES_OK(context,\n                   examples.SampleAdaptiveProbabilities(\n                       options.num_loss_partitions, options.regularizations,\n                       model_weights, example_state_data, options.loss_updater,\n                       /*num_weight_vectors =*/1));\n  } else {\n    examples.RandomShuffle();\n  }\n  struct {\n    mutex mu;\n    Status value TF_GUARDED_BY(mu);\n  } train_step_status;\n  std::atomic<std::int64_t> atomic_index(-1);\n  auto train_step = [&](const int64_t begin, const int64_t end) {\n    // The static_cast here is safe since begin and end can be at most\n    // num_examples which is an int.\n    for (int id = static_cast<int>(begin); id < end; ++id) {\n      const int64_t example_index = examples.sampled_index(++atomic_index);\n      const Example& example = examples.example(example_index);\n      const float dual = example_state_data(example_index, 0);\n      const float example_weight = example.example_weight();\n      float example_label = example.example_label();\n      const Status conversion_status =\n          options.loss_updater->ConvertLabel(&example_label);\n      if (!conversion_status.ok()) {\n        mutex_lock l(train_step_status.mu);\n        train_step_status.value = conversion_status;\n        // Return from this worker thread - the calling thread is\n        // responsible for checking context status and returning on error.\n        return;\n      }\n\n      // Compute wx, example norm weighted by regularization, dual loss,\n      // primal loss.\n      // For binary SDCA, num_weight_vectors should be one.\n      const ExampleStatistics example_statistics =\n          example.ComputeWxAndWeightedExampleNorm(\n              options.num_loss_partitions, model_weights,\n              options.regularizations, 1 /* num_weight_vectors */);\n\n      const double new_dual = options.loss_updater->ComputeUpdatedDual(\n          options.num_loss_partitions, example_label, example_weight, dual,\n          example_statistics.wx[0], example_statistics.normalized_squared_norm);\n\n      // Compute new weights.\n      const double normalized_bounded_dual_delta =\n          (new_dual - dual) * example_weight /\n          options.regularizations.symmetric_l2();\n      model_weights.UpdateDeltaWeights(\n          context->eigen_cpu_device(), example,\n          std::vector<double>{normalized_bounded_dual_delta});\n\n      // Update example data.\n      example_state_data(example_index, 0) = new_dual;\n      example_state_data(example_index, 1) =\n          options.loss_updater->ComputePrimalLoss(\n              example_statistics.prev_wx[0], example_label, example_weight);\n      example_state_data(example_index, 2) =\n          options.loss_updater->ComputeDualLoss(dual, example_label,\n                                                example_weight);\n      example_state_data(example_index, 3) = example_weight;\n    }\n  };\n  // TODO(sibyl-Aix6ihai): Tune this properly based on sparsity of the data,\n  // number of cpus, and cost per example.\n  const int64_t kCostPerUnit = examples.num_features();\n  const DeviceBase::CpuWorkerThreads& worker_threads =\n      *context->device()->tensorflow_cpu_worker_threads();\n\n  Shard(worker_threads.num_threads, worker_threads.workers,\n        examples.num_examples(), kCostPerUnit, train_step);\n  mutex_lock l(train_step_status.mu);\n  OP_REQUIRES_OK(context, train_step_status.value);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def trim_punctuation(lead, middle, trail):\n        \"\"\"\n        Trim trailing and wrapping punctuation from `middle`. Return the items\n        of the new state.\n        \"\"\"\n        # Continue trimming until middle remains unchanged.\n        trimmed_something = True\n        while trimmed_something:\n            trimmed_something = False\n\n            # Trim trailing punctuation.\n            stripped = middle.rstrip(TRAILING_PUNCTUATION_CHARS)\n            if middle != stripped:\n                trail = middle[len(stripped):] + trail\n                middle = stripped\n                trimmed_something = True\n\n            # Trim wrapping punctuation.\n            for opening, closing in WRAPPING_PUNCTUATION:\n                if middle.startswith(opening):\n                    middle = middle[len(opening):]\n                    lead += opening\n                    trimmed_something = True\n                # Keep parentheses at the end only if they're balanced.\n                if (middle.endswith(closing) and\n                        middle.count(closing) == middle.count(opening) + 1):\n                    middle = middle[:-len(closing)]\n                    trail = closing + trail\n                    trimmed_something = True\n        return lead, middle, trail\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _write_private_key_file(self, filename, key, format, password=None):\n        \"\"\"\n        Write an SSH2-format private key file in a form that can be read by\n        paramiko or openssh.  If no password is given, the key is written in\n        a trivially-encoded format (base64) which is completely insecure.  If\n        a password is given, DES-EDE3-CBC is used.\n\n        :param str tag:\n            ``\"RSA\"`` or ``\"DSA\"``, the tag used to mark the data block.\n        :param filename: name of the file to write.\n        :param str data: data blob that makes up the private key.\n        :param str password: an optional password to use to encrypt the file.\n\n        :raises: ``IOError`` -- if there was an error writing the file.\n        \"\"\"\n        # Ensure that we create new key files directly with a user-only mode,\n        # instead of opening, writing, then chmodding, which leaves us open to\n        # CVE-2022-24302.\n        # NOTE: O_TRUNC is a noop on new files, and O_CREAT is a noop on\n        # existing files, so using all 3 in both cases is fine. Ditto the use\n        # of the 'mode' argument; it should be safe to give even for existing\n        # files (though it will not act like a chmod in that case).\n        kwargs = dict(flags=os.O_WRONLY | os.O_TRUNC | os.O_CREAT, mode=o600)\n        # NOTE: yea, you still gotta inform the FLO that it is in \"write\" mode\n        with os.fdopen(os.open(filename, **kwargs), mode=\"w\") as f:\n            # TODO 3.0: remove the now redundant chmod\n            os.chmod(filename, o600)\n            self._write_private_key(f, key, format, password=password)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _property_resolver(arg):\n    \"\"\"\n    When arg is convertible to float, behave like operator.itemgetter(arg)\n    Otherwise, chain __getitem__() and getattr().\n\n    >>> _property_resolver(1)('abc')\n    'b'\n    >>> _property_resolver('1')('abc')\n    Traceback (most recent call last):\n    ...\n    TypeError: string indices must be integers\n    >>> class Foo:\n    ...     a = 42\n    ...     b = 3.14\n    ...     c = 'Hey!'\n    >>> _property_resolver('b')(Foo())\n    3.14\n    \"\"\"\n    try:\n        float(arg)\n    except ValueError:\n        if VARIABLE_ATTRIBUTE_SEPARATOR + '_' in arg or arg[0] == '_':\n            raise AttributeError('Access to private variables is forbidden.')\n        parts = arg.split(VARIABLE_ATTRIBUTE_SEPARATOR)\n\n        def resolve(value):\n            for part in parts:\n                try:\n                    value = value[part]\n                except (AttributeError, IndexError, KeyError, TypeError, ValueError):\n                    value = getattr(value, part)\n            return value\n\n        return resolve\n    else:\n        return itemgetter(arg)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def callback(_, pdu):\n            with logcontext.PreserveLoggingContext(ctx):\n                if not check_event_content_hash(pdu):\n                    logger.warn(\n                        \"Event content has been tampered, redacting %s: %s\",\n                        pdu.event_id, pdu.get_pdu_json()\n                    )\n                    return prune_event(pdu)\n\n                if self.spam_checker.check_event_for_spam(pdu):\n                    logger.warn(\n                        \"Event contains spam, redacting %s: %s\",\n                        pdu.event_id, pdu.get_pdu_json()\n                    )\n                    return prune_event(pdu)\n\n                return pdu\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    PartialTensorShape element_shape;\n    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(0), &element_shape));\n    int32 num_elements = c->input(1).scalar<int32>()();\n    OP_REQUIRES(c, num_elements >= 0,\n                errors::InvalidArgument(\"The num_elements to reserve must be a \"\n                                        \"non negative number, but got \",\n                                        num_elements));\n    TensorList output;\n    output.element_shape = element_shape;\n    output.element_dtype = element_dtype_;\n    output.tensors().resize(num_elements, Tensor(DT_INVALID));\n    Tensor* result;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape{}, &result, attr));\n    result->scalar<Variant>()() = std::move(output);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse(self):\n        \"\"\"\n        Parse the POST data and break it into a FILES MultiValueDict and a POST\n        MultiValueDict.\n\n        Return a tuple containing the POST and FILES dictionary, respectively.\n        \"\"\"\n        from django.http import QueryDict\n\n        encoding = self._encoding\n        handlers = self._upload_handlers\n\n        # HTTP spec says that Content-Length >= 0 is valid\n        # handling content-length == 0 before continuing\n        if self._content_length == 0:\n            return QueryDict(encoding=self._encoding), MultiValueDict()\n\n        # See if any of the handlers take care of the parsing.\n        # This allows overriding everything if need be.\n        for handler in handlers:\n            result = handler.handle_raw_input(\n                self._input_data,\n                self._meta,\n                self._content_length,\n                self._boundary,\n                encoding,\n            )\n            # Check to see if it was handled\n            if result is not None:\n                return result[0], result[1]\n\n        # Create the data structures to be used later.\n        self._post = QueryDict(mutable=True)\n        self._files = MultiValueDict()\n\n        # Instantiate the parser and stream:\n        stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n        # Whether or not to signal a file-completion at the beginning of the loop.\n        old_field_name = None\n        counters = [0] * len(handlers)\n\n        # Number of bytes that have been read.\n        num_bytes_read = 0\n        # To count the number of keys in the request.\n        num_post_keys = 0\n        # To limit the amount of data read from the request.\n        read_size = None\n\n        try:\n            for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                if old_field_name:\n                    # We run this at the beginning of the next loop\n                    # since we cannot be sure a file is complete until\n                    # we hit the next boundary/part of the multipart content.\n                    self.handle_file_complete(old_field_name, counters)\n                    old_field_name = None\n\n                try:\n                    disposition = meta_data['content-disposition'][1]\n                    field_name = disposition['name'].strip()\n                except (KeyError, IndexError, AttributeError):\n                    continue\n\n                transfer_encoding = meta_data.get('content-transfer-encoding')\n                if transfer_encoding is not None:\n                    transfer_encoding = transfer_encoding[0].strip()\n                field_name = force_text(field_name, encoding, errors='replace')\n\n                if item_type == FIELD:\n                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                    num_post_keys += 1\n                    if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and\n                            settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):\n                        raise TooManyFieldsSent(\n                            'The number of GET/POST parameters exceeded '\n                            'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'\n                        )\n\n                    # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                    if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                        read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n\n                    # This is a post field, we can just set it in the post\n                    if transfer_encoding == 'base64':\n                        raw_data = field_stream.read(size=read_size)\n                        num_bytes_read += len(raw_data)\n                        try:\n                            data = base64.b64decode(raw_data)\n                        except binascii.Error:\n                            data = raw_data\n                    else:\n                        data = field_stream.read(size=read_size)\n                        num_bytes_read += len(data)\n\n                    # Add two here to make the check consistent with the\n                    # x-www-form-urlencoded check that includes '&='.\n                    num_bytes_read += len(field_name) + 2\n                    if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and\n                            num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):\n                        raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')\n\n                    self._post.appendlist(field_name, force_text(data, encoding, errors='replace'))\n                elif item_type == FILE:\n                    # This is a file, use the handler...\n                    file_name = disposition.get('filename')\n                    if file_name:\n                        file_name = force_text(file_name, encoding, errors='replace')\n                        file_name = self.sanitize_file_name(file_name)\n                    if not file_name:\n                        continue\n\n                    content_type, content_type_extra = meta_data.get('content-type', ('', {}))\n                    content_type = content_type.strip()\n                    charset = content_type_extra.get('charset')\n\n                    try:\n                        content_length = int(meta_data.get('content-length')[0])\n                    except (IndexError, TypeError, ValueError):\n                        content_length = None\n\n                    counters = [0] * len(handlers)\n                    try:\n                        for handler in handlers:\n                            try:\n                                handler.new_file(\n                                    field_name, file_name, content_type,\n                                    content_length, charset, content_type_extra,\n                                )\n                            except StopFutureHandlers:\n                                break\n\n                        for chunk in field_stream:\n                            if transfer_encoding == 'base64':\n                                # We only special-case base64 transfer encoding\n                                # We should always decode base64 chunks by multiple of 4,\n                                # ignoring whitespace.\n\n                                stripped_chunk = b\"\".join(chunk.split())\n\n                                remaining = len(stripped_chunk) % 4\n                                while remaining != 0:\n                                    over_chunk = field_stream.read(4 - remaining)\n                                    if not over_chunk:\n                                        break\n                                    stripped_chunk += b\"\".join(over_chunk.split())\n                                    remaining = len(stripped_chunk) % 4\n\n                                try:\n                                    chunk = base64.b64decode(stripped_chunk)\n                                except Exception as exc:\n                                    # Since this is only a chunk, any error is an unfixable error.\n                                    raise MultiPartParserError(\"Could not decode base64 data.\") from exc\n\n                            for i, handler in enumerate(handlers):\n                                chunk_length = len(chunk)\n                                chunk = handler.receive_data_chunk(chunk, counters[i])\n                                counters[i] += chunk_length\n                                if chunk is None:\n                                    # Don't continue if the chunk received by\n                                    # the handler is None.\n                                    break\n\n                    except SkipFile:\n                        self._close_files()\n                        # Just use up the rest of this file...\n                        exhaust(field_stream)\n                    else:\n                        # Handle file upload completions on next iteration.\n                        old_field_name = field_name\n                else:\n                    # If this is neither a FIELD or a FILE, just exhaust the stream.\n                    exhaust(stream)\n        except StopUpload as e:\n            self._close_files()\n            if not e.connection_reset:\n                exhaust(self._input_data)\n        else:\n            # Make sure that the request data is all fed\n            exhaust(self._input_data)\n\n        # Signal that the upload has completed.\n        # any() shortcircuits if a handler's upload_complete() returns a value.\n        any(handler.upload_complete() for handler in handlers)\n        self._post._mutable = False\n        return self._post, self._files\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def getbbox(self, text, *args, **kwargs):\n        \"\"\"\n        Returns bounding box (in pixels) of given text.\n\n        .. versionadded:: 9.2.0\n\n        :param text: Text to render.\n        :param mode: Used by some graphics drivers to indicate what mode the\n                     driver prefers; if empty, the renderer may return either\n                     mode. Note that the mode is always a string, to simplify\n                     C-level implementations.\n\n        :return: ``(left, top, right, bottom)`` bounding box\n        \"\"\"\n        _string_length_check(text)\n        width, height = self.font.getsize(text)\n        return 0, 0, width, height\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def read_other_paths(request: Request) -> Response:\n    \"\"\"\n    Return related frontend files. Adapted from https://github.com/tiangolo/fastapi/issues/130\n    \"\"\"\n    # check first if requested file exists (for frontend assets)\n    path = request.path_params[\"catchall\"]\n    logger.debug(f\"Catch all path detected: {path}\")\n    try:\n        path = sanitise_url_path(path)\n    except MalisciousUrlException:\n        # if a maliscious URL is detected, route the user to the index\n        return get_admin_index_as_response()\n\n    # search for matching route in package (i.e. /dataset)\n    ui_file = match_route(get_ui_file_map(), path)\n\n    # if not, check if the requested file is an asset (i.e. /_next/static/...)\n    if not ui_file:\n        ui_file = get_path_to_admin_ui_file(path)\n\n    # Serve up the file as long as it is within the UI directory\n    if ui_file and ui_file.is_file():\n        if not path_is_in_ui_directory(ui_file):\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND, detail=\"Item not found\"\n            )\n        logger.debug(\n            \"catchall request path '{}' matched static admin UI file: {}\",\n            path,\n            ui_file,\n        )\n        return FileResponse(ui_file)\n\n    # raise 404 for anything that should be backend endpoint but we can't find it\n    if path.startswith(API_PREFIX[1:]):\n        logger.debug(\n            \"catchall request path '{}' matched an invalid API route, return 404\",\n            path,\n        )\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Item not found\"\n        )\n\n    # otherwise return the index\n    logger.debug(\n        \"catchall request path '{}' did not match any admin UI routes, return generic admin UI index\",\n        path,\n    )\n    return get_admin_index_as_response()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _recv_packet(self) -> bool:\n        \"\"\"Receive the remainder of an SSH packet and process it\"\"\"\n\n        rem = 4 + self._pktlen + self._recv_macsize - self._recv_blocksize\n        if len(self._inpbuf) < rem:\n            return False\n\n        seq = self._recv_seq\n        rest = self._inpbuf[:rem-self._recv_macsize]\n        mac = self._inpbuf[rem-self._recv_macsize:rem]\n\n        if self._recv_encryption:\n            packet_data = self._recv_encryption.decrypt_packet(\n                seq, self._packet, rest, 4, mac)\n\n            if not packet_data:\n                raise MACError('MAC verification failed')\n        else:\n            packet_data = self._packet[4:] + rest\n\n        self._inpbuf = self._inpbuf[rem:]\n        self._packet = b''\n\n        orig_payload = packet_data[1:-packet_data[0]]\n\n        if self._decompressor and (self._auth_complete or\n                                   not self._decompress_after_auth):\n            payload = self._decompressor.decompress(orig_payload)\n\n            if payload is None:\n                raise CompressionError('Decompression failed')\n        else:\n            payload = orig_payload\n\n        packet = SSHPacket(payload)\n        pkttype = packet.get_byte()\n        handler: SSHPacketHandler = self\n        skip_reason = ''\n        exc_reason = ''\n\n        if MSG_KEX_FIRST <= pkttype <= MSG_KEX_LAST:\n            if self._kex:\n                if self._ignore_first_kex: # pragma: no cover\n                    skip_reason = 'ignored first kex'\n                    self._ignore_first_kex = False\n                else:\n                    handler = self._kex\n            else:\n                skip_reason = 'kex not in progress'\n                exc_reason = 'Key exchange not in progress'\n        elif MSG_USERAUTH_FIRST <= pkttype <= MSG_USERAUTH_LAST:\n            if self._auth:\n                handler = self._auth\n            else:\n                skip_reason = 'auth not in progress'\n                exc_reason = 'Authentication not in progress'\n        elif pkttype > MSG_KEX_LAST and not self._recv_encryption:\n            skip_reason = 'invalid request before kex complete'\n            exc_reason = 'Invalid request before key exchange was complete'\n        elif pkttype > MSG_USERAUTH_LAST and not self._auth_complete:\n            skip_reason = 'invalid request before auth complete'\n            exc_reason = 'Invalid request before authentication was complete'\n        elif MSG_CHANNEL_FIRST <= pkttype <= MSG_CHANNEL_LAST:\n            try:\n                recv_chan = packet.get_uint32()\n            except PacketDecodeError:\n                skip_reason = 'incomplete channel request'\n                exc_reason = 'Incomplete channel request received'\n            else:\n                try:\n                    handler = self._channels[recv_chan]\n                except KeyError:\n                    skip_reason = 'invalid channel number'\n                    exc_reason = 'Invalid channel number %d ' \\\n                                 'received' % recv_chan\n\n        handler.log_received_packet(pkttype, seq, packet, skip_reason)\n\n        if not skip_reason:\n            try:\n                processed = handler.process_packet(pkttype, seq, packet)\n            except PacketDecodeError as exc:\n                raise ProtocolError(str(exc)) from None\n\n            if not processed:\n                self.logger.debug1('Unknown packet type %d received', pkttype)\n                self.send_packet(MSG_UNIMPLEMENTED, UInt32(seq))\n\n        if exc_reason:\n            raise ProtocolError(exc_reason)\n\n        if pkttype > MSG_USERAUTH_LAST:\n            self._auth_final = True\n\n        if self._transport:\n            self._recv_seq = (seq + 1) & 0xffffffff\n            self._recv_handler = self._recv_pkthdr\n\n        return True\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def visit_Attribute(self, node):\n        \"\"\"Checks and mutates attribute access/assignment.\n\n        'a.b' becomes '_getattr_(a, \"b\")'\n        'a.b = c' becomes '_write_(a).b = c'\n        'del a.b' becomes 'del _write_(a).b'\n\n        The _write_ function should return a security proxy.\n        \"\"\"\n        if node.attr.startswith('_') and node.attr != '_':\n            self.error(\n                node,\n                '\"{name}\" is an invalid attribute name because it starts '\n                'with \"_\".'.format(name=node.attr))\n\n        if node.attr.endswith('__roles__'):\n            self.error(\n                node,\n                '\"{name}\" is an invalid attribute name because it ends '\n                'with \"__roles__\".'.format(name=node.attr))\n\n        if node.attr in INSPECT_ATTRIBUTES:\n            self.error(\n                node,\n                f'\"{node.attr}\" is a restricted name,'\n                ' that is forbidden to access in RestrictedPython.',\n            )\n\n        if isinstance(node.ctx, ast.Load):\n            node = self.node_contents_visit(node)\n            new_node = ast.Call(\n                func=ast.Name('_getattr_', ast.Load()),\n                args=[node.value, ast.Str(node.attr)],\n                keywords=[])\n\n            copy_locations(new_node, node)\n            return new_node\n\n        elif isinstance(node.ctx, (ast.Store, ast.Del)):\n            node = self.node_contents_visit(node)\n            new_value = ast.Call(\n                func=ast.Name('_write_', ast.Load()),\n                args=[node.value],\n                keywords=[])\n\n            copy_locations(new_value, node.value)\n            node.value = new_value\n            return node\n\n        else:  # pragma: no cover\n            # Impossible Case only ctx Load, Store and Del are defined in ast.\n            raise NotImplementedError(\n                f\"Unknown ctx type: {type(node.ctx)}\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse_message(self, lines: List[bytes]) -> RawRequestMessage:\n        # request line\n        line = lines[0].decode(\"utf-8\", \"surrogateescape\")\n        try:\n            method, path, version = line.split(maxsplit=2)\n        except ValueError:\n            raise BadStatusLine(line) from None\n\n        if len(path) > self.max_line_size:\n            raise LineTooLong(\n                \"Status line is too long\", str(self.max_line_size), str(len(path))\n            )\n\n        # method\n        if not METHRE.match(method):\n            raise BadStatusLine(method)\n\n        # version\n        match = VERSRE.match(version)\n        if match is None:\n            raise BadStatusLine(line)\n        version_o = HttpVersion(int(match.group(1)), int(match.group(2)))\n\n        if method == \"CONNECT\":\n            # authority-form,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.3\n            url = URL.build(authority=path, encoded=True)\n        elif path.startswith(\"/\"):\n            # origin-form,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.1\n            path_part, _hash_separator, url_fragment = path.partition(\"#\")\n            path_part, _question_mark_separator, qs_part = path_part.partition(\"?\")\n\n            # NOTE: `yarl.URL.build()` is used to mimic what the Cython-based\n            # NOTE: parser does, otherwise it results into the same\n            # NOTE: HTTP Request-Line input producing different\n            # NOTE: `yarl.URL()` objects\n            url = URL.build(\n                path=path_part,\n                query_string=qs_part,\n                fragment=url_fragment,\n                encoded=True,\n            )\n        else:\n            # absolute-form for proxy maybe,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.2\n            url = URL(path, encoded=True)\n\n        # read headers\n        (\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        ) = self.parse_headers(lines)\n\n        if close is None:  # then the headers weren't set in the request\n            if version_o <= HttpVersion10:  # HTTP 1.0 must asks to not close\n                close = True\n            else:  # HTTP 1.1 must ask to close.\n                close = False\n\n        return RawRequestMessage(\n            method,\n            path,\n            version_o,\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n            url,\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def has_access(permissions: Sequence[tuple[str, str]] | None = None) -> Callable[[T], T]:\n    \"\"\"Factory for decorator that checks current user's permissions against required permissions.\"\"\"\n\n    def requires_access_decorator(func: T):\n        @wraps(func)\n        def decorated(*args, **kwargs):\n            __tracebackhide__ = True  # Hide from pytest traceback.\n\n            appbuilder = current_app.appbuilder\n\n            dag_id_kwargs = kwargs.get(\"dag_id\")\n            dag_id_args = request.args.get(\"dag_id\")\n            dag_id_form = request.form.get(\"dag_id\")\n            dag_id_json = request.json.get(\"dag_id\") if request.is_json else None\n            all_dag_ids = [dag_id_kwargs, dag_id_args, dag_id_form, dag_id_json]\n            unique_dag_ids = set(dag_id for dag_id in all_dag_ids if dag_id is not None)\n\n            if len(unique_dag_ids) > 1:\n                log.warning(\n                    f\"There are different dag_ids passed in the request: {unique_dag_ids}. Returning 403.\"\n                )\n                log.warning(\n                    f\"kwargs: {dag_id_kwargs}, args: {dag_id_args}, \"\n                    f\"form: {dag_id_form}, json: {dag_id_json}\"\n                )\n                return (\n                    render_template(\n                        \"airflow/no_roles_permissions.html\",\n                        hostname=get_hostname()\n                        if conf.getboolean(\"webserver\", \"EXPOSE_HOSTNAME\")\n                        else \"redact\",\n                        logout_url=get_auth_manager().get_url_logout(),\n                    ),\n                    403,\n                )\n            dag_id = unique_dag_ids.pop() if unique_dag_ids else None\n\n            if appbuilder.sm.check_authorization(permissions, dag_id):\n                return func(*args, **kwargs)\n            elif get_auth_manager().is_logged_in() and not g.user.perms:\n                return (\n                    render_template(\n                        \"airflow/no_roles_permissions.html\",\n                        hostname=get_hostname()\n                        if conf.getboolean(\"webserver\", \"EXPOSE_HOSTNAME\")\n                        else \"redact\",\n                        logout_url=get_auth_manager().get_url_logout(),\n                    ),\n                    403,\n                )\n            else:\n                access_denied = get_access_denied_message()\n                flash(access_denied, \"danger\")\n            return redirect(get_auth_manager().get_url_login(next=request.url))\n\n        return cast(T, decorated)\n\n    return requires_access_decorator\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_part_begin(self) -> None:\n        self._current_part = MultipartPart()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void pointZZ_pDouble(PointZZ_p * rop, const PointZZ_p * op, const CurveZZ_p * curve) {\n    if(pointZZ_pIsIdentityElement(op)) {\n        return pointZZ_pSetToIdentityElement(rop);\n    }\n\n    mpz_t numer, denom, lambda;\n    mpz_inits(numer, denom, lambda, NULL);\n\n    // calculate lambda\n    mpz_mul(numer, op->x, op->x);\n    mpz_mul_ui(numer, numer, 3);\n    mpz_add(numer, numer, curve->a);\n    mpz_mul_ui(denom, op->y, 2);\n    mpz_invert(denom, denom, curve->p);  // TODO check status\n    mpz_mul(lambda, numer, denom);\n    mpz_mod(lambda, lambda, curve->p);\n\n    // calculate resulting x coord\n    mpz_mul(rop->x, lambda, lambda);\n    mpz_sub(rop->x, rop->x, op->x);\n    mpz_sub(rop->x, rop->x, op->x);\n    mpz_mod(rop->x, rop->x, curve->p);\n\n    //calculate resulting y coord\n    mpz_sub(rop->y, op->x, rop->x);\n    mpz_mul(rop->y, lambda, rop->y);\n    mpz_sub(rop->y, rop->y, op->y);\n    mpz_mod(rop->y, rop->y, curve->p);\n\n    mpz_clears(numer, denom, lambda, NULL);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def group_detail(group_id):\n    if not current_user.admin and not \\\n            current_user.has_permission(PermissionType.admin):\n        abort(403)\n    group = Group.query.get_or_404(group_id)\n    submit = map(lambda p: p.division,\n            filter(lambda p: p.permission == PermissionType.submit,\n                group.permissions))\n    review = map(lambda p: p.division,\n            filter(lambda p: p.permission == PermissionType.review,\n                group.permissions))\n    pay = map(lambda p: p.division,\n            filter(lambda p: p.permission == PermissionType.pay,\n                group.permissions))\n    resp = {\n        u'name': group.name,\n        u'users': list(group.users),\n        u'divisions': {\n            u'submit': list(set(submit)),\n            u'review': list(set(review)),\n            u'pay': list(set(pay)),\n        },\n    }\n    return jsonify(**resp)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def perform_parsing(self):\n        try:\n            return yaml.safe_load(self.content)\n        except yaml.error.YAMLError:\n            raise ResponseParseError(self._error_message, self.content)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _save(self, name, content):\n        full_path = self.path(name)\n\n        # Create any intermediate directories that do not exist.\n        directory = os.path.dirname(full_path)\n        try:\n            if self.directory_permissions_mode is not None:\n                # Set the umask because os.makedirs() doesn't apply the \"mode\"\n                # argument to intermediate-level directories.\n                old_umask = os.umask(0o777 & ~self.directory_permissions_mode)\n                try:\n                    os.makedirs(directory, self.directory_permissions_mode, exist_ok=True)\n                finally:\n                    os.umask(old_umask)\n            else:\n                os.makedirs(directory, exist_ok=True)\n        except FileExistsError:\n            raise FileExistsError('%s exists and is not a directory.' % directory)\n\n        # There's a potential race condition between get_available_name and\n        # saving the file; it's possible that two threads might return the\n        # same name, at which point all sorts of fun happens. So we need to\n        # try to create the file, but if it already exists we have to go back\n        # to get_available_name() and try again.\n\n        while True:\n            try:\n                # This file has a file path that we can move.\n                if hasattr(content, 'temporary_file_path'):\n                    file_move_safe(content.temporary_file_path(), full_path)\n\n                # This is a normal uploadedfile that we can stream.\n                else:\n                    # The current umask value is masked out by os.open!\n                    fd = os.open(full_path, self.OS_OPEN_FLAGS, 0o666)\n                    _file = None\n                    try:\n                        locks.lock(fd, locks.LOCK_EX)\n                        for chunk in content.chunks():\n                            if _file is None:\n                                mode = 'wb' if isinstance(chunk, bytes) else 'wt'\n                                _file = os.fdopen(fd, mode)\n                            _file.write(chunk)\n                    finally:\n                        locks.unlock(fd)\n                        if _file is not None:\n                            _file.close()\n                        else:\n                            os.close(fd)\n            except FileExistsError:\n                # A new name is needed if the file exists.\n                name = self.get_available_name(name)\n                full_path = self.path(name)\n            else:\n                # OK, the file save worked. Break out of the loop.\n                break\n\n        if self.file_permissions_mode is not None:\n            os.chmod(full_path, self.file_permissions_mode)\n\n        # Ensure the saved path is always relative to the storage root.\n        name = os.path.relpath(full_path, self.location)\n        # Store filenames with forward slashes, even on Windows.\n        return str(name).replace('\\\\', '/')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_params(self, context, cluster_template, cluster, **kwargs):\n        osc = self.get_osc(context)\n\n        extra_params = kwargs.pop('extra_params', {})\n        extra_params['trustee_domain_id'] = osc.keystone().trustee_domain_id\n        extra_params['trustee_user_id'] = cluster.trustee_user_id\n        extra_params['trustee_username'] = cluster.trustee_username\n        extra_params['trustee_password'] = cluster.trustee_password\n\n        # Only pass trust ID into the template when it is needed.\n        if (cluster_template.volume_driver == 'rexray' or\n                cluster_template.registry_enabled):\n            if CONF.trust.cluster_user_trust:\n                extra_params['trust_id'] = cluster.trust_id\n            else:\n                missing_setting = ('trust/cluster_user_trust = True')\n                msg = _LE('This cluster can only be created with %s in '\n                          'magnum.conf')\n                raise exception.ConfigInvalid(msg % missing_setting)\n        else:\n            extra_params['trust_id'] = \"\"\n\n        extra_params['auth_url'] = context.auth_url\n\n        return super(BaseTemplateDefinition,\n                     self).get_params(context, cluster_template, cluster,\n                                      extra_params=extra_params,\n                                      **kwargs)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def send(self, anyway=False, prefetch=False):\n        \"\"\"Sends the request. Returns True of successful, False if not.\n        If there was an HTTPError during transmission,\n        self.response.status_code will contain the HTTPError code.\n\n        Once a request is successfully sent, `sent` will equal True.\n\n        :param anyway: If True, request will be sent, even if it has\n        already been sent.\n        \"\"\"\n\n        # Build the URL\n        url = self.full_url\n\n        # Logging\n        if self.config.get('verbose'):\n            self.config.get('verbose').write('%s   %s   %s\\n' % (\n                datetime.now().isoformat(), self.method, url\n            ))\n\n        # Nottin' on you.\n        body = None\n        content_type = None\n\n        # Multi-part file uploads.\n        if self.files:\n            (body, content_type) = self._enc_files\n        else:\n            if self.data:\n\n                body = self._enc_data\n                if isinstance(self.data, str):\n                    content_type = None\n                else:\n                    content_type = 'application/x-www-form-urlencoded'\n\n        # Add content-type if it wasn't explicitly provided.\n        if (content_type) and (not 'content-type' in self.headers):\n            self.headers['Content-Type'] = content_type\n\n        # Use .netrc auth if none was provided.\n        if not self.auth and self.config.get('trust_env'):\n            self.auth = get_netrc_auth(url)\n\n        if self.auth:\n            if isinstance(self.auth, tuple) and len(self.auth) == 2:\n                # special-case basic HTTP auth\n                self.auth = HTTPBasicAuth(*self.auth)\n\n            # Allow auth to make its changes.\n            r = self.auth(self)\n\n            # Update self to reflect the auth changes.\n            self.__dict__.update(r.__dict__)\n\n        _p = urlparse(url)\n        proxy = self.proxies.get(_p.scheme)\n\n        if proxy:\n            conn = poolmanager.proxy_from_url(proxy)\n            _proxy = urlparse(proxy)\n            if '@' in _proxy.netloc:\n                auth, url = _proxy.netloc.split('@', 1)\n                self.proxy_auth = HTTPProxyAuth(*auth.split(':', 1))\n                r = self.proxy_auth(self)\n                self.__dict__.update(r.__dict__)\n        else:\n            # Check to see if keep_alive is allowed.\n            if self.config.get('keep_alive'):\n                conn = self._poolmanager.connection_from_url(url)\n            else:\n                conn = connectionpool.connection_from_url(url)\n\n        if url.startswith('https') and self.verify:\n\n            cert_loc = None\n\n            # Allow self-specified cert location.\n            if self.verify is not True:\n                cert_loc = self.verify\n\n            # Look for configuration.\n            if not cert_loc and self.config.get('trust_env'):\n                cert_loc = os.environ.get('REQUESTS_CA_BUNDLE')\n\n            # Curl compatiblity.\n            if not cert_loc and self.config.get('trust_env'):\n                cert_loc = os.environ.get('CURL_CA_BUNDLE')\n\n            # Use the awesome certifi list.\n            if not cert_loc:\n                cert_loc = __import__('certifi').where()\n\n            conn.cert_reqs = 'CERT_REQUIRED'\n            conn.ca_certs = cert_loc\n        else:\n            conn.cert_reqs = 'CERT_NONE'\n            conn.ca_certs = None\n\n        if self.cert and self.verify:\n            if len(self.cert) == 2:\n                conn.cert_file = self.cert[0]\n                conn.key_file = self.cert[1]\n            else:\n                conn.cert_file = self.cert\n\n        if not self.sent or anyway:\n\n            if self.cookies:\n\n                # Skip if 'cookie' header is explicitly set.\n                if 'cookie' not in self.headers:\n\n                    # Simple cookie with our dict.\n                    c = SimpleCookie()\n                    for (k, v) in list(self.cookies.items()):\n                        c[k] = v\n\n                    # Turn it into a header.\n                    cookie_header = c.output(header='', sep='; ').strip()\n\n                    # Attach Cookie header to request.\n                    self.headers['Cookie'] = cookie_header\n\n            # Pre-request hook.\n            r = dispatch_hook('pre_request', self.hooks, self)\n            self.__dict__.update(r.__dict__)\n\n            try:\n                # The inner try .. except re-raises certain exceptions as\n                # internal exception types; the outer suppresses exceptions\n                # when safe mode is set.\n                try:\n                    # Send the request.\n                    r = conn.urlopen(\n                        method=self.method,\n                        url=self.path_url,\n                        body=body,\n                        headers=self.headers,\n                        redirect=False,\n                        assert_same_host=False,\n                        preload_content=False,\n                        decode_content=False,\n                        retries=self.config.get('max_retries', 0),\n                        timeout=self.timeout,\n                    )\n                    self.sent = True\n\n                except MaxRetryError as e:\n                    raise ConnectionError(e)\n\n                except (_SSLError, _HTTPError) as e:\n                    if self.verify and isinstance(e, _SSLError):\n                        raise SSLError(e)\n\n                    raise Timeout('Request timed out.')\n\n            except RequestException as e:\n                if self.config.get('safe_mode', False):\n                    # In safe mode, catch the exception and attach it to\n                    # a blank urllib3.HTTPResponse object.\n                    r = HTTPResponse()\n                    r.error = e\n                else:\n                    raise\n\n            self._build_response(r)\n\n            # Response manipulation hook.\n            self.response = dispatch_hook('response', self.hooks, self.response)\n\n            # Post-request hook.\n            r = dispatch_hook('post_request', self.hooks, self)\n            self.__dict__.update(r.__dict__)\n\n            # If prefetch is True, mark content as consumed.\n            if prefetch:\n                # Save the response.\n                self.response.content\n\n            if self.config.get('danger_mode'):\n                self.response.raise_for_status()\n\n            return self.sent\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_send_leave_request(self, origin, content, room_id):\n        logger.debug(\"on_send_leave_request: content: %s\", content)\n\n        room_version = yield self.store.get_room_version(room_id)\n        format_ver = room_version_to_event_format(room_version)\n        pdu = event_from_pdu_json(content, format_ver)\n\n        origin_host, _ = parse_server_name(origin)\n        yield self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_leave_request: pdu sigs: %s\", pdu.signatures)\n\n        pdu = yield self._check_sigs_and_hash(room_version, pdu)\n\n        yield self.handler.on_send_leave_request(origin, pdu)\n        return 200, {}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_safe_url(url, host=None):\n    \"\"\"\n    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to\n    a different host and uses a safe scheme).\n\n    Always returns ``False`` on an empty url.\n    \"\"\"\n    if not url:\n        return False\n    url = url.strip()\n    # Chrome treats \\ completely as /\n    url = url.replace('\\\\', '/')\n    # Chrome considers any URL with more than two slashes to be absolute, but\n    # urlaprse is not so flexible. Treat any url with three slashes as unsafe.\n    if url.startswith('///'):\n        return False\n    url_info = urlparse(url)\n    # Forbid URLs like http:///example.com - with a scheme, but without a hostname.\n    # In that URL, example.com is not the hostname but, a path component. However,\n    # Chrome will still consider example.com to be the hostname, so we must not\n    # allow this syntax.\n    if not url_info.netloc and url_info.scheme:\n        return False\n    return (not url_info.netloc or url_info.netloc == host) and \\\n        (not url_info.scheme or url_info.scheme in ['http', 'https'])\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def dagrun_success(self):\n        dag_id = request.form.get('dag_id')\n        execution_date = request.form.get('execution_date')\n        confirmed = request.form.get('confirmed') == 'true'\n        origin = get_safe_url(request.form.get('origin'))\n        return self._mark_dagrun_state_as_success(dag_id, execution_date,\n                                                  confirmed, origin)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _load_pkcs7_certificates(self, p7) -> typing.List[x509.Certificate]:\n        nid = self._lib.OBJ_obj2nid(p7.type)\n        self.openssl_assert(nid != self._lib.NID_undef)\n        if nid != self._lib.NID_pkcs7_signed:\n            raise UnsupportedAlgorithm(\n                \"Only basic signed structures are currently supported. NID\"\n                \" for this data was {}\".format(nid),\n                _Reasons.UNSUPPORTED_SERIALIZATION,\n            )\n\n        certs: list[x509.Certificate] = []\n        if p7.d.sign == self._ffi.NULL:\n            return certs\n\n        sk_x509 = p7.d.sign.cert\n        num = self._lib.sk_X509_num(sk_x509)\n        for i in range(num):\n            x509 = self._lib.sk_X509_value(sk_x509, i)\n            self.openssl_assert(x509 != self._ffi.NULL)\n            cert = self._ossl2cert(x509)\n            certs.append(cert)\n\n        return certs\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def as_sql(self, compiler, connection):\n        key_transforms = [self.key_name]\n        previous = self.lhs\n        while isinstance(previous, KeyTransform):\n            key_transforms.insert(0, previous.key_name)\n            previous = previous.lhs\n        lhs, params = compiler.compile(previous)\n        if len(key_transforms) > 1:\n            return \"(%s %s %%s)\" % (lhs, self.nested_operator), [key_transforms] + params\n        try:\n            lookup = int(self.key_name)\n        except ValueError:\n            lookup = self.key_name\n        return '(%s %s %%s)' % (lhs, self.operator), [lookup] + params\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def main():\n    _validate_untrusted_args(sys.argv)\n    parser = get_argparser()\n    argv = sys.argv[1:]\n    args = parser.parse_args(argv)\n    if args.json_args is not None:\n        args = _unpack_json_args(args)\n    earlyinit.early_init(args)\n    # We do this imports late as earlyinit needs to be run first (because of\n    # version checking and other early initialization)\n    from qutebrowser import app\n    return app.run(args)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def json_errors(method):\n    \"\"\"Decorate methods with this to return GitHub style JSON errors.\n    \n    This should be used on any JSON API on any handler method that can raise HTTPErrors.\n    \n    This will grab the latest HTTPError exception using sys.exc_info\n    and then:\n    \n    1. Set the HTTP status code based on the HTTPError\n    2. Create and return a JSON body with a message field describing\n       the error in a human readable form.\n    \"\"\"\n    @functools.wraps(method)\n    def wrapper(self, *args, **kwargs):\n        try:\n            result = method(self, *args, **kwargs)\n        except web.HTTPError as e:\n            status = e.status_code\n            message = e.log_message\n            self.log.warn(message)\n            self.set_status(e.status_code)\n            self.set_header('Content-Type', 'application/json')\n            self.finish(json.dumps(dict(message=message)))\n        except Exception:\n            self.log.error(\"Unhandled error in API request\", exc_info=True)\n            status = 500\n            message = \"Unknown server error\"\n            t, value, tb = sys.exc_info()\n            self.set_status(status)\n            tb_text = ''.join(traceback.format_exception(t, value, tb))\n            reply = dict(message=message, traceback=tb_text)\n            self.set_header('Content-Type', 'application/json')\n            self.finish(json.dumps(reply))\n        else:\n            return result\n    return wrapper\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "int ossl_dh_compute_key(unsigned char *key, const BIGNUM *pub_key, DH *dh)\n{\n    BN_CTX *ctx = NULL;\n    BN_MONT_CTX *mont = NULL;\n    BIGNUM *z = NULL, *pminus1;\n    int ret = -1;\n\n    if (BN_num_bits(dh->params.p) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n        goto err;\n    }\n\n    if (dh->params.q != NULL\n        && BN_num_bits(dh->params.q) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_Q_TOO_LARGE);\n        goto err;\n    }\n\n    if (BN_num_bits(dh->params.p) < DH_MIN_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_SMALL);\n        return 0;\n    }\n\n    ctx = BN_CTX_new_ex(dh->libctx);\n    if (ctx == NULL)\n        goto err;\n    BN_CTX_start(ctx);\n    pminus1 = BN_CTX_get(ctx);\n    z = BN_CTX_get(ctx);\n    if (z == NULL)\n        goto err;\n\n    if (dh->priv_key == NULL) {\n        ERR_raise(ERR_LIB_DH, DH_R_NO_PRIVATE_VALUE);\n        goto err;\n    }\n\n    if (dh->flags & DH_FLAG_CACHE_MONT_P) {\n        mont = BN_MONT_CTX_set_locked(&dh->method_mont_p,\n                                      dh->lock, dh->params.p, ctx);\n        BN_set_flags(dh->priv_key, BN_FLG_CONSTTIME);\n        if (!mont)\n            goto err;\n    }\n\n    /* (Step 1) Z = pub_key^priv_key mod p */\n    if (!dh->meth->bn_mod_exp(dh, z, pub_key, dh->priv_key, dh->params.p, ctx,\n                              mont)) {\n        ERR_raise(ERR_LIB_DH, ERR_R_BN_LIB);\n        goto err;\n    }\n\n    /* (Step 2) Error if z <= 1 or z = p - 1 */\n    if (BN_copy(pminus1, dh->params.p) == NULL\n        || !BN_sub_word(pminus1, 1)\n        || BN_cmp(z, BN_value_one()) <= 0\n        || BN_cmp(z, pminus1) == 0) {\n        ERR_raise(ERR_LIB_DH, DH_R_INVALID_SECRET);\n        goto err;\n    }\n\n    /* return the padded key, i.e. same number of bytes as the modulus */\n    ret = BN_bn2binpad(z, key, BN_num_bytes(dh->params.p));\n err:\n    BN_clear(z); /* (Step 2) destroy intermediate values */\n    BN_CTX_end(ctx);\n    BN_CTX_free(ctx);\n    return ret;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def sendfile(request, filename, **kwargs):\n    response = HttpResponse()\n    response['X-Accel-Redirect'] = _convert_file_to_url(filename)\n\n    return response\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def predict(self, model_uri, input_path, output_path, content_type):\n        \"\"\"\n        Generate predictions using generic python model saved with MLflow. The expected format of\n        the input JSON is the Mlflow scoring format.\n        Return the prediction results as a JSON.\n        \"\"\"\n        local_path = _download_artifact_from_uri(model_uri)\n        # NB: Absolute windows paths do not work with mlflow apis, use file uri to ensure\n        # platform compatibility.\n        local_uri = path_to_local_file_uri(local_path)\n\n        if self._env_manager != _EnvManager.LOCAL:\n            predict_cmd = [\n                \"python\",\n                _mlflow_pyfunc_backend_predict.__file__,\n                \"--model-uri\",\n                str(local_uri),\n                \"--content-type\",\n                shlex.quote(str(content_type)),\n            ]\n            if input_path:\n                predict_cmd += [\"--input-path\", shlex.quote(str(input_path))]\n            if output_path:\n                predict_cmd += [\"--output-path\", shlex.quote(str(output_path))]\n            return self.prepare_env(local_path).execute(\" \".join(predict_cmd))\n        else:\n            scoring_server._predict(local_uri, input_path, output_path, content_type)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def lookup_path(\n        self, path: str\n    ) -> typing.Tuple[str, typing.Optional[os.stat_result]]:\n        for directory in self.all_directories:\n            joined_path = os.path.join(directory, path)\n            if self.follow_symlink:\n                full_path = os.path.abspath(joined_path)\n            else:\n                full_path = os.path.realpath(joined_path)\n            directory = os.path.realpath(directory)\n            if os.path.commonpath([full_path, directory]) != directory:\n                # Don't allow misbehaving clients to break out of the static files\n                # directory.\n                continue\n            try:\n                return full_path, os.stat(full_path)\n            except (FileNotFoundError, NotADirectoryError):\n                continue\n        return \"\", None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render_POST(self, request):\n        \"\"\"\n        Register with the Identity Server\n        \"\"\"\n        send_cors(request)\n\n        args = get_args(request, ('matrix_server_name', 'access_token'))\n\n        matrix_server = args['matrix_server_name'].lower()\n\n        if not is_valid_matrix_server_name(matrix_server):\n            request.setResponseCode(400)\n            return {\n                'errcode': 'M_INVALID_PARAM',\n                'error': 'matrix_server_name must be a valid Matrix server name (IP address or hostname)'\n            }\n\n        result = yield self.client.get_json(\n            \"matrix://%s/_matrix/federation/v1/openid/userinfo?access_token=%s\"\n            % (\n                matrix_server,\n                urllib.parse.quote(args['access_token']),\n            ),\n            1024 * 5,\n        )\n\n        if 'sub' not in result:\n            raise Exception(\"Invalid response from homeserver\")\n\n        user_id = result['sub']\n\n        if not isinstance(user_id, str):\n            request.setResponseCode(500)\n            return {\n                'errcode': 'M_UNKNOWN',\n                'error': 'The Matrix homeserver returned a malformed reply'\n            }\n\n        user_id_components = user_id.split(':', 1)\n\n        # Ensure there's a localpart and domain in the returned user ID.\n        if len(user_id_components) != 2:\n            request.setResponseCode(500)\n            return {\n                'errcode': 'M_UNKNOWN',\n                'error': 'The Matrix homeserver returned an invalid MXID'\n            }\n\n        user_id_server = user_id_components[1]\n\n        if not is_valid_matrix_server_name(user_id_server):\n            request.setResponseCode(500)\n            return {\n                'errcode': 'M_UNKNOWN',\n                'error': 'The Matrix homeserver returned an invalid MXID'\n            }\n\n        if user_id_server != matrix_server:\n            request.setResponseCode(500)\n            return {\n                'errcode': 'M_UNKNOWN',\n                'error': 'The Matrix homeserver returned a MXID belonging to another homeserver'\n            }\n\n        tok = yield issueToken(self.sydent, user_id)\n\n        # XXX: `token` is correct for the spec, but we released with `access_token`\n        # for a substantial amount of time. Serve both to make spec-compliant clients\n        # happy.\n        defer.returnValue({\n            \"access_token\": tok,\n            \"token\": tok,\n        })\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def failure(cls, request):\n        '''\n        Creates an RPCMessage instance containing exception information,\n        when the exception is from user-defined handlers or upper adaptation layers.\n\n        It must be called in an exception handler context, where ``sys.exc_info()``\n        returns a non-null tuple.\n        '''\n        exc_info = sys.exc_info()\n        return cls(\n            request.peer_id,\n            RPCMessageTypes.FAILURE,\n            request.method, request.order_key, request.client_seq_id,\n            ErrorMetadata(exc_info[0].__name__, traceback.format_exc()),\n            None,\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    PartialTensorShape element_shape_except_first_dim;\n    if (!element_shape_.unknown_rank()) {\n      auto dim_sizes = element_shape_.dim_sizes();\n      OP_REQUIRES(c, !dim_sizes.empty(),\n                  errors::InvalidArgument(\"element_shape must not be empty\"));\n      element_shape_except_first_dim =\n          PartialTensorShape(gtl::ArraySlice<int64_t>(dim_sizes).subspan(1));\n    }\n    // Check that the input Variant tensor is indeed a TensorList and has the\n    // correct element type.\n    const TensorList* tensor_list = nullptr;\n    OP_REQUIRES_OK(c, GetInputList(c, 0, &tensor_list));\n    OP_REQUIRES(\n        c, element_dtype_ == tensor_list->element_dtype,\n        errors::InvalidArgument(\n            \"Invalid data types; op elements \", DataTypeString(element_dtype_),\n            \" but list elements \", DataTypeString(tensor_list->element_dtype)));\n    // The leading dimension of all list elements if they are all the same.\n    // This is used as the leading dim of uninitialized tensors in the list\n    // if leading_dims is not provided.\n    int64_t first_dim = -1;\n    if (c->num_inputs() > 1) {\n      // TensorListConcatV2\n      PartialTensorShape element_shape;\n      OP_REQUIRES_OK(\n          c, GetElementShapeFromInput(c, *tensor_list, 1, &element_shape));\n      OP_REQUIRES(c, element_shape.unknown_rank() || element_shape.dims() >= 1,\n                  errors::InvalidArgument(\n                      \"Concat requires elements to be at least vectors, \",\n                      \"found scalars instead.\"));\n      // Split `element_shape` into `first_dim` and\n      // `element_shape_except_first_dim`.\n      first_dim = element_shape.dim_size(0);\n      element_shape_except_first_dim = element_shape;\n      element_shape_except_first_dim.RemoveDim(0);\n    }\n    // If the TensorList is empty, element_shape_except_first_dim must be fully\n    // defined.\n    OP_REQUIRES(c,\n                !tensor_list->tensors().empty() ||\n                    element_shape_except_first_dim.IsFullyDefined(),\n                errors::InvalidArgument(\n                    \"All except the first dimension must be fully defined \",\n                    \"when concating an empty tensor list. element_shape: \",\n                    element_shape_except_first_dim.DebugString()));\n    // 1. Check that `element_shape_except_first_dim` input tensor is\n    //    compatible with the shapes of element tensors.\n    // 2. Check that the elements have the same shape except the first dim.\n    // 3. If `first_dim` is known, check that it is compatible with the leading\n    //    dims of all elements.\n    // 4. If `first_dim` is unknown (-1), check whether all initialized\n    //    elements have the same leading dim and if so set `first_dim` to that\n    //    value.\n    if (!tensor_list->element_shape.IsFullyDefined()) {\n      bool check_dim = (first_dim == -1);\n      int64_t inferred_first_dim = first_dim;\n      for (int i = 0; i < tensor_list->tensors().size(); ++i) {\n        const Tensor& t = tensor_list->tensors()[i];\n        if (t.dtype() != DT_INVALID) {\n          PartialTensorShape tmp = element_shape_except_first_dim;\n          OP_REQUIRES(\n              c, TensorShapeUtils::IsVectorOrHigher(t.shape()),\n              errors::InvalidArgument(\"Concat saw a scalar shape at index \", i,\n                                      \" but requires at least vectors.\"));\n          TensorShape shape_except_first_dim = TensorShape(\n              gtl::ArraySlice<int64_t>(t.shape().dim_sizes()).subspan(1));\n          OP_REQUIRES_OK(c, tmp.MergeWith(shape_except_first_dim,\n                                          &element_shape_except_first_dim));\n          OP_REQUIRES(c, first_dim == -1 || first_dim == t.shape().dim_size(0),\n                      errors::InvalidArgument(\n                          \"First entry of element_shape input does not match \",\n                          \"the first dim of list element at index: \", i,\n                          \" Expected: \", first_dim,\n                          \" Actual: \", t.shape().dim_size(0)));\n          if (check_dim) {\n            if (inferred_first_dim == -1) {\n              inferred_first_dim = t.shape().dim_size(0);\n            } else if (inferred_first_dim != t.shape().dim_size(0)) {\n              inferred_first_dim = -1;\n              check_dim = false;\n            }\n          }\n        }\n      }\n      first_dim = inferred_first_dim;\n    }\n    TensorShape output_shape;\n    OP_REQUIRES(c, element_shape_except_first_dim.AsTensorShape(&output_shape),\n                errors::InvalidArgument(\n                    \"Trying to concat list with only uninitialized tensors \",\n                    \"but element_shape_except_first_dim is not fully defined: \",\n                    element_shape_except_first_dim.DebugString()));\n    // Build the lengths_tensor and leading dim of the output tensor by\n    // iterating over all element tensors.\n    Tensor* lengths_tensor = nullptr;\n    OP_REQUIRES_OK(c, c->allocate_output(1,\n                                         TensorShape({static_cast<int64_t>(\n                                             tensor_list->tensors().size())}),\n                                         &lengths_tensor));\n    auto lengths_tensor_vec = lengths_tensor->vec<int64_t>();\n    int64_t leading_dim = 0;\n    for (size_t i = 0; i < tensor_list->tensors().size(); i++) {\n      int64_t dim;\n      if (tensor_list->tensors()[i].dtype() != DT_INVALID) {\n        dim = tensor_list->tensors()[i].shape().dim_size(0);\n      } else {\n        // If leading_dims is not provided or does not contain an entry for\n        // index i use the inferred `first_dim` if set.\n        if ((c->num_inputs() <= 2 || i >= c->input(2).NumElements()) &&\n            first_dim != -1) {\n          dim = first_dim;\n        } else {\n          OP_REQUIRES(c, c->num_inputs() > 2,\n                      errors::InvalidArgument(\n                          \"Concating lists with uninitialized tensors is not \",\n                          \"supported in this version of TensorListConcat. \",\n                          \"Consider updating your GraphDef to run the newer \",\n                          \"version.\"));\n          OP_REQUIRES(c, i < c->input(2).NumElements(),\n                      errors::InvalidArgument(\n                          \"List contains uninitialized tensor at index \", i,\n                          \" but leading_dims has only \",\n                          c->input(2).NumElements(), \" elements.\"));\n          dim = c->input(2).vec<int64_t>()(i);\n        }\n      }\n      leading_dim += dim;\n      lengths_tensor_vec(i) = dim;\n    }\n    output_shape.InsertDim(0, leading_dim);\n    Tensor* output;\n    // Allocate the output tensor and fill it up with the concated element\n    // tensors.\n    OP_REQUIRES_OK(c, c->allocate_output(0, output_shape, &output));\n    if (output->NumElements() == 0) {\n      return;\n    }\n\n    ConstMatrixVector inputs_flat;\n    inputs_flat.reserve(tensor_list->tensors().size());\n    // Store the zeros tensors in a vector to prevent them from being GC'ed till\n    // concat is complete.\n    std::vector<Tensor> zeros_vec;\n    for (int i = 0; i < tensor_list->tensors().size(); i++) {\n      const Tensor& element_tensor = tensor_list->tensors()[i];\n      if (element_tensor.dtype() != DT_INVALID) {\n        if (element_tensor.NumElements() > 0) {\n          inputs_flat.emplace_back(new typename TTypes<T, 2>::ConstMatrix(\n              element_tensor.shaped<T, 2>({1, element_tensor.NumElements()})));\n        }\n      } else {\n        AllocatorAttributes attr;\n        if (element_dtype_ == DT_VARIANT) {\n          attr.set_on_host(true);\n        }\n        TensorShape element_shape = output_shape;\n        element_shape.set_dim(0, lengths_tensor_vec(i));\n        zeros_vec.emplace_back();\n        Tensor& zeros = zeros_vec.back();\n        OP_REQUIRES_OK(\n            c, c->allocate_temp(element_dtype_, element_shape, &zeros, attr));\n        SetZero<Device, T>(c, zeros);\n        inputs_flat.emplace_back(new typename TTypes<T, 2>::ConstMatrix(\n            const_cast<const Tensor&>(zeros).shaped<T, 2>(\n                {1, zeros.NumElements()})));\n      }\n    }\n    auto output_flat = output->shaped<T, 2>({1, output->NumElements()});\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n    if (std::is_same<Device, Eigen::GpuDevice>::value) {\n      ConcatGPU<T>(c, inputs_flat, output, &output_flat);\n      return;\n    }\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n    if (IsPluggableDevice(c)) {\n      ConcatPluggableDevice<T>(c, inputs_flat, &output_flat);\n    } else {\n      ConcatCPU<T>(c->device(), inputs_flat, &output_flat);\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_index_absolute_path(index):\n    return safe_join(INDEXDIR, index)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def start(self):\n        \"\"\"\n        Prepare the server to start serving connections.\n\n        Configure the server socket handler and establish a TLS wrapping\n        socket from which all client connections descend. Bind this TLS\n        socket to the specified network address for the server.\n\n        Raises:\n            NetworkingError: Raised if the TLS socket cannot be bound to the\n                network address.\n        \"\"\"\n        self.manager = multiprocessing.Manager()\n        self.policies = self.manager.dict()\n        policies = copy.deepcopy(operation_policy.policies)\n        for policy_name, policy_set in six.iteritems(policies):\n            self.policies[policy_name] = policy_set\n\n        self.policy_monitor = monitor.PolicyDirectoryMonitor(\n            self.config.settings.get('policy_path'),\n            self.policies,\n            self.live_policies\n        )\n\n        def interrupt_handler(trigger, frame):\n            self.policy_monitor.stop()\n        signal.signal(signal.SIGINT, interrupt_handler)\n        signal.signal(signal.SIGTERM, interrupt_handler)\n\n        self.policy_monitor.start()\n\n        self._engine = engine.KmipEngine(\n            policies=self.policies,\n            database_path=self.config.settings.get('database_path')\n        )\n\n        self._logger.info(\"Starting server socket handler.\")\n\n        # Create a TCP stream socket and configure it for immediate reuse.\n        socket.setdefaulttimeout(10)\n        self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n\n        self._logger.debug(\n            \"Configured cipher suites: {0}\".format(\n                len(self.config.settings.get('tls_cipher_suites'))\n            )\n        )\n        for cipher in self.config.settings.get('tls_cipher_suites'):\n            self._logger.debug(cipher)\n        auth_suite_ciphers = self.auth_suite.ciphers.split(':')\n        self._logger.debug(\n            \"Authentication suite ciphers to use: {0}\".format(\n                len(auth_suite_ciphers)\n            )\n        )\n        for cipher in auth_suite_ciphers:\n            self._logger.debug(cipher)\n\n        self._socket = ssl.wrap_socket(\n            self._socket,\n            keyfile=self.config.settings.get('key_path'),\n            certfile=self.config.settings.get('certificate_path'),\n            server_side=True,\n            cert_reqs=ssl.CERT_REQUIRED,\n            ssl_version=self.auth_suite.protocol,\n            ca_certs=self.config.settings.get('ca_path'),\n            do_handshake_on_connect=True,\n            suppress_ragged_eofs=True,\n            ciphers=self.auth_suite.ciphers\n        )\n\n        try:\n            self._socket.bind(\n                (\n                    self.config.settings.get('hostname'),\n                    int(self.config.settings.get('port'))\n                )\n            )\n        except Exception as e:\n            self._logger.exception(e)\n            raise exceptions.NetworkingError(\n                \"Server failed to bind socket handler to {0}:{1}\".format(\n                    self.config.settings.get('hostname'),\n                    self.config.settings.get('port')\n                )\n            )\n        else:\n            self._logger.info(\n                \"Server successfully bound socket handler to {0}:{1}\".format(\n                    self.config.settings.get('hostname'),\n                    self.config.settings.get('port')\n                )\n            )\n            self._is_serving = True\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def delete_department(department_id=False):\n    if department_id:\n\n        # check user is authorised to delete departments. Only admin or super_user can do this.\n        if not any([g.user.is_admin, g.user.is_super_user]):\n            flash(gettext('You are not authorised to delete departments.'), category='warning')\n            return redirect('flicket_bp.departments')\n\n        form = ConfirmPassword()\n\n        #\n        departments = FlicketCategory.query.filter_by(department_id=department_id)\n        department = FlicketDepartment.query.filter_by(id=department_id).first()\n\n        # we can't delete any departments associated with categories.\n        if departments.count() > 0:\n            flash(gettext(\n                ('Department has categories linked to it. Department can not be deleted unless all categories are '\n                 'first removed.')),\n                category=\"danger\")\n            return redirect(url_for('flicket_bp.departments'))\n\n        if form.validate_on_submit():\n            # delete category from database\n            department = FlicketDepartment.query.filter_by(id=department_id).first()\n\n            db.session.delete(department)\n            # commit changes\n            db.session.commit()\n            flash('Department \"{}\" deleted.'.format(department.department), category='success')\n            return redirect(url_for('flicket_bp.departments'))\n\n        notification = gettext(\n            \"You are trying to delete department: %(value)s.\",\n            value=department.department.upper())\n\n        title = gettext('Delete Department')\n\n        return render_template('flicket_delete.html',\n                               form=form,\n                               notification=notification,\n                               title=title)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def release(self):\n        validate_savepoint_name(self.name)\n        await self.transaction.connection.execute(\n            f\"RELEASE SAVEPOINT {self.name}\"\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def application(environ, start_response):\n\n    my_uri = urllib.unquote(environ['REQUEST_URI'])\n    \n    form = {}\n\n    if my_uri.find(\"?\") == -1:\n       # support fake query strings\n       # something log /cobbler/web/op/ks/server/foo\n       # which is needed because of xend parser errors\n       # not tolerating \";\" and also libvirt on 5.1 not\n       # tolerating \"&amp;\" (nor \"&\").\n\n       # canonicalizes uri, mod_python does this, mod_wsgi does not\n       my_uri = os.path.realpath(my_uri)\n\n       tokens = my_uri.split(\"/\")\n       tokens = tokens[3:]\n       label = True\n       field = \"\"\n       for t in tokens:\n          if label:\n             field = t\n          else:\n             form[field] = t\n          label = not label\n\n    # This MAC header is set by anaconda during a kickstart booted with the \n    # kssendmac kernel option. The field will appear here as something \n    # like: eth0 XX:XX:XX:XX:XX:XX\n    form[\"REMOTE_MAC\"]  = form.get(\"HTTP_X_RHN_PROVISIONING_MAC_0\", None)\n\n    # Read config for the XMLRPC port to connect to:\n    fd = open(\"/etc/cobbler/settings\")\n    data = fd.read()\n    fd.close()\n    ydata = yaml.safe_load(data)\n    remote_port = ydata.get(\"xmlrpc_port\",25151)\n\n    # instantiate a CobblerWeb object\n    cw = CobblerSvc(server = \"http://127.0.0.1:%s\" % remote_port)\n\n    # check for a valid path/mode\n    # handle invalid paths gracefully\n    mode = form.get('op','index')\n\n    # TODO: We could do proper exception handling here and return\n    # corresponding HTTP status codes:\n\n    # Execute corresponding operation on the CobblerSvc object:\n    func = getattr( cw, mode )\n    content = func( **form )\n\n    content = unicode(content).encode('utf-8')\n    status = '200 OK'\n    \n    if content.find(\"# *** ERROR ***\") != -1:\n        status = '500 SERVER ERROR'\n        print(\"possible cheetah template error\")\n\n    # TODO: Not sure these strings are the right ones to look for...\n    elif content.find(\"# profile not found\") != -1 or \\\n            content.find(\"# system not found\") != -1 or \\\n            content.find(\"# object not found\") != -1:\n        print(\"content not found: %s\" % my_uri)\n        status = \"404 NOT FOUND\"\n\n #   req.content_type = \"text/plain;charset=utf-8\"\n    response_headers = [('Content-type', 'text/plain;charset=utf-8'),\n                        ('Content-Length', str(len(content)))]\n    start_response(status, response_headers)\n\n    return [content]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get(self, key, default=None, version=None):\n        key = self.make_key(key, version=version)\n        self.validate_key(key)\n        return self._cache.get(key, default)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _redirect_request_using_get(self, request, redirect_url):\n        redirect_request = _build_redirect_request(\n            request,\n            url=redirect_url,\n            method='GET',\n            body='',\n        )\n        redirect_request.headers.pop('Content-Type', None)\n        redirect_request.headers.pop('Content-Length', None)\n        return redirect_request\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def bad_url(prefix):\n    \"\"\"Redirect unknown root URLs to /simple/.\"\"\"\n    return redirect(core.get_bad_url_redirect_path(request, prefix))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render_entries(\n        self, entries: Entries\n    ) -> Generator[Markup, None, None]:\n        \"\"\"Return entries in Beancount format.\n\n        Only renders :class:`.Balance` and :class:`.Transaction`.\n\n        Args:\n            entries: A list of entries.\n\n        Yields:\n            The entries rendered in Beancount format.\n        \"\"\"\n        indent = self.ledger.fava_options.indent\n        for entry in entries:\n            if isinstance(entry, (Balance, Transaction)):\n                if isinstance(entry, Transaction) and entry.flag in EXCL_FLAGS:\n                    continue\n                try:\n                    yield Markup(get_entry_slice(entry)[0] + \"\\n\")\n                except (KeyError, FileNotFoundError):\n                    yield Markup(\n                        _format_entry(\n                            entry,\n                            self.ledger.fava_options.currency_column,\n                            indent,\n                        )\n                    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "int VP8LBuildHuffmanTable(HuffmanTables* const root_table, int root_bits,\n                          const int code_lengths[], int code_lengths_size) {\n  const int total_size =\n      BuildHuffmanTable(NULL, root_bits, code_lengths, code_lengths_size, NULL);\n  assert(code_lengths_size <= MAX_CODE_LENGTHS_SIZE);\n  if (total_size == 0 || root_table == NULL) return total_size;\n\n  if (root_table->curr_segment->curr_table + total_size >=\n      root_table->curr_segment->start + root_table->curr_segment->size) {\n    // If 'root_table' does not have enough memory, allocate a new segment.\n    // The available part of root_table->curr_segment is left unused because we\n    // need a contiguous buffer.\n    const int segment_size = root_table->curr_segment->size;\n    struct HuffmanTablesSegment* next =\n        (HuffmanTablesSegment*)WebPSafeMalloc(1, sizeof(*next));\n    if (next == NULL) return 0;\n    // Fill the new segment.\n    // We need at least 'total_size' but if that value is small, it is better to\n    // allocate a big chunk to prevent more allocations later. 'segment_size' is\n    // therefore chosen (any other arbitrary value could be chosen).\n    next->size = total_size > segment_size ? total_size : segment_size;\n    next->start =\n        (HuffmanCode*)WebPSafeMalloc(next->size, sizeof(*next->start));\n    if (next->start == NULL) {\n      WebPSafeFree(next);\n      return 0;\n    }\n    next->curr_table = next->start;\n    next->next = NULL;\n    // Point to the new segment.\n    root_table->curr_segment->next = next;\n    root_table->curr_segment = next;\n  }\n  if (code_lengths_size <= SORTED_SIZE_CUTOFF) {\n    // use local stack-allocated array.\n    uint16_t sorted[SORTED_SIZE_CUTOFF];\n    BuildHuffmanTable(root_table->curr_segment->curr_table, root_bits,\n                      code_lengths, code_lengths_size, sorted);\n  } else {  // rare case. Use heap allocation.\n    uint16_t* const sorted =\n        (uint16_t*)WebPSafeMalloc(code_lengths_size, sizeof(*sorted));\n    if (sorted == NULL) return 0;\n    BuildHuffmanTable(root_table->curr_segment->curr_table, root_bits,\n                      code_lengths, code_lengths_size, sorted);\n    WebPSafeFree(sorted);\n  }\n  return total_size;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    StagingMap<Ordered>* map = nullptr;\n    OP_REQUIRES_OK(ctx, GetStagingMap(ctx, def(), &map));\n    core::ScopedUnref scope(map);\n    typename StagingMap<Ordered>::OptionalTuple tuple;\n\n    const Tensor* key_tensor;\n    const Tensor* indices_tensor;\n    OpInputList values_tensor;\n\n    OP_REQUIRES_OK(ctx, ctx->input(\"key\", &key_tensor));\n    OP_REQUIRES_OK(ctx, ctx->input(\"indices\", &indices_tensor));\n    OP_REQUIRES_OK(ctx, ctx->input_list(\"values\", &values_tensor));\n    OP_REQUIRES(ctx, key_tensor->NumElements() > 0,\n                errors::InvalidArgument(\"key must not be empty\"));\n\n    OP_REQUIRES(ctx, key_tensor->NumElements() == 1,\n                errors::InvalidArgument(\n                    \"key must be an int64 scalar, got tensor with shape: \",\n                    key_tensor->shape()));\n\n    // Create copy for insertion into Staging Area\n    Tensor key(*key_tensor);\n\n    // Create the tuple to store\n    for (std::size_t i = 0; i < values_tensor.size(); ++i) {\n      tuple.push_back(values_tensor[i]);\n    }\n\n    // Store the tuple in the map\n    OP_REQUIRES_OK(ctx, map->put(&key, indices_tensor, &tuple));\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def handle_response(self, response, **kwargs):\n        \"\"\"Takes the given response and tries kerberos-auth, as needed.\"\"\"\n\n        if self.pos is not None:\n            # Rewind the file position indicator of the body to where\n            # it was to resend the request.\n            response.request.body.seek(self.pos)\n\n        if response.status_code == 401:\n            _r = self.handle_401(response, **kwargs)\n            log.debug(\"handle_response(): returning {0}\".format(_r))\n            return self.handle_response(_r, **kwargs)\n        else:\n            _r = self.handle_other(response)\n            log.debug(\"handle_response(): returning {0}\".format(_r))\n            return _r\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _build_pretty_email_regex() -> re.Pattern[str]:\n    name_chars = r'[\\w!#$%&\\'*+\\-/=?^_`{|}~]'\n    unquoted_name_group = fr'((?:{name_chars}+\\s+)*{name_chars}+)'\n    quoted_name_group = r'\"((?:[^\"]|\\\")+)\"'\n    email_group = r'<\\s*(.{0,254})\\s*>'\n    return re.compile(rf'\\s*(?:{unquoted_name_group}|{quoted_name_group})?\\s*{email_group}\\s*')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def complete_reg(request):\n    \"\"\"Completes the registeration, called by API\"\"\"\n    try:\n        if not \"fido_state\" in request.session:\n            return JsonResponse({'status': 'ERR', \"message\": \"FIDO Status can't be found, please try again\"})\n        data = cbor.decode(request.body)\n\n        client_data = CollectedClientData(data['clientDataJSON'])\n        att_obj = AttestationObject((data['attestationObject']))\n        server = getServer()\n        auth_data = server.register_complete(\n            request.session.pop['fido_state'],\n            client_data,\n            att_obj\n        )\n        encoded = websafe_encode(auth_data.credential_data)\n        uk = User_Keys()\n        uk.username = request.user.username\n        uk.properties = {\"device\": encoded, \"type\": att_obj.fmt, }\n        uk.owned_by_enterprise = getattr(settings, \"MFA_OWNED_BY_ENTERPRISE\", False)\n        uk.key_type = \"FIDO2\"\n        uk.save()\n        return HttpResponse(simplejson.dumps({'status': 'OK'}))\n    except Exception as exp:\n        import traceback\n        print(traceback.format_exc())\n        try:\n            from raven.contrib.django.raven_compat.models import client\n            client.captureException()\n        except:\n            pass\n        return JsonResponse({'status': 'ERR', \"message\": \"Error on server, please try again later\"})\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_current_phase(self, requested_phase_identifier):\n        found = False\n        for phase in self.phases:\n            if phase.is_valid():\n                phase.process()\n            if found or not requested_phase_identifier or requested_phase_identifier == phase.identifier:\n                found = True  # We're at or past the requested phase\n                if not phase.should_skip():\n                    return phase\n            if not phase.should_skip() and not phase.is_valid():  # A past phase is not valid, that's the current one\n                return phase\n        raise Http404(\"Error! Phase with identifier `%s` not found.\" % escape(requested_phase_identifier))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def put_table_description() -> Response:\n\n    @action_logging\n    def _log_put_table_description(*, table_key: str, description: str, source: str) -> None:\n        pass  # pragma: no cover\n\n    try:\n        args = request.get_json()\n        table_endpoint = _get_table_endpoint()\n\n        table_key = get_query_param(args, 'key')\n\n        description = get_query_param(args, 'description')\n        src = get_query_param(args, 'source')\n\n        table_uri = TableUri.from_uri(table_key)\n        if not is_table_editable(table_uri.schema, table_uri.table):\n            return make_response('', HTTPStatus.FORBIDDEN)\n\n        url = '{0}/{1}/description'.format(table_endpoint, table_key)\n        _log_put_table_description(table_key=table_key, description=description, source=src)\n\n        response = request_metadata(url=url, method='PUT', data=json.dumps({'description': description}))\n        status_code = response.status_code\n\n        if status_code == HTTPStatus.OK:\n            message = 'Success'\n        else:\n            message = 'Update table description failed'\n\n        payload = jsonify({'msg': message})\n        return make_response(payload, status_code)\n    except Exception as e:\n        payload = jsonify({'msg': 'Encountered exception: ' + str(e)})\n        return make_response(payload, HTTPStatus.INTERNAL_SERVER_ERROR)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  QInt32() : value(0) {}\n  QInt32(const int8_t v) : value(v) {}"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init_on_load__(self):\n        # RdiffRepo required an absolute full path, When the user_root is invalid, let generate an invalid full path.\n        if not self.user.user_root:\n            full_path = os.path.join('/user_has_an_empty_user_root/', self.repopath.strip('/'))\n        elif not os.path.isabs(self.user.user_root):\n            full_path = os.path.join('/user_has_a_relative_user_root/', self.repopath.strip('/'))\n        else:\n            full_path = os.path.join(self.user.user_root, self.repopath.strip('/'))\n        RdiffRepo.__init__(self, full_path, encoding=self.encoding or RepoObject.DEFAULT_REPO_ENCODING)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_initial(self):\n        \"\"\"Override of CreateView method, queries for which T&C to accept and catches returnTo from URL\"\"\"\n        LOGGER.debug(\"termsandconditions.views.AcceptTermsView.get_initial\")\n\n        terms = self.get_terms(self.kwargs)\n        return_to = self.get_return_to(self.request.GET)\n\n        return {\"terms\": terms, \"returnTo\": return_to}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, instance, filename):\n        \"\"\"\n        Apply (if callable) or prepend (if a string) upload_to to the filename,\n        then delegate further processing of the name to the storage backend.\n        Until the storage layer, all file paths are expected to be Unix style\n        (with forward slashes).\n        \"\"\"\n        filename = validate_file_name(filename)\n        if callable(self.upload_to):\n            filename = self.upload_to(instance, filename)\n        else:\n            dirname = datetime.datetime.now().strftime(self.upload_to)\n            filename = posixpath.join(dirname, filename)\n        return self.storage.generate_filename(filename)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def from_crawler(cls, crawler):\n        s = crawler.settings\n        splash_base_url = s.get('SPLASH_URL', cls.default_splash_url)\n        log_400 = s.getbool('SPLASH_LOG_400', True)\n        slot_policy = s.get('SPLASH_SLOT_POLICY', cls.default_policy)\n        if slot_policy not in SlotPolicy._known:\n            raise NotConfigured(\"Incorrect slot policy: %r\" % slot_policy)\n\n        splash_user = s.get('SPLASH_USER', '')\n        splash_pass = s.get('SPLASH_PASS', '')\n        auth = None\n        if splash_user or splash_pass:\n            auth = basic_auth_header(splash_user, splash_pass)\n        return cls(crawler, splash_base_url, slot_policy, log_400, auth)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void RepeatsTensor2IndexTensor(const Context& ctx,\n                               const DenseTensor& repeats,\n                               DenseTensor* index) {\n  DenseTensor repeats_cpu_copy;\n  if (repeats.place().GetType() != phi::AllocationType::CPU) {\n    phi::Copy(ctx, repeats, phi::CPUPlace(), true, &repeats_cpu_copy);\n  }\n  const RepeatsT* repeats_data =\n      repeats.place().GetType() == phi::AllocationType::CPU\n          ? repeats.data<RepeatsT>()\n          : repeats_cpu_copy.data<RepeatsT>();\n\n  int64_t index_size = 0;\n  for (int i = 0; i < repeats.dims()[0]; i++) {\n    PADDLE_ENFORCE_GE(repeats_data[i],\n                      0,\n                      phi::errors::InvalidArgument(\n                          \"repeats must grater or equal than 0, but got %d\",\n                          repeats_data[i]));\n    index_size += repeats_data[i];\n  }\n  std::vector<RepeatsT> index_vec(index_size);\n  int offset = 0;\n  for (int i = 0; i < repeats.dims()[0]; i++) {\n    std::fill_n(index_vec.begin() + offset, repeats_data[i], i);\n    offset += repeats_data[i];\n  }\n  index->Resize(phi::make_ddim({index_size}));\n\n  phi::TensorFromVector<RepeatsT>(index_vec, ctx, index);\n}\n}  // namespace funcs"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    // Get inputs\n    const Tensor& input_tensor = context->input(0);\n    const auto input_tensor_flat = input_tensor.flat<int32>();\n    const Tensor& input_splits = context->input(1);\n    const auto input_splits_flat = input_splits.flat<SPLITS_TYPE>();\n\n    OP_REQUIRES(\n        context, input_splits.NumElements() > 0,\n        errors::InvalidArgument(\"Input_splits should contain elements, but \"\n                                \"given input_values has 0 elements\"));\n    // Operation will treat first argument in input_splits as if it were zero\n    // regardless of its actual value since splits should begin with zero and\n    // end with the length of the input values vector.\n    OP_REQUIRES(\n        context, input_splits_flat(0) == 0,\n        errors::InvalidArgument(\"First value in input_splits must be zero.\"));\n    OP_REQUIRES(context,\n                input_splits_flat(input_splits_flat.size() - 1) ==\n                    input_tensor_flat.size(),\n                errors::InvalidArgument(\"Last value in input_splits must be \"\n                                        \"equal to length of input_tensor.\"));\n    // Since we limit to a 2-D input (flat_values of rank 1 and a single splits\n    // tensor), our output dimension will be 1 with it's size equal to the\n    // number of splits (outer dimension or ragged tensor).\n    TensorShape output_shape({input_splits.dim_size(0) - 1});\n    Tensor* output_tensor;\n    OP_REQUIRES_OK(context, context->allocate_output(\"output\", output_shape,\n                                                     &output_tensor));\n    auto output_tensor_flat = output_tensor->flat<tstring>();\n\n    // Use a single index over the flattened input values tensor.\n    int idx = 0;\n    // Loop through our split dimension to create a new string at each split.\n    for (int i = 1; i < input_splits_flat.size(); ++i) {\n      icu::UnicodeString unicode_string;\n      icu::UnicodeStringAppendable appendable_unicode_string(unicode_string);\n      OP_REQUIRES(\n          context, input_splits_flat(i - 1) <= input_splits_flat(i),\n          errors::InvalidArgument(\n              \"Values in input_splits must be equal or in ascending order.\"));\n      OP_REQUIRES(\n          context, input_splits_flat(i) <= input_tensor_flat.size(),\n          errors::InvalidArgument(\"Values in input_splits must be less than or \"\n                                  \"equal to input_tensor length.\"));\n      for (; idx < input_splits_flat(i); ++idx) {\n        int32_t code_point = input_tensor_flat(idx);\n        // Check for invalid code point\n        if (!U_IS_UNICODE_CHAR(code_point)) {\n          if (error_options_.error_on_malformatting) {\n            context->CtxFailure(errors::InvalidArgument(\n                \"Code point is out of range for Unicode, or a noncharacter.\"));\n            return;\n          } else if (!error_options_.elide_replacement) {\n            code_point = error_options_.subst;\n          }\n        }\n        appendable_unicode_string.appendCodePoint(code_point);\n      }\n      // Encode our string and save in the output.\n      tstring result;\n      Encode(encoding_, unicode_string, &result);\n      output_tensor_flat(i - 1) = std::move(result);\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void operator()(OpKernelContext* context, const Tensor& x_input,\n                  const Tensor& scale_input, const Tensor& offset_input,\n                  const Tensor& estimated_mean_input,\n                  const Tensor& estimated_variance_input,\n                  const Tensor* side_input, U epsilon, U exponential_avg_factor,\n                  FusedBatchNormActivationMode activation_mode,\n                  Tensor* y_output, Tensor* batch_mean_output,\n                  Tensor* batch_var_output, Tensor* saved_mean_output,\n                  Tensor* saved_var_output, TensorFormat tensor_format,\n                  bool use_reserved_space) {\n    OP_REQUIRES(context, side_input == nullptr,\n                errors::Internal(\n                    \"The CPU implementation of FusedBatchNorm does not support \"\n                    \"side input.\"));\n    OP_REQUIRES(context,\n                activation_mode == FusedBatchNormActivationMode::kIdentity,\n                errors::Internal(\"The CPU implementation of FusedBatchNorm \"\n                                 \"does not support activations.\"));\n\n    if (use_reserved_space) {\n      Tensor* dummy_reserve_space = nullptr;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(5, {}, &dummy_reserve_space));\n      // Initialize the memory, to avoid sanitizer alerts.\n      dummy_reserve_space->flat<U>()(0) = U();\n    }\n    Tensor transformed_x;\n    Tensor transformed_y;\n    if (tensor_format == FORMAT_NCHW) {\n      const int64 in_batch = GetTensorDim(x_input, tensor_format, 'N');\n      const int64 in_rows = GetTensorDim(x_input, tensor_format, 'H');\n      const int64 in_cols = GetTensorDim(x_input, tensor_format, 'W');\n      const int64 in_depths = GetTensorDim(x_input, tensor_format, 'C');\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_y));\n      // Perform NCHW to NHWC\n      std::vector<int32> perm = {0, 2, 3, 1};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             x_input, perm, &transformed_x));\n    } else {\n      transformed_x = x_input;\n      transformed_y = *y_output;\n    }\n    typename TTypes<T, 4>::Tensor x(transformed_x.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec offset(offset_input.vec<U>());\n    typename TTypes<U>::ConstVec estimated_mean(estimated_mean_input.vec<U>());\n    typename TTypes<U>::ConstVec estimated_variance(\n        estimated_variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor y(transformed_y.tensor<T, 4>());\n    typename TTypes<U>::Vec batch_mean(batch_mean_output->vec<U>());\n    typename TTypes<U>::Vec batch_variance(batch_var_output->vec<U>());\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    const int depth = x.dimension(3);\n    OP_REQUIRES(\n        context, depth != 0,\n        errors::Internal(\"The 4th element in the input shape cannot be 0.\"));\n    const int size = x.size();\n    const int rest_size = size / depth;\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 1> reduce_dims({0});\n    Eigen::array<int, 2> bcast_spec({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> bcast_spec;\n    bcast_spec.set(0, rest_size);\n#endif\n\n    auto x_rest_by_depth = x.reshape(rest_by_depth).template cast<U>();\n    auto x_centered =\n        x_rest_by_depth -\n        estimated_mean.reshape(one_by_depth).broadcast(bcast_spec);\n    auto scaling_factor = ((estimated_variance + epsilon).rsqrt() * scale)\n                              .eval()\n                              .reshape(one_by_depth)\n                              .broadcast(bcast_spec);\n    auto x_scaled = x_centered * scaling_factor;\n    auto x_shifted =\n        (x_scaled + offset.reshape(one_by_depth).broadcast(bcast_spec))\n            .template cast<T>();\n\n    y.reshape(rest_by_depth).device(d) = x_shifted;\n    batch_mean.device(d) = estimated_mean;\n    batch_variance.device(d) = estimated_variance;\n\n    if (tensor_format == FORMAT_NCHW) {\n      // Perform NHWC to NCHW\n      const std::vector<int32> perm = {0, 3, 1, 2};\n      const Status s = ::tensorflow::DoTranspose(\n          context->eigen_device<CPUDevice>(), transformed_y, perm, y_output);\n      if (!s.ok()) {\n        context->SetStatus(errors::InvalidArgument(\"Transpose failed: \", s));\n      }\n    }\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _cli_deploy_image(args):\n\n    docker_repository = _resolve_property(args, 'docker_repository')\n    vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)\n    vault_endpoint = _resolve_vault_endpoint(args)\n    vault_endpoint_verify = _resolve_property(args, 'vault_endpoint_verify', ignore_value_error=True)\n    image_tar_path = args.image_tar_path if args.image_tar_path else find_image_file()\n\n    bigflow.deploy.deploy_docker_image(\n        image_tar_path=image_tar_path,\n        auth_method=args.auth_method,\n        docker_repository=docker_repository,\n        vault_endpoint=vault_endpoint,\n        vault_endpoint_verify=vault_endpoint_verify,\n        vault_secret=vault_secret,\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def index_html(self, REQUEST, RESPONSE):\n        \"\"\"\n        The default view of the contents of a File or Image.\n\n        Returns the contents of the file or image.  Also, sets the\n        Content-Type HTTP header to the objects content type.\n        \"\"\"\n\n        if self._if_modified_since_request_handler(REQUEST, RESPONSE):\n            # we were able to handle this by returning a 304\n            # unfortunately, because the HTTP cache manager uses the cache\n            # API, and because 304 responses are required to carry the Expires\n            # header for HTTP/1.1, we need to call ZCacheable_set here.\n            # This is nonsensical for caches other than the HTTP cache manager\n            # unfortunately.\n            self.ZCacheable_set(None)\n            return b''\n\n        if self.precondition and hasattr(self, str(self.precondition)):\n            # Grab whatever precondition was defined and then\n            # execute it.  The precondition will raise an exception\n            # if something violates its terms.\n            c = getattr(self, str(self.precondition))\n            if hasattr(c, 'isDocTemp') and c.isDocTemp:\n                c(REQUEST['PARENTS'][1], REQUEST)\n            else:\n                c()\n\n        if self._range_request_handler(REQUEST, RESPONSE):\n            # we served a chunk of content in response to a range request.\n            return b''\n\n        RESPONSE.setHeader('Last-Modified', rfc1123_date(self._p_mtime))\n        RESPONSE.setHeader('Content-Type', self.content_type)\n        RESPONSE.setHeader('Content-Length', self.size)\n        RESPONSE.setHeader('Accept-Ranges', 'bytes')\n\n        if self._should_force_download():\n            # We need a filename, even a dummy one if needed.\n            filename = self.getId()\n            if \".\" not in filename:\n                # This either returns None or \".some_extension\"\n                ext = guess_extension(self.content_type, strict=False)\n                if not ext:\n                    # image/svg+xml -> svg\n                    ext = \".\" + self.content_type.split(\"/\")[-1].split(\"+\")[0]\n                filename += f\"{ext}\"\n            filename = quote(filename.encode(\"utf8\"))\n            RESPONSE.setHeader(\n                \"Content-Disposition\",\n                f\"attachment; filename*=UTF-8''{filename}\",\n            )\n\n        if self.ZCacheable_isCachingEnabled():\n            result = self.ZCacheable_get(default=None)\n            if result is not None:\n                # We will always get None from RAMCacheManager and HTTP\n                # Accelerated Cache Manager but we will get\n                # something implementing the IStreamIterator interface\n                # from a \"FileCacheManager\"\n                return result\n\n        self.ZCacheable_set(None)\n\n        data = self.data\n        if isinstance(data, bytes):\n            RESPONSE.setBase(None)\n            return data\n\n        while data is not None:\n            RESPONSE.write(data.data)\n            data = data.next\n\n        return b''\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  static port::StatusOr<CudnnRnnSequenceTensorDescriptor> Create(\n      GpuExecutor* parent, int max_seq_length, int batch_size, int data_size,\n      cudnnDataType_t data_type) {\n    if (max_seq_length <= 0) {\n      return port::Status(port::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n    }\n    int dims[] = {batch_size, data_size, 1};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    TensorDescriptor tensor_desc = CreateTensorDescriptor();\n    RETURN_IF_CUDNN_ERROR(cudnnSetTensorNdDescriptor(\n        /*tensorDesc=*/tensor_desc.get(), /*dataType=*/data_type,\n        /*nbDims=*/sizeof(dims) / sizeof(dims[0]), /*dimA=*/dims,\n        /*strideA=*/strides));\n    return CudnnRnnSequenceTensorDescriptor(parent, max_seq_length, batch_size,\n                                            data_size, data_type,\n                                            nullptr,\n                                            std::move(tensor_desc));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def real_download(self, filename, info_dict):\n        self.report_destination(filename)\n        tmpfilename = self.temp_name(filename)\n        self._cookies_tempfile = None\n\n        try:\n            started = time.time()\n            retval = self._call_downloader(tmpfilename, info_dict)\n        except KeyboardInterrupt:\n            if not info_dict.get('is_live'):\n                raise\n            # Live stream downloading cancellation should be considered as\n            # correct and expected termination thus all postprocessing\n            # should take place\n            retval = 0\n            self.to_screen('[%s] Interrupted by user' % self.get_basename())\n        finally:\n            if self._cookies_tempfile:\n                self.try_remove(self._cookies_tempfile)\n\n        if retval == 0:\n            status = {\n                'filename': filename,\n                'status': 'finished',\n                'elapsed': time.time() - started,\n            }\n            if filename != '-':\n                fsize = os.path.getsize(encodeFilename(tmpfilename))\n                self.try_rename(tmpfilename, filename)\n                status.update({\n                    'downloaded_bytes': fsize,\n                    'total_bytes': fsize,\n                })\n            self._hook_progress(status, info_dict)\n            return True\n        else:\n            self.to_stderr('\\n')\n            self.report_error('%s exited with code %d' % (\n                self.get_basename(), retval))\n            return False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _runtime_ir(runtime_functions, all_sigs, global_ctx):\n    # categorize the runtime functions because we will organize the runtime\n    # code into the following sections:\n    # payable functions, nonpayable functions, fallback function, internal_functions\n    internal_functions = [f for f in runtime_functions if _is_internal(f)]\n\n    external_functions = [f for f in runtime_functions if not _is_internal(f)]\n    default_function = next((f for f in external_functions if _is_default_func(f)), None)\n\n    # functions that need to go exposed in the selector section\n    regular_functions = [f for f in external_functions if not _is_default_func(f)]\n    payables = [f for f in regular_functions if _is_payable(f)]\n    nonpayables = [f for f in regular_functions if not _is_payable(f)]\n\n    # create a map of the IR functions since they might live in both\n    # runtime and deploy code (if init function calls them)\n    internal_functions_map: Dict[str, IRnode] = {}\n\n    for func_ast in internal_functions:\n        func_ir = generate_ir_for_function(func_ast, all_sigs, global_ctx, False)\n        internal_functions_map[func_ast.name] = func_ir\n\n    # for some reason, somebody may want to deploy a contract with no\n    # external functions, or more likely, a \"pure data\" contract which\n    # contains immutables\n    if len(external_functions) == 0:\n        # TODO: prune internal functions in this case?\n        runtime = [\"seq\"] + list(internal_functions_map.values())\n        return runtime, internal_functions_map\n\n    # note: if the user does not provide one, the default fallback function\n    # reverts anyway. so it does not hurt to batch the payable check.\n    default_is_nonpayable = default_function is None or not _is_payable(default_function)\n\n    # when a contract has a nonpayable default function,\n    # we can do a single check for all nonpayable functions\n    batch_payable_check = len(nonpayables) > 0 and default_is_nonpayable\n    skip_nonpayable_check = batch_payable_check\n\n    selector_section = [\"seq\"]\n\n    for func_ast in payables:\n        func_ir = generate_ir_for_function(func_ast, all_sigs, global_ctx, False)\n        selector_section.append(func_ir)\n\n    if batch_payable_check:\n        selector_section.append([\"assert\", [\"iszero\", \"callvalue\"]])\n\n    for func_ast in nonpayables:\n        func_ir = generate_ir_for_function(func_ast, all_sigs, global_ctx, skip_nonpayable_check)\n        selector_section.append(func_ir)\n\n    if default_function:\n        fallback_ir = generate_ir_for_function(\n            default_function, all_sigs, global_ctx, skip_nonpayable_check\n        )\n    else:\n        fallback_ir = IRnode.from_list(\n            [\"revert\", 0, 0], annotation=\"Default function\", error_msg=\"fallback function\"\n        )\n\n    # ensure the external jumptable section gets closed out\n    # (for basic block hygiene and also for zksync interpreter)\n    # NOTE: this jump gets optimized out in assembly since the\n    # fallback label is the immediate next instruction,\n    close_selector_section = [\"goto\", \"fallback\"]\n\n    runtime = [\n        \"seq\",\n        [\"with\", \"_calldata_method_id\", shr(224, [\"calldataload\", 0]), selector_section],\n        close_selector_section,\n        [\"label\", \"fallback\", [\"var_list\"], fallback_ir],\n    ]\n\n    # TODO: prune unreachable functions?\n    runtime.extend(internal_functions_map.values())\n\n    return runtime, internal_functions_map\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n\n  int batches = input->dims->data[0];\n  int height = input->dims->data[1];\n  int width = input->dims->data[2];\n  int channels_out = input->dims->data[3];\n\n  // Matching GetWindowedOutputSize in TensorFlow.\n  auto padding = params->padding;\n  int out_width, out_height;\n\n  // Prevent division by 0 in optimized pooling implementations\n  TF_LITE_ENSURE(context, params->stride_height > 0);\n  TF_LITE_ENSURE(context, params->stride_width > 0);\n\n  data->padding = ComputePaddingHeightWidth(\n      params->stride_height, params->stride_width, 1, 1, height, width,\n      params->filter_height, params->filter_width, padding, &out_height,\n      &out_width);\n\n  if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8) {\n    if (pool_type == kAverage || pool_type == kMax) {\n      TFLITE_DCHECK_LE(std::abs(input->params.scale - output->params.scale),\n                       1.0e-6);\n      TFLITE_DCHECK_EQ(input->params.zero_point, output->params.zero_point);\n    }\n    if (pool_type == kL2) {\n      // We currently don't have a quantized implementation of L2Pool\n      TF_LITE_ENSURE_TYPES_EQ(context, input->type, kTfLiteFloat32);\n    }\n  }\n\n  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);\n  output_size->data[0] = batches;\n  output_size->data[1] = out_height;\n  output_size->data[2] = out_width;\n  output_size->data[3] = channels_out;\n  return context->ResizeTensor(context, output, output_size);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create_service(self, context, OS_KSADM_service):\n        self.assert_admin(context)\n        service_id = uuid.uuid4().hex\n        service_ref = OS_KSADM_service.copy()\n        service_ref['id'] = service_id\n        new_service_ref = self.catalog_api.create_service(\n                context, service_id, service_ref)\n        return {'OS-KSADM:service': new_service_ref}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs(\n    OpKernelContext* context, TensorInputs* inputs,\n    TensorShapes* input_matrix_shapes, TensorShape* batch_shape) {\n  int input_rank = -1;\n  for (int i = 0; i < NumMatrixInputs(context); ++i) {\n    const Tensor& in = context->input(i);\n    if (i == 0) {\n      input_rank = in.dims();\n      OP_REQUIRES(\n          context, input_rank >= 2,\n          errors::InvalidArgument(\"Input tensor \", i,\n                                  \" must have rank >= 2, got \", input_rank));\n      // If the tensor rank is greater than 2, we consider the inner-most\n      // dimensions as matrices, and loop over all the other outer (\"batch\")\n      // dimensions to compute the results.\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        batch_shape->AddDim(in.dim_size(dim));\n      }\n    } else {\n      // Make sure that all inputs have the same rank and outer dimensions.\n      OP_REQUIRES(context, input_rank == in.dims(),\n                  errors::InvalidArgument(\n                      \"All input tensors must have the same rank.\"));\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        OP_REQUIRES(\n            context, in.dim_size(dim) == batch_shape->dim_size(dim),\n            errors::InvalidArgument(\n                \"All input tensors must have the same outer dimensions.\"));\n      }\n    }\n\n    const int row_dimension = input_rank - 2;\n    const int col_dimension = input_rank - 1;\n    const int64_t num_rows = in.dim_size(row_dimension);\n    const int64_t num_cols = in.dim_size(col_dimension);\n    input_matrix_shapes->emplace_back(\n        std::initializer_list<int64_t>({num_rows, num_cols}));\n    inputs->emplace_back(&in);\n    OP_REQUIRES(\n        context, in.dtype() == DataTypeToEnum<InputScalar>::v(),\n        errors::InvalidArgument(\"Invalid input dtype \", in.dtype(), \" vs \",\n                                DataTypeToEnum<InputScalar>::v()));\n  }\n  // Have the derived class validate that the inputs are as expected.\n  ValidateInputMatrixShapes(context, *input_matrix_shapes);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def yum_sync(self, repo):\n\n        \"\"\"\n        Handle copying of http:// and ftp:// yum repos.\n        \"\"\"\n\n        # create the config file the hosts will use to access the repository.\n        repo_mirror = repo.mirror.strip()\n        dest_path = os.path.join(self.settings.webdir + \"/repo_mirror\", repo.name.strip())\n        self.create_local_file(dest_path, repo)\n\n        if not repo.mirror_locally:\n            return\n\n        cmd = self.reposync_cmd()  # command to run\n        has_rpm_list = False       # flag indicating not to pull the whole repo\n\n        # detect cases that require special handling\n\n        if repo.rpm_list != \"\" and repo.rpm_list != []:\n            has_rpm_list = True\n\n        # create yum config file for use by reposync\n        temp_path = os.path.join(dest_path, \".origin\")\n\n        if not os.path.isdir(temp_path):\n            # FIXME: there's a chance this might break the RHN D/L case\n            os.makedirs(temp_path)\n\n        temp_file = self.create_local_file(temp_path, repo, output=False)\n\n        if not has_rpm_list:\n            # if we have not requested only certain RPMs, use reposync\n            cmd = \"%s %s --config=%s --repoid=%s --download_path=%s\" % (cmd, self.rflags, temp_file, pipes.quote(repo.name), pipes.quote(self.settings.webdir + \"/repo_mirror\"))\n            if repo.arch != \"\":\n                if repo.arch == \"x86\":\n                    repo.arch = \"i386\"      # FIX potential arch errors\n                if repo.arch == \"i386\":\n                    # counter-intuitive, but we want the newish kernels too\n                    cmd = \"%s -a i686\" % (cmd)\n                else:\n                    cmd = \"%s -a %s\" % (cmd, repo.arch)\n\n        else:\n\n            # create the output directory if it doesn't exist\n            if not os.path.exists(dest_path):\n                os.makedirs(dest_path)\n\n            use_source = \"\"\n            if repo.arch == \"src\":\n                use_source = \"--source\"\n\n            # older yumdownloader sometimes explodes on --resolvedeps\n            # if this happens to you, upgrade yum & yum-utils\n            extra_flags = self.settings.yumdownloader_flags\n            cmd = \"\"\n            if os.path.exists(\"/usr/bin/dnf\"):\n                cmd = \"/usr/bin/dnf download\"\n            else:\n                cmd = \"/usr/bin/yumdownloader\"\n            cmd = \"%s %s %s --disablerepo=* --enablerepo=%s -c %s --destdir=%s %s\" % (cmd, extra_flags, use_source, pipes.quote(repo.name), temp_file, pipes.quote(dest_path), \" \".join(repo.rpm_list))\n\n        # now regardless of whether we're doing yumdownloader or reposync\n        # or whether the repo was http://, ftp://, or rhn://, execute all queued\n        # commands here.  Any failure at any point stops the operation.\n\n        rc = utils.subprocess_call(self.logger, cmd)\n        if rc != 0:\n            utils.die(self.logger, \"cobbler reposync failed\")\n\n        repodata_path = os.path.join(dest_path, \"repodata\")\n\n        # grab repomd.xml and use it to download any metadata we can use\n        proxies = None\n        if repo.proxy == '<<inherit>>':\n            proxies = {'http': self.settings.proxy_url_ext}\n        elif repo.proxy != '<<None>>' and repo.proxy != '':\n            proxies = {'http': repo.proxy, 'https': repo.proxy}\n        src = repo_mirror + \"/repodata/repomd.xml\"\n        dst = temp_path + \"/repomd.xml\"\n        urlgrab_ssl_opts = self.gen_urlgrab_ssl_opts(repo.yumopts)\n        try:\n            urlgrabber.grabber.urlgrab(src, filename=dst, proxies=proxies, **urlgrab_ssl_opts)\n        except Exception as e:\n            utils.die(self.logger, \"failed to fetch \" + src + \" \" + e.args)\n\n        # create our repodata directory now, as any extra metadata we're\n        # about to download probably lives there\n        if not os.path.isdir(repodata_path):\n            os.makedirs(repodata_path)\n        rmd = yum.repoMDObject.RepoMD('', \"%s/repomd.xml\" % (temp_path))\n        for mdtype in rmd.repoData.keys():\n            # don't download metadata files that are created by default\n            if mdtype not in [\"primary\", \"primary_db\", \"filelists\", \"filelists_db\", \"other\", \"other_db\"]:\n                mdfile = rmd.getData(mdtype).location[1]\n                src = repo_mirror + \"/\" + mdfile\n                dst = dest_path + \"/\" + mdfile\n                try:\n                    urlgrabber.grabber.urlgrab(src, filename=dst, proxies=proxies, **urlgrab_ssl_opts)\n                except Exception as e:\n                    utils.die(self.logger, \"failed to fetch \" + src + \" \" + e.args)\n\n        # now run createrepo to rebuild the index\n        if repo.mirror_locally:\n            os.path.walk(dest_path, self.createrepo_walker, repo)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, *, tokens: List[str], max_cache_size: int = config.DEFAULT_CACHE_SIZE):\n        self._tokens = tokens\n        self._max_cache_size = max_cache_size\n        self._check_args()\n        self._tokens = sorted(self._tokens)  # Sorted for subsequent reproducible randomization.\n\n        self._shorten_url = lru_cache(maxsize=self._max_cache_size)(self._shorten_url)  # type: ignore  # Instance level cache\n        self._init_executor()\n        if config.TEST_API_ON_INIT:\n            self._test()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _set_image_file_metadata(self):\n        self.file.open()\n\n        # Set new image file size\n        self.file_size = self.file.size\n\n        # Set new image file hash\n        self._set_file_hash()\n        self.file.seek(0)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def publish(self, id_, identity, uow=None):\n        \"\"\"Publish a draft.\n\n        Idea:\n            - Get the draft from the data layer (draft is not passed in)\n            - Validate it more strictly than when it was originally saved\n              (drafts can be incomplete but only complete drafts can be turned\n              into records)\n            - Create or update associated (published) record with data\n        \"\"\"\n        # Get the draft\n        draft = self.draft_cls.pid.resolve(id_, registered_only=False)\n        self.require_permission(identity, \"publish\", record=draft)\n\n        # Validate the draft strictly - since a draft can be saved with errors\n        # we do a strict validation here to make sure only valid drafts can be\n        # published.\n        self._validate_draft(identity, draft)\n\n        # Create the record from the draft\n        latest_id = draft.versions.latest_id\n        record = self.record_cls.publish(draft)\n\n        # Run components\n        self.run_components(\n            'publish', identity, draft=draft, record=record, uow=uow)\n\n        # Commit and index\n        uow.register(RecordCommitOp(record, indexer=self.indexer))\n        uow.register(RecordDeleteOp(draft, force=False, indexer=self.indexer))\n\n        if latest_id:\n            self._reindex_latest(latest_id, uow=uow)\n\n        return self.result_item(\n            self, identity, record, links_tpl=self.links_item_tpl)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def shutdown():\n    task = request.get_json().get('parameter', -1)\n    showtext = {}\n    if task in (0, 1):  # valid commandos received\n        # close all database connections\n        calibre_db.dispose()\n        ub.dispose()\n\n        if task == 0:\n            showtext['text'] = _(u'Server restarted, please reload page')\n        else:\n            showtext['text'] = _(u'Performing shutdown of server, please close window')\n        # stop gevent/tornado server\n        web_server.stop(task == 0)\n        return json.dumps(showtext)\n\n    if task == 2:\n        log.warning(\"reconnecting to calibre database\")\n        calibre_db.reconnect_db(config, ub.app_DB_path)\n        showtext['text'] = _(u'Reconnect successful')\n        return json.dumps(showtext)\n\n    showtext['text'] = _(u'Unknown command')\n    return json.dumps(showtext), 400\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status FractionalPoolShapeFn(InferenceContext* c) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n  std::vector<float> pooling_ratio;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"pooling_ratio\", &pooling_ratio));\n  if (pooling_ratio.size() != 4) {\n    return errors::InvalidArgument(\n        \"pooling_ratio field must specify 4 dimensions\");\n  }\n  std::vector<DimensionHandle> output_dims;\n  for (int i = 0; i < 4; ++i) {\n    DimensionHandle d = c->Dim(input, i);\n    if (c->ValueKnown(d)) {\n      // This must match the same logic in the kernel function in\n      // core/kernels/fractional_max_pool_op.cc.\n      auto val =\n          static_cast<int64_t>(std::floor(c->Value(d) / pooling_ratio[i]));\n      if (val < 0) {\n        return errors::InvalidArgument(\"Size computed for dim \", i,\n                                       \" is negative: \", val);\n      }\n      output_dims.push_back(c->MakeDim(val));\n    } else {\n      output_dims.push_back(c->UnknownDim());\n    }\n  }\n\n  for (std::size_t i = 0; i < pooling_ratio.size(); ++i) {\n    if (pooling_ratio[i] < 1) {\n      return errors::InvalidArgument(\n          \"pooling_ratio cannot be smaller than 1, got: \", pooling_ratio[i]);\n    }\n  }\n\n  c->set_output(0, c->MakeShape(output_dims));\n  c->set_output(1, c->Vector(output_dims[1]));\n  c->set_output(2, c->Vector(output_dims[2]));\n  return OkStatus();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _handle_carbon_received(self, msg):\n        if msg['from'].bare == self.xmpp.boundjid.bare:\n            self.xmpp.event('carbon_received', msg)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& gradient = ctx->input(0);\n    const Tensor& input = ctx->input(1);\n    Tensor* input_backprop = nullptr;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, input.shape(), &input_backprop));\n    OP_REQUIRES(\n        ctx, axis_ >= -1,\n        errors::InvalidArgument(\"Axis must be at least -1. Found \", axis_));\n    OP_REQUIRES(ctx, (axis_ == -1 || axis_ < input.shape().dims()),\n                errors::InvalidArgument(\n                    \"Axis should be -1 or 0 or a positive value less than \",\n                    input.shape().dims(), \"but given axis value was \", axis_));\n\n    OP_REQUIRES(\n        ctx, input.IsSameSize(gradient),\n        errors::InvalidArgument(\"gradient and input must be the same size\"));\n    const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n    const Tensor& input_min_tensor = ctx->input(2);\n    OP_REQUIRES(ctx,\n                input_min_tensor.dims() == 0 || input_min_tensor.dims() == 1,\n                errors::InvalidArgument(\n                    \"Input min tensor must have dimension 0 or 1. Received \",\n                    input_min_tensor.dims(), \".\"));\n    const Tensor& input_max_tensor = ctx->input(3);\n    OP_REQUIRES(ctx,\n                input_max_tensor.dims() == 0 || input_max_tensor.dims() == 1,\n                errors::InvalidArgument(\n                    \"Input max tensor must have dimension 0 or 1. Received \",\n                    input_max_tensor.dims(), \".\"));\n    if (axis_ != -1) {\n      OP_REQUIRES(\n          ctx, input_min_tensor.dim_size(0) == depth,\n          errors::InvalidArgument(\"min has incorrect size, expected \", depth,\n                                  \" was \", input_min_tensor.dim_size(0)));\n      OP_REQUIRES(\n          ctx, input_max_tensor.dim_size(0) == depth,\n          errors::InvalidArgument(\"max has incorrect size, expected \", depth,\n                                  \" was \", input_max_tensor.dim_size(0)));\n    }\n\n    TensorShape min_max_shape(input_min_tensor.shape());\n    Tensor* input_min_backprop;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(1, min_max_shape, &input_min_backprop));\n\n    Tensor* input_max_backprop;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(2, min_max_shape, &input_max_backprop));\n\n    if (axis_ == -1) {\n      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_min_tensor.shape()),\n                  errors::InvalidArgument(\n                      \"input_min must be a scalar if axis is unspecified\"));\n      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_max_tensor.shape()),\n                  errors::InvalidArgument(\n                      \"input_max must be a scalar if axis is unspecified\"));\n      functor::QuantizeAndDequantizeOneScaleGradientFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(), gradient.template flat<T>(),\n        input.template flat<T>(), input_min_tensor.scalar<T>(),\n        input_max_tensor.scalar<T>(), input_backprop->template flat<T>(),\n        input_min_backprop->template scalar<T>(),\n        input_max_backprop->template scalar<T>());\n    } else {\n      functor::QuantizeAndDequantizePerChannelGradientFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(),\n        gradient.template flat_inner_outer_dims<T, 3>(axis_ - 1),\n        input.template flat_inner_outer_dims<T, 3>(axis_ - 1),\n        &input_min_tensor, &input_max_tensor,\n        input_backprop->template flat_inner_outer_dims<T, 3>(axis_ - 1),\n        input_min_backprop->template flat<T>(),\n        input_max_backprop->template flat<T>());\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def autolink(self, link, is_email=False):\n        \"\"\"Rendering a given link or email address.\n\n        :param link: link content or email address.\n        :param is_email: whether this is an email or not.\n        \"\"\"\n        text = link = escape_link(link)\n        if is_email:\n            link = 'mailto:%s' % link\n        return '<a href=\"%s\">%s</a>' % (link, text)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def received(self, s):\n        # Returns the number of bytes consumed.\n\n        if self.completed:\n            return 0\n        orig_size = len(s)\n\n        while s:\n            rm = self.chunk_remainder\n\n            if rm > 0:\n                # Receive the remainder of a chunk.\n                to_write = s[:rm]\n                self.buf.append(to_write)\n                written = len(to_write)\n                s = s[written:]\n\n                self.chunk_remainder -= written\n\n                if self.chunk_remainder == 0:\n                    self.validate_chunk_end = True\n            elif self.validate_chunk_end:\n                pos = s.find(b\"\\r\\n\")\n\n                if pos == 0:\n                    # Chop off the terminating CR LF from the chunk\n                    s = s[2:]\n                else:\n                    self.error = BadRequest(\"Chunk not properly terminated\")\n                    self.all_chunks_received = True\n\n                # Always exit this loop\n                self.validate_chunk_end = False\n            elif not self.all_chunks_received:\n                # Receive a control line.\n                s = self.control_line + s\n                pos = s.find(b\"\\r\\n\")\n\n                if pos < 0:\n                    # Control line not finished.\n                    self.control_line = s\n                    s = \"\"\n                else:\n                    # Control line finished.\n                    line = s[:pos]\n                    s = s[pos + 2 :]\n                    self.control_line = b\"\"\n                    line = line.strip()\n\n                    if line:\n                        # Begin a new chunk.\n                        semi = line.find(b\";\")\n\n                        if semi >= 0:\n                            # discard extension info.\n                            line = line[:semi]\n                        try:\n                            sz = int(line.strip(), 16)  # hexadecimal\n                        except ValueError:  # garbage in input\n                            self.error = BadRequest(\"garbage in chunked encoding input\")\n                            sz = 0\n\n                        if sz > 0:\n                            # Start a new chunk.\n                            self.chunk_remainder = sz\n                        else:\n                            # Finished chunks.\n                            self.all_chunks_received = True\n                    # else expect a control line.\n            else:\n                # Receive the trailer.\n                trailer = self.trailer + s\n\n                if trailer.startswith(b\"\\r\\n\"):\n                    # No trailer.\n                    self.completed = True\n\n                    return orig_size - (len(trailer) - 2)\n                pos = find_double_newline(trailer)\n\n                if pos < 0:\n                    # Trailer not finished.\n                    self.trailer = trailer\n                    s = b\"\"\n                else:\n                    # Finished the trailer.\n                    self.completed = True\n                    self.trailer = trailer[:pos]\n\n                    return orig_size - (len(trailer) - pos)\n\n        return orig_size\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _set_headers(self):\n        response = cherrypy.serving.response\n        # Define X-Frame-Options to avoid Clickjacking\n        response.headers['X-Frame-Options'] = 'DENY'\n\n        # Enforce security on cookies\n        cookie = response.cookie.get('session_id', None)\n        if cookie:\n            # Awaiting bug fix in cherrypy\n            # https://github.com/cherrypy/cherrypy/issues/1767\n            # Force SameSite to Lax\n            cookie['samesite'] = 'Lax'\n            # Check if https is enabled\n            https = cherrypy.request.base.startswith('https')\n            if https:\n                cookie['secure'] = 1\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "encode_unicode(PyObject *unicode)\n{\n    PyObject *repr;\n    Py_UNICODE *s;\n    Py_ssize_t size;\n    char *p;\n\n    static const char *hexdigit = \"0123456789abcdef\";\n#ifdef Py_UNICODE_WIDE\n    const Py_ssize_t expandsize = 10;\n#else\n    const Py_ssize_t expandsize = 6;\n#endif\n\n    /* Initial allocation is based on the longest-possible unichr\n       escape.\n\n       In wide (UTF-32) builds '\\U00xxxxxx' is 10 chars per source\n       unichr, so in this case it's the longest unichr escape. In\n       narrow (UTF-16) builds this is five chars per source unichr\n       since there are two unichrs in the surrogate pair, so in narrow\n       (UTF-16) builds it's not the longest unichr escape.\n\n       In wide or narrow builds '\\uxxxx' is 6 chars per source unichr,\n       so in the narrow (UTF-16) build case it's the longest unichr\n       escape.\n    */\n\n    s = PyUnicode_AS_UNICODE(unicode);\n    size = PyUnicode_GET_SIZE(unicode);\n\n    if (size > (PY_SSIZE_T_MAX-2-1)/expandsize) {\n        PyErr_SetString(PyExc_OverflowError,\n                        \"unicode object is too large to make repr\");\n        return NULL;\n    }\n\n    repr = PyString_FromStringAndSize(NULL, 2 + expandsize*size + 1);\n    if (repr == NULL)\n        return NULL;\n\n    p = PyString_AS_STRING(repr);\n\n    *p++ = '\"';\n\n    while (size-- > 0) {\n        Py_UNICODE ch = *s++;\n\n        /* Escape quotes */\n        if ((ch == (Py_UNICODE) PyString_AS_STRING(repr)[0] || ch == '\\\\')) {\n            *p++ = '\\\\';\n            *p++ = (char) ch;\n            continue;\n        }\n\n#ifdef Py_UNICODE_WIDE\n        /* Map 21-bit characters to '\\U00xxxxxx' */\n        else if (ch >= 0x10000) {\n            *p++ = '\\\\';\n            *p++ = 'U';\n            *p++ = hexdigit[(ch >> 28) & 0x0000000F];\n            *p++ = hexdigit[(ch >> 24) & 0x0000000F];\n            *p++ = hexdigit[(ch >> 20) & 0x0000000F];\n            *p++ = hexdigit[(ch >> 16) & 0x0000000F];\n            *p++ = hexdigit[(ch >> 12) & 0x0000000F];\n            *p++ = hexdigit[(ch >> 8) & 0x0000000F];\n            *p++ = hexdigit[(ch >> 4) & 0x0000000F];\n            *p++ = hexdigit[ch & 0x0000000F];\n            continue;\n        }\n#endif\n        /* Map UTF-16 surrogate pairs to Unicode \\UXXXXXXXX escapes */\n        else if (ch >= 0xD800 && ch < 0xDC00) {\n            Py_UNICODE ch2;\n            Py_UCS4 ucs;\n\n            ch2 = *s++;\n            size--;\n            if (ch2 >= 0xDC00 && ch2 <= 0xDFFF) {\n                ucs = (((ch & 0x03FF) << 10) | (ch2 & 0x03FF)) + 0x00010000;\n                *p++ = '\\\\';\n                *p++ = 'U';\n                *p++ = hexdigit[(ucs >> 28) & 0x0000000F];\n                *p++ = hexdigit[(ucs >> 24) & 0x0000000F];\n                *p++ = hexdigit[(ucs >> 20) & 0x0000000F];\n                *p++ = hexdigit[(ucs >> 16) & 0x0000000F];\n                *p++ = hexdigit[(ucs >> 12) & 0x0000000F];\n                *p++ = hexdigit[(ucs >> 8) & 0x0000000F];\n                *p++ = hexdigit[(ucs >> 4) & 0x0000000F];\n                *p++ = hexdigit[ucs & 0x0000000F];\n                continue;\n            }\n            /* Fall through: isolated surrogates are copied as-is */\n            s--;\n            size++;\n        }\n\n        /* Map 16-bit characters to '\\uxxxx' */\n        if (ch >= 256) {\n            *p++ = '\\\\';\n            *p++ = 'u';\n            *p++ = hexdigit[(ch >> 12) & 0x000F];\n            *p++ = hexdigit[(ch >> 8) & 0x000F];\n            *p++ = hexdigit[(ch >> 4) & 0x000F];\n            *p++ = hexdigit[ch & 0x000F];\n        }\n\n        /* Map special whitespace to '\\t', \\n', '\\r', '\\f', '\\b' */\n        else if (ch == '\\t') {\n            *p++ = '\\\\';\n            *p++ = 't';\n        }\n        else if (ch == '\\n') {\n            *p++ = '\\\\';\n            *p++ = 'n';\n        }\n        else if (ch == '\\r') {\n            *p++ = '\\\\';\n            *p++ = 'r';\n        }\n        else if (ch == '\\f') {\n            *p++ = '\\\\';\n            *p++ = 'f';\n        }\n        else if (ch == '\\b') {\n            *p++ = '\\\\';\n            *p++ = 'b';\n        }\n\n        /* Map non-printable US ASCII to '\\u00hh' */\n        else if (ch < ' ' || ch >= 0x7F) {\n            *p++ = '\\\\';\n            *p++ = 'u';\n            *p++ = '0';\n            *p++ = '0';\n            *p++ = hexdigit[(ch >> 4) & 0x000F];\n            *p++ = hexdigit[ch & 0x000F];\n        }\n\n        /* Copy everything else as-is */\n        else\n            *p++ = (char) ch;\n    }\n\n    *p++ = PyString_AS_STRING(repr)[0];\n\n    *p = '\\0';\n    _PyString_Resize(&repr, p - PyString_AS_STRING(repr));\n    return repr;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    // Get the input Tensors.\n\n    OpInputList params_nested_splits_in;\n    OP_REQUIRES_OK(context, context->input_list(\"params_nested_splits\",\n                                                &params_nested_splits_in));\n    OP_REQUIRES(\n        context, params_nested_splits_in.size() > 0,\n        errors::InvalidArgument(\"params_nested_splits must be non empty\"));\n\n    const Tensor& params_dense_values_in =\n        context->input(params_nested_splits_in.size());\n    const Tensor& indices_in =\n        context->input(params_nested_splits_in.size() + 1);\n\n    OP_REQUIRES(context, params_nested_splits_in[0].dims() > 0,\n                errors::InvalidArgument(\"Split tensors must not be scalars\"));\n    SPLITS_TYPE num_params = params_nested_splits_in[0].dim_size(0) - 1;\n    OP_REQUIRES_OK(context, ValidateIndices(indices_in, num_params));\n\n    OP_REQUIRES(context, params_dense_values_in.dims() > 0,\n                errors::InvalidArgument(\"params.rank must be nonzero\"));\n    SPLITS_TYPE num_params_dense_values = params_dense_values_in.dim_size(0);\n\n    // Calculate the `splits`, and store the value slices that we need to\n    // copy in `value_slices`.\n    std::vector<std::pair<SPLITS_TYPE, SPLITS_TYPE>> value_slices;\n    SPLITS_TYPE num_values = 0;\n    std::vector<std::vector<SPLITS_TYPE>> out_splits;\n    OP_REQUIRES_OK(context, MakeSplits(indices_in, params_nested_splits_in,\n                                       num_params_dense_values, &out_splits,\n                                       &value_slices, &num_values));\n\n    // Write the output tensors.\n    OP_REQUIRES_OK(context, WriteSplits(out_splits, context));\n    OP_REQUIRES_OK(context,\n                   WriteValues(params_dense_values_in, value_slices,\n                               out_splits.size(), num_values, context));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def default(self, path=b\"\", action=None, **kwargs):\n        repo_obj = self.app.store.get_repo(path)\n        if cherrypy.request.method == 'POST':\n            if kwargs.get('keepdays'):\n                return self._remove_older(repo_obj, **kwargs)\n            elif kwargs.get('new_encoding'):\n                return self._set_encoding(repo_obj, **kwargs)\n            elif kwargs.get('maxage'):\n                return self._set_maxage(repo_obj, **kwargs)\n        # Get page data.\n        params = {\n            'repo': repo_obj,\n            'keepdays': repo_obj.keepdays,\n        }\n        # Generate page.\n        return self._compile_template(\"settings.html\", **params)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def put_file(self, in_path, out_path):\n        ''' transfer a file from local to jail '''\n\n        vvv(\"PUT %s TO %s\" % (in_path, out_path), host=self.jail)\n\n        with open(in_path, 'rb') as in_file:\n            p = self._buffered_exec_command('dd of=%s' % out_path, None, stdin=in_file)\n            try:\n                stdout, stderr = p.communicate()\n            except:\n                traceback.print_exc()\n                raise errors.AnsibleError(\"failed to transfer file to %s\" % out_path)\n            if p.returncode != 0:\n                raise errors.AnsibleError(\"failed to transfer file to %s:\\n%s\\n%s\" % (out_path, stdout, stderr))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def validate_value(self, value):\n        if not is_list_like(value):\n            self.invalidate_value(\n                value,\n                msg=\"must be list-like object such as array or tuple\",\n            )\n\n        if len(value) == 0:\n            self.invalidate_value(\n                value,\n                msg='Zero-sized tuple types \"()\" are not supported.',\n            )\n\n        if len(value) != len(self.encoders):\n            self.invalidate_value(\n                value,\n                exc=ValueOutOfBounds,\n                msg=\"value has {} items when {} were expected\".format(\n                    len(value),\n                    len(self.encoders),\n                ),\n            )\n\n        for item, encoder in zip(value, self.encoders):\n            try:\n                encoder.validate_value(item)\n            except AttributeError:\n                encoder(item)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "m3ApiRawFunction(m3_wasi_generic_fd_read)\n{\n    m3ApiReturnType  (uint32_t)\n    m3ApiGetArg      (uvwasi_fd_t          , fd)\n    m3ApiGetArgMem   (wasi_iovec_t *       , wasi_iovs)\n    m3ApiGetArg      (uvwasi_size_t        , iovs_len)\n    m3ApiGetArgMem   (uvwasi_size_t *      , nread)\n\n    m3ApiCheckMem(wasi_iovs,    iovs_len * sizeof(wasi_iovec_t));\n    m3ApiCheckMem(nread,        sizeof(uvwasi_size_t));\n\n#if defined(M3_COMPILER_MSVC)\n    if (iovs_len > 32) m3ApiReturn(UVWASI_EINVAL);\n    uvwasi_ciovec_t  iovs[32];\n#else\n    if (iovs_len > 128) m3ApiReturn(UVWASI_EINVAL);\n    uvwasi_ciovec_t  iovs[iovs_len];\n#endif\n    uvwasi_size_t num_read;\n    uvwasi_errno_t ret;\n\n    for (uvwasi_size_t i = 0; i < iovs_len; ++i) {\n        iovs[i].buf = m3ApiOffsetToPtr(m3ApiReadMem32(&wasi_iovs[i].buf));\n        iovs[i].buf_len = m3ApiReadMem32(&wasi_iovs[i].buf_len);\n        m3ApiCheckMem(iovs[i].buf,     iovs[i].buf_len);\n        //fprintf(stderr, \"> fd_read fd:%d iov%d.len:%d\\n\", fd, i, iovs[i].buf_len);\n    }\n\n    ret = uvwasi_fd_read(&uvwasi, fd, (const uvwasi_iovec_t *) iovs, iovs_len, &num_read);\n\n    WASI_TRACE(\"fd:%d | nread:%d\", fd, num_read);\n\n    m3ApiWriteMem32(nread, num_read);\n    m3ApiReturn(ret);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _list_artifacts_mlflow_artifacts():\n    \"\"\"\n    A request handler for `GET /mlflow-artifacts/artifacts?path=<value>` to list artifacts in `path`\n    (a relative path from the root artifact directory).\n    \"\"\"\n    request_message = _get_request_message(ListArtifactsMlflowArtifacts())\n    path = validate_path_is_safe(request_message.path) if request_message.HasField(\"path\") else None\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    files = []\n    for file_info in artifact_repo.list_artifacts(path):\n        basename = posixpath.basename(file_info.path)\n        new_file_info = FileInfo(basename, file_info.is_dir, file_info.file_size)\n        files.append(new_file_info.to_proto())\n    response_message = ListArtifacts.Response()\n    response_message.files.extend(files)\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def login(cls, username: str, password: str) -> t.Optional[int]:\n        \"\"\"\n        Make sure the user exists and the password is valid. If so, the\n        ``last_login`` value is updated in the database.\n\n        :returns:\n            The id of the user if a match is found, otherwise ``None``.\n\n        \"\"\"\n        if len(username) > cls.username.length:\n            logger.warning(\"Excessively long username provided.\")\n            return None\n\n        if len(password) > cls._max_password_length:\n            logger.warning(\"Excessively long password provided.\")\n            return None\n\n        response = (\n            await cls.select(cls._meta.primary_key, cls.password)\n            .where(cls.username == username)\n            .first()\n            .run()\n        )\n        if not response:\n            # No match found. We still call hash_password\n            # here to mitigate the ability to enumerate\n            # users via response timings\n            cls.hash_password(password)\n            return None\n\n        stored_password = response[\"password\"]\n\n        algorithm, iterations_, salt, hashed = cls.split_stored_password(\n            stored_password\n        )\n        iterations = int(iterations_)\n\n        if cls.hash_password(password, salt, iterations) == stored_password:\n            # If the password was hashed in an earlier Piccolo version, update\n            # it so it's hashed with the currently recommended number of\n            # iterations:\n            if iterations != cls._pbkdf2_iteration_count:\n                await cls.update_password(username, password)\n\n            await cls.update({cls.last_login: datetime.datetime.now()}).where(\n                cls.username == username\n            )\n            return response[\"id\"]\n        else:\n            return None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def cors_after_request(resp):\n        # If CORS headers are set in a view decorator, pass\n        if resp.headers is not None and resp.headers.get(ACL_ORIGIN):\n            LOG.debug('CORS have been already evaluated, skipping')\n            return resp\n        normalized_path = unquote_plus(request.path)\n        for res_regex, res_options in resources:\n            if try_match(normalized_path, res_regex):\n                LOG.debug(\"Request to '%s' matches CORS resource '%s'. Using options: %s\",\n                      request.path, get_regexp_pattern(res_regex), res_options)\n                set_cors_headers(resp, res_options)\n                break\n        else:\n            LOG.debug('No CORS rule matches')\n        return resp\n    return cors_after_request"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def boboAwareZopeTraverse(object, path_items, econtext):\n    \"\"\"Traverses a sequence of names, first trying attributes then items.\n\n    This uses zope.traversing path traversal where possible and interacts\n    correctly with objects providing OFS.interface.ITraversable when\n    necessary (bobo-awareness).\n    \"\"\"\n    request = getattr(econtext, 'request', None)\n    path_items = list(path_items)\n    path_items.reverse()\n\n    while path_items:\n        name = path_items.pop()\n\n        if name == '_':\n            warnings.warn('Traversing to the name `_` is deprecated '\n                          'and will be removed in Zope 6.',\n                          DeprecationWarning)\n        elif name.startswith('_'):\n            raise NotFound(name)\n\n        if OFS.interfaces.ITraversable.providedBy(object):\n            object = object.restrictedTraverse(name)\n        else:\n            object = traversePathElement(object, name, path_items,\n                                         request=request)\n    return object\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self) -> Model:\n        self.validate()\n        try:\n            dashboard = DashboardDAO.create(self._properties, commit=True)\n        except DAOCreateFailedError as ex:\n            logger.exception(ex.exception)\n            raise DashboardCreateFailedError() from ex\n        return dashboard\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _get_most_recent_version(self, ttl_action):\n        # type: (bool) -> CryptographicMaterialsProvider\n        \"\"\"Get the most recent version of the provider.\n\n        If allowing local and we cannot obtain the lock, just return the most recent local\n        version. Otherwise, wait for the lock and ask the provider store for the most recent\n        version of the provider.\n\n        :param TtlActions ttl_action: The ttl action to take for this version\n        :returns: version and corresponding cryptographic materials provider\n        :rtype: CryptographicMaterialsProvider\n        \"\"\"\n        blocking_wait = bool(ttl_action is TtlActions.EXPIRED)\n        acquired = self._lock.acquire(blocking_wait)\n\n        if not acquired:\n            # We failed to acquire the lock.\n            # If blocking, we will never reach this point.\n            # If not blocking, we want whatever the latest local version is.\n            _LOGGER.debug(\"Failed to acquire lock. Returning the last cached version.\")\n            version = self._version\n            _, provider = self._cache.get(version)\n            return provider\n\n        try:\n            # If the entry was expired then we blocked waiting for the lock, so it's possible some other thread already\n            # queried the provider store and re-populated the cache. If so, we don't want to re-query the provider\n            # store, so check if the entry is back in the cache first\n            if ttl_action is TtlActions.EXPIRED:\n                try:\n                    _, provider = self._cache.get(self._version)\n                    return provider\n                except KeyError:\n                    pass\n\n            max_version = self._get_max_version()\n            try:\n                _, provider = self._cache.get(max_version)\n            except KeyError:\n                provider = self._get_provider(max_version)\n            received_version = self._provider_store.version_from_material_description(\n                provider._material_description  # pylint: disable=protected-access\n            )\n\n            _LOGGER.debug(\"Caching materials provider version %d\", received_version)\n            self._version = received_version  # pylint: disable=attribute-defined-outside-init\n            self._last_updated = time.time()  # pylint: disable=attribute-defined-outside-init\n            self._cache.put(received_version, (self._last_updated, provider))\n        finally:\n            self._lock.release()\n\n        _LOGGER.debug(\"New latest version is %d\", self._version)\n\n        return provider\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def authorisation_window_loaded(self):\n        for window in webview.windows[:]:  # iterate over a copy; remove (in destroy()) from original\n            if not hasattr(window, 'get_title'):\n                continue  # skip dummy window\n\n            url = window.get_current_url()\n            username = window.get_title(window).split(' ')[-1]  # see note above: title *must* match this format\n            if not url or not username:\n                continue  # skip any invalid windows\n\n            # respond to both the original request and any duplicates in the list\n            completed_request = None\n            for request in self.authorisation_requests[:]:  # iterate over a copy; remove from original\n                if url.startswith(request['redirect_uri']) and username == request['username']:\n                    Log.info('Successfully authorised request for', request['username'])\n                    RESPONSE_QUEUE.put(\n                        {'permission_url': request['permission_url'], 'response_url': url, 'username': username})\n                    self.authorisation_requests.remove(request)\n                    completed_request = request\n                else:\n                    Log.debug('Waiting for URL matching `redirect_uri`; following browser redirection to',\n                              '%s/[...]' % urllib.parse.urlparse(url).hostname)\n\n            if completed_request is None:\n                continue  # no requests processed for this window - nothing to do yet\n\n            window.destroy()\n            self.icon.update_menu()\n\n            # note that in this part of the interaction we don't actually check the *use* of the authorisation code,\n            # but just whether it was successfully acquired - if there is an error in the subsequent access/refresh\n            # token request then we still send an 'authentication completed' notification here, but in the background\n            # we close the connection with a failure message and re-request authorisation next time the client\n            # interacts, which may potentially lead to repeated and conflicting (and confusing) notifications - improve?\n            if len(self.authorisation_requests) > 0:\n                self.notify(APP_NAME,\n                            'Authentication completed for %s. Please authorise an additional account %s from the '\n                            'menu' % (completed_request['username'], self.authorisation_requests[0]['username']))\n            else:\n                self.notify(APP_NAME, 'Authentication completed for %s' % completed_request['username'])\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _async_render_POST(self, request):\n        try:\n            try:\n                # json.loads doesn't allow bytes in Python 3.5\n                body = json.loads(request.content.read().decode(\"UTF-8\"))\n            except ValueError:\n                request.setResponseCode(400)\n                request.write(dict_to_json_bytes({'errcode': 'M_BAD_JSON', 'error': 'Malformed JSON'}))\n                request.finish()\n                return\n\n            missing = [k for k in (\"threepid\", \"mxid\") if k not in body]\n            if len(missing) > 0:\n                request.setResponseCode(400)\n                msg = \"Missing parameters: \"+(\",\".join(missing))\n                request.write(dict_to_json_bytes({'errcode': 'M_MISSING_PARAMS', 'error': msg}))\n                request.finish()\n                return\n\n            threepid = body['threepid']\n            mxid = body['mxid']\n\n            if 'medium' not in threepid or 'address' not in threepid:\n                request.setResponseCode(400)\n                request.write(dict_to_json_bytes({'errcode': 'M_MISSING_PARAMS', 'error': 'Threepid lacks medium / address'}))\n                request.finish()\n                return\n\n            # We now check for authentication in two different ways, depending\n            # on the contents of the request. If the user has supplied \"sid\"\n            # (the Session ID returned by Sydent during the original binding)\n            # and \"client_secret\" fields, they are trying to prove that they\n            # were the original author of the bind. We then check that what\n            # they supply matches and if it does, allow the unbind.\n            #\n            # However if these fields are not supplied, we instead check\n            # whether the request originated from a homeserver, and if so the\n            # same homeserver that originally created the bind. We do this by\n            # checking the signature of the request. If it all matches up, we\n            # allow the unbind.\n            #\n            # Only one method of authentication is required.\n            if 'sid' in body and 'client_secret' in body:\n                sid = body['sid']\n                client_secret = body['client_secret']\n\n                if not is_valid_client_secret(client_secret):\n                    request.setResponseCode(400)\n                    request.write(dict_to_json_bytes({\n                        'errcode': 'M_INVALID_PARAM',\n                        'error': 'Invalid client_secret provided'\n                    }))\n                    request.finish()\n                    return\n\n                valSessionStore = ThreePidValSessionStore(self.sydent)\n\n                try:\n                    s = valSessionStore.getValidatedSession(sid, client_secret)\n                except (IncorrectClientSecretException, InvalidSessionIdException):\n                    request.setResponseCode(401)\n                    request.write(dict_to_json_bytes({\n                        'errcode': 'M_NO_VALID_SESSION',\n                        'error': \"No valid session was found matching that sid and client secret\"\n                    }))\n                    request.finish()\n                    return\n                except SessionNotValidatedException:\n                    request.setResponseCode(403)\n                    request.write(dict_to_json_bytes({\n                        'errcode': 'M_SESSION_NOT_VALIDATED',\n                        'error': \"This validation session has not yet been completed\"\n                    }))\n                    return\n\n                if s.medium != threepid['medium'] or s.address != threepid['address']:\n                    request.setResponseCode(403)\n                    request.write(dict_to_json_bytes({\n                        'errcode': 'M_FORBIDDEN',\n                        'error': 'Provided session information does not match medium/address combo',\n                    }))\n                    request.finish()\n                    return\n            else:\n                try:\n                    origin_server_name = yield self.sydent.sig_verifier.authenticate_request(request, body)\n                except SignatureVerifyException as ex:\n                    request.setResponseCode(401)\n                    request.write(dict_to_json_bytes({'errcode': 'M_FORBIDDEN', 'error': str(ex)}))\n                    request.finish()\n                    return\n                except NoAuthenticationError as ex:\n                    request.setResponseCode(401)\n                    request.write(dict_to_json_bytes({'errcode': 'M_FORBIDDEN', 'error': str(ex)}))\n                    request.finish()\n                    return\n                except InvalidServerName as ex:\n                    request.setResponseCode(400)\n                    request.write(dict_to_json_bytes({'errcode': 'M_INVALID_PARAM', 'error': str(ex)}))\n                    request.finish()\n                    return\n                except Exception:\n                    logger.exception(\"Exception whilst authenticating unbind request\")\n                    request.setResponseCode(500)\n                    request.write(dict_to_json_bytes({'errcode': 'M_UNKNOWN', 'error': 'Internal Server Error'}))\n                    request.finish()\n                    return\n\n                if not mxid.endswith(':' + origin_server_name):\n                    request.setResponseCode(403)\n                    request.write(dict_to_json_bytes({'errcode': 'M_FORBIDDEN', 'error': 'Origin server name does not match mxid'}))\n                    request.finish()\n                    return\n\n            self.sydent.threepidBinder.removeBinding(threepid, mxid)\n\n            request.write(dict_to_json_bytes({}))\n            request.finish()\n        except Exception as ex:\n            logger.exception(\"Exception whilst handling unbind\")\n            request.setResponseCode(500)\n            request.write(dict_to_json_bytes({'errcode': 'M_UNKNOWN', 'error': str(ex)}))\n            request.finish()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_python_instance_state(self, instance, state, unsafe=False):\n        if hasattr(instance, '__setstate__'):\n            instance.__setstate__(state)\n        else:\n            slotstate = {}\n            if isinstance(state, tuple) and len(state) == 2:\n                state, slotstate = state\n            if hasattr(instance, '__dict__'):\n                if not unsafe and state:\n                    for key in state.keys():\n                        self.check_state_key(key)\n                instance.__dict__.update(state)\n            elif state:\n                slotstate.update(state)\n            for key, value in slotstate.items():\n                if not unsafe:\n                    self.check_state_key(key)\n                setattr(instance, key, value)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def handler_for(calldata_kwargs, default_kwargs):\n        calldata_args = sig.base_args + calldata_kwargs\n        # create a fake type so that get_element_ptr works\n        calldata_args_t = TupleT(list(arg.typ for arg in calldata_args))\n\n        abi_sig = sig.abi_signature_for_kwargs(calldata_kwargs)\n        method_id = _annotated_method_id(abi_sig)\n\n        calldata_kwargs_ofst = IRnode(\n            4, location=CALLDATA, typ=calldata_args_t, encoding=Encoding.ABI\n        )\n\n        # a sequence of statements to strictify kwargs into memory\n        ret = [\"seq\"]\n\n        # ensure calldata is at least of minimum length\n        args_abi_t = calldata_args_t.abi_type\n        calldata_min_size = args_abi_t.min_size() + 4\n        ret.append([\"assert\", [\"ge\", \"calldatasize\", calldata_min_size]])\n\n        # TODO optimize make_setter by using\n        # TupleT(list(arg.typ for arg in calldata_kwargs + default_kwargs))\n        # (must ensure memory area is contiguous)\n\n        n_base_args = len(sig.base_args)\n\n        for i, arg_meta in enumerate(calldata_kwargs):\n            k = n_base_args + i\n\n            dst = context.lookup_var(arg_meta.name).pos\n\n            lhs = IRnode(dst, location=MEMORY, typ=arg_meta.typ)\n\n            rhs = get_element_ptr(calldata_kwargs_ofst, k, array_bounds_check=False)\n\n            copy_arg = make_setter(lhs, rhs)\n            copy_arg.source_pos = getpos(arg_meta.ast_source)\n            ret.append(copy_arg)\n\n        for x in default_kwargs:\n            dst = context.lookup_var(x.name).pos\n            lhs = IRnode(dst, location=MEMORY, typ=x.typ)\n            lhs.source_pos = getpos(x.ast_source)\n            kw_ast_val = sig.default_values[x.name]  # e.g. `3` in x: int = 3\n            rhs = Expr(kw_ast_val, context).ir_node\n\n            copy_arg = make_setter(lhs, rhs)\n            copy_arg.source_pos = getpos(x.ast_source)\n            ret.append(copy_arg)\n\n        ret.append([\"goto\", sig.external_function_base_entry_label])\n\n        method_id_check = [\"eq\", \"_calldata_method_id\", method_id]\n\n        # if there is a function whose selector is 0, it won't be distinguished\n        # from the case where nil calldata is supplied, b/c calldataload loads\n        # 0s past the end of physical calldata (cf. yellow paper).\n        # since supplying 0 calldata is expected to trigger the fallback fn,\n        # we check that calldatasize > 0, which distinguishes the 0 selector\n        # from the fallback function \"selector\"\n        # (equiv. to \"all selectors not in the selector table\").\n\n        # note: cases where not enough calldata is supplied (besides\n        # calldatasize==0) are not addressed here b/c a calldatasize\n        # well-formedness check is already present in the function body\n        # as part of abi validation\n        if method_id.value == 0:\n            method_id_check = [\"and\", [\"gt\", \"calldatasize\", 0], method_id_check]\n\n        ret = [\"if\", method_id_check, ret]\n        return ret\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_oauth2_credentials(username, password, recurse_retries=True):\n        \"\"\"Using the given username (i.e., email address) and password, reads account details from AppConfig and\n        handles OAuth 2.0 token request and renewal, saving the updated details back to AppConfig (or removing them\n        if invalid). Returns either (True, '[OAuth2 string for authentication]') or (False, '[Error message]')\"\"\"\n        if username not in AppConfig.accounts():\n            Log.error('Proxy config file entry missing for account', username, '- aborting login')\n            return (False, '%s: No config file entry found for account %s - please add a new section with values '\n                           'for permission_url, token_url, oauth2_scope, redirect_uri, client_id and '\n                           'client_secret' % (APP_NAME, username))\n\n        config = AppConfig.get()\n        current_time = int(time.time())\n\n        permission_url = config.get(username, 'permission_url', fallback=None)\n        token_url = config.get(username, 'token_url', fallback=None)\n        oauth2_scope = config.get(username, 'oauth2_scope', fallback=None)\n        redirect_uri = config.get(username, 'redirect_uri', fallback=None)\n        redirect_listen_address = config.get(username, 'redirect_listen_address', fallback=None)\n        client_id = config.get(username, 'client_id', fallback=None)\n        client_secret = config.get(username, 'client_secret', fallback=None)\n\n        # note that we don't require client_secret here because it can be optional for Office 365 configurations\n        if not (permission_url and token_url and oauth2_scope and redirect_uri and client_id):\n            Log.error('Proxy config file entry incomplete for account', username, '- aborting login')\n            return (False, '%s: Incomplete config file entry found for account %s - please make sure all required '\n                           'fields are added (permission_url, token_url, oauth2_scope, redirect_uri, client_id '\n                           'and client_secret)' % (APP_NAME, username))\n\n        # while not technically forbidden (RFC 6749, A.1 and A.2), it is highly unlikely the example value is valid\n        example_client_value = '*** your client'\n        example_client_status = [example_client_value in i for i in [client_id, client_secret] if i]\n        if any(example_client_status):\n            if all(example_client_status) or example_client_value in client_id:\n                Log.info('Warning: client configuration for account', username, 'seems to contain example values -',\n                         'if authentication fails, please double-check these values are correct')\n            elif example_client_value in client_secret:\n                Log.info('Warning: client secret for account', username, 'seems to contain the example value - if you',\n                         'are using an Office 365 setup that does not need a secret, please delete this line entirely;',\n                         'otherwise, if authentication fails, please double-check this value is correct')\n\n        token_salt = config.get(username, 'token_salt', fallback=None)\n        access_token = config.get(username, 'access_token', fallback=None)\n        access_token_expiry = config.getint(username, 'access_token_expiry', fallback=current_time)\n        refresh_token = config.get(username, 'refresh_token', fallback=None)\n\n        # we hash locally-stored tokens with the given password\n        if not token_salt:\n            token_salt = base64.b64encode(os.urandom(16)).decode('utf-8')\n\n        # generate encrypter/decrypter based on password and random salt\n        key_derivation_function = PBKDF2HMAC(algorithm=hashes.SHA256(), length=32,\n                                             salt=base64.b64decode(token_salt.encode('utf-8')), iterations=100000,\n                                             backend=default_backend())\n        key = base64.urlsafe_b64encode(key_derivation_function.derive(password.encode('utf-8')))\n        cryptographer = Fernet(key)\n\n        try:\n            if access_token:\n                if access_token_expiry - current_time < TOKEN_EXPIRY_MARGIN:\n                    if refresh_token:\n                        # if expiring soon, refresh token (if possible)\n                        response = OAuth2Helper.refresh_oauth2_access_token(token_url, client_id, client_secret,\n                                                                            OAuth2Helper.decrypt(cryptographer,\n                                                                                                 refresh_token))\n\n                        access_token = response['access_token']\n                        config.set(username, 'access_token', OAuth2Helper.encrypt(cryptographer, access_token))\n                        config.set(username, 'access_token_expiry', str(current_time + response['expires_in']))\n                        if 'refresh_token' in response:\n                            config.set(username, 'refresh_token',\n                                       OAuth2Helper.encrypt(cryptographer, response['refresh_token']))\n                        AppConfig.save()\n\n                    elif access_token_expiry <= current_time:\n                        # cannot get another access token without a refresh token - must submit another manual request\n                        access_token = None\n                else:\n                    access_token = OAuth2Helper.decrypt(cryptographer, access_token)\n\n            if not access_token:\n                permission_url = OAuth2Helper.construct_oauth2_permission_url(permission_url, redirect_uri, client_id,\n                                                                              oauth2_scope, username)\n                # note: get_oauth2_authorisation_code is a blocking call\n                success, authorisation_code = OAuth2Helper.get_oauth2_authorisation_code(permission_url, redirect_uri,\n                                                                                         redirect_listen_address,\n                                                                                         username)\n                if not success:\n                    Log.info('Authentication request failed or expired for account', username, '- aborting login')\n                    return False, '%s: Login failed - the authentication request expired or was cancelled for ' \\\n                                  'account %s' % (APP_NAME, username)\n\n                response = OAuth2Helper.get_oauth2_authorisation_tokens(token_url, redirect_uri, client_id,\n                                                                        client_secret, authorisation_code)\n\n                access_token = response['access_token']\n                config.set(username, 'token_salt', token_salt)\n                config.set(username, 'access_token', OAuth2Helper.encrypt(cryptographer, access_token))\n                config.set(username, 'access_token_expiry', str(current_time + response['expires_in']))\n                if 'refresh_token' in response:\n                    config.set(username, 'refresh_token',\n                               OAuth2Helper.encrypt(cryptographer, response['refresh_token']))\n                else:\n                    Log.info('Warning: no refresh token returned for', username, '- you will need to re-authenticate',\n                             'each time the access token expires (does your `oauth2_scope` value allow `offline` use?)')\n                AppConfig.save()\n\n            # send authentication command to server (response checked in ServerConnection) - note: we only support\n            # single-trip authentication (SASL) without actually checking the server's capabilities - improve?\n            oauth2_string = OAuth2Helper.construct_oauth2_string(username, access_token)\n            return True, oauth2_string\n\n        except InvalidToken as e:\n            # if invalid details are the reason for failure we remove our cached version and re-authenticate - this can\n            # be disabled by a configuration setting, but note that we always remove credentials on 400 Bad Request\n            if e.args == (400, APP_PACKAGE) or AppConfig.globals().getboolean('delete_account_token_on_password_error',\n                                                                              fallback=True):\n                config.remove_option(username, 'token_salt')\n                config.remove_option(username, 'access_token')\n                config.remove_option(username, 'access_token_expiry')\n                config.remove_option(username, 'refresh_token')\n                AppConfig.save()\n            else:\n                recurse_retries = False  # no need to recurse if we are just trying the same credentials again\n\n            if recurse_retries:\n                Log.info('Retrying login due to exception while requesting OAuth 2.0 credentials:', Log.error_string(e))\n                return OAuth2Helper.get_oauth2_credentials(username, password, recurse_retries=False)\n            else:\n                Log.error('Invalid password to decrypt', username, 'credentials - aborting login:', Log.error_string(e))\n                return False, '%s: Login failed - the password for account %s is incorrect' % (APP_NAME, username)\n\n        except Exception as e:\n            # note that we don't currently remove cached credentials here, as failures on the initial request are before\n            # caching happens, and the assumption is that refresh token request exceptions are temporal (e.g., network\n            # errors: URLError(OSError(50, 'Network is down'))) - access token 400 Bad Request HTTPErrors with messages\n            # such as 'authorisation code was already redeemed' are caused by our support for simultaneous requests,\n            # and will work from the next request; however, please report an issue if you encounter problems here\n            Log.info('Caught exception while requesting OAuth 2.0 credentials:', Log.error_string(e))\n            return False, '%s: Login failed for account %s - please check your internet connection and retry' % (\n                APP_NAME, username)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _configure_session(cls, app):\n        tempdir = app.config[\"PYLOAD_API\"].get_cachedir()\n        cache_path = os.path.join(tempdir, \"flask\")\n        os.makedirs(cache_path, exist_ok=True)\n\n        app.config[\"SESSION_FILE_DIR\"] = cache_path\n        app.config[\"SESSION_TYPE\"] = \"filesystem\"\n        app.config[\"SESSION_COOKIE_NAME\"] = \"pyload_session\"\n        app.config[\"SESSION_COOKIE_SAMESITE\"] = \"Strict\"\n        app.config[\"SESSION_COOKIE_SECURE\"] = app.config[\"PYLOAD_API\"].get_config_value(\"webui\", \"use_ssl\")\n        app.config[\"SESSION_PERMANENT\"] = False\n\n        session_lifetime = max(app.config[\"PYLOAD_API\"].get_config_value(\"webui\", \"session_lifetime\"), 1) * 60\n        app.config[\"PERMANENT_SESSION_LIFETIME\"] = session_lifetime\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __post_init__(self) -> None:\n        self.python_name = utils.snake_case(self.name)\n        if self.default is not None:\n            self.default = self._validate_default(default=self.default)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def build_IR(self, expr, args, kwargs, context):\n        unwrap_tuple = kwargs[\"unwrap_tuple\"]\n\n        data = args[0]\n        output_typ = args[1]\n        wrapped_typ = output_typ\n\n        if unwrap_tuple is True:\n            wrapped_typ = calculate_type_for_external_return(output_typ)\n\n        abi_size_bound = wrapped_typ.abi_type.size_bound()\n        abi_min_size = wrapped_typ.abi_type.min_size()\n\n        # Get the size of data\n        input_max_len = data.typ.maxlen\n\n        assert abi_min_size <= abi_size_bound, \"bad abi type\"\n        if input_max_len < abi_size_bound:\n            raise StructureException(\n                (\n                    \"Mismatch between size of input and size of decoded types. \"\n                    f\"length of ABI-encoded {wrapped_typ} must be equal to or greater \"\n                    f\"than {abi_size_bound}\"\n                ),\n                expr.args[0],\n            )\n\n        data = ensure_in_memory(data, context)\n\n        with data.cache_when_complex(\"to_decode\") as (b1, data):\n            data_ptr = bytes_data_ptr(data)\n            data_len = get_bytearray_length(data)\n\n            ret = [\"seq\"]\n\n            if abi_min_size == abi_size_bound:\n                ret.append([\"assert\", [\"eq\", abi_min_size, data_len]])\n            else:\n                # runtime assert: abi_min_size <= data_len <= abi_size_bound\n                ret.append(clamp2(abi_min_size, data_len, abi_size_bound, signed=False))\n\n            to_decode = IRnode.from_list(\n                data_ptr,\n                typ=wrapped_typ,\n                location=data.location,\n                encoding=Encoding.ABI,\n                annotation=f\"abi_decode({output_typ})\",\n            )\n            to_decode.encoding = Encoding.ABI\n\n            # TODO optimization: skip make_setter when we don't need\n            # input validation\n\n            output_buf = context.new_internal_variable(wrapped_typ)\n            output = IRnode.from_list(output_buf, typ=wrapped_typ, location=MEMORY)\n\n            # sanity check buffer size for wrapped output type will not buffer overflow\n            assert wrapped_typ.memory_bytes_required == output_typ.memory_bytes_required\n            ret.append(make_setter(output, to_decode))\n\n            ret.append(output)\n            # finalize. set the type and location for the return buffer.\n            # (note: unwraps the tuple type if necessary)\n            ret = IRnode.from_list(ret, typ=output_typ, location=MEMORY)\n            return b1.resolve(ret)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get(self, path: str) -> None:\n        parts = path.split(\"/\")\n        component_name = parts[0]\n        component_root = self._registry.get_component_path(component_name)\n        if component_root is None:\n            self.write(\"not found\")\n            self.set_status(404)\n            return\n\n        # follow symlinks to get an accurate normalized path\n        component_root = os.path.realpath(component_root)\n        filename = \"/\".join(parts[1:])\n        abspath = os.path.realpath(os.path.join(component_root, filename))\n\n        # Do NOT expose anything outside of the component root.\n        if os.path.commonpath([component_root, abspath]) != component_root:\n            self.write(\"forbidden\")\n            self.set_status(403)\n            return\n        try:\n            with open(abspath, \"rb\") as file:\n                contents = file.read()\n        except OSError as e:\n            _LOGGER.error(\n                \"ComponentRequestHandler: GET %s read error\", abspath, exc_info=e\n            )\n            self.write(\"read error\")\n            self.set_status(404)\n            return\n\n        self.write(contents)\n        self.set_header(\"Content-Type\", self.get_content_type(abspath))\n\n        self.set_extra_headers(path)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def generate_nonce(length=8):\n    \"\"\"Generate pseudorandom number.\"\"\"\n    return ''.join([str(random.SystemRandom().randint(0, 9)) for i in range(length)])\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, data):\n        if self.USE_TIMESTAMP and self.TIMESTAMP_FIELD not in data:\n            data = data.copy()\n            data[self.TIMESTAMP_FIELD] = get_current_timestamp()\n        self._data = data\n        self._salt = self._calculate_salt(data)\n        self._signer = Signer(salt=self._salt)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def handle(request, message=None, redirect=None, ignore=False, escalate=False):\n    \"\"\" Centralized error handling for Horizon.\n\n    Because Horizon consumes so many different APIs with completely\n    different ``Exception`` types, it's necessary to have a centralized\n    place for handling exceptions which may be raised.\n\n    Exceptions are roughly divided into 3 types:\n\n    #. ``UNAUTHORIZED``: Errors resulting from authentication or authorization\n       problems. These result in being logged out and sent to the login screen.\n    #. ``NOT_FOUND``: Errors resulting from objects which could not be\n       located via the API. These generally result in a user-facing error\n       message, but are otherwise returned to the normal code flow. Optionally\n       a redirect value may be passed to the error handler so users are\n       returned to a different view than the one requested in addition to the\n       error message.\n    #. RECOVERABLE: Generic API errors which generate a user-facing message\n       but drop directly back to the regular code flow.\n\n    All other exceptions bubble the stack as normal unless the ``ignore``\n    argument is passed in as ``True``, in which case only unrecognized\n    errors are bubbled.\n\n    If the exception is not re-raised, an appropriate wrapper exception\n    class indicating the type of exception that was encountered will be\n    returned.\n    \"\"\"\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n\n    # Because the same exception may travel through this method more than\n    # once (if it's re-raised) we may want to treat it differently\n    # the second time (e.g. no user messages/logging).\n    handled = issubclass(exc_type, HandledException)\n    wrap = False\n\n    # Restore our original exception information, but re-wrap it at the end\n    if handled:\n        exc_type, exc_value, exc_traceback = exc_value.wrapped\n        wrap = True\n\n    # We trust messages from our own exceptions\n    if issubclass(exc_type, HorizonException):\n        message = exc_value\n    # If the message has a placeholder for the exception, fill it in\n    elif message and \"%(exc)s\" in message:\n        message = message % {\"exc\": exc_value}\n\n    if issubclass(exc_type, UNAUTHORIZED):\n        if ignore:\n            return NotAuthorized\n        request.user_logout()\n        if not handled:\n            LOG.debug(\"Unauthorized: %s\" % exc_value)\n            # We get some pretty useless error messages back from\n            # some clients, so let's define our own fallback.\n            fallback = _(\"Unauthorized. Please try logging in again.\")\n            messages.error(request, message or fallback, extra_tags=\"login\")\n        raise NotAuthorized  # Redirect handled in middleware\n\n    if issubclass(exc_type, NOT_FOUND):\n        wrap = True\n        if not ignore and not handled:\n            LOG.debug(\"Not Found: %s\" % exc_value)\n            messages.error(request, message or exc_value)\n        if redirect:\n            raise Http302(redirect)\n        if not escalate:\n            return NotFound  # return to normal code flow\n\n    if issubclass(exc_type, RECOVERABLE):\n        wrap = True\n        if not ignore and not handled:\n            LOG.debug(\"Recoverable error: %s\" % exc_value)\n            messages.error(request, message or exc_value)\n        if redirect:\n            raise Http302(redirect)\n        if not escalate:\n            return RecoverableError  # return to normal code flow\n\n    # If we've gotten here, time to wrap and/or raise our exception.\n    if wrap:\n        raise HandledException([exc_type, exc_value, exc_traceback])\n    raise exc_type, exc_value, exc_traceback\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _handle_incoming_transaction(self, origin, transaction, request_time):\n        \"\"\" Process an incoming transaction and return the HTTP response\n\n        Args:\n            origin (unicode): the server making the request\n            transaction (Transaction): incoming transaction\n            request_time (int): timestamp that the HTTP request arrived at\n\n        Returns:\n            Deferred[(int, object)]: http response code and body\n        \"\"\"\n        response = yield self.transaction_actions.have_responded(origin, transaction)\n\n        if response:\n            logger.debug(\n                \"[%s] We've already responded to this request\",\n                transaction.transaction_id\n            )\n            defer.returnValue(response)\n            return\n\n        logger.debug(\"[%s] Transaction is new\", transaction.transaction_id)\n\n        received_pdus_counter.inc(len(transaction.pdus))\n\n        origin_host, _ = parse_server_name(origin)\n\n        pdus_by_room = {}\n\n        for p in transaction.pdus:\n            if \"unsigned\" in p:\n                unsigned = p[\"unsigned\"]\n                if \"age\" in unsigned:\n                    p[\"age\"] = unsigned[\"age\"]\n            if \"age\" in p:\n                p[\"age_ts\"] = request_time - int(p[\"age\"])\n                del p[\"age\"]\n\n            event = event_from_pdu_json(p)\n            room_id = event.room_id\n            pdus_by_room.setdefault(room_id, []).append(event)\n\n        pdu_results = {}\n\n        # we can process different rooms in parallel (which is useful if they\n        # require callouts to other servers to fetch missing events), but\n        # impose a limit to avoid going too crazy with ram/cpu.\n\n        @defer.inlineCallbacks\n        def process_pdus_for_room(room_id):\n            logger.debug(\"Processing PDUs for %s\", room_id)\n            try:\n                yield self.check_server_matches_acl(origin_host, room_id)\n            except AuthError as e:\n                logger.warn(\n                    \"Ignoring PDUs for room %s from banned server\", room_id,\n                )\n                for pdu in pdus_by_room[room_id]:\n                    event_id = pdu.event_id\n                    pdu_results[event_id] = e.error_dict()\n                return\n\n            for pdu in pdus_by_room[room_id]:\n                event_id = pdu.event_id\n                try:\n                    yield self._handle_received_pdu(\n                        origin, pdu\n                    )\n                    pdu_results[event_id] = {}\n                except FederationError as e:\n                    logger.warn(\"Error handling PDU %s: %s\", event_id, e)\n                    pdu_results[event_id] = {\"error\": str(e)}\n                except Exception as e:\n                    f = failure.Failure()\n                    pdu_results[event_id] = {\"error\": str(e)}\n                    logger.error(\n                        \"Failed to handle PDU %s: %s\",\n                        event_id, f.getTraceback().rstrip(),\n                    )\n\n        yield async.concurrently_execute(\n            process_pdus_for_room, pdus_by_room.keys(),\n            TRANSACTION_CONCURRENCY_LIMIT,\n        )\n\n        if hasattr(transaction, \"edus\"):\n            for edu in (Edu(**x) for x in transaction.edus):\n                yield self.received_edu(\n                    origin,\n                    edu.edu_type,\n                    edu.content\n                )\n\n        response = {\n            \"pdus\": pdu_results,\n        }\n\n        logger.debug(\"Returning: %s\", str(response))\n\n        yield self.transaction_actions.set_response(\n            origin,\n            transaction,\n            200, response\n        )\n        defer.returnValue((200, response))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _not_here_yet(request, *args, **kwargs):\n    return HttpResponse(\"Not here yet: %s (%r, %r)\" % (request.path, escape(args), escape(kwargs)), status=410)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_comparison_class_for_block(block):\n    if hasattr(block, 'get_comparison_class'):\n        return block.get_comparison_class()\n    elif isinstance(block, (blocks.CharBlock, blocks.TextBlock)):\n        return CharBlockComparison\n    elif isinstance(block, blocks.RawHTMLBlock):\n        # Compare raw HTML blocks as if they were plain text, so that tags are shown explicitly\n        return CharBlockComparison\n    elif isinstance(block, blocks.RichTextBlock):\n        return RichTextBlockComparison\n    elif isinstance(block, blocks.StructBlock):\n        return StructBlockComparison\n    else:\n        # As all stream field blocks have a HTML representation, fall back to diffing that.\n        return RichTextBlockComparison\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_task_instances_batch(session: Session = NEW_SESSION) -> APIResponse:\n    \"\"\"Get list of task instances.\"\"\"\n    body = get_json_request_dict()\n    try:\n        data = task_instance_batch_form.load(body)\n    except ValidationError as err:\n        raise BadRequest(detail=str(err.messages))\n    dag_ids = data[\"dag_ids\"]\n    if dag_ids:\n        cannot_access_dag_ids = set()\n        for id in dag_ids:\n            if not get_airflow_app().appbuilder.sm.can_read_dag(id, g.user):\n                cannot_access_dag_ids.add(id)\n        if cannot_access_dag_ids:\n            raise PermissionDenied(\n                detail=f\"User not allowed to access these DAGs: {list(cannot_access_dag_ids)}\"\n            )\n    else:\n        dag_ids = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)\n\n    states = _convert_ti_states(data[\"state\"])\n    base_query = select(TI).join(TI.dag_run)\n\n    base_query = _apply_array_filter(base_query, key=TI.dag_id, values=dag_ids)\n    base_query = _apply_array_filter(base_query, key=TI.run_id, values=data[\"dag_run_ids\"])\n    base_query = _apply_array_filter(base_query, key=TI.task_id, values=data[\"task_ids\"])\n    base_query = _apply_range_filter(\n        base_query,\n        key=DR.execution_date,\n        value_range=(data[\"execution_date_gte\"], data[\"execution_date_lte\"]),\n    )\n    base_query = _apply_range_filter(\n        base_query,\n        key=TI.start_date,\n        value_range=(data[\"start_date_gte\"], data[\"start_date_lte\"]),\n    )\n    base_query = _apply_range_filter(\n        base_query, key=TI.end_date, value_range=(data[\"end_date_gte\"], data[\"end_date_lte\"])\n    )\n    base_query = _apply_range_filter(\n        base_query, key=TI.duration, value_range=(data[\"duration_gte\"], data[\"duration_lte\"])\n    )\n    base_query = _apply_array_filter(base_query, key=TI.state, values=states)\n    base_query = _apply_array_filter(base_query, key=TI.pool, values=data[\"pool\"])\n    base_query = _apply_array_filter(base_query, key=TI.queue, values=data[\"queue\"])\n\n    # Count elements before joining extra columns\n    total_entries = get_query_count(base_query, session=session)\n    # Add join\n    base_query = base_query.join(\n        SlaMiss,\n        and_(\n            SlaMiss.dag_id == TI.dag_id,\n            SlaMiss.task_id == TI.task_id,\n            SlaMiss.execution_date == DR.execution_date,\n        ),\n        isouter=True,\n    ).add_columns(SlaMiss)\n    ti_query = base_query.options(joinedload(TI.rendered_task_instance_fields))\n    # using execute because we want the SlaMiss entity. Scalars don't return None for missing entities\n    task_instances = session.execute(ti_query).all()\n\n    return task_instance_collection_schema.dump(\n        TaskInstanceCollection(task_instances=task_instances, total_entries=total_entries)\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& diagonal = context->input(0);\n\n    // MatrixDiag and MatrixDiagV2 both use this OpKernel. MatrixDiag only has\n    // one input, so we have to check the number of inputs before reading\n    // additional parameters in MatrixDiagV2.\n    int32_t lower_diag_index = 0;\n    int32_t upper_diag_index = 0;\n    int32_t num_rows = -1;\n    int32_t num_cols = -1;\n    T padding_value(0);\n\n    // MatrixDiagOpV2-specific.\n    if (context->num_inputs() > kNumV1Inputs) {\n      auto& diag_index = context->input(1);\n      OP_REQUIRES(context,\n                  TensorShapeUtils::IsScalar(diag_index.shape()) ||\n                      TensorShapeUtils::IsVector(diag_index.shape()),\n                  errors::InvalidArgument(\n                      \"diag_index must be a scalar or vector, received shape: \",\n                      diag_index.shape().DebugString()));\n      OP_REQUIRES(context, diag_index.NumElements() > 0,\n                  errors::InvalidArgument(\n                      \"Expected diag_index to have at least 1 element\"));\n      lower_diag_index = diag_index.flat<int32>()(0);\n      upper_diag_index = lower_diag_index;\n      if (TensorShapeUtils::IsVector(diag_index.shape())) {\n        auto diag_index_size = diag_index.dim_size(0);\n        OP_REQUIRES(\n            context, 0 < diag_index_size && diag_index_size <= 2,\n            errors::InvalidArgument(\n                \"diag_index must have only one or two elements, received \",\n                diag_index_size, \" elements.\"));\n        if (diag_index_size > 1) {\n          upper_diag_index = diag_index.flat<int32>()(1);\n        }\n      }\n\n      auto& num_rows_tensor = context->input(2);\n      OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_rows_tensor.shape()),\n                  errors::InvalidArgument(\"num_rows must be a scalar\"));\n      num_rows = num_rows_tensor.flat<int32>()(0);\n\n      auto& num_cols_tensor = context->input(3);\n      OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_cols_tensor.shape()),\n                  errors::InvalidArgument(\"num_cols must be a scalar\"));\n      num_cols = num_cols_tensor.flat<int32>()(0);\n\n      auto& padding_value_tensor = context->input(4);\n      OP_REQUIRES(context,\n                  TensorShapeUtils::IsScalar(padding_value_tensor.shape()),\n                  errors::InvalidArgument(\"padding_value must be a scalar\"));\n      padding_value = padding_value_tensor.flat<T>()(0);\n    }\n\n    // Size validations.\n    const TensorShape& diagonal_shape = diagonal.shape();\n    const int diag_rank = diagonal_shape.dims();\n    const Eigen::Index num_diags = upper_diag_index - lower_diag_index + 1;\n    OP_REQUIRES(context, TensorShapeUtils::IsVectorOrHigher(diagonal_shape),\n                errors::InvalidArgument(\n                    \"diagonal must be at least 1-dim, received shape: \",\n                    diagonal.shape().DebugString()));\n    OP_REQUIRES(\n        context, lower_diag_index <= upper_diag_index,\n        errors::InvalidArgument(\n            \"lower_diag_index must not be larger than upper_diag_index: \",\n            lower_diag_index, \" > \", upper_diag_index));\n    OP_REQUIRES(context,\n                lower_diag_index == upper_diag_index ||\n                    diagonal_shape.dim_size(diag_rank - 2) == num_diags,\n                errors::InvalidArgument(\n                    \"The number of diagonals provided in the input does not \"\n                    \"match the lower_diag_index and upper_diag_index range.\"));\n\n    const Eigen::Index max_diag_len = diagonal_shape.dim_size(diag_rank - 1);\n    const int32_t min_num_rows = max_diag_len - std::min(upper_diag_index, 0);\n    const int32_t min_num_cols = max_diag_len + std::max(lower_diag_index, 0);\n    OP_REQUIRES(context, num_rows == -1 || num_rows >= min_num_rows,\n                errors::InvalidArgument(\"The number of rows is too small.\"));\n    OP_REQUIRES(context, num_cols == -1 || num_cols >= min_num_cols,\n                errors::InvalidArgument(\"The number of columns is too small.\"));\n\n    // If both num_rows and num_cols are unknown, assume that output is square.\n    // Otherwise, use smallest possible values.\n    if (num_rows == -1 && num_cols == -1) {\n      num_rows = std::max(min_num_rows, min_num_cols);\n      num_cols = num_rows;\n    } else if (num_rows == -1) {\n      num_rows = min_num_rows;\n    } else if (num_cols == -1) {\n      num_cols = min_num_cols;\n    }\n    OP_REQUIRES(context, num_rows == min_num_rows || num_cols == min_num_cols,\n                errors::InvalidArgument(\n                    \"The number of rows or columns is not consistent with \"\n                    \"the specified d_lower, d_upper, and diagonal.\"));\n\n    TensorShape output_shape = diagonal_shape;\n    if (num_diags == 1) {  // Output has rank `rank+1`.\n      output_shape.set_dim(diag_rank - 1, num_rows);\n      output_shape.AddDim(num_cols);\n    } else {  // Output has rank `rank`.\n      output_shape.set_dim(diag_rank - 2, num_rows);\n      output_shape.set_dim(diag_rank - 1, num_cols);\n    }\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &output));\n    auto output_reshaped = output->flat_inner_dims<T, 3>();\n    auto diag_reshaped = diagonal.flat<T>();\n    functor::MatrixDiag<Device, T>::Compute(\n        context, context->eigen_device<Device>(), diag_reshaped,\n        output_reshaped, lower_diag_index, upper_diag_index, max_diag_len,\n        padding_value, left_align_superdiagonal_, left_align_subdiagonal_);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_path(self, scope):\n        return urllib.parse.quote(scope.get(\"root_path\", \"\") + scope[\"path\"])\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_email(value: str) -> tuple[str, str]:\n    \"\"\"Email address validation using [email-validator](https://pypi.org/project/email-validator/).\n\n    Note:\n        Note that:\n\n        * Raw IP address (literal) domain parts are not allowed.\n        * `\"John Doe <local_part@domain.com>\"` style \"pretty\" email addresses are processed.\n        * Spaces are striped from the beginning and end of addresses, but no error is raised.\n    \"\"\"\n    if email_validator is None:\n        import_email_validator()\n\n    if len(value) > MAX_EMAIL_LENGTH:\n        raise PydanticCustomError(\n            'value_error',\n            'value is not a valid email address: {reason}',\n            {'reason': f'Length must not exceed {MAX_EMAIL_LENGTH} characters'},\n        )\n\n    m = pretty_email_regex.fullmatch(value)\n    name: str | None = None\n    if m:\n        unquoted_name, quoted_name, value = m.groups()\n        name = unquoted_name or quoted_name\n\n    email = value.strip()\n\n    try:\n        parts = email_validator.validate_email(email, check_deliverability=False)\n    except email_validator.EmailNotValidError as e:\n        raise PydanticCustomError(\n            'value_error', 'value is not a valid email address: {reason}', {'reason': str(e.args[0])}\n        ) from e\n\n    email = parts.normalized\n    assert email is not None\n    name = name or parts.local_part\n    return name, email\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def settings(request):\n   \"\"\"\n   This page presents a list of all the settings to the user.  They are not editable.\n   \"\"\"\n   if not test_user_authenticated(request): return login(request, next=\"/cobbler_web/settings\")\n   settings = remote.get_settings()\n   skeys = settings.keys()\n   skeys.sort()\n\n   results = []\n   for k in skeys:\n      results.append([k,settings[k]])\n\n   t = get_template('settings.tmpl')\n   html = t.render(RequestContext(request,{\n        'settings' : results,\n        'version'  : remote.version(request.session['token']),\n        'username' : username,\n   }))\n   return HttpResponse(html)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "                        def get_content_length() -> Optional[int]:\n                            # payload length\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\n                            if length_hdr is None:\n                                return None\n\n                            # Shouldn't allow +/- or other number formats.\n                            # https://www.rfc-editor.org/rfc/rfc9110#section-8.6-2\n                            if not length_hdr.strip(\" \\t\").isdigit():\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            return int(length_hdr)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_local_uri(uri):\n    \"\"\"Returns true if this is a local file path (/foo or file:/foo).\"\"\"\n    if uri == \"databricks\":\n        return False\n\n    if is_windows() and uri.startswith(\"\\\\\\\\\"):\n        # windows network drive path looks like: \"\\\\<server name>\\path\\...\"\n        return False\n\n    parsed_uri = urllib.parse.urlparse(uri)\n    if parsed_uri.hostname and not (\n        parsed_uri.hostname == \".\"\n        or parsed_uri.hostname.startswith(\"localhost\")\n        or parsed_uri.hostname.startswith(\"127.0.0.1\")\n    ):\n        return False\n\n    scheme = parsed_uri.scheme\n    if scheme == \"\" or scheme == \"file\":\n        return True\n\n    if is_windows() and len(scheme) == 1 and scheme.lower() == pathlib.Path(uri).drive.lower()[0]:\n        return True\n\n    return False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "inline TfLiteTensor* GetTemporary(TfLiteContext* context,\n                                  const TfLiteNode* node, int index) {\n  if (index >= 0 && index < node->temporaries->size) {\n    const int tensor_index = node->temporaries->data[index];\n    if (tensor_index != kTfLiteOptionalTensor) {\n      if (context->tensors != nullptr) {\n        return &context->tensors[tensor_index];\n      }\n    }\n  }\n  return nullptr;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static int ReadHuffmanCodes(VP8LDecoder* const dec, int xsize, int ysize,\n                            int color_cache_bits, int allow_recursion) {\n  int i, j;\n  VP8LBitReader* const br = &dec->br_;\n  VP8LMetadata* const hdr = &dec->hdr_;\n  uint32_t* huffman_image = NULL;\n  HTreeGroup* htree_groups = NULL;\n  HuffmanTables* huffman_tables = &hdr->huffman_tables_;\n  int num_htree_groups = 1;\n  int num_htree_groups_max = 1;\n  const int max_alphabet_size =\n      kAlphabetSize[0] + ((color_cache_bits > 0) ? 1 << color_cache_bits : 0);\n  int* code_lengths = NULL;\n  const int table_size = kTableSize[color_cache_bits];\n  int* mapping = NULL;\n  int ok = 0;\n\n  // Check the table has been 0 initialized (through InitMetadata).\n  assert(huffman_tables->root.start == NULL);\n  assert(huffman_tables->curr_segment == NULL);\n\n  if (allow_recursion && VP8LReadBits(br, 1)) {\n    // use meta Huffman codes.\n    const int huffman_precision = VP8LReadBits(br, 3) + 2;\n    const int huffman_xsize = VP8LSubSampleSize(xsize, huffman_precision);\n    const int huffman_ysize = VP8LSubSampleSize(ysize, huffman_precision);\n    const int huffman_pixs = huffman_xsize * huffman_ysize;\n    if (!DecodeImageStream(huffman_xsize, huffman_ysize, /*is_level0=*/0, dec,\n                           &huffman_image)) {\n      goto Error;\n    }\n    hdr->huffman_subsample_bits_ = huffman_precision;\n    for (i = 0; i < huffman_pixs; ++i) {\n      // The huffman data is stored in red and green bytes.\n      const int group = (huffman_image[i] >> 8) & 0xffff;\n      huffman_image[i] = group;\n      if (group >= num_htree_groups_max) {\n        num_htree_groups_max = group + 1;\n      }\n    }\n    // Check the validity of num_htree_groups_max. If it seems too big, use a\n    // smaller value for later. This will prevent big memory allocations to end\n    // up with a bad bitstream anyway.\n    // The value of 1000 is totally arbitrary. We know that num_htree_groups_max\n    // is smaller than (1 << 16) and should be smaller than the number of pixels\n    // (though the format allows it to be bigger).\n    if (num_htree_groups_max > 1000 || num_htree_groups_max > xsize * ysize) {\n      // Create a mapping from the used indices to the minimal set of used\n      // values [0, num_htree_groups)\n      mapping = (int*)WebPSafeMalloc(num_htree_groups_max, sizeof(*mapping));\n      if (mapping == NULL) {\n        VP8LSetError(dec, VP8_STATUS_OUT_OF_MEMORY);\n        goto Error;\n      }\n      // -1 means a value is unmapped, and therefore unused in the Huffman\n      // image.\n      memset(mapping, 0xff, num_htree_groups_max * sizeof(*mapping));\n      for (num_htree_groups = 0, i = 0; i < huffman_pixs; ++i) {\n        // Get the current mapping for the group and remap the Huffman image.\n        int* const mapped_group = &mapping[huffman_image[i]];\n        if (*mapped_group == -1) *mapped_group = num_htree_groups++;\n        huffman_image[i] = *mapped_group;\n      }\n    } else {\n      num_htree_groups = num_htree_groups_max;\n    }\n  }\n\n  if (br->eos_) goto Error;\n\n  code_lengths = (int*)WebPSafeCalloc((uint64_t)max_alphabet_size,\n                                      sizeof(*code_lengths));\n  htree_groups = VP8LHtreeGroupsNew(num_htree_groups);\n\n  if (htree_groups == NULL || code_lengths == NULL ||\n      !VP8LHuffmanTablesAllocate(num_htree_groups * table_size,\n                                 huffman_tables)) {\n    VP8LSetError(dec, VP8_STATUS_OUT_OF_MEMORY);\n    goto Error;\n  }\n\n  for (i = 0; i < num_htree_groups_max; ++i) {\n    // If the index \"i\" is unused in the Huffman image, just make sure the\n    // coefficients are valid but do not store them.\n    if (mapping != NULL && mapping[i] == -1) {\n      for (j = 0; j < HUFFMAN_CODES_PER_META_CODE; ++j) {\n        int alphabet_size = kAlphabetSize[j];\n        if (j == 0 && color_cache_bits > 0) {\n          alphabet_size += (1 << color_cache_bits);\n        }\n        // Passing in NULL so that nothing gets filled.\n        if (!ReadHuffmanCode(alphabet_size, dec, code_lengths, NULL)) {\n          goto Error;\n        }\n      }\n    } else {\n      HTreeGroup* const htree_group =\n          &htree_groups[(mapping == NULL) ? i : mapping[i]];\n      HuffmanCode** const htrees = htree_group->htrees;\n      int size;\n      int total_size = 0;\n      int is_trivial_literal = 1;\n      int max_bits = 0;\n      for (j = 0; j < HUFFMAN_CODES_PER_META_CODE; ++j) {\n        int alphabet_size = kAlphabetSize[j];\n        if (j == 0 && color_cache_bits > 0) {\n          alphabet_size += (1 << color_cache_bits);\n        }\n        size =\n            ReadHuffmanCode(alphabet_size, dec, code_lengths, huffman_tables);\n        htrees[j] = huffman_tables->curr_segment->curr_table;\n        if (size == 0) {\n          goto Error;\n        }\n        if (is_trivial_literal && kLiteralMap[j] == 1) {\n          is_trivial_literal = (htrees[j]->bits == 0);\n        }\n        total_size += htrees[j]->bits;\n        huffman_tables->curr_segment->curr_table += size;\n        if (j <= ALPHA) {\n          int local_max_bits = code_lengths[0];\n          int k;\n          for (k = 1; k < alphabet_size; ++k) {\n            if (code_lengths[k] > local_max_bits) {\n              local_max_bits = code_lengths[k];\n            }\n          }\n          max_bits += local_max_bits;\n        }\n      }\n      htree_group->is_trivial_literal = is_trivial_literal;\n      htree_group->is_trivial_code = 0;\n      if (is_trivial_literal) {\n        const int red = htrees[RED][0].value;\n        const int blue = htrees[BLUE][0].value;\n        const int alpha = htrees[ALPHA][0].value;\n        htree_group->literal_arb = ((uint32_t)alpha << 24) | (red << 16) | blue;\n        if (total_size == 0 && htrees[GREEN][0].value < NUM_LITERAL_CODES) {\n          htree_group->is_trivial_code = 1;\n          htree_group->literal_arb |= htrees[GREEN][0].value << 8;\n        }\n      }\n      htree_group->use_packed_table =\n          !htree_group->is_trivial_code && (max_bits < HUFFMAN_PACKED_BITS);\n      if (htree_group->use_packed_table) BuildPackedTable(htree_group);\n    }\n  }\n  ok = 1;\n\n  // All OK. Finalize pointers.\n  hdr->huffman_image_ = huffman_image;\n  hdr->num_htree_groups_ = num_htree_groups;\n  hdr->htree_groups_ = htree_groups;\n\n Error:\n  WebPSafeFree(code_lengths);\n  WebPSafeFree(mapping);\n  if (!ok) {\n    WebPSafeFree(huffman_image);\n    VP8LHuffmanTablesDeallocate(huffman_tables);\n    VP8LHtreeGroupsFree(htree_groups);\n  }\n  return ok;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def run(argv: List[str]) -> None:\n    args = parse_command_line(argv)\n    remarshal(\n        args.input,\n        args.output,\n        args.input_format,\n        args.output_format,\n        json_indent=args.json_indent,\n        max_nodes=args.max_nodes,\n        ordered=args.ordered,\n        stringify=args.stringify,\n        unwrap=args.unwrap,\n        wrap=args.wrap,\n        yaml_options=args.yaml_options,\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  explicit FractionalAvgPoolOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pooling_ratio\", &pooling_ratio_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"pseudo_random\", &pseudo_random_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n    OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                errors::InvalidArgument(\n                    \"pooling_ratio field must specify 4 dimensions\"));\n    for (std::size_t i = 0; i < pooling_ratio_.size(); ++i) {\n      OP_REQUIRES(context, pooling_ratio_[i] >= 1,\n                  errors::InvalidArgument(\n                      \"pooling_ratio cannot be smaller than 1, got: \",\n                      pooling_ratio_[i]));\n    }\n    OP_REQUIRES(\n        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n        errors::Unimplemented(\"Fractional average pooling is not yet \"\n                              \"supported on the batch nor channel dimension.\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed2\", &seed2_));\n    if (deterministic_) {\n      // If both seeds are not set when deterministic_ is true, force set seeds.\n      if ((seed_ == 0) && (seed2_ == 0)) {\n        seed_ = random::New64();\n        seed2_ = random::New64();\n      }\n    } else {\n      OP_REQUIRES(\n          context, (seed_ == 0) && (seed2_ == 0),\n          errors::InvalidArgument(\n              \"Both seed and seed2 should be 0 if deterministic is false.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def generic_edit(request, what=None, obj_name=None, editmode=\"new\"):\n\n   \"\"\"\n   Presents an editor page for any type of object.\n   While this is generally standardized, systems are a little bit special.\n   \"\"\"\n   target = \"\"\n   if obj_name != None:\n       target = \"/%s\" % obj_name\n   if not test_user_authenticated(request): return login(request, next=\"/cobbler_web/%s/edit%s\" % (what,target))\n\n   obj = None\n\n   settings = remote.get_settings()\n\n   child = False\n   if what == \"subprofile\":\n      what = \"profile\"\n      child = True\n   \n\n   if not obj_name is None:\n      editable = remote.check_access_no_fail(request.session['token'], \"modify_%s\" % what, obj_name)\n      obj = remote.get_item(what, obj_name, True)\n   #\n   #   if obj.has_key('ctime'):\n   #      obj['ctime'] = time.ctime(obj['ctime'])\n   #   if obj.has_key('mtime'):\n   #      obj['mtime'] = time.ctime(obj['mtime'])\n\n   else:\n       editable = remote.check_access_no_fail(request.session['token'], \"new_%s\" % what, None)\n       obj = None\n\n\n   interfaces = {}\n   if what == \"system\":\n       if obj:\n           interfaces = obj.get(\"interfaces\",{})\n       else:\n           interfaces = {}\n\n   fields = get_fields(what, child, obj)\n\n   # populate some select boxes\n   # FIXME: we really want to just populate with the names, right?\n   if what == \"profile\":\n      if (obj and obj[\"parent\"] not in (None,\"\")) or child:\n         __tweak_field(fields, \"parent\", \"choices\", __names_from_dicts(remote.get_profiles()))\n      else:\n         __tweak_field(fields, \"distro\", \"choices\", __names_from_dicts(remote.get_distros()))\n      __tweak_field(fields, \"repos\", \"choices\",     __names_from_dicts(remote.get_repos()))\n   elif what == \"system\":\n      __tweak_field(fields, \"profile\", \"choices\",      __names_from_dicts(remote.get_profiles()))\n      __tweak_field(fields, \"image\", \"choices\",        __names_from_dicts(remote.get_images(),optional=True))\n   elif what == \"mgmtclass\":\n        __tweak_field(fields, \"packages\", \"choices\", __names_from_dicts(remote.get_packages()))\n        __tweak_field(fields, \"files\", \"choices\",    __names_from_dicts(remote.get_files()))\n\n   if what in (\"distro\",\"profile\",\"system\"):\n       __tweak_field(fields, \"mgmt_classes\", \"choices\", __names_from_dicts(remote.get_mgmtclasses(),optional=False))\n\n   # save the fields in the session for comparison later\n   request.session['%s_%s' % (what,obj_name)] = fields\n\n   t = get_template('generic_edit.tmpl')\n   inames = interfaces.keys()\n   inames.sort()\n   html = t.render(RequestContext(request,{\n       'what'            : what, \n       'fields'          : fields, \n       'subobject'       : child,\n       'editmode'        : editmode, \n       'editable'        : editable,\n       'interfaces'      : interfaces,\n       'interface_names' : inames,\n       'interface_length': len(inames),\n       'version'         : remote.version(request.session['token']),\n       'username'        : username,\n       'name'            : obj_name\n   }))\n\n   return HttpResponse(html)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse_headers(\n        self, lines: List[bytes]\n    ) -> Tuple[\"CIMultiDictProxy[str]\", RawHeaders]:\n        headers: CIMultiDict[str] = CIMultiDict()\n        raw_headers = []\n\n        lines_idx = 1\n        line = lines[1]\n        line_count = len(lines)\n\n        while line:\n            # Parse initial header name : value pair.\n            try:\n                bname, bvalue = line.split(b\":\", 1)\n            except ValueError:\n                raise InvalidHeader(line) from None\n\n            # https://www.rfc-editor.org/rfc/rfc9112.html#section-5.1-2\n            if {bname[0], bname[-1]} & {32, 9}:  # {\" \", \"\\t\"}\n                raise InvalidHeader(line)\n\n            bvalue = bvalue.lstrip(b\" \\t\")\n            if HDRRE.search(bname):\n                raise InvalidHeader(bname)\n            if len(bname) > self.max_field_size:\n                raise LineTooLong(\n                    \"request header name {}\".format(\n                        bname.decode(\"utf8\", \"xmlcharrefreplace\")\n                    ),\n                    str(self.max_field_size),\n                    str(len(bname)),\n                )\n\n            header_length = len(bvalue)\n\n            # next line\n            lines_idx += 1\n            line = lines[lines_idx]\n\n            # consume continuation lines\n            continuation = line and line[0] in (32, 9)  # (' ', '\\t')\n\n            # Deprecated: https://www.rfc-editor.org/rfc/rfc9112.html#name-obsolete-line-folding\n            if continuation:\n                bvalue_lst = [bvalue]\n                while continuation:\n                    header_length += len(line)\n                    if header_length > self.max_field_size:\n                        raise LineTooLong(\n                            \"request header field {}\".format(\n                                bname.decode(\"utf8\", \"xmlcharrefreplace\")\n                            ),\n                            str(self.max_field_size),\n                            str(header_length),\n                        )\n                    bvalue_lst.append(line)\n\n                    # next line\n                    lines_idx += 1\n                    if lines_idx < line_count:\n                        line = lines[lines_idx]\n                        if line:\n                            continuation = line[0] in (32, 9)  # (' ', '\\t')\n                    else:\n                        line = b\"\"\n                        break\n                bvalue = b\"\".join(bvalue_lst)\n            else:\n                if header_length > self.max_field_size:\n                    raise LineTooLong(\n                        \"request header field {}\".format(\n                            bname.decode(\"utf8\", \"xmlcharrefreplace\")\n                        ),\n                        str(self.max_field_size),\n                        str(header_length),\n                    )\n\n            bvalue = bvalue.strip(b\" \\t\")\n            name = bname.decode(\"utf-8\", \"surrogateescape\")\n            value = bvalue.decode(\"utf-8\", \"surrogateescape\")\n\n            # https://www.rfc-editor.org/rfc/rfc9110.html#section-5.5-5\n            if \"\\n\" in value or \"\\r\" in value or \"\\x00\" in value:\n                raise InvalidHeader(bvalue)\n\n            headers.add(name, value)\n            raw_headers.append((bname, bvalue))\n\n        return (CIMultiDictProxy(headers), tuple(raw_headers))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _build_docker_image(\n    project_spec: BigflowProjectSpec,\n    tag: str,\n    cache_params: BuildImageCacheParams | None,\n):\n\n    logger.debug(\"Run docker build...\")\n    cmd = [\"docker\", \"build\", project_spec.project_dir, \"--tag\", tag]\n\n    if cache_params:\n\n        logger.debug(\"Authenticate to docker registry\")\n        bigflow.deploy.authenticate_to_registry(\n            auth_method=cache_params.auth_method or bigflow.deploy.AuthorizationType.LOCAL_ACCOUNT,\n            vault_endpoint=cache_params.vault_endpoint,\n            vault_secret=cache_params.vault_secret,\n            vault_endpoint_verify=cache_params.vault_endpoint_verify\n        )\n\n        for image in (cache_params.cache_from_image or []):\n            logger.debug(\"Add --cache-from=%s to `docker build`\", image)\n            cmd.extend([\"--cache-from\", image])\n\n        for version in (cache_params.cache_from_version or []):\n            image = f\"{project_spec.docker_repository}:{version}\"\n            logger.debug(\"Add --cache-from=%s to `docker build`\", image)\n            cmd.extend([\"--cache-from\", image])\n\n        # noop when building backend is not a buildkit\n        logger.debug(\"Enable buildkit inline cache\")\n        cmd.extend([\"--build-arg\", \"BUILDKIT_INLINE_CACHE=1\"])\n\n    return bf_commons.run_process(cmd)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, hs):\n        super().__init__(hs)\n        self.clock = hs.get_clock()\n        self.client = hs.get_federation_http_client()\n        self.key_servers = self.config.key_servers\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def redirect_request(self, req, fp, code, msg, headers, newurl):\n        if code not in (301, 302, 303, 307, 308):\n            raise urllib.error.HTTPError(req.full_url, code, msg, headers, fp)\n\n        new_method = req.get_method()\n        new_data = req.data\n\n        # Technically the Cookie header should be in unredirected_hdrs,\n        # however in practice some may set it in normal headers anyway.\n        # We will remove it here to prevent any leaks.\n        remove_headers = ['Cookie']\n\n        # A 303 must either use GET or HEAD for subsequent request\n        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.4\n        if code == 303 and req.get_method() != 'HEAD':\n            new_method = 'GET'\n        # 301 and 302 redirects are commonly turned into a GET from a POST\n        # for subsequent requests by browsers, so we'll do the same.\n        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.2\n        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.3\n        elif code in (301, 302) and req.get_method() == 'POST':\n            new_method = 'GET'\n\n        # only remove payload if method changed (e.g. POST to GET)\n        if new_method != req.get_method():\n            new_data = None\n            remove_headers.extend(['Content-Length', 'Content-Type'])\n\n        new_headers = {k: v for k, v in req.headers.items() if k.title() not in remove_headers}\n\n        return urllib.request.Request(\n            newurl, headers=new_headers, origin_req_host=req.origin_req_host,\n            unverifiable=True, method=new_method, data=new_data)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def export_with_optimum(args):\n    if is_optimum_available():\n        from optimum.version import __version__ as optimum_version\n\n        parsed_optimum_version = version.parse(optimum_version)\n        if parsed_optimum_version < version.parse(MIN_OPTIMUM_VERSION):\n            raise RuntimeError(\n                f\"transformers.onnx requires optimum >= {MIN_OPTIMUM_VERSION} but {optimum_version} is installed. You \"\n                \"can upgrade optimum by running: pip install -U optimum[exporters]\"\n            )\n    else:\n        raise RuntimeError(\n            \"transformers.onnx requires optimum to run, you can install the library by running: pip install \"\n            \"optimum[exporters]\"\n        )\n    cmd_line = [\n        sys.executable,\n        \"-m\",\n        \"optimum.exporters.onnx\",\n        f\"--model {args.model}\",\n        f\"--task {args.feature}\",\n        f\"--framework {args.framework}\" if args.framework is not None else \"\",\n        f\"{args.output}\",\n    ]\n    proc = subprocess.Popen(cmd_line, stdout=subprocess.PIPE)\n    proc.wait()\n\n    logger.info(\n        \"The export was done by optimum.exporters.onnx. We recommend using to use this package directly in future, as \"\n        \"transformers.onnx is deprecated, and will be removed in v5. You can find more information here: \"\n        \"https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model.\"\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_file_hash(self):\n        if self.file_hash == \"\":\n            self._set_file_hash()\n            self.save(update_fields=[\"file_hash\"])\n\n        return self.file_hash\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status InferenceContext::Multiply(DimensionHandle first,\n                                  DimensionOrConstant second,\n                                  DimensionHandle* out) {\n  const int64_t first_value = Value(first);\n  const int64_t second_value = Value(second);\n  // Special cases.\n  if (first_value == 0) {\n    *out = first;\n  } else if (second_value == 0) {\n    *out = MakeDim(second);\n  } else if (first_value == 1) {\n    *out = MakeDim(second);\n  } else if (second_value == 1) {\n    *out = first;\n  } else if (first_value == kUnknownDim || second_value == kUnknownDim) {\n    *out = UnknownDim();\n  } else {\n    // Invariant: Both values are known and greater than 1.\n    const int64_t product = MultiplyWithoutOverflow(first_value, second_value);\n    if (product < 0) {\n      return errors::InvalidArgument(\n          \"Negative dimension size caused by overflow when multiplying \",\n          first_value, \" and \", second_value);\n    }\n    *out = MakeDim(product);\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& true_classes = context->input(0);\n    OP_REQUIRES(context, true_classes.dims() == 2,\n                errors::InvalidArgument(\"true_classes must be a matrix\"));\n    const int32_t batch_size = true_classes.dim_size(0);\n    OP_REQUIRES(\n        context, true_classes.dim_size(1) == num_true_,\n        errors::InvalidArgument(\"true_classes must have \"\n                                \"num_true columns, expected: \",\n                                true_classes.dim_size(1), \" was: \", num_true_));\n    CHECK(sampler_) << \"CandidateSamplerOp did not set sampler_\";\n\n    if (unique_) {\n      OP_REQUIRES(context, num_sampled_ <= sampler_->range(),\n                  errors::InvalidArgument(\"Sampler's range is too small.\"));\n    }\n\n    // Output candidates and expected_count.\n    Tensor* out_sampled_candidates = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({num_sampled_}),\n                                            &out_sampled_candidates));\n\n    Tensor* out_true_expected_count = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                1, TensorShape({batch_size, num_true_}),\n                                &out_true_expected_count));\n    Tensor* out_sampled_expected_count = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(2, TensorShape({num_sampled_}),\n                                            &out_sampled_expected_count));\n\n    gtl::ArraySlice<int64_t> true_candidate(\n        true_classes.matrix<int64_t>().data(), batch_size * num_true_);\n\n    for (const auto& candidate : true_candidate) {\n      OP_REQUIRES(context, candidate >= 0 && candidate < sampler_->range(),\n                  errors::InvalidArgument(\"`true_candidate` out of range [\", 0,\n                                          \", \", sampler_->range(),\n                                          \"), received \", candidate));\n    }\n\n    gtl::MutableArraySlice<int64_t> sampled_candidate(\n        out_sampled_candidates->vec<int64_t>().data(), num_sampled_);\n    gtl::MutableArraySlice<float> true_expected_count(\n        out_true_expected_count->matrix<float>().data(),\n        batch_size * num_true_);\n    gtl::MutableArraySlice<float> sampled_expected_count(\n        out_sampled_expected_count->vec<float>().data(), num_sampled_);\n\n    // Approximately conservatively estimate the number of samples required.\n    // In cases where rejection sampling is used we may occasionally use more\n    // samples than expected, which will result in reused random bits.\n    const int64_t samples32 = 2048 * num_sampled_;\n\n    // Pick sampled candidates.\n    auto local_gen = generator_.ReserveSamples32(samples32);\n    random::SimplePhilox random(&local_gen);\n    sampler_->SampleBatchGetExpectedCount(&random, unique_, sampled_candidate,\n                                          sampled_expected_count,\n                                          true_candidate, true_expected_count);\n\n    if (sampler_->NeedsUpdates()) {\n      sampler_->Update(true_candidate);\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def format_errormsg(message: str) -> Markup:\n    \"\"\"Match account names in error messages and insert HTML links for them.\"\"\"\n    match = re.search(ACCOUNT_RE, message)\n    if not match:\n        return Markup(message)\n    account = match.group()\n    url = url_for(\"account\", name=account)\n    return Markup(\n        message.replace(account, f'<a href=\"{url}\">{account}</a>')\n        .replace(\"for '\", \"for \")\n        .replace(\"': \", \": \")\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __call__(self, value):\n        if not isinstance(value, str) or len(value) > self.max_length:\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n        if self.unsafe_chars.intersection(value):\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n        # Check if the scheme is valid.\n        scheme = value.split(\"://\")[0].lower()\n        if scheme not in self.schemes:\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n\n        # Then check full URL\n        try:\n            splitted_url = urlsplit(value)\n        except ValueError:\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n        try:\n            super().__call__(value)\n        except ValidationError as e:\n            # Trivial case failed. Try for possible IDN domain\n            if value:\n                scheme, netloc, path, query, fragment = splitted_url\n                try:\n                    netloc = punycode(netloc)  # IDN -> ACE\n                except UnicodeError:  # invalid domain part\n                    raise e\n                url = urlunsplit((scheme, netloc, path, query, fragment))\n                super().__call__(url)\n            else:\n                raise\n        else:\n            # Now verify IPv6 in the netloc part\n            host_match = re.search(r\"^\\[(.+)\\](?::[0-9]{1,5})?$\", splitted_url.netloc)\n            if host_match:\n                potential_ip = host_match[1]\n                try:\n                    validate_ipv6_address(potential_ip)\n                except ValidationError:\n                    raise ValidationError(\n                        self.message, code=self.code, params={\"value\": value}\n                    )\n\n        # The maximum length of a full host name is 253 characters per RFC 1034\n        # section 3.1. It's defined to be 255 bytes or less, but this includes\n        # one byte for the length of the name and one byte for the trailing dot\n        # that's used to indicate absolute names in DNS.\n        if splitted_url.hostname is None or len(splitted_url.hostname) > 253:\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def element():\n    empty_gif = base64.b64decode(\n        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n    element_url = src_url = request.args.get('url')\n    if element_url.startswith('gAAAAA'):\n        try:\n            cipher_suite = Fernet(g.session_key)\n            src_url = cipher_suite.decrypt(element_url.encode()).decode()\n        except (InvalidSignature, InvalidToken) as e:\n            return render_template(\n                'error.html',\n                error_message=str(e)), 401\n\n    src_type = request.args.get('type')\n\n    # Ensure requested element is from a valid domain\n    domain = urlparse.urlparse(src_url).netloc\n    if not validators.domain(domain):\n        return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n\n    try:\n        file_data = g.user_request.send(base_url=src_url).content\n        tmp_mem = io.BytesIO()\n        tmp_mem.write(file_data)\n        tmp_mem.seek(0)\n\n        return send_file(tmp_mem, mimetype=src_type)\n    except exceptions.RequestException:\n        pass\n\n    return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def explain_query_prefix(self, format=None, **options):\n        extra = {}\n        # Normalize options.\n        if options:\n            options = {\n                name.upper(): \"true\" if value else \"false\"\n                for name, value in options.items()\n            }\n            for valid_option in self.explain_options:\n                value = options.pop(valid_option, None)\n                if value is not None:\n                    extra[valid_option.upper()] = value\n        prefix = super().explain_query_prefix(format, **options)\n        if format:\n            extra[\"FORMAT\"] = format\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status BuildInputArgIndex(const OpDef::ArgDef& arg_def, AttrSlice attr_values,\n                            const FunctionDef::ArgAttrs* arg_attrs,\n                            bool ints_on_device,\n                            int64_t resource_arg_unique_id) {\n    bool is_type_list;\n    DataTypeVector dtypes;\n    TF_RETURN_IF_ERROR(\n        ArgNumType(attr_values, arg_def, &is_type_list, &dtypes));\n    if (dtypes.size() < size_t{1}) {\n      return errors::Internal(\"Expected a list of at least one dtype\");\n    }\n    int arg_index = result_.nodes.size();\n    TF_RETURN_IF_ERROR(\n        AddItem(arg_def.name(), {true, arg_index, 0, is_type_list, dtypes}));\n    // Creates dtypes.size() nodes in the graph.\n    for (size_t i = 0; i < dtypes.size(); ++i) {\n      TF_RETURN_IF_ERROR(AddItem(strings::StrCat(arg_def.name(), \":\", i),\n                                 {true, arg_index, 0, false, {dtypes[i]}}));\n      if (arg_index != result_.nodes.size()) {\n        return errors::Internal(\n            \"Expected arg_index to be equal to the number of nodes in result.\",\n            \" Got \", arg_index, \" and \", result_.nodes.size());\n      }\n      string name = arg_def.name();\n      if (dtypes.size() > 1) {\n        strings::StrAppend(&name, \"_\", i);\n      }\n      NodeDef* gnode = AddNode(name);\n      if (ints_on_device && dtypes[i] == DataType::DT_INT32) {\n        gnode->set_op(FunctionLibraryDefinition::kDeviceArgOp);\n      } else {\n        gnode->set_op(FunctionLibraryDefinition::kArgOp);\n      }\n      DataType dtype = arg_def.is_ref() ? MakeRefType(dtypes[i]) : dtypes[i];\n      AddAttr(\"T\", dtype, gnode);\n      AddAttr(\"index\", arg_index, gnode);\n      if (resource_arg_unique_id >= 0) {\n        AddAttr(\"_resource_arg_unique_id\", resource_arg_unique_id, gnode);\n      }\n      if (arg_attrs) {\n        for (const auto& arg_attr : arg_attrs->attr()) {\n          AddAttr(arg_attr.first, arg_attr.second, gnode->mutable_attr());\n        }\n      }\n      result_.arg_types.push_back(dtypes[i]);\n      ++arg_index;\n    }\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool MfccMelFilterbank::Initialize(int input_length, double input_sample_rate,\n                                   int output_channel_count,\n                                   double lower_frequency_limit,\n                                   double upper_frequency_limit) {\n  num_channels_ = output_channel_count;\n  sample_rate_ = input_sample_rate;\n  input_length_ = input_length;\n\n  if (num_channels_ < 1) {\n    LOG(ERROR) << \"Number of filterbank channels must be positive.\";\n    return false;\n  }\n\n  if (sample_rate_ <= 0) {\n    LOG(ERROR) << \"Sample rate must be positive.\";\n    return false;\n  }\n\n  if (input_length < 2) {\n    LOG(ERROR) << \"Input length must greater than 1.\";\n    return false;\n  }\n\n  if (lower_frequency_limit < 0) {\n    LOG(ERROR) << \"Lower frequency limit must be nonnegative.\";\n    return false;\n  }\n\n  if (upper_frequency_limit <= lower_frequency_limit) {\n    LOG(ERROR) << \"Upper frequency limit must be greater than \"\n               << \"lower frequency limit.\";\n    return false;\n  }\n\n  // An extra center frequency is computed at the top to get the upper\n  // limit on the high side of the final triangular filter.\n  std::size_t center_frequencies_size = std::size_t(num_channels_) + 1;\n  if (center_frequencies_size >= std::numeric_limits<int>::max() ||\n      center_frequencies_size > center_frequencies_.max_size()) {\n    LOG(ERROR) << \"Number of filterbank channels must be less than \"\n               << std::numeric_limits<int>::max()\n               << \" and less than or equal to \"\n               << center_frequencies_.max_size();\n    return false;\n  }\n  center_frequencies_.resize(center_frequencies_size);\n\n  const double mel_low = FreqToMel(lower_frequency_limit);\n  const double mel_hi = FreqToMel(upper_frequency_limit);\n  const double mel_span = mel_hi - mel_low;\n  const double mel_spacing = mel_span / static_cast<double>(num_channels_ + 1);\n  for (int i = 0; i < num_channels_ + 1; ++i) {\n    center_frequencies_[i] = mel_low + (mel_spacing * (i + 1));\n  }\n\n  // Always exclude DC; emulate HTK.\n  const double hz_per_sbin =\n      0.5 * sample_rate_ / static_cast<double>(input_length_ - 1);\n  start_index_ = static_cast<int>(1.5 + (lower_frequency_limit / hz_per_sbin));\n  end_index_ = static_cast<int>(upper_frequency_limit / hz_per_sbin);\n\n  // Maps the input spectrum bin indices to filter bank channels/indices. For\n  // each FFT bin, band_mapper tells us which channel this bin contributes to\n  // on the right side of the triangle.  Thus this bin also contributes to the\n  // left side of the next channel's triangle response.\n  band_mapper_.resize(input_length_);\n  int channel = 0;\n  for (int i = 0; i < input_length_; ++i) {\n    double melf = FreqToMel(i * hz_per_sbin);\n    if ((i < start_index_) || (i > end_index_)) {\n      band_mapper_[i] = -2;  // Indicate an unused Fourier coefficient.\n    } else {\n      while ((channel < num_channels_) &&\n             (center_frequencies_[channel] < melf)) {\n        ++channel;\n      }\n      band_mapper_[i] = channel - 1;  // Can be == -1\n    }\n  }\n\n  // Create the weighting functions to taper the band edges.  The contribution\n  // of any one FFT bin is based on its distance along the continuum between two\n  // mel-channel center frequencies.  This bin contributes weights_[i] to the\n  // current channel and 1-weights_[i] to the next channel.\n  weights_.resize(input_length_);\n  for (int i = 0; i < input_length_; ++i) {\n    channel = band_mapper_[i];\n    if ((i < start_index_) || (i > end_index_)) {\n      weights_[i] = 0.0;\n    } else {\n      if (channel >= 0) {\n        weights_[i] =\n            (center_frequencies_[channel + 1] - FreqToMel(i * hz_per_sbin)) /\n            (center_frequencies_[channel + 1] - center_frequencies_[channel]);\n      } else {\n        weights_[i] = (center_frequencies_[0] - FreqToMel(i * hz_per_sbin)) /\n                      (center_frequencies_[0] - mel_low);\n      }\n    }\n  }\n  // Check the sum of FFT bin weights for every mel band to identify\n  // situations where the mel bands are so narrow that they don't get\n  // significant weight on enough (or any) FFT bins -- i.e., too many\n  // mel bands have been requested for the given FFT size.\n  std::vector<int> bad_channels;\n  for (int c = 0; c < num_channels_; ++c) {\n    float band_weights_sum = 0.0;\n    for (int i = 0; i < input_length_; ++i) {\n      if (band_mapper_[i] == c - 1) {\n        band_weights_sum += (1.0 - weights_[i]);\n      } else if (band_mapper_[i] == c) {\n        band_weights_sum += weights_[i];\n      }\n    }\n    // The lowest mel channels have the fewest FFT bins and the lowest\n    // weights sum.  But given that the target gain at the center frequency\n    // is 1.0, if the total sum of weights is 0.5, we're in bad shape.\n    if (band_weights_sum < 0.5) {\n      bad_channels.push_back(c);\n    }\n  }\n  if (!bad_channels.empty()) {\n    LOG(ERROR) << \"Missing \" << bad_channels.size() << \" bands \"\n               << \" starting at \" << bad_channels[0]\n               << \" in mel-frequency design. \"\n               << \"Perhaps too many channels or \"\n               << \"not enough frequency resolution in spectrum. (\"\n               << \"input_length: \" << input_length\n               << \" input_sample_rate: \" << input_sample_rate\n               << \" output_channel_count: \" << output_channel_count\n               << \" lower_frequency_limit: \" << lower_frequency_limit\n               << \" upper_frequency_limit: \" << upper_frequency_limit;\n  }\n  initialized_ = true;\n  return true;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(tensorflow::OpKernelContext* context) override {\n    for (int ngram_width : ngram_widths_) {\n      OP_REQUIRES(\n          context, ngram_width > 0,\n          errors::InvalidArgument(\"ngram_widths must contain positive values\"));\n    }\n\n    const tensorflow::Tensor* data;\n    OP_REQUIRES_OK(context, context->input(\"data\", &data));\n    const auto& input_data = data->flat<tstring>().data();\n\n    const tensorflow::Tensor* splits;\n    OP_REQUIRES_OK(context, context->input(\"data_splits\", &splits));\n    const auto& splits_vec = splits->flat<SPLITS_TYPE>();\n\n    // Validate that the splits are valid indices into data, only if there are\n    // splits specified.\n    const int input_data_size = data->flat<tstring>().size();\n    const int splits_vec_size = splits_vec.size();\n    if (splits_vec_size > 0) {\n      int prev_split = splits_vec(0);\n      OP_REQUIRES(context, prev_split == 0,\n                  errors::InvalidArgument(\"First split value must be 0, got \",\n                                          prev_split));\n      for (int i = 1; i < splits_vec_size; ++i) {\n        bool valid_splits = splits_vec(i) >= prev_split;\n        valid_splits = valid_splits && (splits_vec(i) <= input_data_size);\n        OP_REQUIRES(context, valid_splits,\n                    errors::InvalidArgument(\n                        \"Invalid split value \", splits_vec(i), \", must be in [\",\n                        prev_split, \", \", input_data_size, \"]\"));\n        prev_split = splits_vec(i);\n      }\n      OP_REQUIRES(context, prev_split == input_data_size,\n                  errors::InvalidArgument(\n                      \"Last split value must be data size. Expected \",\n                      input_data_size, \", got \", prev_split));\n    }\n\n    int num_batch_items = splits_vec.size() - 1;\n    tensorflow::Tensor* ngrams_splits;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(1, splits->shape(), &ngrams_splits));\n    auto ngrams_splits_data = ngrams_splits->flat<SPLITS_TYPE>().data();\n\n    // If there is no data or size, return an empty RT.\n    if (data->flat<tstring>().size() == 0 || splits_vec.size() == 0) {\n      tensorflow::Tensor* empty;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(0, data->shape(), &empty));\n      for (int i = 0; i <= num_batch_items; ++i) {\n        ngrams_splits_data[i] = 0;\n      }\n      return;\n    }\n\n    ngrams_splits_data[0] = 0;\n    for (int i = 1; i <= num_batch_items; ++i) {\n      int length = splits_vec(i) - splits_vec(i - 1);\n      int num_ngrams = 0;\n      for (int ngram_width : ngram_widths_)\n        num_ngrams += get_num_ngrams(length, ngram_width);\n      if (preserve_short_ && length > 0 && num_ngrams == 0) {\n        num_ngrams = 1;\n      }\n      ngrams_splits_data[i] = ngrams_splits_data[i - 1] + num_ngrams;\n    }\n\n    tensorflow::Tensor* ngrams;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            0, TensorShape({ngrams_splits_data[num_batch_items]}), &ngrams));\n    auto ngrams_data = ngrams->flat<tstring>().data();\n\n    for (int i = 0; i < num_batch_items; ++i) {\n      auto data_start = &input_data[splits_vec(i)];\n      int output_start_idx = ngrams_splits_data[i];\n      for (int ngram_width : ngram_widths_) {\n        auto output_start = &ngrams_data[output_start_idx];\n        int length = splits_vec(i + 1) - splits_vec(i);\n        int num_ngrams = get_num_ngrams(length, ngram_width);\n        CreateNgrams(data_start, output_start, num_ngrams, ngram_width);\n        output_start_idx += num_ngrams;\n      }\n      // If we're preserving short sequences, check to see if no sequence was\n      // generated by comparing the current output start idx to the original\n      // one (ngram_splits_data). If no ngrams were generated, then they will\n      // be equal (since we increment output_start_idx by num_ngrams every\n      // time we create a set of ngrams.)\n      if (preserve_short_ && output_start_idx == ngrams_splits_data[i]) {\n        int data_length = splits_vec(i + 1) - splits_vec(i);\n        // One legitimate reason to not have any ngrams when preserve_short_\n        // is true is if the sequence itself is empty. In that case, move on.\n        if (data_length == 0) {\n          continue;\n        }\n        // We don't have to worry about dynamic padding sizes here: if padding\n        // was dynamic, every sequence would have had sufficient padding to\n        // generate at least one ngram.\n        int ngram_width = data_length + 2 * pad_width_;\n        auto output_start = &ngrams_data[output_start_idx];\n        int num_ngrams = 1;\n        CreateNgrams(data_start, output_start, num_ngrams, ngram_width);\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor x = context->input(0);\n    const Tensor& scale = context->input(1);\n    const Tensor& offset = context->input(2);\n    const Tensor& estimated_mean = context->input(3);\n    const Tensor& estimated_variance = context->input(4);\n    const Tensor* side_input = has_side_input_ ? &context->input(5) : nullptr;\n\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(context, offset.dims() == 1,\n                errors::InvalidArgument(\"offset must be 1-dimensional\",\n                                        offset.shape().DebugString()));\n    OP_REQUIRES(context, estimated_mean.dims() == 1,\n                errors::InvalidArgument(\"estimated_mean must be 1-dimensional\",\n                                        estimated_mean.shape().DebugString()));\n    OP_REQUIRES(\n        context, estimated_variance.dims() == 1,\n        errors::InvalidArgument(\"estimated_variance must be 1-dimensional\",\n                                estimated_variance.shape().DebugString()));\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64 in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64 in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64 in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64 in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64 in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    const auto num_channels = GetTensorDim(x, tensor_format_, 'C');\n    OP_REQUIRES(\n        context, scale.NumElements() == num_channels,\n        errors::InvalidArgument(\"scale must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                scale.NumElements(), \" and \", num_channels));\n    OP_REQUIRES(\n        context, offset.NumElements() == num_channels,\n        errors::InvalidArgument(\"offset must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                offset.NumElements(), \" and \", num_channels));\n    if (estimated_mean.NumElements() != 0) {\n      OP_REQUIRES(context, estimated_mean.NumElements() == num_channels,\n                  errors::InvalidArgument(\n                      \"mean must be empty or have the same number of \"\n                      \"elements as the channels of x, got \",\n                      estimated_mean.NumElements(), \" and \", num_channels));\n    }\n    if (estimated_variance.NumElements() != 0) {\n      OP_REQUIRES(context, estimated_variance.NumElements() == num_channels,\n                  errors::InvalidArgument(\n                      \"variance must be empty or have the same number of \"\n                      \"elements as the channels of x, got \",\n                      estimated_variance.NumElements(), \" and \", num_channels));\n    }\n\n    if (has_side_input_) {\n      OP_REQUIRES(context, side_input->shape() == x.shape(),\n                  errors::InvalidArgument(\n                      \"side_input shape must be equal to input shape: \",\n                      side_input->shape().DebugString(),\n                      \" != \", x.shape().DebugString()));\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity) {\n      // NOTE(ezhulenev): This requirement is coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx.\n      OP_REQUIRES(\n          context, !is_training_ || num_channels % 4 == 0,\n          errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                  \"channel dimension to be a multiple of 4.\"));\n    }\n\n    Tensor* y = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, alloc_shape, &y));\n\n    Tensor* batch_mean = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {3}, 1, scale.shape(), &batch_mean));\n    Tensor* batch_var = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {4}, 2, scale.shape(), &batch_var));\n    Tensor* saved_mean = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(3, scale.shape(), &saved_mean));\n    Tensor* saved_maybe_inv_var = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(4, scale.shape(),\n                                                     &saved_maybe_inv_var));\n\n    if (is_training_) {\n      functor::FusedBatchNorm<Device, T, U, true>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    } else {\n      functor::FusedBatchNorm<Device, T, U, false>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, y->CopyFrom(*y, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _w(self, content):\n        self.output.append(bleach.clean(str(format_data(content, format_money_values=True)), strip=True))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        self.save = function () {\n            if (!CONFIG_ACCESS_CONTROL) return;\n\n            self.userSettingsDialog.trigger(\"beforeSave\");\n\n            function process() {\n                var settings = {\n                    interface: {\n                        language: self.interface_language()\n                    }\n                };\n                self.updateSettings(self.currentUser().name, settings).done(function () {\n                    // close dialog\n                    self.currentUser(undefined);\n                    self.userSettingsDialog.modal(\"hide\");\n                    self.loginState.reloadUser();\n                });\n            }\n\n            if (self.access_password() && !self.passwordMismatch()) {\n                self.users\n                    .updatePassword(\n                        self.currentUser().name,\n                        self.access_password(),\n                        self.access_currentPassword()\n                    )\n                    .done(function () {\n                        process();\n                    })\n                    .fail(function (xhr) {\n                        if (xhr.status === 403) {\n                            self.access_currentPasswordMismatch(true);\n                        }\n                    });\n            } else {\n                process();\n            }\n        };\n\n        self.copyApikey = function () {\n            copyToClipboard(self.access_apikey());\n        };\n\n        self.generateApikey = function () {\n            if (!CONFIG_ACCESS_CONTROL) return;\n\n            var generate = function () {\n                self.users\n                    .generateApikey(self.currentUser().name)\n                    .done(function (response) {\n                        self.access_apikey(response.apikey);\n                    });\n            };\n\n            if (self.access_apikey()) {\n                showConfirmationDialog(\n                    gettext(\n                        \"This will generate a new API Key. The old API Key will cease to function immediately.\"\n                    ),\n                    generate\n                );\n            } else {\n                generate();\n            }\n        };\n\n        self.deleteApikey = function () {\n            if (!CONFIG_ACCESS_CONTROL) return;\n            if (!self.access_apikey()) return;\n\n            showConfirmationDialog(\n                gettext(\n                    \"This will delete the API Key. It will cease to to function immediately.\"\n                ),\n                function () {\n                    self.users.deleteApikey(self.currentUser().name).done(function () {\n                        self.access_apikey(undefined);\n                    });\n                }\n            );\n        };\n\n        self.updateSettings = function (username, settings) {\n            return OctoPrint.access.users.saveSettings(username, settings);\n        };\n\n        self.saveEnabled = function () {\n            return !self.passwordMismatch();\n        };\n\n        self.onStartup = function () {\n            self.userSettingsDialog = $(\"#usersettings_dialog\");\n        };\n\n        self.onAllBound = function (allViewModels) {\n            self.userSettingsDialog.on(\"show\", function () {\n                callViewModels(allViewModels, \"onUserSettingsShown\");\n            });\n            self.userSettingsDialog.on(\"hidden\", function () {\n                callViewModels(allViewModels, \"onUserSettingsHidden\");\n            });\n            self.userSettingsDialog.on(\"beforeSave\", function () {\n                callViewModels(allViewModels, \"onUserSettingsBeforeSave\");\n            });\n        };\n    }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ConstantFolding::EvaluateOneFoldable(const NodeDef& node,\n                                            std::vector<NodeDef>* outputs,\n                                            bool* result_too_large) {\n  TensorVector inputs;\n  TensorVector output_tensors;\n  auto inputs_cleanup = gtl::MakeCleanup([&inputs, &output_tensors] {\n    for (const auto& input : inputs) {\n      delete input.tensor;\n    }\n    for (const auto& output : output_tensors) {\n      if (output.tensor) {\n        delete output.tensor;\n      }\n    }\n  });\n\n  size_t total_inputs_size = 0;\n  for (const auto& input : node.input()) {\n    const TensorId input_tensor = ParseTensorName(input);\n    if (input_tensor.index() < 0) {\n      // Control dependency\n      break;\n    }\n    const NodeDef* input_node = node_map_->GetNode(input);\n    if (!IsReallyConstant(*input_node)) {\n      return Status(error::INVALID_ARGUMENT,\n                    strings::StrCat(\"Can't fold \", node.name(), \", its \", input,\n                                    \" isn't constant\"));\n    }\n    TF_RETURN_IF_ERROR(CheckAttrExists(*input_node, \"value\"));\n    const TensorProto& raw_val = input_node->attr().at(\"value\").tensor();\n    if (raw_val.dtype() == DT_INVALID) {\n      return Status(\n          error::INVALID_ARGUMENT,\n          strings::StrCat(\"A tensor in the input node, with TensorId of \",\n                          input_tensor.ToString(),\n                          \" has a dtype of DT_INVALID.\"));\n    }\n    if (IsRefType(raw_val.dtype())) {\n      return errors::InvalidArgument(\n          \"Not allowed to construct a tensor with reference dtype, got \",\n          DataTypeString(raw_val.dtype()));\n    }\n    Tensor* value = new Tensor(raw_val.dtype(), raw_val.tensor_shape());\n    if (!value->FromProto(raw_val)) {\n      delete (value);\n      return errors::InvalidArgument(\"Unable to make Tensor from proto for \",\n                                     node.name(), \" with shape \",\n                                     raw_val.tensor_shape().DebugString());\n    }\n    inputs.emplace_back(value);\n    total_inputs_size += value->TotalBytes();\n  }\n\n  TF_RETURN_IF_ERROR(EvaluateNode(node, inputs, &output_tensors));\n  if (output_tensors.empty()) {\n    return Status(error::INVALID_ARGUMENT, \"Expected at least one output.\");\n  }\n\n  outputs->resize(output_tensors.size());\n  for (size_t i = 0; i < output_tensors.size(); i++) {\n    string node_name = OptimizedNodeName(node, \"-folded\");\n    if (output_tensors.size() > 1) {\n      node_name = strings::StrCat(node_name, \"-\", i);\n    }\n    if (output_tensors[i].tensor) {\n      Status s = CreateNodeDef(node_name, output_tensors[i], &outputs->at(i),\n                               total_inputs_size);\n      if (!s.ok()) {\n        *result_too_large = true;\n        return s;\n      }\n    } else {\n      // Create an empty NodeDef to identify dead outputs (e.g. the output of a\n      // switch that's not selected by the switch predicate).\n      outputs->at(i) = NodeDef();\n    }\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def to_python(self, value):\n        value = super().to_python(value)\n        if self.max_length is not None and len(value) > self.max_length:\n            # Normalization can increase the string length (e.g.\n            # \"\ufb00\" -> \"ff\", \"\u00bd\" -> \"1\u20442\") but cannot reduce it, so there is no\n            # point in normalizing invalid data. Moreover, Unicode\n            # normalization is very slow on Windows and can be a DoS attack\n            # vector.\n            return value\n        return unicodedata.normalize(\"NFKC\", value)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def urlopen(self, method, url, redirect=True, **kw):\n        \"\"\"\n        Same as :meth:`urllib3.connectionpool.HTTPConnectionPool.urlopen`\n        with custom cross-host redirect logic and only sends the request-uri\n        portion of the ``url``.\n\n        The given ``url`` parameter must be absolute, such that an appropriate\n        :class:`urllib3.connectionpool.ConnectionPool` can be chosen for it.\n        \"\"\"\n        u = parse_url(url)\n        conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)\n\n        kw['assert_same_host'] = False\n        kw['redirect'] = False\n\n        if 'headers' not in kw:\n            kw['headers'] = self.headers.copy()\n\n        if self.proxy is not None and u.scheme == \"http\":\n            response = conn.urlopen(method, url, **kw)\n        else:\n            response = conn.urlopen(method, u.request_uri, **kw)\n\n        redirect_location = redirect and response.get_redirect_location()\n        if not redirect_location:\n            return response\n\n        # Support relative URLs for redirecting.\n        redirect_location = urljoin(url, redirect_location)\n\n        # RFC 7231, Section 6.4.4\n        if response.status == 303:\n            method = 'GET'\n\n        retries = kw.get('retries')\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect)\n\n        # Strip headers marked as unsafe to forward to the redirected location.\n        # Check remove_headers_on_redirect to avoid a potential network call within\n        # conn.is_same_host() which may use socket.gethostbyname() in the future.\n        if (retries.remove_headers_on_redirect\n                and not conn.is_same_host(redirect_location)):\n            headers = list(six.iterkeys(kw['headers']))\n            for header in headers:\n                if header.lower() in retries.remove_headers_on_redirect:\n                    kw['headers'].pop(header, None)\n\n        try:\n            retries = retries.increment(method, url, response=response, _pool=conn)\n        except MaxRetryError:\n            if retries.raise_on_redirect:\n                raise\n            return response\n\n        kw['retries'] = retries\n        kw['redirect'] = redirect\n\n        log.info(\"Redirecting %s -> %s\", url, redirect_location)\n        return self.urlopen(method, redirect_location, **kw)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Subgraph::Invoke() {\n  if (!consistent_) {\n    ReportError(\"Invoke called on model that is not consistent.\");\n    return kTfLiteError;\n  }\n\n  TfLiteStatus status = kTfLiteOk;\n  if (state_ == kStateUninvokable) {\n    ReportError(\"Invoke called on model that is not ready.\");\n    return kTfLiteError;\n  } else if (memory_planner_ && !memory_planner_->HasNonPersistentMemory()) {\n    ReportError(\"Non-persistent memory is not available.\");\n    return kTfLiteError;\n  }\n  TFLITE_SCOPED_TAGGED_DEFAULT_PROFILE(profiler_.get(), \"Invoke\");\n\n  // Invocations are always done in node order.\n  // Note that calling Invoke repeatedly will cause the original memory plan to\n  // be reused, unless either ResizeInputTensor() or AllocateTensors() has been\n  // called.\n  for (int execution_plan_index = 0;\n       execution_plan_index < execution_plan_.size(); execution_plan_index++) {\n    if (execution_plan_index == next_execution_plan_index_to_prepare_) {\n      TF_LITE_ENSURE_STATUS(PrepareOpsAndTensors());\n      TF_LITE_ENSURE(&context_, next_execution_plan_index_to_prepare_ >=\n                                    execution_plan_index);\n    }\n    int node_index = execution_plan_[execution_plan_index];\n    TfLiteNode& node = nodes_and_registration_[node_index].first;\n    const TfLiteRegistration& registration =\n        nodes_and_registration_[node_index].second;\n\n    const char* op_name = nullptr;\n    if (profiler_) op_name = GetTFLiteOpName(registration);\n    TFLITE_SCOPED_TAGGED_OPERATOR_PROFILE(profiler_.get(), op_name, node_index);\n\n    for (int i = 0; i < node.inputs->size; ++i) {\n      int tensor_index = node.inputs->data[i];\n      if (tensor_index == kTfLiteOptionalTensor) {\n        continue;\n      }\n      TfLiteTensor* tensor = &tensors_[tensor_index];\n      if (tensor->delegate && tensor->delegate != node.delegate &&\n          tensor->data_is_stale) {\n        TF_LITE_ENSURE_STATUS(EnsureTensorDataIsReadable(tensor_index));\n      }\n      if (tensor->data.raw == nullptr && tensor->bytes > 0) {\n        if (registration.builtin_code == kTfLiteBuiltinReshape && i == 1 &&\n            tensor->dims->size != 1) {\n          // In general, having a tensor here with no buffer will be an error.\n          // However, for the reshape operator, the second input tensor is\n          // sometimes only used for the shape, not for the data. Thus, null\n          // buffer is ok in this situation.\n          // The situation where null buffer is not ok for reshape operator is\n          // only when there are 2 inputs given to the node and the one\n          // corresponding to the shape (i == 1) is a vector that contains all\n          // dimensions. See `GetOutputShape()` function in\n          // `tensorflow/lite/kernels/reshape.cc`\n          continue;\n        } else {\n          // In all other cases, we need to return an error as otherwise we will\n          // trigger a null pointer dereference (likely).\n          ReportError(\"Input tensor %d lacks data\", tensor_index);\n          return kTfLiteError;\n        }\n      }\n    }\n\n    if (check_cancelled_func_ != nullptr &&\n        check_cancelled_func_(cancellation_data_)) {\n      ReportError(\"Client requested cancel during Invoke()\");\n      return kTfLiteError;\n    }\n\n    EnsureTensorsVectorCapacity();\n    tensor_resized_since_op_invoke_ = false;\n    if (OpInvoke(registration, &node) != kTfLiteOk) {\n      return ReportOpError(&context_, node, registration, node_index,\n                           \"failed to invoke\");\n    }\n\n    // Force execution prep for downstream ops if the latest op triggered the\n    // resize of a dynamic tensor.\n    if (tensor_resized_since_op_invoke_ &&\n        HasDynamicTensor(context_, node.outputs)) {\n      next_execution_plan_index_to_prepare_ = execution_plan_index + 1;\n\n      // This happens when an intermediate dynamic tensor is resized.\n      // We don't have to prepare all the ops, but we need to recompute\n      // the allocation plan.\n      if (next_execution_plan_index_to_plan_allocation_ >\n          next_execution_plan_index_to_prepare_) {\n        next_execution_plan_index_to_plan_allocation_ =\n            next_execution_plan_index_to_prepare_;\n        if (memory_planner_) {\n          TF_LITE_ENSURE_STATUS(memory_planner_->ResetAllocationsAfter(\n              next_execution_plan_index_to_plan_allocation_ - 1));\n        }\n      }\n    }\n  }\n\n  return status;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def edit_file(self):\n\n        if not HAS_AES or not HAS_COUNTER or not HAS_PBKDF2 or not HAS_HASH:\n            raise errors.AnsibleError(CRYPTO_UPGRADE)\n\n        # make sure the umask is set to a sane value\n        old_mask = os.umask(0077)\n\n        # decrypt to tmpfile\n        tmpdata = self.read_data(self.filename)\n        this_vault = VaultLib(self.password)\n        dec_data = this_vault.decrypt(tmpdata)\n        _, tmp_path = tempfile.mkstemp()\n        self.write_data(dec_data, tmp_path)\n\n        # drop the user into vim on the tmp file\n        EDITOR = os.environ.get('EDITOR','vim')\n        call([EDITOR, tmp_path])\n        new_data = self.read_data(tmp_path)\n\n        # create new vault\n        new_vault = VaultLib(self.password)\n\n        # we want the cipher to default to AES256\n        #new_vault.cipher_name = this_vault.cipher_name\n\n        # encrypt new data a write out to tmp\n        enc_data = new_vault.encrypt(new_data)\n        self.write_data(enc_data, tmp_path)\n\n        # shuffle tmp file into place\n        self.shuffle_files(tmp_path, self.filename)\n\n        # and restore the old umask\n        os.umask(old_mask)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def execute(self, image_id, file_path):\n\n        # NOTE(flaper87): A format must be explicitly\n        # specified. There's no \"sane\" default for this\n        # because the dest format may work differently depending\n        # on the environment OpenStack is running in.\n        conversion_format = CONF.taskflow_executor.conversion_format\n        if conversion_format is None:\n            if not _Convert.conversion_missing_warned:\n                msg = _LW('The conversion format is None, please add a value '\n                          'for it in the config file for this task to '\n                          'work: %s')\n                LOG.warn(msg, self.task_id)\n                _Convert.conversion_missing_warned = True\n            return\n\n        image_obj = self.image_repo.get(image_id)\n        src_format = image_obj.disk_format\n\n        # TODO(flaper87): Check whether the image is in the desired\n        # format already. Probably using `qemu-img` just like the\n        # `Introspection` task.\n\n        # NOTE(hemanthm): We add '-f' parameter to the convert command here so\n        # that the image format need not be inferred by qemu utils. This\n        # shields us from being vulnerable to an attack vector described here\n        # https://bugs.launchpad.net/glance/+bug/1449062\n\n        dest_path = os.path.join(CONF.task.work_dir, \"%s.converted\" % image_id)\n        stdout, stderr = putils.trycmd('qemu-img', 'convert',\n                                       '-f', src_format,\n                                       '-O', conversion_format,\n                                       file_path, dest_path,\n                                       log_errors=putils.LOG_ALL_ERRORS)\n\n        if stderr:\n            raise RuntimeError(stderr)\n\n        os.rename(dest_path, file_path.split(\"file://\")[-1])\n        return file_path\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _read(self, ti, try_number, metadata=None):\n        \"\"\"\n        Template method that contains custom logic of reading\n        logs given the try_number.\n\n        :param ti: task instance record\n        :param try_number: current try_number to read log from\n        :param metadata: log metadata,\n                         can be used for steaming log reading and auto-tailing.\n        :return: log message as a string and metadata.\n        \"\"\"\n        from airflow.utils.jwt_signer import JWTSigner\n\n        # Task instance here might be different from task instance when\n        # initializing the handler. Thus explicitly getting log location\n        # is needed to get correct log path.\n        log_relative_path = self._render_filename(ti, try_number)\n        location = os.path.join(self.local_base, log_relative_path)\n\n        log = \"\"\n\n        if os.path.exists(location):\n            try:\n                with open(location, encoding=\"utf-8\", errors=\"surrogateescape\") as file:\n                    log += f\"*** Reading local file: {location}\\n\"\n                    log += \"\".join(file.readlines())\n            except Exception as e:\n                log = f\"*** Failed to load local log file: {location}\\n\"\n                log += f\"*** {str(e)}\\n\"\n        elif conf.get('core', 'executor') == 'KubernetesExecutor':\n            try:\n                from airflow.kubernetes.kube_client import get_kube_client\n\n                kube_client = get_kube_client()\n\n                if len(ti.hostname) >= 63:\n                    # Kubernetes takes the pod name and truncates it for the hostname. This truncated hostname\n                    # is returned for the fqdn to comply with the 63 character limit imposed by DNS standards\n                    # on any label of a FQDN.\n                    pod_list = kube_client.list_namespaced_pod(conf.get('kubernetes', 'namespace'))\n                    matches = [\n                        pod.metadata.name\n                        for pod in pod_list.items\n                        if pod.metadata.name.startswith(ti.hostname)\n                    ]\n                    if len(matches) == 1:\n                        if len(matches[0]) > len(ti.hostname):\n                            ti.hostname = matches[0]\n\n                log += f'*** Trying to get logs (last 100 lines) from worker pod {ti.hostname} ***\\n\\n'\n\n                res = kube_client.read_namespaced_pod_log(\n                    name=ti.hostname,\n                    namespace=conf.get('kubernetes', 'namespace'),\n                    container='base',\n                    follow=False,\n                    tail_lines=100,\n                    _preload_content=False,\n                )\n\n                for line in res:\n                    log += line.decode()\n\n            except Exception as f:\n                log += f'*** Unable to fetch logs from worker pod {ti.hostname} ***\\n{str(f)}\\n\\n'\n        else:\n            import httpx\n\n            url = urljoin(\n                f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path\n            )\n            log += f\"*** Log file does not exist: {location}\\n\"\n            log += f\"*** Fetching from: {url}\\n\"\n            try:\n                timeout = None  # No timeout\n                try:\n                    timeout = conf.getint('webserver', 'log_fetch_timeout_sec')\n                except (AirflowConfigException, ValueError):\n                    pass\n\n                signer = JWTSigner(\n                    secret_key=conf.get('webserver', 'secret_key'),\n                    expiration_time_in_seconds=conf.getint(\n                        'webserver', 'log_request_clock_grace', fallback=30\n                    ),\n                    audience=\"task-instance-logs\",\n                )\n                response = httpx.get(\n                    url,\n                    timeout=timeout,\n                    headers={b'Authorization': signer.generate_signed_token({\"filename\": log_relative_path})},\n                )\n                response.encoding = \"utf-8\"\n\n                if response.status_code == 403:\n                    log += (\n                        \"*** !!!! Please make sure that all your Airflow components (e.g. \"\n                        \"schedulers, webservers and workers) have \"\n                        \"the same 'secret_key' configured in 'webserver' section and \"\n                        \"time is synchronized on all your machines (for example with ntpd) !!!!!\\n***\"\n                    )\n                    log += (\n                        \"*** See more at https://airflow.apache.org/docs/apache-airflow/\"\n                        \"stable/configurations-ref.html#secret-key\\n***\"\n                    )\n                # Check if the resource was properly fetched\n                response.raise_for_status()\n\n                log += '\\n' + response.text\n            except Exception as e:\n                log += f\"*** Failed to fetch log file from worker. {str(e)}\\n\"\n\n        return log, {'end_of_log': True}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def login_record(success=True):\n    denied_hosts = read_hosts_deny()\n    val = (0, 0)\n    if success and request.client in denied_hosts:\n        del denied_hosts[request.client]\n    elif not success:\n        val = denied_hosts.get(request.client, (0, 0))\n        if time.time() - val[1] < expiration_failed_logins \\\n            and val[0] >= allowed_number_of_attempts:\n            return val[0]  # locked out\n        time.sleep(2 ** val[0])\n        val = (val[0] + 1, int(time.time()))\n        denied_hosts[request.client] = val\n    write_hosts_deny(denied_hosts)\n    return val[0]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def xsrf_token(self):\n        \"\"\"The XSRF-prevention token for the current user/session.\n\n        To prevent cross-site request forgery, we set an '_xsrf' cookie\n        and include the same '_xsrf' value as an argument with all POST\n        requests. If the two do not match, we reject the form submission\n        as a potential forgery.\n\n        See http://en.wikipedia.org/wiki/Cross-site_request_forgery\n        \"\"\"\n        if not hasattr(self, \"_xsrf_token\"):\n            version, token, timestamp = self._get_raw_xsrf_token()\n            mask = os.urandom(4)\n            self._xsrf_token = b\"|\".join([\n                b\"2\",\n                binascii.b2a_hex(mask),\n                binascii.b2a_hex(_websocket_mask(mask, token)),\n                utf8(str(int(timestamp)))])\n            if version is None or version != 2:\n                expires_days = 30 if self.current_user else None\n                self.set_cookie(\"_xsrf\", self._xsrf_token,\n                                expires_days=expires_days)\n        return self._xsrf_token\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse_native_url(url):\n        \"\"\"\n        Support https://maker.ifttt.com/use/WEBHOOK_ID/EVENT_ID\n        \"\"\"\n\n        result = re.match(\n            r'^https?://maker\\.ifttt\\.com/use/'\n            r'(?P<webhook_id>[A-Z0-9_-]+)'\n            r'((?P<events>(/[A-Z0-9_-]+)+))?'\n            r'/?(?P<params>\\?.+)?$', url, re.I)\n\n        if result:\n            return NotifyIFTTT.parse_url(\n                '{schema}://{webhook_id}{events}{params}'.format(\n                    schema=NotifyIFTTT.secure_protocol,\n                    webhook_id=result.group('webhook_id'),\n                    events='' if not result.group('events')\n                    else '@{}'.format(result.group('events')),\n                    params='' if not result.group('params')\n                    else result.group('params')))\n\n        return None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _require_verified_user(self, request):\n        user = request.user\n\n        if not settings.WAGTAIL_2FA_REQUIRED:\n            # If two factor authentication is disabled in the settings\n            return False\n\n        if not user.is_authenticated:\n            return False\n\n        # If the user has no access to the admin anyway then don't require a\n        # verified user here\n        if not (\n            user.is_staff\n            or user.is_superuser\n            or user.has_perms([\"wagtailadmin.access_admin\"])\n        ):\n            return False\n\n        # Allow the user to a fixed number of paths when not verified\n        user_has_device = django_otp.user_has_device(user, confirmed=True)\n        if request.path in self._get_allowed_paths(user_has_device):\n            return False\n\n        # For all other cases require that the user is verfied via otp\n        return True\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _glob_matches(glob: str, value: str, word_boundary: bool = False) -> bool:\n    \"\"\"Tests if value matches glob.\n\n    Args:\n        glob\n        value: String to test against glob.\n        word_boundary: Whether to match against word boundaries or entire\n            string. Defaults to False.\n    \"\"\"\n\n    try:\n        r = regex_cache.get((glob, True, word_boundary), None)\n        if not r:\n            r = glob_to_regex(glob, word_boundary)\n            regex_cache[(glob, True, word_boundary)] = r\n        return bool(r.search(value))\n    except re.error:\n        logger.warning(\"Failed to parse glob to regex: %r\", glob)\n        return False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "                repeatedPassword: ko.observable(undefined),\n                passwordMismatch: ko.pureComputed(function () {\n                    return self.editor.password() !== self.editor.repeatedPassword();\n                }),\n                currentPasswordMismatch: ko.observable(false),\n                apikey: ko.observable(undefined),\n                active: ko.observable(undefined),\n                permissionSelectable: function (permission) {\n                    return true;\n                },\n                permissionSelected: function (permission) {\n                    var index = self.editor.permissions().indexOf(permission);\n                    return index >= 0;\n                },\n                togglePermission: function (permission) {\n                    var permissions = self.editor.permissions();\n                    var index = permissions.indexOf(permission);\n                    if (index < 0) {\n                        permissions.push(permission);\n                    } else {\n                        permissions.splice(index, 1);\n                    }\n                    self.editor.permissions(permissions);\n                },\n                groupSelected: function (group) {\n                    var index = self.editor.groups().indexOf(group);\n                    return index >= 0;\n                },\n                toggleGroup: function (group) {\n                    var groups = self.editor.groups();\n                    var index = groups.indexOf(group);\n                    if (index < 0) {\n                        groups.push(group);\n                    } else {\n                        groups.splice(index, 1);\n                    }\n                    self.editor.groups(groups);\n                },\n                joinedGroupPermissions: function (group) {\n                    return access.permissionList(group);\n                },\n                header: ko.observable(undefined),\n                new: ko.observable(true),\n                confirm: undefined,\n                valid: ko.pureComputed(function () {\n                    return (\n                        self.editor.name() &&\n                        self.editor.name().trim() &&\n                        (!self.editor.new() ||\n                            (self.editor.password() &&\n                                self.editor.password().trim() &&\n                                !self.editor.passwordMismatch()))\n                    );\n                }),\n                dangerRestricted: function () {\n                    return false;\n                },\n                dangerRestrictedText: gettext(\n                    \"This user may not have dangerous permissions.\"\n                )\n            };\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def read_config(self, config, **kwargs):\n        consent_config = config.get(\"user_consent\")\n        self.terms_template = self.read_template(\"terms.html\")\n\n        if consent_config is None:\n            return\n        self.user_consent_version = str(consent_config[\"version\"])\n        self.user_consent_template_dir = self.abspath(consent_config[\"template_dir\"])\n        if not path.isdir(self.user_consent_template_dir):\n            raise ConfigError(\n                \"Could not find template directory '%s'\"\n                % (self.user_consent_template_dir,)\n            )\n        self.user_consent_server_notice_content = consent_config.get(\n            \"server_notice_content\"\n        )\n        self.block_events_without_consent_error = consent_config.get(\n            \"block_events_error\"\n        )\n        self.user_consent_server_notice_to_guests = bool(\n            consent_config.get(\"send_server_notice_to_guests\", False)\n        )\n        self.user_consent_at_registration = bool(\n            consent_config.get(\"require_at_registration\", False)\n        )\n        self.user_consent_policy_name = consent_config.get(\n            \"policy_name\", \"Privacy Policy\"\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    INDEX_TYPE first_dimension;\n    const Tensor first_partition_tensor =\n        context->input(kFirstPartitionInputIndex);\n    OP_REQUIRES(context, first_partition_tensor.NumElements() > 0,\n                errors::InvalidArgument(\"Invalid first partition input. Tensor \"\n                                        \"requires at least one element.\"));\n    OP_REQUIRES_OK(context, GetFirstDimensionSize(context, &first_dimension));\n    vector<INDEX_TYPE> output_size;\n    OP_REQUIRES_OK(context,\n                   CalculateOutputSize(first_dimension, context, &output_size));\n    vector<INDEX_TYPE> multiplier;\n    multiplier.resize(ragged_rank_ + 1);\n\n    multiplier[multiplier.size() - 1] = 1;\n    for (int i = multiplier.size() - 2; i >= 0; --i) {\n      multiplier[i] = multiplier[i + 1] * output_size[i + 1];\n    }\n    // Full size of the tensor.\n    TensorShape output_shape;\n    OP_REQUIRES_OK(context,\n                   TensorShapeUtils::MakeShape(output_size, &output_shape));\n    Tensor* output_tensor = nullptr;\n\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, output_shape, &output_tensor));\n    const INDEX_TYPE full_size = multiplier[0] * output_size[0];\n    if (full_size > 0) {\n      vector<INDEX_TYPE> output_index, new_output_index;\n      int nvals = context->input(kValueInputIndex).shape().dim_size(0);\n      output_index.reserve(nvals);\n      new_output_index.reserve(nvals);\n\n      CalculateFirstParentOutputIndex(first_dimension, multiplier[0],\n                                      output_size[0], &output_index);\n      for (int i = 1; i <= ragged_rank_; ++i) {\n        OP_REQUIRES_OK(context, CalculateOutputIndex(\n                                    context, i - 1, output_index, multiplier[i],\n                                    output_size[i], &new_output_index));\n        output_index.swap(new_output_index);\n        new_output_index.clear();\n      }\n\n      SetOutput(context, ragged_rank_, output_index, output_tensor);\n    }\n  }\n  virtual void SetOutput(OpKernelContext* context, int ragged_rank,"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_users(self, email):\n        \"\"\"Given an email, return matching user(s) who should receive a reset.\n\n        This allows subclasses to more easily customize the default policies\n        that prevent inactive users and users with unusable passwords from\n        resetting their password.\n        \"\"\"\n        email_field_name = UserModel.get_email_field_name()\n        active_users = UserModel._default_manager.filter(**{\n            '%s__iexact' % email_field_name: email,\n            'is_active': True,\n        })\n        return (\n            u for u in active_users\n            if u.has_usable_password() and\n            _unicode_ci_compare(email, getattr(u, email_field_name))\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _read_lockfile(self, lockfile: Mapping[str, Any]) -> None:\n        root = self.environment.project.root\n        static_urls = self.environment.project.lockfile.static_urls\n        with cd(root):\n            for package in lockfile.get(\"package\", []):\n                version = package.get(\"version\")\n                if version:\n                    package[\"version\"] = f\"=={version}\"\n                package_name = package.pop(\"name\")\n                req_dict = {\n                    k: v for k, v in package.items() if k not in (\"dependencies\", \"requires_python\", \"summary\", \"files\")\n                }\n                req = Requirement.from_req_dict(package_name, req_dict)\n                if req.is_file_or_url and req.path and not req.url:  # type: ignore[attr-defined]\n                    req.url = path_to_url(posixpath.join(root, req.path))  # type: ignore[attr-defined]\n                can = make_candidate(req, name=package_name, version=version)\n                can.hashes = package.get(\"files\", [])\n                if not static_urls and any(\"url\" in f for f in can.hashes):\n                    raise PdmException(\n                        \"Static URLs are not allowed in lockfile unless enabled by `pdm lock --static-urls`.\"\n                    )\n                can_id = self._identify_candidate(can)\n                self.packages[can_id] = can\n                candidate_info: CandidateInfo = (\n                    package.get(\"dependencies\", []),\n                    package.get(\"requires_python\", \"\"),\n                    package.get(\"summary\", \"\"),\n                )\n                self.candidate_info[can_id] = candidate_info\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def deserialize_raw(collection_type):\n    __connect()\n    contents = simplejson.loads(couchdb.listDoc(collection_type))\n    items = []\n    if contents.has_key(\"error\") and contents.get(\"reason\",\"\").find(\"Missing\") != -1:\n        # no items in the DB yet\n        return []\n    for x in contents[\"rows\"]:\n       items.append(x[\"key\"])\n\n    if collection_type == \"settings\":\n         fd = open(\"/etc/cobbler/settings\")\n         datastruct = yaml.safe_load(fd.read())\n         fd.close()\n         return datastruct\n    else:\n         results = []\n         for f in items:\n             data = couchdb.openDoc(collection_type, f)\n             datastruct = simplejson.loads(data, encoding='utf-8')\n             results.append(datastruct)\n         return results    \n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _open_index(self, index=1):\n        #\n        # get the Image Contents Property Set\n\n        prop = self.ole.getproperties(\n            [\"Data Object Store %06d\" % index, \"\\005Image Contents\"]\n        )\n\n        # size (highest resolution)\n\n        self._size = prop[0x1000002], prop[0x1000003]\n\n        size = max(self.size)\n        i = 1\n        while size > 64:\n            size = size / 2\n            i += 1\n        self.maxid = i - 1\n\n        # mode.  instead of using a single field for this, flashpix\n        # requires you to specify the mode for each channel in each\n        # resolution subimage, and leaves it to the decoder to make\n        # sure that they all match.  for now, we'll cheat and assume\n        # that this is always the case.\n\n        id = self.maxid << 16\n\n        s = prop[0x2000002 | id]\n\n        colors = []\n        bands = i32(s, 4)\n        if bands > 4:\n            raise IOError(\"Invalid number of bands\")\n        for i in range(bands):\n            # note: for now, we ignore the \"uncalibrated\" flag\n            colors.append(i32(s, 8 + i * 4) & 0x7FFFFFFF)\n\n        self.mode, self.rawmode = MODES[tuple(colors)]\n\n        # load JPEG tables, if any\n        self.jpeg = {}\n        for i in range(256):\n            id = 0x3000001 | (i << 16)\n            if id in prop:\n                self.jpeg[i] = prop[id]\n\n        self._open_subimage(1, self.maxid)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def remove_auth_hashes(input: Optional[str]):\n    if not input:\n        return input\n    # If there are no hashes, skip the RE for performance.\n    input_lower = input.lower()\n    if not any([pw_hash.lower() in input_lower for pw_hash in PASSWORD_HASHERS_ALL.keys()]):\n        return input\n    return re_remove_passwords.sub(r'\\1 %s  # Filtered for security' % PASSWORD_HASH_DUMMY_VALUE, input)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "export const redirectAnyRequest = response => {\n  const {headers: {location}} = Response.message (response);\n  const original = Response.request (response);\n  const oldUrl = Request.url (original);\n  const newUrl = mergeUrls (oldUrl, location);\n  const request = Request (Request.options (original))\n                          (newUrl)\n                          (Request.body (original));\n\n  return sameOrigin (oldUrl, newUrl) ? request : overHeaders (request, xs => (\n    xs.filter (([name]) => !confidentialHeaders.includes (name.toLowerCase ()))\n  ));\n};\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def load(self):\n        start = time.time()\n        end = start + 15\n        current = start\n        while current <= end:\n            try:\n                if self.saving_method == \"mongo\":\n                    data = self._load_mongo()\n                elif self.saving_method == \"local\":\n                    data = self._load_local()\n\n                data = data[\"members\"][self.sid]\n                self.member.data = GroupMemberData(**data)\n\n                return\n\n            except KeyError:\n                time.sleep(1)\n                current = time.time()\n                msg = \"Error while loading.\"\n                if current <= end:\n                    msg += \" Trying again.\"\n                else:\n                    msg += \" Timeout reached. Aborting.\"\n\n                self.exp.log.exception(msg)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def remove_file_ref(filename):\n        logger.debug('removing ref on %s', filename)\n        # TODO: fix the race condition below\n        HTMLAsset.objects.filter(filename=filename, ref_count__gt=0).update(ref_count=models.F('ref_count') - 1)\n        refs_count = HTMLAsset.objects.filter(filename=filename).aggregate(models.Sum('ref_count'))\n        refs_count = refs_count.get('ref_count__sum')\n\n        # refs_count is None when HTMLAsset.objects.filter(...) returns an empty set\n        if refs_count is None or refs_count <= 0:\n            logger.debug('removing file %s', filename)\n            remove_html_asset_file(settings.SOSSE_HTML_SNAPSHOT_DIR + filename)\n\n        HTMLAsset.objects.filter(filename=filename, ref_count=0).delete()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& indices_tensor = ctx->input(0);\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(indices_tensor.shape()) ||\n                    TensorShapeUtils::IsScalar(indices_tensor.shape()),\n                errors::InvalidArgument(\n                    \"The indices can only be scalar or vector, got \\\"\",\n                    indices_tensor.shape().DebugString(), \"\\\"\"));\n\n    const Tensor& dims_tensor = ctx->input(1);\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(dims_tensor.shape()),\n        errors::InvalidArgument(\"The indices can only be 1-D, got \\\"\",\n                                dims_tensor.shape().DebugString(), \"\\\"\"));\n\n    auto dims = dims_tensor.vec<Tidx>();\n    // Make sure dims does not contain a zero\n    double prod = 1;\n    uint64_t limit;\n    if (dtidx_ == DataType::DT_INT64) {\n      limit = kint64max;\n    } else {\n      limit = kint32max;\n    }\n\n    for (int i = 0; i < dims.size(); i++) {\n      OP_REQUIRES(\n          ctx, dims(i) != 0,\n          errors::InvalidArgument(\"Input dims cannot contain a dim of zero, \"\n                                  \"but dims contains zero at index \",\n                                  i));\n      OP_REQUIRES(ctx, dims(i) > 0,\n                  errors::InvalidArgument(\n                      \"Input dims cannot be negative. Got dim = \", dims(i),\n                      \" at index \", i));\n      // Check interger overflow\n      OP_REQUIRES(\n          ctx, prod <= limit / dims(i),\n          errors::InvalidArgument(\"Input dims product is causing integer \"\n                                  \"overflow: (\",\n                                  dims, \")\"));\n      prod = (prod * dims(i));\n    }\n\n    // Check to make sure indices is not out of boundary\n    Eigen::Tensor<Tidx, 0, Eigen::RowMajor> dims_prod_eigen = dims.prod();\n    Tidx dims_prod = dims_prod_eigen();\n    const Tidx* indices = indices_tensor.flat<Tidx>().data();\n    int64_t size = indices_tensor.NumElements();\n    bool check = std::all_of(indices, indices + size,\n                             [&](Tidx index) { return index < dims_prod; });\n    OP_REQUIRES(ctx, check,\n                errors::InvalidArgument(\"index is out of bound as with dims\"));\n\n    Eigen::array<bool, 1> reverse({true});\n\n    Tensor strides_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<Tidx>::value,\n                                      TensorShape({dims_tensor.NumElements()}),\n                                      &strides_tensor));\n\n    auto strides = strides_tensor.vec<Tidx>();\n    strides = dims.reverse(reverse)\n                  .scan(0, Eigen::internal::ProdReducer<Tidx>(), false)\n                  .reverse(reverse);\n\n    Tensor strides_shifted_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<Tidx>::value,\n                                      TensorShape({dims_tensor.NumElements()}),\n                                      &strides_shifted_tensor));\n\n    auto strides_shifted = strides_shifted_tensor.vec<Tidx>();\n    strides_shifted = dims.reverse(reverse)\n                          .scan(0, Eigen::internal::ProdReducer<Tidx>(), true)\n                          .reverse(reverse);\n\n    Tensor* output_tensor = nullptr;\n    if (TensorShapeUtils::IsScalar(indices_tensor.shape())) {\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({dims_tensor.NumElements()}),\n                                    &output_tensor));\n\n      auto output = output_tensor->vec<Tidx>();\n\n      output = output.constant(indices_tensor.scalar<Tidx>()());\n      output = output.binaryExpr(strides, mod_op<Tidx>()) / strides_shifted;\n    } else {\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0,\n                                    TensorShape({dims_tensor.NumElements(),\n                                                 indices_tensor.NumElements()}),\n                                    &output_tensor));\n\n      auto output = output_tensor->matrix<Tidx>();\n\n      Eigen::array<Eigen::Index, 2> reshape{\n          {static_cast<Eigen::Index>(dims_tensor.NumElements()), 1}};\n      Eigen::array<Eigen::Index, 2> bcast(\n          {1, static_cast<Eigen::Index>(indices_tensor.NumElements())});\n      Eigen::array<Eigen::Index, 2> indices_reshape{\n          {1, static_cast<Eigen::Index>(indices_tensor.NumElements())}};\n      Eigen::array<Eigen::Index, 2> indices_bcast(\n          {static_cast<Eigen::Index>(dims_tensor.NumElements()), 1});\n\n      output = indices_tensor.vec<Tidx>()\n                   .reshape(indices_reshape)\n                   .broadcast(indices_bcast);\n      output = output.binaryExpr(strides.reshape(reshape).broadcast(bcast),\n                                 mod_op<Tidx>()) /\n               strides_shifted.reshape(reshape).broadcast(bcast);\n    }\n  }\n  const DataType dtidx_;"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def decrypt(self, password_encrypted):\n        raise NotImplementedError()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def upload_template(filename, destination, context=None, use_jinja=False,\n    template_dir=None, use_sudo=False, backup=True, mirror_local_mode=False,\n    mode=None):\n    \"\"\"\n    Render and upload a template text file to a remote host.\n\n    ``filename`` should be the path to a text file, which may contain `Python\n    string interpolation formatting\n    <http://docs.python.org/release/2.5.4/lib/typesseq-strings.html>`_ and will\n    be rendered with the given context dictionary ``context`` (if given.)\n\n    Alternately, if ``use_jinja`` is set to True and you have the Jinja2\n    templating library available, Jinja will be used to render the template\n    instead. Templates will be loaded from the invoking user's current working\n    directory by default, or from ``template_dir`` if given.\n\n    The resulting rendered file will be uploaded to the remote file path\n    ``destination``.  If the destination file already exists, it will be\n    renamed with a ``.bak`` extension unless ``backup=False`` is specified.\n\n    By default, the file will be copied to ``destination`` as the logged-in\n    user; specify ``use_sudo=True`` to use `sudo` instead.\n\n    In some use cases, it is desirable to force a newly uploaded file to match\n    the mode of its local counterpart (such as when uploading executable\n    scripts). To do this, specify ``mirror_local_mode=True``.\n\n    Alternately, you may use the ``mode`` kwarg to specify an exact mode, in\n    the same vein as ``os.chmod`` or the Unix ``chmod`` command.\n    \"\"\"\n    func = use_sudo and sudo or run\n    # Normalize destination to be an actual filename, due to using StringIO\n    with settings(hide('everything'), warn_only=True):\n        if func('test -d %s' % destination).succeeded:\n            sep = \"\" if destination.endswith('/') else \"/\"\n            destination += sep + os.path.basename(filename)\n\n    # Process template\n    text = None\n    if use_jinja:\n        try:\n            from jinja2 import Environment, FileSystemLoader\n            jenv = Environment(loader=FileSystemLoader(template_dir or '.'))\n            text = jenv.get_template(filename).render(**context or {})\n        except ImportError, e:\n            abort(\"tried to use Jinja2 but was unable to import: %s\" % e)\n    else:\n        with open(filename) as inputfile:\n            text = inputfile.read()\n        if context:\n            text = text % context\n\n    # Back up original file\n    if backup and exists(destination):\n        func(\"cp %s{,.bak}\" % destination)\n\n    # Upload the file.\n    put(\n        local_path=StringIO(text),\n        remote_path=destination,\n        use_sudo=use_sudo,\n        mirror_local_mode=mirror_local_mode,\n        mode=mode\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        name = str(name).replace('\\\\', '/')\n        dir_name, file_name = os.path.split(name)\n        if '..' in pathlib.PurePath(dir_name).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n        validate_file_name(file_name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, generate an alternative filename\n        # until it doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, self.get_alternative_name(file_root, file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, self.get_alternative_name(file_root, file_ext))\n        return name\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def write_exception_sync(exception, before=None, after=None, file=None, *, filter=None, highlighter=None):\n    \"\"\"\n    Writes the given exception's traceback.\n    \n    Parameters\n    ----------\n    exception : ``BaseException``\n        The exception to render.\n    \n    before : `str`, `list` of `str` = `None`, Optional\n        Any content, what should go before the exception's traceback.\n        \n        If given as `str`, or if `list`, then the last element of it should end with linebreak.\n    \n    after : `str`, `list` of `str` = `None`, Optional\n        Any content, what should go after the exception's traceback.\n        \n        If given as `str`, or if `list`, then the last element of it should end with linebreak.\n    \n    file : `None`, `I/O stream` = `None`, Optional\n        The file to print the stack to. Defaults to `sys.stderr`.\n    \n    filter : `None`, `callable` = `None`, Optional (Keyword only)\n        Additional filter to check whether a frame should be shown.\n        \n        Called with 4 parameters:\n        \n        +---------------+-----------+---------------------------------------------------------------+\n        | Name          | Type      | Description                                                   |\n        +===============+===========+===============================================================+\n        | file_name     | `str`     | The frame's file's name.                                      |\n        +---------------+-----------+---------------------------------------------------------------+\n        | name          | `str`     | The name of the function.                                     |\n        +---------------+-----------+---------------------------------------------------------------+\n        | line_number   | `int`     | The line's index of the file where the exception occurred.    |\n        +---------------+-----------+---------------------------------------------------------------+\n        | line          | `str`     | The line of the file.                                         |\n        +---------------+-----------+---------------------------------------------------------------+\n    \n    highlighter : `None`, ``HighlightFormatterContext`` = `None`, Optional (Keyword only)\n        Formatter storing highlighting details.\n    \"\"\"\n    extracted = []\n    \n    if before is None:\n        pass\n    elif isinstance(before, str):\n        extracted.append(before)\n    elif isinstance(before, list):\n        for element in before:\n            if isinstance(element, str):\n                extracted.append(element)\n            else:\n                extracted.append(repr(element))\n                extracted.append('\\n')\n    else:\n        # ignore exception cases\n        extracted.append(repr(before))\n        extracted.append('\\n')\n    \n    if (file is None) and (highlighter is None):\n        highlighter = get_default_trace_writer_highlighter()\n    \n    render_exception_into(exception, extracted, filter=filter, highlighter=highlighter)\n    \n    if after is None:\n        pass\n    elif isinstance(after, str):\n        extracted.append(after)\n    elif isinstance(after, list):\n        for element in after:\n            if isinstance(element, str):\n                extracted.append(element)\n            else:\n                extracted.append(repr(element))\n                extracted.append('\\n')\n    else:\n        extracted.append(repr(after))\n        extracted.append('\\n')\n    \n    if file is None:\n        # ignore exception cases\n        file = sys.stderr\n    \n    file.write(''.join(extracted))\n    file.flush()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def dataReceived(self, data: bytes) -> None:\n        self.stream.write(data)\n        self.length += len(data)\n        if self.max_size is not None and self.length >= self.max_size:\n            self.deferred.errback(BodyExceededMaxSize())\n            self.deferred = defer.Deferred()\n            self.transport.loseConnection()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def get(self, request: web.Request) -> web.Response:\n        \"\"\"Get available auth providers.\"\"\"\n        hass: HomeAssistant = request.app[\"hass\"]\n        if not onboarding.async_is_user_onboarded(hass):\n            return self.json_message(\n                message=\"Onboarding not finished\",\n                status_code=HTTPStatus.BAD_REQUEST,\n                message_code=\"onboarding_required\",\n            )\n\n        try:\n            remote_address = ip_address(request.remote)  # type: ignore[arg-type]\n        except ValueError:\n            return self.json_message(\n                message=\"Invalid remote IP\",\n                status_code=HTTPStatus.BAD_REQUEST,\n                message_code=\"invalid_remote_ip\",\n            )\n\n        cloud_connection = is_cloud_connection(hass)\n\n        providers = []\n        for provider in hass.auth.auth_providers:\n            if provider.type == \"trusted_networks\":\n                if cloud_connection:\n                    # Skip quickly as trusted networks are not available on cloud\n                    continue\n\n                try:\n                    cast(\"TrustedNetworksAuthProvider\", provider).async_validate_access(\n                        remote_address\n                    )\n                except InvalidAuthError:\n                    # Not a trusted network, so we don't expose that trusted_network authenticator is setup\n                    continue\n\n            providers.append(\n                {\n                    \"name\": provider.name,\n                    \"id\": provider.id,\n                    \"type\": provider.type,\n                }\n            )\n\n        return self.json(providers)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __call__(self, request):\n        file_fields = request.POST.getlist(\"s3file\")\n        for field_name in file_fields:\n\n            paths = request.POST.getlist(field_name)\n            if paths:\n                try:\n                    signature = request.POST[f\"{field_name}-s3f-signature\"]\n                except KeyError:\n                    raise PermissionDenied(\"No signature provided.\")\n                try:\n                    request.FILES.setlist(\n                        field_name, list(self.get_files_from_storage(paths, signature))\n                    )\n                except SuspiciousFileOperation as e:\n                    raise PermissionDenied(\"Illegal file name!\") from e\n\n        if local_dev and request.path == \"/__s3_mock__/\":\n            return views.S3MockView.as_view()(request)\n\n        return self.get_response(request)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& indices = ctx->input(0);\n    const Tensor& values = ctx->input(1);\n    const auto values_flat = values.flat<Tidx>();\n    const Tensor& dense_shape = ctx->input(2);\n    const Tensor& size_t = ctx->input(3);\n    const auto weights = ctx->input(4).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    OP_REQUIRES(ctx, size_t.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_t.dims()));\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n    OP_REQUIRES_OK(\n        ctx, sparse_utils::ValidateSparseTensor<int64_t>(\n                 indices, values, dense_shape, /*validate_indices=*/true));\n\n    bool is_1d = dense_shape.NumElements() == 1;\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (is_1d) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(ctx,\n                       functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                           ctx, values_flat, weights, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, values_flat, weights, out, size));\n      }\n    } else {\n      const auto shape = dense_shape.flat<int64_t>();\n      const int64_t num_rows = shape(0);\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      const auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      const auto indices_mat = indices.matrix<int64_t>();\n      for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {\n        const int64_t batch = indices_mat(i, 0);\n        const Tidx bin = values_flat(i);\n        OP_REQUIRES(\n            ctx, batch < out.dimension(0),\n            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,\n                                    \") must be less than the dimension size (\",\n                                    out.dimension(0), \").\"));\n        OP_REQUIRES(\n            ctx, bin < out.dimension(1),\n            errors::InvalidArgument(\"Index out ouf bound. `bin` (\", bin,\n                                    \") must be less then the dimension size (\",\n                                    out.dimension(1), \").\"));\n        if (bin < size) {\n          if (binary_output_) {\n            out(batch, bin) = T(1);\n          } else {\n            if (weights_size) {\n              out(batch, bin) += weights(i);\n            } else {\n              out(batch, bin) += T(1);\n            }\n          }\n        }\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "_tiffReadProc(thandle_t hdata, tdata_t buf, tsize_t size) {\n    TIFFSTATE *state = (TIFFSTATE *)hdata;\n    tsize_t to_read;\n\n    TRACE((\"_tiffReadProc: %d \\n\", (int)size));\n    dump_state(state);\n\n    if (state->loc > state->eof) {\n        TIFFError(\"_tiffReadProc\", \"Invalid Read at loc %d, eof: %d\", state->loc, state->eof);\n        return 0;\n    }\n    to_read = min(size, min(state->size, (tsize_t)state->eof) - (tsize_t)state->loc);\n    TRACE((\"to_read: %d\\n\", (int)to_read));\n\n    _TIFFmemcpy(buf, (UINT8 *)state->data + state->loc, to_read);\n    state->loc += (toff_t)to_read;\n\n    TRACE((\"location: %u\\n\", (uint)state->loc));\n    return to_read;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def unarchive(request, bookmark_id: int):\n    try:\n        bookmark = Bookmark.objects.get(pk=bookmark_id, owner=request.user)\n    except Bookmark.DoesNotExist:\n        raise Http404('Bookmark does not exist')\n\n    unarchive_bookmark(bookmark)\n    return_url = request.GET.get('return_url')\n    return_url = return_url if return_url else reverse('bookmarks:archived')\n    return HttpResponseRedirect(return_url)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def execute(self, raise_on_error: bool = True):\n        \"\"\"Execute all the commands in the current pipeline\"\"\"\n        stack = self.command_stack\n        if not stack and not self.watching:\n            return []\n        if self.scripts:\n            await self.load_scripts()\n        if self.is_transaction or self.explicit_transaction:\n            execute = self._execute_transaction\n        else:\n            execute = self._execute_pipeline\n\n        conn = self.connection\n        if not conn:\n            conn = await self.connection_pool.get_connection(\"MULTI\", self.shard_hint)\n            # assign to self.connection so reset() releases the connection\n            # back to the pool after we're done\n            self.connection = conn\n        conn = cast(Connection, conn)\n\n        try:\n            return await asyncio.shield(\n                conn.retry.call_with_retry(\n                    lambda: execute(conn, stack, raise_on_error),\n                    lambda error: self._disconnect_raise_reset(conn, error),\n                )\n            )\n        except asyncio.CancelledError:\n            # not supposed to be possible, yet here we are\n            await conn.disconnect(nowait=True)\n            raise\n        finally:\n            await self.reset()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _on_ssl_errors(self, error):\n        url = error.url()\n        self._insecure_hosts.add(url.host())\n\n        log.webview.debug(\"Certificate error: {}\".format(error))\n\n        if error.is_overridable():\n            error.ignore = shared.ignore_certificate_errors(\n                url, [error], abort_on=[self.abort_questions])\n        else:\n            log.webview.error(\"Non-overridable certificate error: \"\n                              \"{}\".format(error))\n\n        log.webview.debug(\"ignore {}, URL {}, requested {}\".format(\n            error.ignore, url, self.url(requested=True)))\n\n        # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-56207\n        # We can't really know when to show an error page, as the error might\n        # have happened when loading some resource.\n        # However, self.url() is not available yet and the requested URL\n        # might not match the URL we get from the error - so we just apply a\n        # heuristic here.\n        if (not qtutils.version_check('5.9') and\n                not error.ignore and\n                url.matches(self.url(requested=True), QUrl.RemoveScheme)):\n            self._show_error_page(url, str(error))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def to_yaml(self, **kwargs):\n    \"\"\"Returns a yaml string containing the network configuration.\n\n    Note: Since TF 2.6, this method is no longer supported and will raise a\n    RuntimeError.\n\n    To load a network from a yaml save file, use\n    `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n\n    `custom_objects` should be a dictionary mapping\n    the names of custom losses / layers / etc to the corresponding\n    functions / classes.\n\n    Args:\n        **kwargs: Additional keyword arguments\n            to be passed to `yaml.dump()`.\n\n    Returns:\n        A YAML string.\n\n    Raises:\n        RuntimeError: announces that the method poses a security risk\n    \"\"\"\n    raise RuntimeError(\n        'Method `model.to_yaml()` has been removed due to security risk of '\n        'arbitrary code execution. Please use `model.to_json()` instead.'\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool SetOpAttrList(TFE_Context* ctx, TFE_Op* op, const char* key,\n                   PyObject* py_list, TF_AttrType type,\n                   tensorflow::gtl::FlatMap<string, int64_t>* attr_list_sizes,\n                   TF_Status* status) {\n  if (!PySequence_Check(py_list)) {\n    TF_SetStatus(\n        status, TF_INVALID_ARGUMENT,\n        tensorflow::strings::StrCat(\"Expecting sequence value for attr \", key,\n                                    \", got \", py_list->ob_type->tp_name)\n            .c_str());\n    return false;\n  }\n  const int num_values = PySequence_Size(py_list);\n  if (attr_list_sizes != nullptr) (*attr_list_sizes)[key] = num_values;\n\n#define PARSE_LIST(c_type, parse_fn)                                       \\\n  std::unique_ptr<c_type[]> values(new c_type[num_values]);                \\\n  for (int i = 0; i < num_values; ++i) {                                   \\\n    tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));    \\\n    if (py_value == nullptr) {                                             \\\n      TF_SetStatus(status, TF_INVALID_ARGUMENT,                            \\\n                   tensorflow::strings::StrCat(                            \\\n                       \"Expecting sequence of \" #c_type \" for attr \", key, \\\n                       \", got \", py_list->ob_type->tp_name)                \\\n                       .c_str());                                          \\\n      return false;                                                        \\\n    } else if (!parse_fn(key, py_value.get(), status, &values[i])) {       \\\n      return false;                                                        \\\n    }                                                                      \\\n  }\n\n  if (type == TF_ATTR_STRING) {\n    std::unique_ptr<const void*[]> values(new const void*[num_values]);\n    std::unique_ptr<size_t[]> lengths(new size_t[num_values]);\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::StringPiece value;\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      if (!ParseStringValue(key, py_value.get(), status, &value)) return false;\n      values[i] = value.data();\n      lengths[i] = value.size();\n    }\n    TFE_OpSetAttrStringList(op, key, values.get(), lengths.get(), num_values);\n  } else if (type == TF_ATTR_INT) {\n    PARSE_LIST(int64_t, ParseInt64Value);\n    TFE_OpSetAttrIntList(op, key, values.get(), num_values);\n  } else if (type == TF_ATTR_FLOAT) {\n    PARSE_LIST(float, ParseFloatValue);\n    TFE_OpSetAttrFloatList(op, key, values.get(), num_values);\n  } else if (type == TF_ATTR_BOOL) {\n    PARSE_LIST(unsigned char, ParseBoolValue);\n    TFE_OpSetAttrBoolList(op, key, values.get(), num_values);\n  } else if (type == TF_ATTR_TYPE) {\n    PARSE_LIST(int, ParseTypeValue);\n    TFE_OpSetAttrTypeList(op, key,\n                          reinterpret_cast<const TF_DataType*>(values.get()),\n                          num_values);\n  } else if (type == TF_ATTR_SHAPE) {\n    // Make one pass through the input counting the total number of\n    // dims across all the input lists.\n    int total_dims = 0;\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      if (py_value.get() != Py_None) {\n        if (!PySequence_Check(py_value.get())) {\n          TF_SetStatus(\n              status, TF_INVALID_ARGUMENT,\n              tensorflow::strings::StrCat(\n                  \"Expecting None or sequence value for element\", i,\n                  \" of attr \", key, \", got \", py_value->ob_type->tp_name)\n                  .c_str());\n          return false;\n        }\n        const auto size = TensorShapeNumDims(py_value.get());\n        if (size >= 0) {\n          total_dims += size;\n        }\n      }\n    }\n    // Allocate a buffer that can fit all of the dims together.\n    std::unique_ptr<int64_t[]> buffer(new int64_t[total_dims]);\n    // Copy the input dims into the buffer and set dims to point to\n    // the start of each list's dims.\n    std::unique_ptr<const int64_t*[]> dims(new const int64_t*[num_values]);\n    std::unique_ptr<int[]> num_dims(new int[num_values]);\n    int64_t* offset = buffer.get();\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      if (py_value.get() == Py_None) {\n        dims[i] = nullptr;\n        num_dims[i] = -1;\n      } else {\n        const auto size = TensorShapeNumDims(py_value.get());\n        if (size == -1) {\n          dims[i] = nullptr;\n          num_dims[i] = -1;\n          continue;\n        }\n        dims[i] = offset;\n        num_dims[i] = size;\n        for (int j = 0; j < size; ++j) {\n          tensorflow::Safe_PyObjectPtr inner_py_value(\n              PySequence_ITEM(py_value.get(), j));\n          if (inner_py_value.get() == Py_None) {\n            *offset = -1;\n          } else if (!ParseDimensionValue(key, inner_py_value.get(), status,\n                                          offset)) {\n            return false;\n          }\n          ++offset;\n        }\n      }\n    }\n    TFE_OpSetAttrShapeList(op, key, dims.get(), num_dims.get(), num_values,\n                           status);\n    if (!status->status.ok()) return false;\n  } else if (type == TF_ATTR_FUNC) {\n    std::unique_ptr<const TFE_Op*[]> funcs(new const TFE_Op*[num_values]);\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      // Allow:\n      // (1) String function name, OR\n      // (2) A Python object with a .name attribute\n      //     (A crude test for being a\n      //     tensorflow.python.framework.function._DefinedFunction)\n      //     (which is what the various \"defun\" or \"Defun\" decorators do).\n      // And in the future also allow an object that can encapsulate\n      // the function name and its attribute values.\n      tensorflow::StringPiece func_name;\n      if (!ParseStringValue(key, py_value.get(), status, &func_name)) {\n        PyObject* name_attr = PyObject_GetAttrString(py_value.get(), \"name\");\n        if (name_attr == nullptr ||\n            !ParseStringValue(key, name_attr, status, &func_name)) {\n          TF_SetStatus(\n              status, TF_INVALID_ARGUMENT,\n              tensorflow::strings::StrCat(\n                  \"unable to set function value attribute from a \",\n                  py_value.get()->ob_type->tp_name,\n                  \" object. If you think this is an error, please file an \"\n                  \"issue at \"\n                  \"https://github.com/tensorflow/tensorflow/issues/new\")\n                  .c_str());\n          return false;\n        }\n      }\n      funcs[i] = TFE_NewOp(ctx, func_name.data(), status);\n      if (!status->status.ok()) return false;\n    }\n    TFE_OpSetAttrFunctionList(op, key, funcs.get(), num_values);\n    if (!status->status.ok()) return false;\n  } else {\n    TF_SetStatus(status, TF_UNIMPLEMENTED,\n                 tensorflow::strings::StrCat(\"Attr \", key,\n                                             \" has unhandled list type \", type)\n                     .c_str());\n    return false;\n  }\n#undef PARSE_LIST\n  return true;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static int BuildHuffmanTable(HuffmanCode* const root_table, int root_bits,\n                             const int code_lengths[], int code_lengths_size,\n                             uint16_t sorted[]) {\n  HuffmanCode* table = root_table;  // next available space in table\n  int total_size = 1 << root_bits;  // total size root table + 2nd level table\n  int len;                          // current code length\n  int symbol;                       // symbol index in original or sorted table\n  // number of codes of each length:\n  int count[MAX_ALLOWED_CODE_LENGTH + 1] = { 0 };\n  // offsets in sorted table for each length:\n  int offset[MAX_ALLOWED_CODE_LENGTH + 1];\n\n  assert(code_lengths_size != 0);\n  assert(code_lengths != NULL);\n  assert((root_table != NULL && sorted != NULL) ||\n         (root_table == NULL && sorted == NULL));\n  assert(root_bits > 0);\n\n  // Build histogram of code lengths.\n  for (symbol = 0; symbol < code_lengths_size; ++symbol) {\n    if (code_lengths[symbol] > MAX_ALLOWED_CODE_LENGTH) {\n      return 0;\n    }\n    ++count[code_lengths[symbol]];\n  }\n\n  // Error, all code lengths are zeros.\n  if (count[0] == code_lengths_size) {\n    return 0;\n  }\n\n  // Generate offsets into sorted symbol table by code length.\n  offset[1] = 0;\n  for (len = 1; len < MAX_ALLOWED_CODE_LENGTH; ++len) {\n    if (count[len] > (1 << len)) {\n      return 0;\n    }\n    offset[len + 1] = offset[len] + count[len];\n  }\n\n  // Sort symbols by length, by symbol order within each length.\n  for (symbol = 0; symbol < code_lengths_size; ++symbol) {\n    const int symbol_code_length = code_lengths[symbol];\n    if (code_lengths[symbol] > 0) {\n      if (sorted != NULL) {\n        sorted[offset[symbol_code_length]++] = symbol;\n      } else {\n        offset[symbol_code_length]++;\n      }\n    }\n  }\n\n  // Special case code with only one value.\n  if (offset[MAX_ALLOWED_CODE_LENGTH] == 1) {\n    if (sorted != NULL) {\n      HuffmanCode code;\n      code.bits = 0;\n      code.value = (uint16_t)sorted[0];\n      ReplicateValue(table, 1, total_size, code);\n    }\n    return total_size;\n  }\n\n  {\n    int step;              // step size to replicate values in current table\n    uint32_t low = 0xffffffffu;        // low bits for current root entry\n    uint32_t mask = total_size - 1;    // mask for low bits\n    uint32_t key = 0;      // reversed prefix code\n    int num_nodes = 1;     // number of Huffman tree nodes\n    int num_open = 1;      // number of open branches in current tree level\n    int table_bits = root_bits;        // key length of current table\n    int table_size = 1 << table_bits;  // size of current table\n    symbol = 0;\n    // Fill in root table.\n    for (len = 1, step = 2; len <= root_bits; ++len, step <<= 1) {\n      num_open <<= 1;\n      num_nodes += num_open;\n      num_open -= count[len];\n      if (num_open < 0) {\n        return 0;\n      }\n      if (root_table == NULL) continue;\n      for (; count[len] > 0; --count[len]) {\n        HuffmanCode code;\n        code.bits = (uint8_t)len;\n        code.value = (uint16_t)sorted[symbol++];\n        ReplicateValue(&table[key], step, table_size, code);\n        key = GetNextKey(key, len);\n      }\n    }\n\n    // Fill in 2nd level tables and add pointers to root table.\n    for (len = root_bits + 1, step = 2; len <= MAX_ALLOWED_CODE_LENGTH;\n         ++len, step <<= 1) {\n      num_open <<= 1;\n      num_nodes += num_open;\n      num_open -= count[len];\n      if (num_open < 0) {\n        return 0;\n      }\n      for (; count[len] > 0; --count[len]) {\n        HuffmanCode code;\n        if ((key & mask) != low) {\n          if (root_table != NULL) table += table_size;\n          table_bits = NextTableBitSize(count, len, root_bits);\n          table_size = 1 << table_bits;\n          total_size += table_size;\n          low = key & mask;\n          if (root_table != NULL) {\n            root_table[low].bits = (uint8_t)(table_bits + root_bits);\n            root_table[low].value = (uint16_t)((table - root_table) - low);\n          }\n        }\n        if (root_table != NULL) {\n          code.bits = (uint8_t)(len - root_bits);\n          code.value = (uint16_t)sorted[symbol++];\n          ReplicateValue(&table[key >> root_bits], step, table_size, code);\n        }\n        key = GetNextKey(key, len);\n      }\n    }\n\n    // Check if tree is full.\n    if (num_nodes != 2 * offset[MAX_ALLOWED_CODE_LENGTH] - 1) {\n      return 0;\n    }\n  }\n\n  return total_size;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get(self, key, default=None, version=None):\n        key = self.make_key(key, version=version)\n        self.validate_key(key)\n        val = self._cache.get(key)\n        if val is None:\n            return default\n        return val\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def load(self):\n\n        if len(self.tile) != 1 or self.tile[0][0] != \"iptc\":\n            return ImageFile.ImageFile.load(self)\n\n        type, tile, box = self.tile[0]\n\n        encoding, offset = tile\n\n        self.fp.seek(offset)\n\n        # Copy image data to temporary file\n        o_fd, outfile = tempfile.mkstemp(text=False)\n        o = os.fdopen(o_fd)\n        if encoding == \"raw\":\n            # To simplify access to the extracted file,\n            # prepend a PPM header\n            o.write(\"P5\\n%d %d\\n255\\n\" % self.size)\n        while True:\n            type, size = self.field()\n            if type != (8, 10):\n                break\n            while size > 0:\n                s = self.fp.read(min(size, 8192))\n                if not s:\n                    break\n                o.write(s)\n                size = size - len(s)\n        o.close()\n\n        try:\n            try:\n                # fast\n                self.im = Image.core.open_ppm(outfile)\n            except:\n                # slightly slower\n                im = Image.open(outfile)\n                im.load()\n                self.im = im.im\n        finally:\n            try: os.unlink(outfile)\n            except: pass\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& indices = context->input(0);\n    const Tensor& values = context->input(1);\n    const Tensor& shape = context->input(2);\n    const Tensor& weights = context->input(3);\n    bool use_weights = weights.NumElements() > 0;\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(indices.shape()),\n                errors::InvalidArgument(\n                    \"Input indices must be a 2-dimensional tensor. Got: \",\n                    indices.shape().DebugString()));\n\n    if (use_weights) {\n      OP_REQUIRES(\n          context, weights.shape() == values.shape(),\n          errors::InvalidArgument(\n              \"Weights and values must have the same shape. Weight shape: \",\n              weights.shape().DebugString(),\n              \"; values shape: \", values.shape().DebugString()));\n    }\n\n    bool is_1d = shape.NumElements() == 1;\n    int num_batches = is_1d ? 1 : shape.flat<int64>()(0);\n    int num_values = values.NumElements();\n\n    OP_REQUIRES(context, num_values == indices.shape().dim_size(0),\n                errors::InvalidArgument(\n                    \"Number of values must match first dimension of indices.\",\n                    \"Got \", num_values,\n                    \" values, indices shape: \", indices.shape().DebugString()));\n\n    const auto indices_values = indices.matrix<int64>();\n    const auto values_values = values.flat<T>();\n    const auto weight_values = weights.flat<W>();\n\n    auto per_batch_counts = BatchedMap<W>(num_batches);\n\n    T max_value = 0;\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      int batch = is_1d ? 0 : indices_values(idx, 0);\n      const auto& value = values_values(idx);\n      if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {\n        if (binary_output_) {\n          per_batch_counts[batch][value] = 1;\n        } else if (use_weights) {\n          per_batch_counts[batch][value] += weight_values(idx);\n        } else {\n          per_batch_counts[batch][value]++;\n        }\n        if (value > max_value) {\n          max_value = value;\n        }\n      }\n    }\n\n    int num_output_values = GetOutputSize(max_value, maxlength_, minlength_);\n    OP_REQUIRES_OK(context, OutputSparse<W>(per_batch_counts, num_output_values,\n                                            is_1d, context));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "pyi_get_temp_path(char *buffer, char *runtime_tmpdir)\n{\n    int i;\n    wchar_t *wchar_ret;\n    wchar_t prefix[16];\n    wchar_t wchar_buffer[PATH_MAX];\n    char *original_tmpdir;\n    char runtime_tmpdir_abspath[PATH_MAX + 1];\n\n    if (runtime_tmpdir != NULL) {\n      /*\n       * Get original TMP environment variable so it can be restored\n       * after this is done.\n       */\n      original_tmpdir = pyi_getenv(\"TMP\");\n      /*\n       * Set TMP to runtime_tmpdir for _wtempnam() later\n       */\n      pyi_path_fullpath(runtime_tmpdir_abspath, PATH_MAX, runtime_tmpdir);\n      pyi_setenv(\"TMP\", runtime_tmpdir_abspath);\n    }\n\n    GetTempPathW(PATH_MAX, wchar_buffer);\n\n    swprintf(prefix, 16, L\"_MEI%d\", getpid());\n\n    /*\n     * Windows does not have a race-free function to create a temporary\n     * directory. Thus, we rely on _tempnam, and simply try several times\n     * to avoid stupid race conditions.\n     */\n    for (i = 0; i < 5; i++) {\n        /* TODO use race-free fuction - if any exists? */\n        wchar_ret = _wtempnam(wchar_buffer, prefix);\n\n        if (pyi_win32_mkdir(wchar_ret) == 0) {\n            pyi_win32_utils_to_utf8(buffer, wchar_ret, PATH_MAX);\n            free(wchar_ret);\n            if (runtime_tmpdir != NULL) {\n              /*\n               * Restore TMP to what it was\n               */\n              if (original_tmpdir != NULL) {\n                pyi_setenv(\"TMP\", original_tmpdir);\n                free(original_tmpdir);\n              } else {\n                pyi_unsetenv(\"TMP\");\n              }\n            }\n            return 1;\n        }\n        free(wchar_ret);\n    }\n    if (runtime_tmpdir != NULL) {\n      /*\n       * Restore TMP to what it was\n       */\n      if (original_tmpdir != NULL) {\n        pyi_setenv(\"TMP\", original_tmpdir);\n        free(original_tmpdir);\n      } else {\n        pyi_unsetenv(\"TMP\");\n      }\n    }\n    return 0;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n  int num_inputs = node->inputs->size;\n  // The number of outputs should be the same as number of inputs.\n  TF_LITE_ENSURE_EQ(context, node->outputs->size, num_inputs);\n\n  // Check subgraph indices and get subgraphs.\n  Subgraph* this_subgraph = reinterpret_cast<Subgraph*>(context->impl_);\n  auto* subgraphs = this_subgraph->GetSubgraphs();\n  TF_LITE_ENSURE(context, op_data->cond_subgraph_index < subgraphs->size());\n  TF_LITE_ENSURE(context, op_data->body_subgraph_index < subgraphs->size());\n  TF_LITE_ENSURE(context,\n                 op_data->cond_subgraph_index != op_data->body_subgraph_index);\n\n  Subgraph* cond_subgraph = (*subgraphs)[op_data->cond_subgraph_index].get();\n  Subgraph* body_subgraph = (*subgraphs)[op_data->body_subgraph_index].get();\n\n  // Check input & output count of the condition subgraph.\n  TF_LITE_ENSURE_EQ(context, cond_subgraph->inputs().size(), num_inputs);\n  TF_LITE_ENSURE_EQ(context, cond_subgraph->outputs().size(), 1);\n\n  // Check input & output count of the body subgraph.\n  TF_LITE_ENSURE_EQ(context, body_subgraph->inputs().size(), num_inputs);\n  TF_LITE_ENSURE_EQ(context, body_subgraph->outputs().size(), num_inputs);\n\n  // Prepare and check the condition subgraph.\n  TF_LITE_ENSURE_OK(\n      context, CopyTensorsShapeAndType(\n                   context, this_subgraph, TfLiteIntArrayView(node->inputs),\n                   cond_subgraph, cond_subgraph->inputs(), true));\n  TF_LITE_ENSURE_OK(context, cond_subgraph->AllocateTensors());\n  TfLiteTensor* cond_output =\n      cond_subgraph->tensor(cond_subgraph->outputs()[0]);\n  // This should rarely happens. In most cases the output is static with shape\n  // [1]. However theoretically intermediate tensors in the cond subgraph\n  // can be dynamic.\n  if (IsDynamicTensor(cond_output)) {\n    op_data->cond_has_dynamic_output_tensors = true;\n  } else {\n    TF_LITE_ENSURE_STATUS(CheckCondOutput(context, cond_output));\n  }\n\n  // Prepare and check the body subgraph.\n  TF_LITE_ENSURE_OK(\n      context, CopyTensorsShapeAndType(\n                   context, this_subgraph, TfLiteIntArrayView(node->inputs),\n                   body_subgraph, body_subgraph->inputs(), true));\n  TF_LITE_ENSURE_OK(context, body_subgraph->AllocateTensors());\n  if (body_subgraph->HasDynamicTensors()) {\n    op_data->body_has_dynamic_output_tensors = true;\n  } else {\n    for (int i = 0; i < num_inputs; ++i) {\n      TfLiteTensor* body_input =\n          body_subgraph->tensor(body_subgraph->inputs()[i]);\n      TfLiteTensor* body_output =\n          body_subgraph->tensor(body_subgraph->outputs()[i]);\n      TF_LITE_ENSURE_TYPES_EQ(context, body_input->type, body_output->type);\n\n      TF_LITE_ENSURE(context, !IsDynamicTensor(body_output));\n      if (!TfLiteIntArrayEqual(body_input->dims, body_output->dims)) {\n        // If the output shape of the body subgraph is static w.r.t. a fixed\n        // input size, but it's different from input size, it's still considered\n        // dynamic. For example: If a subgraph keeps padding its input with a\n        // fixed padding, the output shape is static w.r.t the input shape and\n        // padding, but running it in a loop will keep bloating the tensor.\n        op_data->body_has_dynamic_output_tensors = true;\n        break;\n      }\n    }\n  }\n  for (int i = 0; i < num_inputs; ++i) {\n    TfLiteTensor* output;\n    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, i, &output));\n    if (op_data->body_has_dynamic_output_tensors) {\n      SetTensorToDynamic(output);\n    } else {\n      TfLiteTensor* body_output =\n          body_subgraph->tensor(body_subgraph->outputs()[i]);\n      TfLiteIntArray* output_size = TfLiteIntArrayCopy(body_output->dims);\n      TF_LITE_ENSURE_OK(context,\n                        context->ResizeTensor(context, output, output_size));\n    }\n  }\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def save(self) -> FlaskResponse:\n        data = request.form.get(\"data\")\n        if not isinstance(data, str):\n            return json_error_response(_(\"Request missing data field.\"), status=500)\n\n        datasource_dict = json.loads(data)\n        datasource_id = datasource_dict.get(\"id\")\n        datasource_type = datasource_dict.get(\"type\")\n        database_id = datasource_dict[\"database\"].get(\"id\")\n        orm_datasource = ConnectorRegistry.get_datasource(\n            datasource_type, datasource_id, db.session\n        )\n        orm_datasource.database_id = database_id\n\n        if \"owners\" in datasource_dict and orm_datasource.owner_class is not None:\n            # Check ownership\n            try:\n                check_ownership(orm_datasource)\n            except SupersetSecurityException as ex:\n                raise DatasetForbiddenError() from ex\n\n        user = security_manager.get_user_by_id(g.user.id)\n        datasource_dict[\"owners\"] = populate_owners(\n            user, datasource_dict[\"owners\"], default_to_user=False\n        )\n\n        duplicates = [\n            name\n            for name, count in Counter(\n                [col[\"column_name\"] for col in datasource_dict[\"columns\"]]\n            ).items()\n            if count > 1\n        ]\n        if duplicates:\n            return json_error_response(\n                _(\n                    \"Duplicate column name(s): %(columns)s\",\n                    columns=\",\".join(duplicates),\n                ),\n                status=409,\n            )\n        orm_datasource.update_from_object(datasource_dict)\n        data = orm_datasource.data\n        db.session.commit()\n\n        return self.json_response(sanitize_datasource_data(data))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& a = context->input(0);\n    const Tensor& b = context->input(1);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(2).shape()),\n                errors::InvalidArgument(\"min_a must be a scalar, but got shape\",\n                                        context->input(2).shape()));\n    const float min_a = context->input(2).flat<float>()(0);\n    OP_REQUIRES(context, context->input(3).NumElements() == 1,\n                errors::InvalidArgument(\"max_a must be a scalar, but got shape\",\n                                        context->input(3).shape()));\n    const float max_a = context->input(3).flat<float>()(0);\n    OP_REQUIRES(context, context->input(4).NumElements() == 1,\n                errors::InvalidArgument(\"min_b must be a scalar, but got shape\",\n                                        context->input(4).shape()));\n    const float min_b = context->input(4).flat<float>()(0);\n    OP_REQUIRES(context, context->input(5).NumElements() == 1,\n                errors::InvalidArgument(\"max_b must be a scalar, but got shape\",\n                                        context->input(5).shape()));\n    const float max_b = context->input(5).flat<float>()(0);\n\n    // Make sure that we have valid quantization ranges for the input buffers.\n    // If the difference between the min and max is negative or zero, it makes\n    // it hard to do meaningful intermediate operations on the values.\n    OP_REQUIRES(context, (max_a > min_a),\n                errors::InvalidArgument(\"max_a must be larger than min_a.\"));\n    OP_REQUIRES(context, (max_b > min_b),\n                errors::InvalidArgument(\"max_b must be larger than min_b.\"));\n    const int32_t offset_a = FloatToQuantizedUnclamped<T1>(0.0f, min_a, max_a);\n    const int32_t offset_b = FloatToQuantizedUnclamped<T2>(0.0f, min_b, max_b);\n    const int32_t offset_c = 0;\n    const int32_t mult_c = 1;\n    const int32_t shift_c = 0;\n\n    // Check that the dimensions of the two matrices are valid.\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(a.shape()),\n                errors::InvalidArgument(\"In[0] is not a matrix\"));\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(b.shape()),\n                errors::InvalidArgument(\"In[1] is not a matrix\"));\n    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n    dim_pair[0].first = transpose_a_ ? 0 : 1;\n    dim_pair[0].second = transpose_b_ ? 1 : 0;\n\n    OP_REQUIRES(context,\n                a.dim_size(dim_pair[0].first) == b.dim_size(dim_pair[0].second),\n                errors::InvalidArgument(\"Matrix size-incompatible: In[0]: \",\n                                        a.shape().DebugString(),\n                                        \", In[1]: \", b.shape().DebugString()));\n\n    OP_REQUIRES(context, ((shift_c >= 0) && (shift_c <= 31)),\n                errors::InvalidArgument(\"shift_c must be between 0 and 31, \"\n                                        \"inclusive.\"));\n\n    int a_dim_remaining = 1 - dim_pair[0].first;\n    int b_dim_remaining = 1 - dim_pair[0].second;\n    TensorShape out_shape(\n        {a.dim_size(a_dim_remaining), b.dim_size(b_dim_remaining)});\n    Tensor* c = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &c));\n    CHECK(c);\n\n    const T1* a_data = a.flat<T1>().data();\n    const T2* b_data = b.flat<T2>().data();\n    Toutput* c_data = c->flat<Toutput>().data();\n\n    const bool transpose_c = false;\n    const size_t m = a.dim_size(a_dim_remaining);\n    const size_t n = b.dim_size(b_dim_remaining);\n    const size_t k = a.dim_size(dim_pair[0].first);\n    const size_t lda = a.dim_size(1);\n    const size_t ldb = b.dim_size(1);\n    const size_t ldc = n;\n\n    if (meta::IsSupportedAndEnabled() && std::is_same<T1, quint8>() &&\n        std::is_same<T2, quint8>() && std::is_same<Toutput, qint32>() &&\n        (offset_c == 0) && (mult_c == 1) && (shift_c == 0) &&\n        (transpose_c == false) && (k <= 2048)) {\n      // Gemmlowp/meta code path works on 32 & 64 bit Arm with NEON Simd and\n      // allows optimized quantized 8bit to 32bit gemm.\n      meta::QuantizedGemm(context, transpose_a_, transpose_b_, a_data, b_data,\n                          c_data, m, n, k, -offset_a, -offset_b, lda, ldb, ldc);\n    } else if (std::is_same<T1, quint8>() && std::is_same<T2, quint8>() &&\n               std::is_same<Toutput, qint32>() && (offset_c == 0) &&\n               (mult_c == 1) && (shift_c == 0) && (transpose_c == false)) {\n      // The gemmlowp optimized library only works for a particular set of data\n      // types, so check if we meet those requirements and fall back to a slower\n      // reference implementation if not.\n      if (transpose_a_) {\n        if (transpose_b_) {\n          GemmlowpMultiply<true, true, false>(context, a_data, b_data, c_data,\n                                              m, n, k, offset_a, offset_b, lda,\n                                              ldb, ldc);\n        } else {\n          GemmlowpMultiply<true, false, false>(context, a_data, b_data, c_data,\n                                               m, n, k, offset_a, offset_b, lda,\n                                               ldb, ldc);\n        }\n      } else {\n        if (transpose_b_) {\n          GemmlowpMultiply<false, true, false>(context, a_data, b_data, c_data,\n                                               m, n, k, offset_a, offset_b, lda,\n                                               ldb, ldc);\n        } else {\n          GemmlowpMultiply<false, false, false>(context, a_data, b_data, c_data,\n                                                m, n, k, offset_a, offset_b,\n                                                lda, ldb, ldc);\n        }\n      }\n    } else {\n      ReferenceGemm<T1, T2, Toutput>(\n          transpose_a_, transpose_b_, transpose_c, m, n, k, a_data, offset_a,\n          lda, b_data, offset_b, ldb, c_data, shift_c, offset_c, mult_c, ldc);\n    }\n\n    float min_c_value;\n    float max_c_value;\n    QuantizationRangeForMultiplication<T1, T2, Toutput>(\n        min_a, max_a, min_b, max_b, &min_c_value, &max_c_value);\n    Tensor* c_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &c_min));\n    c_min->flat<float>()(0) = min_c_value;\n\n    Tensor* c_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &c_max));\n    c_max->flat<float>()(0) = max_c_value;\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def run_command(cmd, log_output=False):\n    \"\"\"\n    Return (exitcode, output) of executing the provided `cmd` in a shell.\n    `cmd` can be provided as a string or as a list of arguments.\n\n    If `log_output` is True, the stdout and stderr of the process will be captured\n    and streamed to the `logger`.\n    \"\"\"\n    if isinstance(cmd, list):\n        cmd = \" \".join(cmd)\n\n    if not log_output:\n        exitcode, output = subprocess.getstatusoutput(cmd)\n        return exitcode, output\n\n    process = subprocess.Popen(\n        cmd,\n        shell=False,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        universal_newlines=True,\n    )\n\n    while _stream_process(process):\n        sleep(1)\n\n    exitcode = process.poll()\n    return exitcode, \"\"\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, sydent):\n        self.sydent = sydent\n\n        root = Resource()\n        matrix = Resource()\n        identity = Resource()\n        api = Resource()\n        v1 = self.sydent.servlets.v1\n        v2 = self.sydent.servlets.v2\n\n        validate = Resource()\n        validate_v2 = Resource()\n        email = Resource()\n        email_v2 = Resource()\n        msisdn = Resource()\n        msisdn_v2 = Resource()\n\n        threepid_v1 = Resource()\n        threepid_v2 = Resource()\n        unbind = self.sydent.servlets.threepidUnbind\n\n        pubkey = Resource()\n        ephemeralPubkey = Resource()\n\n        root.putChild(b'_matrix', matrix)\n        matrix.putChild(b'identity', identity)\n        identity.putChild(b'api', api)\n        identity.putChild(b'v2', v2)\n        api.putChild(b'v1', v1)\n\n        validate.putChild(b'email', email)\n        validate.putChild(b'msisdn', msisdn)\n\n        validate_v2.putChild(b'email', email_v2)\n        validate_v2.putChild(b'msisdn', msisdn_v2)\n\n        v1.putChild(b'validate', validate)\n\n        v1.putChild(b'lookup', self.sydent.servlets.lookup)\n        v1.putChild(b'bulk_lookup', self.sydent.servlets.bulk_lookup)\n\n        v1.putChild(b'pubkey', pubkey)\n        pubkey.putChild(b'isvalid', self.sydent.servlets.pubkeyIsValid)\n        pubkey.putChild(b'ed25519:0', self.sydent.servlets.pubkey_ed25519)\n        pubkey.putChild(b'ephemeral', ephemeralPubkey)\n        ephemeralPubkey.putChild(b'isvalid', self.sydent.servlets.ephemeralPubkeyIsValid)\n\n        threepid_v2.putChild(b'getValidated3pid', self.sydent.servlets.getValidated3pidV2)\n        threepid_v2.putChild(b'bind', self.sydent.servlets.threepidBindV2)\n        threepid_v2.putChild(b'unbind', unbind)\n\n        threepid_v1.putChild(b'getValidated3pid', self.sydent.servlets.getValidated3pid)\n        threepid_v1.putChild(b'unbind', unbind)\n        if self.sydent.enable_v1_associations:\n            threepid_v1.putChild(b'bind', self.sydent.servlets.threepidBind)\n\n        v1.putChild(b'3pid', threepid_v1)\n\n        email.putChild(b'requestToken', self.sydent.servlets.emailRequestCode)\n        email.putChild(b'submitToken', self.sydent.servlets.emailValidate)\n\n        email_v2.putChild(b'requestToken', self.sydent.servlets.emailRequestCodeV2)\n        email_v2.putChild(b'submitToken', self.sydent.servlets.emailValidateV2)\n\n        msisdn.putChild(b'requestToken', self.sydent.servlets.msisdnRequestCode)\n        msisdn.putChild(b'submitToken', self.sydent.servlets.msisdnValidate)\n\n        msisdn_v2.putChild(b'requestToken', self.sydent.servlets.msisdnRequestCodeV2)\n        msisdn_v2.putChild(b'submitToken', self.sydent.servlets.msisdnValidateV2)\n\n        v1.putChild(b'store-invite', self.sydent.servlets.storeInviteServlet)\n\n        v1.putChild(b'sign-ed25519', self.sydent.servlets.blindlySignStuffServlet)\n\n        # v2\n        # note v2 loses the /api so goes on 'identity' not 'api'\n        identity.putChild(b'v2', v2)\n\n        # v2 exclusive APIs\n        v2.putChild(b'terms', self.sydent.servlets.termsServlet)\n        account = self.sydent.servlets.accountServlet\n        v2.putChild(b'account', account)\n        account.putChild(b'register', self.sydent.servlets.registerServlet)\n        account.putChild(b'logout', self.sydent.servlets.logoutServlet)\n\n        # v2 versions of existing APIs\n        v2.putChild(b'validate', validate_v2)\n        v2.putChild(b'pubkey', pubkey)\n        v2.putChild(b'3pid', threepid_v2)\n        v2.putChild(b'store-invite', self.sydent.servlets.storeInviteServletV2)\n        v2.putChild(b'sign-ed25519', self.sydent.servlets.blindlySignStuffServletV2)\n        v2.putChild(b'lookup', self.sydent.servlets.lookup_v2)\n        v2.putChild(b'hash_details', self.sydent.servlets.hash_details)\n\n        self.factory = Site(root)\n        self.factory.requestFactory = SizeLimitingRequest\n        self.factory.displayTracebacks = False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def logout(self, message, relaystate=None, samlresponse=None):\n        \"\"\"\n        Handle HTTP logout. The supported logout methods are stored\n        in each session. First all the SOAP sessions are logged out\n        then the HTTP Redirect method is used for any remaining\n        sessions.\n\n        The basic process is this:\n         1. A logout request is received. It is processed and the response\n            cached.\n         2. If any other SP's have also logged in as this user then the\n            first such session is popped off and a logout request is\n            generated and forwarded to the SP.\n         3. If a logout response is received then the user is marked\n            as logged out from that SP.\n         Repeat steps 2-3 until only the initial logout request is\n         left unhandled, at which time the pre-generated response is sent\n         back to the SP that originated the logout request.\n\n        The final logout response is always a redirect.\n        \"\"\"\n        logout = self.cfg.idp.get_logout_handler()\n\n        us = UserSession()\n\n        saml_sessions = self.cfg.idp.sessionfactory\n\n        logout_type = None\n        try:\n            if lasso.SAML2_FIELD_REQUEST in message:\n                logout_type = \"request\"\n                self._handle_logout_request(us, logout, saml_sessions,\n                                            message)\n            elif samlresponse:\n                logout_type = \"response\"\n                self._handle_logout_response(us, logout, saml_sessions,\n                                             message, samlresponse)\n            else:\n                raise cherrypy.HTTPError(400, 'Bad Request. Not a ' +\n                                         'logout request or response.')\n        except InvalidRequest as e:\n            raise cherrypy.HTTPError(400, 'Bad Request. %s' % e)\n        except UnknownProvider as e:\n            raise cherrypy.HTTPError(\n                400, 'Invalid logout %s: %s' % (logout_type, e)\n            )\n\n        # Fall through to handle any remaining sessions.\n\n        # Find the next SP to logout and send a LogoutRequest\n        logout_order = [\n            lasso.SAML2_METADATA_BINDING_SOAP,\n            lasso.SAML2_METADATA_BINDING_REDIRECT,\n        ]\n        (logout_mech, session) = saml_sessions.get_next_logout(\n            logout_mechs=logout_order, user=us.user)\n        while session:\n            self.debug('Going to log out %s' % session.provider_id)\n\n            try:\n                logout.setSessionFromDump(session.login_session)\n            except lasso.ProfileBadSessionDumpError as e:\n                self.error('Failed to load session: %s' % e)\n                raise cherrypy.HTTPRedirect(400, 'Failed to log out user: %s '\n                                            % e)\n            try:\n                if logout_mech == lasso.SAML2_METADATA_BINDING_REDIRECT:\n                    logout.initRequest(session.provider_id,\n                                       lasso.HTTP_METHOD_REDIRECT)\n                else:\n                    logout.initRequest(session.provider_id,\n                                       lasso.HTTP_METHOD_SOAP)\n            except lasso.ServerProviderNotFoundError:\n                self.error(\n                    'Service Provider %s not found. Trying next session' %\n                    session.provider_id\n                )\n                saml_sessions.remove_session(session)\n                (logout_mech, session) = saml_sessions.get_next_logout(\n                    logout_mechs=logout_order, user=us.user)\n                continue\n\n            try:\n                logout.buildRequestMsg()\n            except lasso.Error, e:\n                self.error('failure to build logout request msg: %s' % e)\n                raise cherrypy.HTTPRedirect(400, 'Failed to log out user: %s '\n                                            % e)\n\n            # Set the full list of session indexes for this provider to\n            # log out\n            self.debug('logging out provider id %s' % session.provider_id)\n            indexes = saml_sessions.get_session_id_by_provider_id(\n                session.provider_id, us.user\n            )\n            self.debug('Requesting logout for sessions %s' % (indexes,))\n            req = logout.get_request()\n            req.setSessionIndexes(indexes)\n\n            session.set_logoutstate(relaystate=logout.msgUrl,\n                                    request_id=logout.request.id)\n            saml_sessions.start_logout(session, initial=False)\n\n            self.debug('Request logout ID %s for session ID %s' %\n                       (logout.request.id, session.session_id))\n\n            if logout_mech == lasso.SAML2_METADATA_BINDING_REDIRECT:\n                self.debug('Redirecting to another SP to logout on %s at %s' %\n                           (logout.remoteProviderId, logout.msgUrl))\n                raise cherrypy.HTTPRedirect(logout.msgUrl)\n            else:\n                self.debug('SOAP request to another SP to logout on %s at %s' %\n                           (logout.remoteProviderId, logout.msgUrl))\n                if logout.msgBody:\n                    message = self._soap_logout(logout)\n                    try:\n                        self._handle_logout_response(us,\n                                                     logout,\n                                                     saml_sessions,\n                                                     message,\n                                                     samlresponse)\n                    except Exception as e:  # pylint: disable=broad-except\n                        self.error('SOAP SLO failed %s' % e)\n                else:\n                    self.error('Provider does not support SOAP')\n\n            (logout_mech, session) = saml_sessions.get_next_logout(\n                logout_mechs=logout_order, user=us.user)\n\n        # done while\n\n        # All sessions should be logged out now. Respond to the\n        # original request using the response we cached earlier.\n\n        try:\n            session = saml_sessions.get_initial_logout(us.user)\n        except ValueError:\n            self.debug('SLO get_last_session() unable to find last session')\n            raise cherrypy.HTTPError(400, 'Unable to determine logout state')\n\n        redirect = session.relaystate\n        if not redirect:\n            redirect = self.basepath\n\n        saml_sessions.remove_session(session)\n\n        # Log out of cherrypy session\n        user = us.get_user()\n        self._audit('Logged out user: %s [%s] from %s' %\n                    (user.name, user.fullname,\n                     session.provider_id))\n        us.logout(user)\n\n        self.debug('SLO redirect to %s' % redirect)\n\n        raise cherrypy.HTTPRedirect(redirect)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def create_class_from_xml_string(target_class, xml_string):\n    \"\"\"Creates an instance of the target class from a string.\n\n    :param target_class: The class which will be instantiated and populated\n        with the contents of the XML. This class must have a c_tag and a\n        c_namespace class variable.\n    :param xml_string: A string which contains valid XML. The root element\n        of the XML string should match the tag and namespace of the desired\n        class.\n\n    :return: An instance of the target class with members assigned according to\n        the contents of the XML - or None if the root XML tag and namespace did\n        not match those of the target class.\n    \"\"\"\n    if not isinstance(xml_string, six.binary_type):\n        xml_string = xml_string.encode('utf-8')\n    tree = defusedxml.ElementTree.fromstring(xml_string)\n    return create_class_from_element_tree(target_class, tree)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  public static CloseableHttpClient createClient(int maxConnections, int timeoutMilliseconds) {\n    return createClient(\n        maxConnections,\n        timeoutMilliseconds,\n        NO_PROXY,\n        NO_STORE,\n        true,\n        NetworkAddressRules.ALLOW_ALL);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def create_trustee_and_trust(osc, cluster):\n    try:\n        password = utils.generate_password(length=18)\n\n        trustee = osc.keystone().create_trustee(\n            \"%s_%s\" % (cluster.uuid, cluster.project_id),\n            password,\n        )\n\n        cluster.trustee_username = trustee.name\n        cluster.trustee_user_id = trustee.id\n        cluster.trustee_password = password\n\n        trust = osc.keystone().create_trust(\n            cluster.trustee_user_id)\n        cluster.trust_id = trust.id\n\n    except Exception:\n        LOG.exception(\n            _LE('Failed to create trustee and trust for Cluster: %s'),\n            cluster.uuid)\n        raise exception.TrusteeOrTrustToClusterFailed(\n            cluster_uuid=cluster.uuid)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def check_table_permission(cls, user, table, permission):\n        \"\"\"Check if the provided user has the specified permission against the table.\"\"\"\n\n        # Superuser knows no bounds\n        if user.is_superuser:\n            return True\n\n        # If the table does *not* require permissions\n        if table in cls.RULESET_IGNORE:\n            return True\n\n        # Work out which roles touch the given table\n        for role in cls.RULESET_NAMES:\n            if table in cls.RULESET_MODELS[role]:\n\n                if check_user_role(user, role, permission):\n                    return True\n\n        # Check for children models which inherits from parent role\n        for (parent, child) in cls.RULESET_CHANGE_INHERIT:\n            # Get child model name\n            parent_child_string = f'{parent}_{child}'\n\n            if parent_child_string == table:\n                # Check if parent role has change permission\n                if check_user_role(user, parent, 'change'):\n                    return True\n\n        # Print message instead of throwing an error\n        name = getattr(user, 'name', user.pk)\n\n        logger.info(f\"User '{name}' failed permission check for {table}.{permission}\")\n\n        return False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def on_send_join_request(\n        self, origin: str, content: JsonDict\n    ) -> Dict[str, Any]:\n        logger.debug(\"on_send_join_request: content: %s\", content)\n\n        assert_params_in_dict(content, [\"room_id\"])\n        room_version = await self.store.get_room_version(content[\"room_id\"])\n        pdu = event_from_pdu_json(content, room_version)\n\n        origin_host, _ = parse_server_name(origin)\n        await self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_join_request: pdu sigs: %s\", pdu.signatures)\n\n        pdu = await self._check_sigs_and_hash(room_version, pdu)\n\n        res_pdus = await self.handler.on_send_join_request(origin, pdu)\n        time_now = self._clock.time_msec()\n        return {\n            \"state\": [p.get_pdu_json(time_now) for p in res_pdus[\"state\"]],\n            \"auth_chain\": [p.get_pdu_json(time_now) for p in res_pdus[\"auth_chain\"]],\n        }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_model_version_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    name = request_dict.get(\"name\")\n    version = request_dict.get(\"version\")\n    path = request_dict[\"path\"]\n    path = validate_path_is_safe(path)\n    artifact_uri = _get_model_registry_store().get_model_version_download_uri(name, version)\n    if _is_servable_proxied_run_artifact_root(artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_repo = get_artifact_repository(artifact_uri)\n        artifact_path = path\n\n    return _send_artifact(artifact_repo, artifact_path)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) final {\n    const Tensor& superdiag = context->input(0);\n    const Tensor& maindiag = context->input(1);\n    const Tensor& subdiag = context->input(2);\n    const Tensor& rhs = context->input(3);\n\n    const int ndims = rhs.dims();\n    OP_REQUIRES(\n        context, ndims >= 2,\n        errors::InvalidArgument(\"Input must have rank >= 2, but got \", ndims));\n    OP_REQUIRES_OK(context, ValidateInputTensor(superdiag, \"superdiag\", rhs));\n    OP_REQUIRES_OK(context, ValidateInputTensor(maindiag, \"maindiag\", rhs));\n    OP_REQUIRES_OK(context, ValidateInputTensor(subdiag, \"subdiag\", rhs));\n    int64 batch_size = 1;\n    for (int i = 0; i < ndims - 2; i++) {\n      batch_size *= rhs.dim_size(i);\n    }\n    const int m = rhs.dim_size(ndims - 2);\n    const int n = rhs.dim_size(ndims - 1);\n\n    // Allocate output.\n    Tensor* output;\n    OP_REQUIRES_OK(context, context->allocate_output(0, rhs.shape(), &output));\n\n    const Eigen::GpuDevice& device = context->eigen_device<Eigen::GpuDevice>();\n    GpuLaunchConfig cfg = GetGpuLaunchConfig(1, device);\n    TF_CHECK_OK(GpuLaunchKernel(\n        TridiagonalMatMulKernel<Scalar>, cfg.block_count, cfg.thread_per_block,\n        0, device.stream(), batch_size, m, n, superdiag.flat<Scalar>().data(),\n        maindiag.flat<Scalar>().data(), subdiag.flat<Scalar>().data(),\n        rhs.flat<Scalar>().data(), output->flat<Scalar>().data()));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create_service(self, context, OS_KSADM_service):\n        self.assert_admin(context)\n        service_id = uuid.uuid4().hex\n        service_ref = OS_KSADM_service.copy()\n        service_ref['id'] = service_id\n        new_service_ref = self.catalog_api.create_service(\n                context, service_id, service_ref)\n        return {'OS-KSADM:service': new_service_ref}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void operator()(const CPUDevice& d, typename TTypes<T, 4>::ConstTensor input,\n                  typename TTypes<T, 3>::ConstTensor filter,\n                  typename TTypes<T, 4>::ConstTensor out_backprop,\n                  int stride_rows, int stride_cols, int rate_rows,\n                  int rate_cols, int pad_top, int pad_left,\n                  typename TTypes<T, 3>::Tensor filter_backprop) {\n    const int batch = input.dimension(0);\n    const int input_rows = input.dimension(1);\n    const int input_cols = input.dimension(2);\n    const int depth = input.dimension(3);\n\n    const int filter_rows = filter.dimension(0);\n    const int filter_cols = filter.dimension(1);\n\n    const int output_rows = out_backprop.dimension(1);\n    const int output_cols = out_backprop.dimension(2);\n\n    // Initialize gradient with all zeros.\n    filter_backprop.setZero();\n\n    // This is a reference implementation, likely to be slow.\n    // TODO(gpapan): Write multi-threaded implementation.\n    // In the case of multiple argmax branches, we only back-propagate along the\n    // last branch, i.e., the one with largest value of `h * filter_cols + w`,\n    // similarly to the max-pooling backward routines.\n    for (int b = 0; b < batch; ++b) {\n      for (int h_out = 0; h_out < output_rows; ++h_out) {\n        int h_beg = h_out * stride_rows - pad_top;\n        for (int w_out = 0; w_out < output_cols; ++w_out) {\n          int w_beg = w_out * stride_cols - pad_left;\n          for (int d = 0; d < depth; ++d) {\n            T cur_val = Eigen::NumTraits<T>::lowest();\n            int h_max = 0;\n            int w_max = 0;\n            for (int h = 0; h < filter_rows; ++h) {\n              const int h_in = h_beg + h * rate_rows;\n              if (h_in >= 0 && h_in < input_rows) {\n                for (int w = 0; w < filter_cols; ++w) {\n                  const int w_in = w_beg + w * rate_cols;\n                  if (w_in >= 0 && w_in < input_cols) {\n                    const T val = input(b, h_in, w_in, d) + filter(h, w, d);\n                    if (val > cur_val) {\n                      cur_val = val;\n                      h_max = h;\n                      w_max = w;\n                    }\n                  }\n                }\n              }\n            }\n            if (h_max < filter_rows && w_max < filter_cols) {\n              filter_backprop(h_max, w_max, d) +=\n                  out_backprop(b, h_out, w_out, d);\n            }\n          }\n        }\n      }\n    }\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def on_PUT(self, origin, content, query, room_id, event_id):\n        content = await self.handler.on_send_leave_request(origin, content)\n        return 200, (200, content)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _abort_multipart_upload_artifact(artifact_path):\n    \"\"\"\n    A request handler for `POST /mlflow-artifacts/mpu/abort` to abort a multipart upload\n    to `artifact_path` (a relative path from the root artifact directory).\n    \"\"\"\n    artifact_path = validate_path_is_safe(artifact_path)\n\n    request_message = _get_request_message(\n        AbortMultipartUpload(),\n        schema={\n            \"path\": [_assert_required, _assert_string],\n            \"upload_id\": [_assert_string],\n        },\n    )\n    path = request_message.path\n    upload_id = request_message.upload_id\n\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    _validate_support_multipart_upload(artifact_repo)\n\n    artifact_repo.abort_multipart_upload(\n        path,\n        upload_id,\n        artifact_path,\n    )\n    return _wrap_response(AbortMultipartUpload.Response())\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self):\r\n        keys = [key for key in list(request.form.keys()) + list(request.files.keys())]\r\n        field_infos = [FieldInfo.factory(key) for key in keys if key != \"csrf_token\"]\r\n        # important to sort them so they will be in expected order on command line\r\n        self.field_infos = list(sorted(field_infos))\r\n\r"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status CalculateOutputIndex(OpKernelContext* context, int dimension,\n                              const vector<INDEX_TYPE>& parent_output_index,\n                              INDEX_TYPE output_index_multiplier,\n                              INDEX_TYPE output_size,\n                              vector<INDEX_TYPE>* result) {\n    const RowPartitionTensor row_partition_tensor =\n        GetRowPartitionTensor(context, dimension);\n    auto partition_type = GetRowPartitionTypeByDimension(dimension);\n    switch (partition_type) {\n      case RowPartitionType::VALUE_ROWIDS:\n        CalculateOutputIndexValueRowID(\n            context, row_partition_tensor, parent_output_index,\n            output_index_multiplier, output_size, result);\n        return tensorflow::Status::OK();\n      case RowPartitionType::ROW_SPLITS:\n        CalculateOutputIndexRowSplit(\n            context, row_partition_tensor, parent_output_index,\n            output_index_multiplier, output_size, result);\n        return tensorflow::Status::OK();\n      default:\n        return errors::InvalidArgument(\n            \"Unsupported partition type:\",\n            RowPartitionTypeToString(partition_type));\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def save_to_file(self, save_file=None, so_path=None):\n        '''\n        Write the configuration in json format to a file.\n        Save file defaults to '/etc/target/saveconfig.json'.\n        '''\n        if not save_file:\n            save_file = default_save_file\n\n        if so_path:\n            saveconf = self._get_saveconf(so_path, save_file)\n        else:\n            saveconf = self.dump()\n\n        tmp_file = save_file + \".temp\"\n\n        mode = stat.S_IRUSR | stat.S_IWUSR  # 0o600\n        umask = 0o777 ^ mode  # Prevents always downgrading umask to 0\n\n        # For security, remove file with potentially elevated mode\n        try:\n            os.remove(tmp_file)\n        except OSError:\n            pass\n\n        umask_original = os.umask(umask)\n        # Even though the old file is first deleted, a race condition is still\n        # possible. Including os.O_EXCL with os.O_CREAT in the flags will\n        # prevent the file from being created if it exists due to a race\n        try:\n            fdesc = os.open(tmp_file, os.O_WRONLY | os.O_CREAT | os.O_EXCL, mode)\n        finally:\n            os.umask(umask_original)\n\n        with os.fdopen(fdesc, 'w+') as f:\n            f.write(json.dumps(saveconf, sort_keys=True, indent=2))\n            f.write(\"\\n\")\n            f.flush()\n            os.fsync(f.fileno())\n            f.close()\n\n        # copy along with permissions\n        shutil.copy(tmp_file, save_file)\n        os.remove(tmp_file)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def gather_tree(ids, parents):\n    r\"\"\"\n    To be used after beam search. After beam search, we get selected ids at\n    each time step and the corresponding parents in the search tree. Both ids\n    and parents have the layout :attr:`[max_time, batch_size, beam_size]`. Then\n    :attr:`gather_tree` is used to backtrace from the last time step and\n    generate the full sequences by collecting selected ids.\n\n    Here is an example:\n\n    .. code-block:: text\n\n            Given:\n                ids = [[[2 2]\n                        [6 1]]\n                       [[3 9]\n                        [6 1]]\n                       [[0 1]\n                        [9 0]]]\n                parents = [[[0 0]\n                            [1 1]]\n                           [[1 0]\n                            [1 0]]\n                           [[0 0]\n                            [0 1]]]\n\n            Then:\n                gather_tree(ids, parents)\n                         = [[[2 2]\n                             [1 6]]\n                            [[3 3]\n                             [6 1]]\n                            [[0 1]\n                             [9 0]]]\n\n    Args:\n        ids(Tensor): A Tensor with shape :attr:`[length, batch_size, beam_size]`\n            and data type :attr:`int32` or :attr:`int64`. It contains the selected\n            ids of all time steps.\n        parents(Tensor): A Tensor with the same shape and data type as :attr:`ids`,\n            It contains the parents corresponding to selected ids when searching\n            among beams.\n\n    Returns:\n            A Tensor with the same shape and data type as :attr:`ids`. \\\n            It contains the full sequences. The sequences are collected from \\\n            :attr:`ids` by backtracing according to :attr:`parents`.\n\n    Examples:\n        .. code-block:: python\n\n            import paddle\n\n            ids = paddle.to_tensor([[[2, 2], [6, 1]], [[3, 9], [6, 1]], [[0, 1], [9, 0]]])\n\n            parents = paddle.to_tensor([[[0, 0], [1, 1]], [[1, 0], [1, 0]], [[0, 0], [0, 1]]])\n\n            final_sequences = paddle.nn.functional.gather_tree(ids, parents)\n            # [[[2, 2], [1, 6]], [[3, 3], [6, 1]], [[0, 1], [9, 0]]]\n\n    \"\"\"\n    if ids.ndim != 3:\n        raise ValueError(\n            \"The input ids must be a 3D tensor with shape [length, batch_size, beam_size]\"\n        )\n    if ids.ndim != parents.ndim:\n        raise ValueError(\"The ids's shape must be the same as parents' shape. \")\n\n    if in_dygraph_mode():\n        return _C_ops.gather_tree(ids, parents)\n    else:\n        if _in_legacy_dygraph():\n            return _legacy_C_ops.gather_tree(ids, parents)\n        else:\n            helper = LayerHelper('gather_tree', **locals())\n            check_variable_and_dtype(ids, 'ids', ['int32', 'int64'],\n                                     'gather_tree')\n            check_variable_and_dtype(parents, 'parents', ['int32', 'int64'],\n                                     'gather_tree')\n            out = helper.create_variable_for_type_inference(dtype=ids.dtype)\n\n            helper.append_op(type=\"gather_tree\",\n                             inputs={\n                                 \"Ids\": ids,\n                                 \"Parents\": parents\n                             },\n                             outputs={\"Out\": out})\n\n            return out\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def safe_eval(expr, locals=None, include_exceptions=False):\n    '''\n    This is intended for allowing things like:\n    with_items: a_list_variable\n\n    Where Jinja2 would return a string but we do not want to allow it to\n    call functions (outside of Jinja2, where the env is constrained).\n\n    Based on:\n    http://stackoverflow.com/questions/12523516/using-ast-and-whitelists-to-make-pythons-eval-safe\n    '''\n    locals = {} if locals is None else locals\n\n    # define certain JSON types\n    # eg. JSON booleans are unknown to python eval()\n    OUR_GLOBALS = {\n        '__builtins__': {},  # avoid global builtins as per eval docs\n        'false': False,\n        'null': None,\n        'true': True,\n        # also add back some builtins we do need\n        'True': True,\n        'False': False,\n    }\n\n    # this is the whitelist of AST nodes we are going to\n    # allow in the evaluation. Any node type other than\n    # those listed here will raise an exception in our custom\n    # visitor class defined below.\n    SAFE_NODES = set(\n        (\n            ast.Add,\n            ast.BinOp,\n            # ast.Call,\n            ast.Compare,\n            ast.Dict,\n            ast.Div,\n            ast.Expression,\n            ast.List,\n            ast.Load,\n            ast.Mult,\n            ast.Num,\n            ast.Name,\n            ast.Str,\n            ast.Sub,\n            ast.USub,\n            ast.Tuple,\n            ast.UnaryOp,\n        )\n    )\n\n    # AST node types were expanded after 2.6\n    if sys.version_info[:2] >= (2, 7):\n        SAFE_NODES.update(\n            set(\n                (ast.Set,)\n            )\n        )\n\n    # And in Python 3.4 too\n    if sys.version_info[:2] >= (3, 4):\n        SAFE_NODES.update(\n            set(\n                (ast.NameConstant,)\n            )\n        )\n\n    filter_list = []\n    for filter_ in filter_loader.all():\n        filter_list.extend(filter_.filters().keys())\n\n    test_list = []\n    for test in test_loader.all():\n        test_list.extend(test.tests().keys())\n\n    CALL_WHITELIST = C.DEFAULT_CALLABLE_WHITELIST + filter_list + test_list\n\n    class CleansingNodeVisitor(ast.NodeVisitor):\n        def generic_visit(self, node, inside_call=False):\n            if type(node) not in SAFE_NODES:\n                raise Exception(\"invalid expression (%s)\" % expr)\n            elif isinstance(node, ast.Call):\n                inside_call = True\n            elif isinstance(node, ast.Name) and inside_call:\n                # Disallow calls to builtin functions that we have not vetted\n                # as safe.  Other functions are excluded by setting locals in\n                # the call to eval() later on\n                if hasattr(builtins, node.id) and node.id not in CALL_WHITELIST:\n                    raise Exception(\"invalid function: %s\" % node.id)\n            # iterate over all child nodes\n            for child_node in ast.iter_child_nodes(node):\n                self.generic_visit(child_node, inside_call)\n\n    if not isinstance(expr, string_types):\n        # already templated to a datastructure, perhaps?\n        if include_exceptions:\n            return (expr, None)\n        return expr\n\n    cnv = CleansingNodeVisitor()\n    try:\n        parsed_tree = ast.parse(expr, mode='eval')\n        cnv.visit(parsed_tree)\n        compiled = compile(parsed_tree, expr, 'eval')\n        # Note: passing our own globals and locals here constrains what\n        # callables (and other identifiers) are recognized.  this is in\n        # addition to the filtering of builtins done in CleansingNodeVisitor\n        result = eval(compiled, OUR_GLOBALS, dict(locals))\n\n        if include_exceptions:\n            return (result, None)\n        else:\n            return result\n    except SyntaxError as e:\n        # special handling for syntax errors, we just return\n        # the expression string back as-is to support late evaluation\n        if include_exceptions:\n            return (expr, None)\n        return expr\n    except Exception as e:\n        if include_exceptions:\n            return (expr, e)\n        return expr\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _handle_carbon_sent(self, msg):\n        if msg['from'].bare == self.xmpp.boundjid.bare:\n            self.xmpp.event('carbon_sent', msg)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static int generate_key(DH *dh)\n{\n    int ok = 0;\n    int generate_new_key = 0;\n#ifndef FIPS_MODULE\n    unsigned l;\n#endif\n    BN_CTX *ctx = NULL;\n    BIGNUM *pub_key = NULL, *priv_key = NULL;\n\n    if (BN_num_bits(dh->params.p) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n        return 0;\n    }\n\n    if (dh->params.q != NULL\n        && BN_num_bits(dh->params.q) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_Q_TOO_LARGE);\n        return 0;\n    }\n\n    if (BN_num_bits(dh->params.p) < DH_MIN_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_SMALL);\n        return 0;\n    }\n\n    ctx = BN_CTX_new_ex(dh->libctx);\n    if (ctx == NULL)\n        goto err;\n\n    if (dh->priv_key == NULL) {\n        priv_key = BN_secure_new();\n        if (priv_key == NULL)\n            goto err;\n        generate_new_key = 1;\n    } else {\n        priv_key = dh->priv_key;\n    }\n\n    if (dh->pub_key == NULL) {\n        pub_key = BN_new();\n        if (pub_key == NULL)\n            goto err;\n    } else {\n        pub_key = dh->pub_key;\n    }\n    if (generate_new_key) {\n        /* Is it an approved safe prime ?*/\n        if (DH_get_nid(dh) != NID_undef) {\n            int max_strength =\n                    ossl_ifc_ffc_compute_security_bits(BN_num_bits(dh->params.p));\n\n            if (dh->params.q == NULL\n                || dh->length > BN_num_bits(dh->params.q))\n                goto err;\n            /* dh->length = maximum bit length of generated private key */\n            if (!ossl_ffc_generate_private_key(ctx, &dh->params, dh->length,\n                                               max_strength, priv_key))\n                goto err;\n        } else {\n#ifdef FIPS_MODULE\n            if (dh->params.q == NULL)\n                goto err;\n#else\n            if (dh->params.q == NULL) {\n                /* secret exponent length, must satisfy 2^(l-1) <= p */\n                if (dh->length != 0\n                    && dh->length >= BN_num_bits(dh->params.p))\n                    goto err;\n                l = dh->length ? dh->length : BN_num_bits(dh->params.p) - 1;\n                if (!BN_priv_rand_ex(priv_key, l, BN_RAND_TOP_ONE,\n                                     BN_RAND_BOTTOM_ANY, 0, ctx))\n                    goto err;\n                /*\n                 * We handle just one known case where g is a quadratic non-residue:\n                 * for g = 2: p % 8 == 3\n                 */\n                if (BN_is_word(dh->params.g, DH_GENERATOR_2)\n                    && !BN_is_bit_set(dh->params.p, 2)) {\n                    /* clear bit 0, since it won't be a secret anyway */\n                    if (!BN_clear_bit(priv_key, 0))\n                        goto err;\n                }\n            } else\n#endif\n            {\n                /* Do a partial check for invalid p, q, g */\n                if (!ossl_ffc_params_simple_validate(dh->libctx, &dh->params,\n                                                     FFC_PARAM_TYPE_DH, NULL))\n                    goto err;\n                /*\n                 * For FFC FIPS 186-4 keygen\n                 * security strength s = 112,\n                 * Max Private key size N = len(q)\n                 */\n                if (!ossl_ffc_generate_private_key(ctx, &dh->params,\n                                                   BN_num_bits(dh->params.q),\n                                                   MIN_STRENGTH,\n                                                   priv_key))\n                    goto err;\n            }\n        }\n    }\n\n    if (!ossl_dh_generate_public_key(ctx, dh, priv_key, pub_key))\n        goto err;\n\n    dh->pub_key = pub_key;\n    dh->priv_key = priv_key;\n    dh->dirty_cnt++;\n    ok = 1;\n err:\n    if (ok != 1)\n        ERR_raise(ERR_LIB_DH, ERR_R_BN_LIB);\n\n    if (pub_key != dh->pub_key)\n        BN_free(pub_key);\n    if (priv_key != dh->priv_key)\n        BN_free(priv_key);\n    BN_CTX_free(ctx);\n    return ok;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self):\n        dag_id = request.form.get('dag_id')\n        task_id = request.form.get('task_id')\n        origin = get_safe_url(request.form.get('origin'))\n        dag = current_app.dag_bag.get_dag(dag_id)\n        task = dag.get_task(task_id)\n\n        execution_date = request.form.get('execution_date')\n        execution_date = timezone.parse(execution_date)\n        ignore_all_deps = request.form.get('ignore_all_deps') == \"true\"\n        ignore_task_deps = request.form.get('ignore_task_deps') == \"true\"\n        ignore_ti_state = request.form.get('ignore_ti_state') == \"true\"\n\n        executor = ExecutorLoader.get_default_executor()\n        valid_celery_config = False\n        valid_kubernetes_config = False\n\n        try:\n            from airflow.executors.celery_executor import CeleryExecutor\n            valid_celery_config = isinstance(executor, CeleryExecutor)\n        except ImportError:\n            pass\n\n        try:\n            from airflow.executors.kubernetes_executor import KubernetesExecutor\n            valid_kubernetes_config = isinstance(executor, KubernetesExecutor)\n        except ImportError:\n            pass\n\n        if not valid_celery_config and not valid_kubernetes_config:\n            flash(\"Only works with the Celery or Kubernetes executors, sorry\", \"error\")\n            return redirect(origin)\n\n        ti = models.TaskInstance(task=task, execution_date=execution_date)\n        ti.refresh_from_db()\n\n        # Make sure the task instance can be run\n        dep_context = DepContext(\n            deps=RUNNING_DEPS,\n            ignore_all_deps=ignore_all_deps,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state)\n        failed_deps = list(ti.get_failed_dep_statuses(dep_context=dep_context))\n        if failed_deps:\n            failed_deps_str = \", \".join(\n                [\"{}: {}\".format(dep.dep_name, dep.reason) for dep in failed_deps])\n            flash(\"Could not queue task instance for execution, dependencies not met: \"\n                  \"{}\".format(failed_deps_str),\n                  \"error\")\n            return redirect(origin)\n\n        executor.start()\n        executor.queue_task_instance(\n            ti,\n            ignore_all_deps=ignore_all_deps,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state)\n        executor.heartbeat()\n        flash(\n            \"Sent {} to the message queue, \"\n            \"it should start any moment now.\".format(ti))\n        return redirect(origin)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _set_name(self, name):\n        # Sanitize the file name so that it can't be dangerous.\n        if name is not None:\n            # Just use the basename of the file -- anything else is dangerous.\n            name = os.path.basename(name)\n\n            # File names longer than 255 characters can cause problems on older OSes.\n            if len(name) > 255:\n                name, ext = os.path.splitext(name)\n                ext = ext[:255]\n                name = name[:255 - len(ext)] + ext\n\n            name = validate_file_name(name)\n\n        self._name = name\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& indices = ctx->input(0);\n    const auto values = ctx->input(1).flat<Tidx>();\n    const Tensor& dense_shape = ctx->input(2);\n    const Tensor& size_t = ctx->input(3);\n    const auto weights = ctx->input(4).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    OP_REQUIRES(ctx, size_t.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_t.dims()));\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    bool is_1d = dense_shape.NumElements() == 1;\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (is_1d) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(ctx,\n                       functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                           ctx, values, weights, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, values, weights, out, size));\n      }\n    } else {\n      const auto shape = dense_shape.flat<int64_t>();\n      const int64_t num_rows = shape(0);\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      const auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      const auto indices_mat = indices.matrix<int64_t>();\n      for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {\n        const int64_t batch = indices_mat(i, 0);\n        const Tidx bin = values(i);\n        OP_REQUIRES(\n            ctx, batch < out.dimension(0),\n            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,\n                                    \") must be less than the dimension size (\",\n                                    out.dimension(0), \").\"));\n        OP_REQUIRES(\n            ctx, bin < out.dimension(1),\n            errors::InvalidArgument(\"Index out ouf bound. `bin` (\", bin,\n                                    \") must be less then the dimension size (\",\n                                    out.dimension(1), \").\"));\n        if (bin < size) {\n          if (binary_output_) {\n            out(batch, bin) = T(1);\n          } else {\n            if (weights_size) {\n              out(batch, bin) += weights(i);\n            } else {\n              out(batch, bin) += T(1);\n            }\n          }\n        }\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _clone(\n        self,\n        src: \"DictionaryObject\",\n        pdf_dest: PdfWriterProtocol,\n        force_duplicate: bool,\n        ignore_fields: Optional[Sequence[Union[str, int]]],\n        visited: Set[Tuple[int, int]],\n    ) -> None:\n        \"\"\"\n        Update the object from src.\n\n        Args:\n            src: \"DictionaryObject\":\n            pdf_dest:\n            force_duplicate:\n            ignore_fields:\n        \"\"\"\n        # first we remove for the ignore_fields\n        # that are for a limited number of levels\n        x = 0\n        assert ignore_fields is not None\n        ignore_fields = list(ignore_fields)\n        while x < len(ignore_fields):\n            if isinstance(ignore_fields[x], int):\n                if cast(int, ignore_fields[x]) <= 0:\n                    del ignore_fields[x]\n                    del ignore_fields[x]\n                    continue\n                else:\n                    ignore_fields[x] -= 1  # type:ignore\n            x += 1\n        #  First check if this is a chain list, we need to loop to prevent recur\n        if any(\n            field not in ignore_fields\n            and field in src\n            and isinstance(src.raw_get(field), IndirectObject)\n            and isinstance(src[field], DictionaryObject)\n            and (\n                src.get(\"/Type\", None) is None\n                or cast(DictionaryObject, src[field]).get(\"/Type\", None) is None\n                or src.get(\"/Type\", None)\n                == cast(DictionaryObject, src[field]).get(\"/Type\", None)\n            )\n            for field in [\"/Next\", \"/Prev\", \"/N\", \"/V\"]\n        ):\n            ignore_fields = list(ignore_fields)\n            for lst in ((\"/Next\", \"/Prev\"), (\"/N\", \"/V\")):\n                for k in lst:\n                    objs = []\n                    if (\n                        k in src\n                        and k not in self\n                        and isinstance(src.raw_get(k), IndirectObject)\n                        and isinstance(src[k], DictionaryObject)\n                        # IF need to go further the idea is to check\n                        # that the types are the same:\n                        and (\n                            src.get(\"/Type\", None) is None\n                            or cast(DictionaryObject, src[k]).get(\"/Type\", None) is None\n                            or src.get(\"/Type\", None)\n                            == cast(DictionaryObject, src[k]).get(\"/Type\", None)\n                        )\n                    ):\n                        cur_obj: Optional[DictionaryObject] = cast(\n                            \"DictionaryObject\", src[k]\n                        )\n                        prev_obj: Optional[DictionaryObject] = self\n                        while cur_obj is not None:\n                            clon = cast(\n                                \"DictionaryObject\",\n                                cur_obj._reference_clone(\n                                    cur_obj.__class__(), pdf_dest, force_duplicate\n                                ),\n                            )\n                            # check to see if we've previously processed our item\n                            if clon.indirect_reference is not None:\n                                idnum = clon.indirect_reference.idnum\n                                generation = clon.indirect_reference.generation\n                                if (idnum, generation) in visited:\n                                    cur_obj = None\n                                    break\n                                visited.add((idnum, generation))\n                            objs.append((cur_obj, clon))\n                            assert prev_obj is not None\n                            prev_obj[NameObject(k)] = clon.indirect_reference\n                            prev_obj = clon\n                            try:\n                                if cur_obj == src:\n                                    cur_obj = None\n                                else:\n                                    cur_obj = cast(\"DictionaryObject\", cur_obj[k])\n                            except Exception:\n                                cur_obj = None\n                        for s, c in objs:\n                            c._clone(s, pdf_dest, force_duplicate, ignore_fields, visited)\n\n        for k, v in src.items():\n            if k not in ignore_fields:\n                if isinstance(v, StreamObject):\n                    if not hasattr(v, \"indirect_reference\"):\n                        v.indirect_reference = None\n                    vv = v.clone(pdf_dest, force_duplicate, ignore_fields)\n                    assert vv.indirect_reference is not None\n                    self[k.clone(pdf_dest)] = vv.indirect_reference  # type: ignore[attr-defined]\n                elif k not in self:\n                    self[NameObject(k)] = (\n                        v.clone(pdf_dest, force_duplicate, ignore_fields)\n                        if hasattr(v, \"clone\")\n                        else v\n                    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def cli(configfile, observed_since, setup_only=False, troubleshoot=False):\n    '''\n    Tenable.io -> Jira Cloud Transformer & Ingester\n    '''\n    # Load the config, but ensure that any additional fields are additive to the\n    # basic field set.\n    config_from_file = yaml.safe_load(configfile)\n    fields = config_from_file.pop('custom_fields', list())\n    config = dict_merge(base_config(), config_from_file)\n    config['fields'] = config['fields'] + fields\n\n    if config['tenable'].get('tio_transform_tags'):\n        attr_cache = config['tenable'].get('tio_asset_attr_cache', list())\n        tag_attrs = config['tenable'].get('tio_transform_tags', list())\n        config['tenable']['tio_asset_attr_cache'] = attr_cache + tag_attrs\n\n\n    # Get the logging definition and define any defaults as need be.\n    log = config.get('log', {})\n    log_lvls = {'debug': 10, 'info': 20, 'warn': 30, 'error': 40}\n    log['level'] = log_lvls[log.get('level', 'warn')]\n    log['format'] = log.get('format',\n        '%(asctime)-15s %(name)s %(levelname)s %(message)s')\n\n    # Configure the root logging facility\n    if troubleshoot:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=log['format'],\n            filename='tenable_debug.log'\n        )\n    else:\n        logging.basicConfig(**log)\n\n    # Output some basic information detailing the config file used and the\n    # python version & system arch.\n    logging.info('Tenable2JiraCloud Version {}'.format(__version__))\n    logging.info('Using configuration file {}'.format(configfile.name))\n    uname = platform.uname()\n    logging.info('Running on Python {} {}/{}'.format(\n        '.'.join([str(i) for i in sys.version_info][0:3]),\n        uname[0], uname[-2]))\n\n    # instantiate the Jira object\n    jira = Jira(\n        'https://{}/rest/api/3'.format(config['jira']['address']),\n        config['jira']['api_username'],\n        config['jira']['api_token']\n    )\n\n    # Initiate the Tenable.io API model, the Ingester model, and start the\n    # ingestion and data transformation.\n    if config['tenable'].get('platform') == 'tenable.io':\n        if not observed_since:\n            # if no since field is supplied, then look in the config file to see\n            # if an age was applied, if not, then use the default of 30 days.\n            observed_since = arrow.now()\\\n                .shift(days=-config['tenable'].get('tio_age', 30))\\\n                .floor('day').timestamp()\n\n        source = TenableIO(\n            access_key=config['tenable'].get('access_key'),\n            secret_key=config['tenable'].get('secret_key'),\n            vendor='Tenable',\n            product='JiraCloud',\n            build=__version__\n        )\n        if int(source.session.details().get('permissions')) < 64:\n            logging.error('API Keys tie to non-admin user.')\n    elif config['tenable'].get('platform') == 'tenable.sc':\n        source = TenableSC(\n            config['tenable'].get('address'),\n            port=int(config['tenable'].get('port', 443)),\n            username=config['tenable'].get('username'),\n            password=config['tenable'].get('password'),\n            access_key=config['tenable'].get('access_key'),\n            secret_key=config['tenable'].get('secret_key'),\n            vendor='Tenable',\n            product='JiraCloud',\n            build=__version__\n        )\n    else:\n        logging.error('No valid Tenable platform configuration defined.')\n        exit(1)\n    ingest = Tio2Jira(source, jira, config)\n\n    if troubleshoot:\n        # if the troubleshooting flag is set, then we will be collecting some\n        # basic information and outputting it to the screen in a format that\n        # Github issues would expect to format it all pretty.  This should help\n        # reduce the amount of time that is spent with back-and-forth debugging.\n        try:\n            ingest.ingest(int(observed_since))\n        except:\n            logging.exception('Caught the following Exception')\n\n        # Some basic redaction of sensitive data, such as API Keys, Usernames,\n        # Passwords, and hostnames.\n        addr = config_from_file['jira']['address']\n        sc_addr = 'NOTHING_TO_SEE_HERE_AT_ALL'\n        config_from_file['jira']['address'] = '<REDACTED>'\n        config_from_file['jira']['api_token'] = '<REDACTED>'\n        config_from_file['jira']['api_username'] = '<REDACTED>'\n        config_from_file['project']['leadAccountId'] = '<REDACTED>'\n        if config_from_file['tenable'].get('address'):\n            sc_addr = config_from_file['tenable']['address']\n            config_from_file['tenable']['address'] = '<REDACTED>'\n        if config_from_file['tenable'].get('access_key'):\n            config_from_file['tenable']['access_key'] = '<REDACTED>'\n        if config_from_file['tenable'].get('secret_key'):\n            config_from_file['tenable']['secret_key'] = '<REDACTED>'\n        if config_from_file['tenable'].get('username'):\n            config_from_file['tenable']['username'] = '<REDACTED>'\n        if config_from_file['tenable'].get('password'):\n            config_from_file['tenable']['password'] = '<REDACTED>'\n\n        output = troubleshooting.format(\n            configfile=yaml.dump(config_from_file, default_flow_style=False),\n            logging=open('tenable_debug.log').read() \\\n                .replace(addr, '<JIRA_CLOUD_HOST>') \\\n                .replace(sc_addr, '<TENABLE_SC_HOST>'),\n            issuetypes='\\n'.join(\n                [\n                    '{id}: {name}'.format(**a)\n                    for a in jira.issue_types.list()\n                    if a.get('name').lower() in ['task', 'subtask', 'sub-task']\n                ]\n            )\n        )\n        print(output)\n        print('\\n'.join([\n            '/-------------------------------NOTICE-----------------------------------\\\\',\n            '| The output above is helpful for us to troubleshoot exactly what is     |',\n            '| happening within the code and offer a diagnosis for how to correct.    |',\n            '| Please note that while some basic redaction has already been performed |',\n            '| that we ask you to review the information you\\'re about to send and     |',\n            '| ensure that nothing deemed sensitive is transmitted.                   |',\n            '| ---------------------------------------------------------------------- |',\n            '| -- Copy of output saved to \"issue_debug.md\"                            |',\n            '\\\\------------------------------------------------------------------------/'\n        ]))\n        with open('issue_debug.md', 'w') as reportfile:\n            print(output, file=reportfile)\n        os.remove('tenable_debug.log')\n    elif not setup_only:\n        ingest.ingest(observed_since)\n\n        # If we are expected to continually re-run the transformer, then we will\n        # need to track the passage of time and run every X hours, where X is\n        # defined by the user in the configuration.\n        if config.get('service', {}).get('interval', 0) > 0:\n            sleeper = int(config['service']['interval']) * 3600\n            while True:\n                last_run = int(time.time())\n                logging.info(\n                    'Sleeping for {}h'.format(sleeper/3600))\n                time.sleep(sleeper)\n                logging.info(\n                    'Initiating ingest with observed_since={}'.format(last_run))\n                ingest.ingest(last_run)\n    elif setup_only:\n        # In setup-only mode, the ingest will not run, and instead a config file\n        # will be generated that will have all of the JIRA identifiers baked in\n        # and will also inform the integration to ignore the screen builder.\n        # When using this config, if there are any changes to the code, then\n        # this config will need to be re-generated.\n        config['screen']['no_create'] = True\n        logging.info('Set to setup-only.  Will not run ingest.')\n        logging.info('The following is the updated config file from the setup.')\n        with open('generated_config.yaml', 'w') as outfile:\n            outfile.write(yaml.dump(config, Dumper=yaml.Dumper))\n        logging.info('Generated \"generated_config.yaml\" config file.')\n        logging.info('This config file should be updated for every new version of this integration.')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def generate_verifier(length=8):\n    \"\"\"Generate pseudorandom number.\"\"\"\n    return ''.join([str(random.SystemRandom().randint(0, 9)) for i in range(length)])\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def load_hparams_from_yaml(config_yaml: str, use_omegaconf: bool = True) -> Dict[str, Any]:\n    \"\"\"Load hparams from a file.\n\n        Args:\n            config_yaml: Path to config yaml file\n            use_omegaconf: If omegaconf is available and ``use_omegaconf=True``,\n                the hparams will be converted to ``DictConfig`` if possible.\n\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')\n    >>> path_yaml = './testing-hparams.yaml'\n    >>> save_hparams_to_yaml(path_yaml, hparams)\n    >>> hparams_new = load_hparams_from_yaml(path_yaml)\n    >>> vars(hparams) == hparams_new\n    True\n    >>> os.remove(path_yaml)\n    \"\"\"\n    fs = get_filesystem(config_yaml)\n    if not fs.exists(config_yaml):\n        rank_zero_warn(f\"Missing Tags: {config_yaml}.\", category=RuntimeWarning)\n        return {}\n\n    with fs.open(config_yaml, \"r\") as fp:\n        hparams = yaml.full_load(fp)\n\n    if _OMEGACONF_AVAILABLE:\n        if use_omegaconf:\n            try:\n                return OmegaConf.create(hparams)\n            except (UnsupportedValueType, ValidationError):\n                pass\n    return hparams\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const Tensor& input_min = ctx->input(1);\n    const Tensor& input_max = ctx->input(2);\n\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsScalar(input_min.shape()),\n        errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",\n                                input_min.dims()));\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsScalar(input_max.shape()),\n        errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",\n                                input_max.dims()));\n\n    const float input_min_float = input_min.scalar<float>()();\n    const float input_max_float = input_max.scalar<float>()();\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(2, TensorShape({}), &output_max));\n\n    // See QuantizationRangeOp as well, which has a copy of this logic.\n    auto input_array = input.flat<T1>();\n    const int32_t input_lowest_quantized =\n        static_cast<int32>(Eigen::NumTraits<T1>::lowest());\n    const int32_t input_highest_quantized =\n        static_cast<int32>(Eigen::NumTraits<T1>::highest());\n    T1 actual_min_quantized = input_highest_quantized;\n    T1 actual_max_quantized = input_lowest_quantized;\n    for (int i = 0; i < input_array.size(); ++i) {\n      const T1 value = input_array(i);\n      actual_min_quantized = std::min(actual_min_quantized, value);\n      actual_max_quantized = std::max(actual_max_quantized, value);\n    }\n    // We want to make sure that the minimum is no larger than zero, so that the\n    // convolution operation can run efficiently.\n    const float actual_min_float =\n        std::min(0.0f, QuantizedToFloat(actual_min_quantized, input_min_float,\n                                        input_max_float));\n    const float actual_max_float = QuantizedToFloat(\n        actual_max_quantized, input_min_float, input_max_float);\n\n#if 0\n    // This is the reference, non-eigen implementation:\n    auto output_array = output->flat<T2>();\n    RequantizeManyInNewRange<T1, T2>(input_array.data(), input_array.size(),\n                                     input_min_float, input_max_float,\n                                     actual_min_float, actual_max_float,\n                                     output_array.data());\n#endif\n\n    if (input_array.size() > 0) {\n      if (meta::IsSupportedAndEnabled() && std::is_same<T1, qint32>() &&\n          std::is_same<T2, quint8>()) {\n        auto input_i32_array = input.flat<qint32>();\n        meta::Requantize(ctx, input_i32_array.data(), input_i32_array.size(),\n                         input_min_float, input_max_float, actual_min_float,\n                         actual_max_float, output->flat<quint8>().data());\n      } else {\n        RequantizeManyInNewRangeUsingEigen<T1, T2>(\n            ctx->eigen_device<CPUDevice>(), input, input_min_float,\n            input_max_float, actual_min_float, actual_max_float, output);\n      }\n    }\n\n    output_min->flat<float>().setConstant(actual_min_float);\n    output_max->flat<float>().setConstant(actual_max_float);\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def set_metric_variables(self, metric_variable_values: List[Any]) -> None:\n    \"\"\"Set metric variable values to the given values.\"\"\"\n    with self._lock:\n      self._set_metric_variables(metric_variable_values)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def to_python(self, value):\n        value = super().to_python(value)\n        if self.max_length is not None and len(value) > self.max_length:\n            # Normalization can increase the string length (e.g.\n            # \"\ufb00\" -> \"ff\", \"\u00bd\" -> \"1\u20442\") but cannot reduce it, so there is no\n            # point in normalizing invalid data. Moreover, Unicode\n            # normalization is very slow on Windows and can be a DoS attack\n            # vector.\n            return value\n        return unicodedata.normalize(\"NFKC\", value)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_index_absolute_path(index):\n    return safe_join(INDEXDIR, index)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self) -> Model:\n        self.validate()\n        assert self._model\n\n        try:\n            dashboard = DashboardDAO.update(self._model, self._properties, commit=False)\n            if self._properties.get(\"json_metadata\"):\n                dashboard = DashboardDAO.set_dash_metadata(\n                    dashboard,\n                    data=json.loads(self._properties.get(\"json_metadata\", \"{}\")),\n                    commit=False,\n                )\n            db.session.commit()\n        except DAOUpdateFailedError as ex:\n            logger.exception(ex.exception)\n            raise DashboardUpdateFailedError() from ex\n        return dashboard\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def trace_dump(self, pid: int, native: bool = False) -> (bool, str):\n        pyspy = shutil.which(\"py-spy\")\n        if pyspy is None:\n            return False, \"py-spy is not installed\"\n\n        cmd = [pyspy, \"dump\", \"-p\", str(pid)]\n        # We\n        if sys.platform == \"linux\" and native:\n            cmd.append(\"--native\")\n        if await _can_passwordless_sudo():\n            cmd = [\"sudo\", \"-n\"] + cmd\n        process = await asyncio.create_subprocess_exec(\n            *cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        stdout, stderr = await process.communicate()\n        if process.returncode != 0:\n            return False, _format_failed_pyspy_command(cmd, stdout, stderr)\n        else:\n            return True, stdout.decode(\"utf-8\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def process(self, header, body):\n        try:\n            msg = self._connection.receive_from_header_and_body(header, body)\n        except ua.uaerrors.BadRequestTooLarge as e:\n            _logger.warning(\"Recived request that exceed the transport limits\")\n            err = ua.ErrorMessage(ua.StatusCode(e.code), str(e))\n            data = uatcp_to_binary(ua.MessageType.Error, err)\n            self._transport.write(data)\n            return True\n        except ua.uaerrors.BadUserAccessDenied:\n            _logger.warning(\"Unauthenticated user attempted to connect\")\n            return False\n        if isinstance(msg, ua.Message):\n            if header.MessageType == ua.MessageType.SecureOpen:\n                self.open_secure_channel(msg.SecurityHeader(), msg.SequenceHeader(), msg.body())\n            elif header.MessageType == ua.MessageType.SecureClose:\n                self._connection.close()\n                return False\n            elif header.MessageType == ua.MessageType.SecureMessage:\n                return await self.process_message(msg.SequenceHeader(), msg.body())\n        elif isinstance(msg, ua.Hello):\n            ack = self._limits.create_acknowledge_limits(msg)\n            data = uatcp_to_binary(ua.MessageType.Acknowledge, ack)\n            self._transport.write(data)\n        elif isinstance(msg, ua.ErrorMessage):\n            _logger.warning(\"Received an error message type\")\n        elif msg is None:\n            pass  # msg is a ChunkType.Intermediate of an ua.MessageType.SecureMessage\n        else:\n            _logger.warning(\"Unsupported message type: %s\", header.MessageType)\n            raise ServiceError(ua.StatusCodes.BadTcpMessageTypeInvalid)\n        return True\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& logits_in = context->input(0);\n    const Tensor& labels_in = context->input(1);\n\n    TensorShape shape_in = logits_in.shape();\n\n    BCast bcast(BCast::FromShape(logits_in.shape()),\n                BCast::FromShape(labels_in.shape()),\n                /*fewer_dims_optimization=*/false);\n    if (!logits_in.IsSameSize(labels_in)) {\n      OP_REQUIRES(context, bcast.IsValid(),\n                  errors::InvalidArgument(\n                      \"logits and labels must be broadcastable: logits_size=\",\n                      logits_in.shape().DebugString(),\n                      \" labels_size=\", labels_in.shape().DebugString()));\n      shape_in = BCast::ToShape(bcast.output_shape());\n    }\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(shape_in),\n                errors::InvalidArgument(\"logits and labels must be either \"\n                                        \"2-dimensional, or broadcasted to be \"\n                                        \"2-dimensional\"));\n\n    if (std::is_same<Device, GPUDevice>::value) {\n      OP_REQUIRES(context, !OpDeterminismRequired(),\n                  errors::Unimplemented(\n                      \"The GPU implementation of SoftmaxCrossEntropyWithLogits\"\n                      \" that would have been executed is not deterministic.\"\n                      \" Note that the Python API uses an alternative,\"\n                      \" deterministic, GPU-accelerated path when determinism is\"\n                      \" enabled.\"));\n    }\n\n    // loss is 1-D (one per example), and size is batch_size.\n\n    Tensor scratch;\n    OP_REQUIRES_OK(\n        context, context->allocate_temp(DataTypeToEnum<T>::value,\n                                        TensorShape({shape_in.dim_size(0), 1}),\n                                        &scratch));\n\n    Tensor* loss_out = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\n                       0, TensorShape({shape_in.dim_size(0)}), &loss_out));\n    Tensor* back_out = nullptr;\n    // Try to reuse the logits_in buffer for the backprop output.\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 1, shape_in, &back_out));\n    if (shape_in.dim_size(0) > 0) {\n      functor::XentFunctor<Device, T> functor;\n      functor(context->eigen_device<Device>(), shape_in.AsEigenDSizes<2>(),\n              BCast::ToIndexArray<2>(bcast.x_bcast()),\n              BCast::ToIndexArray<2>(bcast.y_bcast()),\n              logits_in.template shaped<T, 2>(bcast.x_reshape()),\n              labels_in.template shaped<T, 2>(bcast.y_reshape()),\n              scratch.matrix<T>(), loss_out->vec<T>(), back_out->matrix<T>());\n    }\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_next_logout(self, user, peek=False):\n        \"\"\"\n        Get the next session in the logged-in state and move\n        it to the logging_out state.  Return the session that is\n        found.\n\n        :param peek: for IdP-initiated logout we can't remove the\n                     session otherwise when the request comes back\n                     in the user won't be seen as being logged-on.\n\n        Return None if no more sessions in LOGGED_IN state.\n        \"\"\"\n        candidates = self._ss.get_user_sessions(user)\n\n        for c in candidates:\n            key = c.keys()[0]\n            if int(c[key].get('logoutstate', 0)) == LOGGED_IN:\n                samlsession = self._data_to_samlsession(key, c[key])\n                self.start_logout(samlsession, initial=False)\n                return samlsession\n        return None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def received(self, data):\n        \"\"\"\n        Receives the HTTP stream for one request.  Returns the number of\n        bytes consumed.  Sets the completed flag once both the header and the\n        body have been received.\n        \"\"\"\n        if self.completed:\n            return 0  # Can't consume any more.\n        datalen = len(data)\n        br = self.body_rcv\n        if br is None:\n            # In header.\n            s = self.header_plus + data\n            index = find_double_newline(s)\n            if index >= 0:\n                # Header finished.\n                header_plus = s[:index]\n                consumed = len(data) - (len(s) - index)\n\n                # Remove preceeding blank lines. This is suggested by\n                # https://tools.ietf.org/html/rfc7230#section-3.5 to support\n                # clients sending an extra CR LF after another request when\n                # using HTTP pipelining\n                header_plus = header_plus.lstrip()\n\n                if not header_plus:\n                    self.empty = True\n                    self.completed = True\n                else:\n                    try:\n                        self.parse_header(header_plus)\n                    except ParsingError as e:\n                        self.error = BadRequest(e.args[0])\n                        self.completed = True\n                    else:\n                        if self.body_rcv is None:\n                            # no content-length header and not a t-e: chunked\n                            # request\n                            self.completed = True\n                        if self.content_length > 0:\n                            max_body = self.adj.max_request_body_size\n                            # we won't accept this request if the content-length\n                            # is too large\n                            if self.content_length >= max_body:\n                                self.error = RequestEntityTooLarge(\n                                    \"exceeds max_body of %s\" % max_body\n                                )\n                                self.completed = True\n                self.headers_finished = True\n                return consumed\n            else:\n                # Header not finished yet.\n                self.header_bytes_received += datalen\n                max_header = self.adj.max_request_header_size\n                if self.header_bytes_received >= max_header:\n                    # malformed header, we need to construct some request\n                    # on our own. we disregard the incoming(?) requests HTTP\n                    # version and just use 1.0. IOW someone just sent garbage\n                    # over the wire\n                    self.parse_header(b\"GET / HTTP/1.0\\n\")\n                    self.error = RequestHeaderFieldsTooLarge(\n                        \"exceeds max_header of %s\" % max_header\n                    )\n                    self.completed = True\n                self.header_plus = s\n                return datalen\n        else:\n            # In body.\n            consumed = br.received(data)\n            self.body_bytes_received += consumed\n            max_body = self.adj.max_request_body_size\n            if self.body_bytes_received >= max_body:\n                # this will only be raised during t-e: chunked requests\n                self.error = RequestEntityTooLarge(\"exceeds max_body of %s\" % max_body)\n                self.completed = True\n            elif br.error:\n                # garbage in chunked encoding input probably\n                self.error = br.error\n                self.completed = True\n            elif br.completed:\n                # The request (with the body) is ready to use.\n                self.completed = True\n                if self.chunked:\n                    # We've converted the chunked transfer encoding request\n                    # body into a normal request body, so we know its content\n                    # length; set the header here.  We already popped the\n                    # TRANSFER_ENCODING header in parse_header, so this will\n                    # appear to the client to be an entirely non-chunked HTTP\n                    # request with a valid content-length.\n                    self.headers[\"CONTENT_LENGTH\"] = str(br.__len__())\n            return consumed\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_object(self, indirect_reference: IndirectObject) -> Optional[PdfObject]:\n        retval = self.cache_get_indirect_object(\n            indirect_reference.generation, indirect_reference.idnum\n        )\n        if retval is not None:\n            return retval\n        if (\n            indirect_reference.generation == 0\n            and indirect_reference.idnum in self.xref_objStm\n        ):\n            retval = self._get_object_from_stream(indirect_reference)  # type: ignore\n        elif (\n            indirect_reference.generation in self.xref\n            and indirect_reference.idnum in self.xref[indirect_reference.generation]\n        ):\n            if self.xref_free_entry.get(indirect_reference.generation, {}).get(\n                indirect_reference.idnum, False\n            ):\n                return NullObject()\n            start = self.xref[indirect_reference.generation][indirect_reference.idnum]\n            self.stream.seek(start, 0)\n            try:\n                idnum, generation = self.read_object_header(self.stream)\n            except Exception:\n                if hasattr(self.stream, \"getbuffer\"):\n                    buf = bytes(self.stream.getbuffer())  # type: ignore\n                else:\n                    p = self.stream.tell()\n                    self.stream.seek(0, 0)\n                    buf = self.stream.read(-1)\n                    self.stream.seek(p, 0)\n                m = re.search(\n                    rf\"\\s{indirect_reference.idnum}\\s+{indirect_reference.generation}\\s+obj\".encode(),\n                    buf,\n                )\n                if m is not None:\n                    logger_warning(\n                        f\"Object ID {indirect_reference.idnum},{indirect_reference.generation} ref repaired\",\n                        __name__,\n                    )\n                    self.xref[indirect_reference.generation][\n                        indirect_reference.idnum\n                    ] = (m.start(0) + 1)\n                    self.stream.seek(m.start(0) + 1)\n                    idnum, generation = self.read_object_header(self.stream)\n                else:\n                    idnum = -1  # exception will be raised below\n            if idnum != indirect_reference.idnum and self.xref_index:\n                # Xref table probably had bad indexes due to not being zero-indexed\n                if self.strict:\n                    raise PdfReadError(\n                        f\"Expected object ID ({indirect_reference.idnum} {indirect_reference.generation}) \"\n                        f\"does not match actual ({idnum} {generation}); \"\n                        \"xref table not zero-indexed.\"\n                    )\n                # xref table is corrected in non-strict mode\n            elif idnum != indirect_reference.idnum and self.strict:\n                # some other problem\n                raise PdfReadError(\n                    f\"Expected object ID ({indirect_reference.idnum} \"\n                    f\"{indirect_reference.generation}) does not match actual \"\n                    f\"({idnum} {generation}).\"\n                )\n            if self.strict:\n                assert generation == indirect_reference.generation\n            retval = read_object(self.stream, self)  # type: ignore\n\n            # override encryption is used for the /Encrypt dictionary\n            if not self._override_encryption and self._encryption is not None:\n                # if we don't have the encryption key:\n                if not self._encryption.is_decrypted():\n                    raise FileNotDecryptedError(\"File has not been decrypted\")\n                # otherwise, decrypt here...\n                retval = cast(PdfObject, retval)\n                retval = self._encryption.decrypt_object(\n                    retval, indirect_reference.idnum, indirect_reference.generation\n                )\n        else:\n            if hasattr(self.stream, \"getbuffer\"):\n                buf = bytes(self.stream.getbuffer())  # type: ignore\n            else:\n                p = self.stream.tell()\n                self.stream.seek(0, 0)\n                buf = self.stream.read(-1)\n                self.stream.seek(p, 0)\n            m = re.search(\n                rf\"\\s{indirect_reference.idnum}\\s+{indirect_reference.generation}\\s+obj\".encode(),\n                buf,\n            )\n            if m is not None:\n                logger_warning(\n                    f\"Object {indirect_reference.idnum} {indirect_reference.generation} found\",\n                    __name__,\n                )\n                if indirect_reference.generation not in self.xref:\n                    self.xref[indirect_reference.generation] = {}\n                self.xref[indirect_reference.generation][indirect_reference.idnum] = (\n                    m.start(0) + 1\n                )\n                self.stream.seek(m.end(0) + 1)\n                skip_over_whitespace(self.stream)\n                self.stream.seek(-1, 1)\n                retval = read_object(self.stream, self)  # type: ignore\n\n                # override encryption is used for the /Encrypt dictionary\n                if not self._override_encryption and self._encryption is not None:\n                    # if we don't have the encryption key:\n                    if not self._encryption.is_decrypted():\n                        raise FileNotDecryptedError(\"File has not been decrypted\")\n                    # otherwise, decrypt here...\n                    retval = cast(PdfObject, retval)\n                    retval = self._encryption.decrypt_object(\n                        retval, indirect_reference.idnum, indirect_reference.generation\n                    )\n            else:\n                logger_warning(\n                    f\"Object {indirect_reference.idnum} {indirect_reference.generation} not defined.\",\n                    __name__,\n                )\n                if self.strict:\n                    raise PdfReadError(\"Could not find object.\")\n        self.cache_indirect_object(\n            indirect_reference.generation, indirect_reference.idnum, retval\n        )\n        return retval\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):\n        if not check_archive_path(arcname):\n            raise ValueError(f\"Specified path is bad: {arcname}\")\n        return self._writestr(data, arcname)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def main(self):\n        while True:\n            request = self.rpc_read()\n            self.log(request)\n            if request is None:\n                self.log('No more requests, stop handler')\n                return\n            cmd = self.conf('cmd')\n            if not cmd:\n                return\n            if request.method == 'notification':\n                icon_data = self.read() if request.params.get('packageIcon', False) else None\n                if request.params.get('event') == 'posted':\n                    text, package = map(request.params.get, ('text', 'package'))\n                    title = request.params.get('title', 'NULL')\n                    name, uin = self.device.name, self.device.uin\n                    if text is None:\n                        text = ''\n                    icon_path = os.path.join(self.conf('icon_dir'), f'{package}.{self.device.uin}.icon.png')\n                    if bool(icon_data):\n                        try:\n                            open(icon_path, 'wb').write(icon_data)\n                        except Exception as e:\n                            self.log(e, logging.WARNING)\n                    icon = icon_path if icon_data else ''\n                    command = cmd.format(uin=quote(uin), name=quote(name), icon=quote(icon), text=quote(text), title=quote(title), package=quote(package))\n                    self.log('Execute: \"{}\"'.format(command))\n                    subprocess.call(command, shell=True)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _save(self, name, content):\n        full_path = self.path(name)\n\n        # Create any intermediate directories that do not exist.\n        directory = os.path.dirname(full_path)\n        if not os.path.exists(directory):\n            try:\n                if self.directory_permissions_mode is not None:\n                    # Set the umask because os.makedirs() doesn't apply the \"mode\"\n                    # argument to intermediate-level directories.\n                    old_umask = os.umask(0o777 & ~self.directory_permissions_mode)\n                    try:\n                        os.makedirs(directory, self.directory_permissions_mode)\n                    finally:\n                        os.umask(old_umask)\n                else:\n                    os.makedirs(directory)\n            except FileExistsError:\n                # There's a race between os.path.exists() and os.makedirs().\n                # If os.makedirs() fails with FileExistsError, the directory\n                # was created concurrently.\n                pass\n        if not os.path.isdir(directory):\n            raise IOError(\"%s exists and is not a directory.\" % directory)\n\n        # There's a potential race condition between get_available_name and\n        # saving the file; it's possible that two threads might return the\n        # same name, at which point all sorts of fun happens. So we need to\n        # try to create the file, but if it already exists we have to go back\n        # to get_available_name() and try again.\n\n        while True:\n            try:\n                # This file has a file path that we can move.\n                if hasattr(content, 'temporary_file_path'):\n                    file_move_safe(content.temporary_file_path(), full_path)\n\n                # This is a normal uploadedfile that we can stream.\n                else:\n                    # The current umask value is masked out by os.open!\n                    fd = os.open(full_path, self.OS_OPEN_FLAGS, 0o666)\n                    _file = None\n                    try:\n                        locks.lock(fd, locks.LOCK_EX)\n                        for chunk in content.chunks():\n                            if _file is None:\n                                mode = 'wb' if isinstance(chunk, bytes) else 'wt'\n                                _file = os.fdopen(fd, mode)\n                            _file.write(chunk)\n                    finally:\n                        locks.unlock(fd)\n                        if _file is not None:\n                            _file.close()\n                        else:\n                            os.close(fd)\n            except FileExistsError:\n                # A new name is needed if the file exists.\n                name = self.get_available_name(name)\n                full_path = self.path(name)\n            else:\n                # OK, the file save worked. Break out of the loop.\n                break\n\n        if self.file_permissions_mode is not None:\n            os.chmod(full_path, self.file_permissions_mode)\n\n        # Ensure the saved path is always relative to the storage root.\n        name = os.path.relpath(full_path, self.location)\n        # Store filenames with forward slashes, even on Windows.\n        return name.replace('\\\\', '/')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def login(self, login, password, url=None):\n        \"\"\"login page\n        \"\"\"\n        auth = self._auth(login, password)\n        cherrypy.session['isadmin'] = auth['isadmin']\n        cherrypy.session['connected'] = auth['connected']\n\n        if auth['connected']:\n            if auth['isadmin']:\n                message = \\\n                    \"login success for user '%(user)s' as administrator\" % {\n                        'user': login\n                    }\n            else:\n                message = \\\n                    \"login success for user '%(user)s' as normal user\" % {\n                        'user': login\n                    }\n            cherrypy.log.error(\n                msg=message,\n                severity=logging.INFO\n            )\n            cherrypy.session[SESSION_KEY] = cherrypy.request.login = login\n            if url is None:\n                redirect = \"/\"\n            else:\n                redirect = url\n            raise cherrypy.HTTPRedirect(redirect)\n        else:\n            message = \"login failed for user '%(user)s'\" % {\n                'user': login\n            }\n            cherrypy.log.error(\n                msg=message,\n                severity=logging.WARNING\n            )\n            if url is None:\n                qs = ''\n            else:\n                qs = '?url=' + urllib.quote_plus(url)\n            raise cherrypy.HTTPRedirect(\"/signin\" + qs)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _prepare_cli_cmd(self) -> list[Any]:\n        \"\"\"This function creates the command list from available information.\"\"\"\n        conn = self.conn\n        hive_bin = \"hive\"\n        cmd_extra = []\n\n        if self.use_beeline:\n            hive_bin = \"beeline\"\n            self._validate_beeline_parameters(conn)\n            jdbc_url = f\"jdbc:hive2://{conn.host}:{conn.port}/{conn.schema}\"\n            if conf.get(\"core\", \"security\") == \"kerberos\":\n                template = conn.extra_dejson.get(\"principal\", \"hive/_HOST@EXAMPLE.COM\")\n                if \"_HOST\" in template:\n                    template = utils.replace_hostname_pattern(utils.get_components(template))\n                proxy_user = self._get_proxy_user()\n                if \";\" in template:\n                    raise RuntimeError(\"The principal should not contain the ';' character\")\n                jdbc_url += f\";principal={template};{proxy_user}\"\n            elif self.auth:\n                jdbc_url += \";auth=\" + self.auth\n\n            jdbc_url = f'\"{jdbc_url}\"'\n\n            cmd_extra += [\"-u\", jdbc_url]\n            if conn.login:\n                cmd_extra += [\"-n\", conn.login]\n            if conn.password:\n                cmd_extra += [\"-p\", conn.password]\n\n        hive_params_list = self.hive_cli_params.split()\n\n        return [hive_bin] + cmd_extra + hive_params_list\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def gravatar(context, user, size=None):\n    \"\"\"\n    Outputs the HTML for displaying a user's gravatar.\n\n    This can take an optional size of the image (defaults to 80 if not\n    specified).\n\n    This is also influenced by the following settings:\n\n        GRAVATAR_SIZE    - Default size for gravatars\n        GRAVATAR_RATING  - Maximum allowed rating (g, pg, r, x)\n        GRAVATAR_DEFAULT - Default image set to show if the user hasn't\n                           specified a gravatar (identicon, monsterid, wavatar)\n\n    See http://www.gravatar.com/ for more information.\n    \"\"\"\n    url = get_gravatar_url(context['request'], user, size)\n\n    if url:\n        return format_html(\n            '<img src=\"{0}\" width=\"{1}\" height=\"{1}\" alt=\"{2}\" '\n            'class=\"gravatar\"/>',\n            url, size, user.get_full_name() or user.username)\n    else:\n        return ''\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _recv_frame(self, message=None):\n        recv = self._get_bytes\n\n        # Unpacking the frame described in Section 5.2 of RFC6455\n        # (https://tools.ietf.org/html/rfc6455#section-5.2)\n        header = recv(2)\n        a, b = struct.unpack('!BB', header)\n        finished = a >> 7 == 1\n        rsv123 = a >> 4 & 7\n        rsv1 = rsv123 & 4\n        if rsv123:\n            if rsv1 and \"permessage-deflate\" not in self.extensions:\n                # must be zero - unless it's compressed then rsv1 is true\n                raise FailedConnectionError(\n                    1002,\n                    \"RSV1, RSV2, RSV3: MUST be 0 unless an extension is\"\n                    \" negotiated that defines meanings for non-zero values.\")\n        opcode = a & 15\n        if opcode not in (0, 1, 2, 8, 9, 0xA):\n            raise FailedConnectionError(1002, \"Unknown opcode received.\")\n        masked = b & 128 == 128\n        if not masked and not self.client:\n            raise FailedConnectionError(1002, \"A client MUST mask all frames\"\n                                        \" that it sends to the server\")\n        length = b & 127\n        if opcode & 8:\n            if not finished:\n                raise FailedConnectionError(1002, \"Control frames must not\"\n                                            \" be fragmented.\")\n            if length > 125:\n                raise FailedConnectionError(\n                    1002,\n                    \"All control frames MUST have a payload length of 125\"\n                    \" bytes or less\")\n        elif opcode and message:\n            raise FailedConnectionError(\n                1002,\n                \"Received a non-continuation opcode within\"\n                \" fragmented message.\")\n        elif not opcode and not message:\n            raise FailedConnectionError(\n                1002,\n                \"Received continuation opcode with no previous\"\n                \" fragments received.\")\n        if length == 126:\n            length = struct.unpack('!H', recv(2))[0]\n        elif length == 127:\n            length = struct.unpack('!Q', recv(8))[0]\n\n        if length > self.max_frame_length:\n            raise FailedConnectionError(1009, \"Incoming frame of {} bytes is above length limit of {} bytes.\".format(\n                length, self.max_frame_length))\n        if masked:\n            mask = struct.unpack('!BBBB', recv(4))\n        received = 0\n        if not message or opcode & 8:\n            decoder = self.UTF8Decoder() if opcode == 1 else None\n            decompressor = self._get_permessage_deflate_dec(rsv1)\n            message = self.Message(opcode, self.max_frame_length, decoder=decoder, decompressor=decompressor)\n        if not length:\n            message.push(b'', final=finished)\n        else:\n            while received < length:\n                d = self.socket.recv(length - received)\n                if not d:\n                    raise ConnectionClosedError()\n                dlen = len(d)\n                if masked:\n                    d = self._apply_mask(d, mask, length=dlen, offset=received)\n                received = received + dlen\n                try:\n                    message.push(d, final=finished)\n                except (UnicodeDecodeError, ValueError):\n                    raise FailedConnectionError(\n                        1007, \"Text data must be valid utf-8\")\n        return message\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        dir_name, file_name = os.path.split(name)\n        if '..' in pathlib.PurePath(dir_name).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n        validate_file_name(file_name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, add an underscore and a random 7\n        # character alphanumeric string (before the file extension, if one\n        # exists) to the filename until the generated filename doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n        return name\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def on_file(file):\n            nonlocal file_object\n            data[\"file\"] = clear_filename(file.file_name.decode())\n            file_object = file.file_object\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    OpInputList indices_list_in;\n    OP_REQUIRES_OK(context, context->input_list(\"indices\", &indices_list_in));\n    OpInputList values_list_in;\n    OP_REQUIRES_OK(context, context->input_list(\"values\", &values_list_in));\n    OpInputList shapes_list_in;\n    OP_REQUIRES_OK(context, context->input_list(\"shapes\", &shapes_list_in));\n    OpInputList dense_list_in;\n    OP_REQUIRES_OK(context,\n                   context->input_list(\"dense_inputs\", &dense_list_in));\n\n    // Set internal_type to invalid_type so that the check will be ignored.\n    DataType internal_type = DT_INVALID;\n    OP_REQUIRES_OK(\n        context, ValidateInput(indices_list_in, values_list_in, shapes_list_in,\n                               dense_list_in, internal_type));\n\n    const Tensor* sep_t;\n    OP_REQUIRES_OK(context, context->input(\"sep\", &sep_t));\n    const tstring separator = sep_t->scalar<tstring>()();\n\n    std::vector<std::unique_ptr<ColumnInterface<tstring>>> columns =\n        GenerateColumnsFromInput<tstring>(indices_list_in, values_list_in,\n                                          shapes_list_in, dense_list_in);\n    Tensor* indices_out;\n    Tensor* values_out;\n    Tensor* shape_out;\n    const int64 batch_size = CalculateBatchSize(shapes_list_in, dense_list_in);\n    std::vector<int64> output_start_indices(batch_size);\n    OP_REQUIRES_OK(\n        context,\n        CreateOutputTensors(columns, batch_size, context, &indices_out,\n                            &values_out, &shape_out, &output_start_indices));\n    StringCrosser<tstring> crosser(columns, 0, 0, separator);\n    OutputUpdater<tstring> updater(output_start_indices, indices_out,\n                                   values_out);\n    auto do_work = [&columns, crosser, updater](int64 begin, int64 end) {\n      for (int b = begin; b < end; b++) {\n        ProductIterator<tstring> product_iterator(columns, b);\n        int64 cross_count = 0;\n        while (product_iterator.HasNext()) {\n          const auto permutation = product_iterator.Next();\n          updater.Update(b, cross_count,\n                         crosser.Generate(b, permutation, false));\n          cross_count++;\n        }\n      }\n    };\n\n    auto* worker_threads = context->device()->tensorflow_cpu_worker_threads();\n    // TODO(zakaria): optimize kCostPerUnit\n    const int kCostPerUnit = 5000 * indices_list_in.size();\n    Shard(worker_threads->num_threads, worker_threads->workers, batch_size,\n          kCostPerUnit, do_work);\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status CompressElement(const std::vector<Tensor>& element,\n                       CompressedElement* out) {\n  // Step 1: Determine the total uncompressed size. This requires serializing\n  // non-memcopyable tensors, which we save to use again later.\n  std::vector<TensorProto> non_memcpy_components;\n  int64 total_size = 0;\n  for (auto& component : element) {\n    if (DataTypeCanUseMemcpy(component.dtype())) {\n      const TensorBuffer* buffer = DMAHelper::buffer(&component);\n      if (buffer) {\n        total_size += buffer->size();\n      }\n    } else {\n      non_memcpy_components.emplace_back();\n      component.AsProtoTensorContent(&non_memcpy_components.back());\n      total_size += non_memcpy_components.back().ByteSizeLong();\n    }\n  }\n\n  // Step 2: Write the tensor data to a buffer, and compress that buffer.\n  // We use tstring for access to resize_uninitialized.\n  tstring uncompressed;\n  uncompressed.resize_uninitialized(total_size);\n  // Position in `uncompressed` to write the next component.\n  char* position = uncompressed.mdata();\n  int non_memcpy_component_index = 0;\n  for (auto& component : element) {\n    CompressedComponentMetadata* metadata =\n        out->mutable_component_metadata()->Add();\n    metadata->set_dtype(component.dtype());\n    component.shape().AsProto(metadata->mutable_tensor_shape());\n    if (DataTypeCanUseMemcpy(component.dtype())) {\n      const TensorBuffer* buffer = DMAHelper::buffer(&component);\n      if (buffer) {\n        memcpy(position, buffer->data(), buffer->size());\n        metadata->set_tensor_size_bytes(buffer->size());\n      }\n    } else {\n      TensorProto& proto = non_memcpy_components[non_memcpy_component_index++];\n      proto.SerializeToArray(position, proto.ByteSizeLong());\n      metadata->set_tensor_size_bytes(proto.ByteSizeLong());\n    }\n    position += metadata->tensor_size_bytes();\n  }\n  DCHECK_EQ(position, uncompressed.mdata() + total_size);\n\n  if (!port::Snappy_Compress(uncompressed.mdata(), total_size,\n                             out->mutable_data())) {\n    return errors::Internal(\"Failed to compress using snappy.\");\n  }\n  VLOG(3) << \"Compressed element from \" << total_size << \" bytes to \"\n          << out->data().size() << \" bytes\";\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def get_metric_variables(self) -> List[Any]:\n    \"\"\"Returns a list containing the metric variable values.\"\"\"\n    with self._lock:\n      return self._get_metric_variables()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext *ctx) override {\n    const Tensor *indices_t, *values_t, *shape_t, *reduction_axes_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_indices\", &indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_values\", &values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_shape\", &shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"reduction_axes\", &reduction_axes_t));\n\n    OP_REQUIRES_OK(ctx, ValidateInputs(shape_t, reduction_axes_t));\n\n    // TODO(zongheng): we will call Reorder() below, which will modify\n    // in-place the underlying indices and values buffers.  To avoid\n    // surprises of this kernel being stateful, we work around the above by\n    // making deep copies here.  Remove this if/when we change Reorder()'s\n    // semantics.\n    const auto shape_vec = shape_t->vec<int64>();\n    SparseTensor sp;\n    OP_REQUIRES_OK(ctx, SparseTensor::Create(\n        tensor::DeepCopy(*indices_t), tensor::DeepCopy(*values_t),\n                    TensorShape(shape_vec), &sp));\n    ReduceDetails reduction = SparseTensorReduceHelper(\n        sp, reduction_axes_t->flat<int32>(), keep_dims_);\n\n    Tensor *out_values;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, reduction.reduced_shape, &out_values));\n    auto out_flat = out_values->flat<T>();\n    out_flat.setZero();\n\n    Tensor tmp_reduced_val;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                           TensorShape({}), &tmp_reduced_val));\n    auto reduced_val = tmp_reduced_val.scalar<T>();\n\n    // Compute strides, and use it to convert coords to flat index.  The\n    // coordinates returned by .group() have the same ndims as group_by_dims.\n    gtl::InlinedVector<int64, 8> output_strides(reduction.group_by_dims.size());\n    if (!output_strides.empty()) {  // Do this iff we don't reduce all.\n      output_strides.back() = 1;\n      for (int d = output_strides.size() - 2; d >= 0; --d) {\n        output_strides[d] =\n            output_strides[d + 1] * shape_vec(reduction.group_by_dims[d + 1]);\n      }\n    }\n\n    auto CoordinatesToFlatIndex = [](ArraySlice<int64> coords,\n                                     ArraySlice<int64> strides) -> int64 {\n      if (strides.empty()) {  // Reduce all.\n        return 0;\n      }\n      CHECK_EQ(coords.size(), strides.size());\n      int64_t idx = 0;\n      for (int i = 0; i < coords.size(); ++i) {\n        idx += coords[i] * strides[i];\n      }\n      return idx;\n    };\n\n    // Each group maps one-on-one onto a value in the reduced tensor.\n    // g.group() provides the coordinates of a particular reduced value.\n    sp.Reorder<T>(reduction.reorder_dims);\n    for (const auto &g : sp.group(reduction.group_by_dims)) {\n      Op::template Run<T>(ctx, reduced_val, g.template values<T>());\n      OP_REQUIRES(ctx,\n                  output_strides.empty() ||\n                  (g.group().size() == output_strides.size()),\n                  errors::Internal(\n                      \"Expected group size and output_strides size to match\",\n                      \", but got \", g.group().size(), \" and \",\n                      output_strides.size()));\n      const int64_t idx = CoordinatesToFlatIndex(g.group(), output_strides);\n      OP_REQUIRES(ctx,\n                  idx >= 0 && idx < out_flat.size(),\n                  errors::Internal(\n                      \"Obtained a write index of \", idx,\n                      \" which is outside of bounds of [0, \",\n                      out_flat.size(), \")\"));\n      out_flat(idx) = reduced_val();\n      VLOG(2) << \"coords: \" << absl::StrJoin(g.group(), \",\")\n              << \"; idx: \" << idx << \"; group \" << Op::Name() << \": \"\n              << reduced_val();\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_files_from_storage(paths, signature):\n        \"\"\"Return S3 file where the name does not include the path.\"\"\"\n        try:\n            location = storage.aws_location\n        except AttributeError:\n            location = storage.location\n        signer = signing.Signer(\n            salt=f\"{S3FileInputMixin.__module__}.{S3FileInputMixin.__name__}\"\n        )\n        for path in paths:\n            path = pathlib.PurePosixPath(path)\n            print(path)\n            print(signer.signature(path.parent), signature)\n            if not constant_time_compare(signer.signature(path.parent), signature):\n                raise PermissionDenied(\"Illegal signature!\")\n            try:\n                relative_path = str(path.relative_to(location))\n            except ValueError as e:\n                raise SuspiciousFileOperation(\n                    f\"Path is not inside the designated upload location: {path}\"\n                ) from e\n\n            try:\n                f = storage.open(relative_path)\n                f.name = path.name\n                yield f\n            except (OSError, ValueError):\n                logger.exception(\"File not found: %s\", path)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def configure_branch(self, branch):\n        \"\"\"Configure repository branch.\"\"\"\n        if not self.on_branch(branch):\n            self.execute([\"update\", \"--\", branch])\n        self.branch = branch\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def import_prompt(request):\n   if not test_user_authenticated(request): return login(request, next=\"/cobbler_web/import/prompt\")\n   t = get_template('import.tmpl')\n   html = t.render(RequestContext(request,{\n       'version'  : remote.version(request.session['token']),\n       'username' : username,\n   }))\n   return HttpResponse(html)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self, *args, **data):\n        \"\"\"Process a single Onboarding Task instance.\"\"\"\n        self.logger.info(\"START: onboard device\")\n        self._parse_credentials(data[\"credentials\"])\n        platform = data[\"platform\"]\n        netdev = NetdevKeeper(\n            hostname=data[\"ip_address\"],\n            port=data[\"port\"],\n            timeout=data[\"timeout\"],\n            username=self.username,\n            password=self.password,\n            secret=self.secret,\n            napalm_driver=platform.napalm_driver if platform and platform.napalm_driver else None,\n            optional_args=platform.napalm_args if platform and platform.napalm_args else settings.NAPALM_ARGS,\n        )\n        netdev.get_onboarding_facts()\n        netdev_dict = netdev.get_netdev_dict()\n\n        onboarding_kwargs = {\n            # Kwargs extracted from OnboardingTask:\n            \"netdev_mgmt_ip_address\": data[\"ip_address\"],\n            \"netdev_nb_location_name\": data[\"location\"].name,\n            \"netdev_nb_device_type_name\": data[\"device_type\"],\n            \"netdev_nb_role_name\": data[\"role\"].name if data[\"role\"] else PLUGIN_SETTINGS[\"default_device_role\"],\n            \"netdev_nb_role_color\": PLUGIN_SETTINGS[\"default_device_role_color\"],\n            \"netdev_nb_platform_name\": data[\"platform\"].name if data[\"platform\"] else None,\n            # Kwargs discovered on the Onboarded Device:\n            \"netdev_hostname\": netdev_dict[\"netdev_hostname\"],\n            \"netdev_vendor\": netdev_dict[\"netdev_vendor\"],\n            \"netdev_model\": netdev_dict[\"netdev_model\"],\n            \"netdev_serial_number\": netdev_dict[\"netdev_serial_number\"],\n            \"netdev_mgmt_ifname\": netdev_dict[\"netdev_mgmt_ifname\"],\n            \"netdev_mgmt_pflen\": netdev_dict[\"netdev_mgmt_pflen\"],\n            \"netdev_netmiko_device_type\": netdev_dict[\"netdev_netmiko_device_type\"],\n            \"onboarding_class\": netdev_dict[\"onboarding_class\"],\n            \"driver_addon_result\": netdev_dict[\"driver_addon_result\"],\n        }\n        onboarding_cls = netdev_dict[\"onboarding_class\"]()\n        onboarding_cls.credentials = {\"username\": self.username, \"password\": self.password, \"secret\": self.secret}\n        onboarding_cls.run(onboarding_kwargs=onboarding_kwargs)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def parse_url(url):\n    \"\"\"\n    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is\n    performed to parse incomplete urls. Fields not provided will be None.\n    This parser is RFC 3986 compliant.\n\n    :param str url: URL to parse into a :class:`.Url` namedtuple.\n\n    Partly backwards-compatible with :mod:`urlparse`.\n\n    Example::\n\n        >>> parse_url('http://google.com/mail/')\n        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)\n        >>> parse_url('google.com:80')\n        Url(scheme=None, host='google.com', port=80, path=None, ...)\n        >>> parse_url('/foo?bar')\n        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)\n    \"\"\"\n    if not url:\n        # Empty\n        return Url()\n\n    is_string = not isinstance(url, six.binary_type)\n\n    # RFC 3986 doesn't like URLs that have a host but don't start\n    # with a scheme and we support URLs like that so we need to\n    # detect that problem and add an empty scheme indication.\n    # We don't get hurt on path-only URLs here as it's stripped\n    # off and given an empty scheme anyways.\n    if not SCHEME_REGEX.search(url):\n        url = \"//\" + url\n\n    def idna_encode(name):\n        if name and any([ord(x) > 128 for x in name]):\n            try:\n                import idna\n            except ImportError:\n                raise LocationParseError(\"Unable to parse URL without the 'idna' module\")\n            try:\n                return idna.encode(name.lower(), strict=True, std3_rules=True)\n            except idna.IDNAError:\n                raise LocationParseError(u\"Name '%s' is not a valid IDNA label\" % name)\n        return name\n\n    try:\n        split_iri = misc.IRI_MATCHER.match(compat.to_str(url)).groupdict()\n        iri_ref = rfc3986.IRIReference(\n            split_iri['scheme'], split_iri['authority'],\n            _encode_invalid_chars(split_iri['path'], PATH_CHARS),\n            _encode_invalid_chars(split_iri['query'], QUERY_CHARS),\n            _encode_invalid_chars(split_iri['fragment'], FRAGMENT_CHARS)\n        )\n        has_authority = iri_ref.authority is not None\n        uri_ref = iri_ref.encode(idna_encoder=idna_encode)\n    except (ValueError, RFC3986Exception):\n        return six.raise_from(LocationParseError(url), None)\n\n    # rfc3986 strips the authority if it's invalid\n    if has_authority and uri_ref.authority is None:\n        raise LocationParseError(url)\n\n    # Only normalize schemes we understand to not break http+unix\n    # or other schemes that don't follow RFC 3986.\n    if uri_ref.scheme is None or uri_ref.scheme.lower() in NORMALIZABLE_SCHEMES:\n        uri_ref = uri_ref.normalize()\n\n    # Validate all URIReference components and ensure that all\n    # components that were set before are still set after\n    # normalization has completed.\n    validator = Validator()\n    try:\n        validator.check_validity_of(\n            *validator.COMPONENT_NAMES\n        ).validate(uri_ref)\n    except ValidationError:\n        return six.raise_from(LocationParseError(url), None)\n\n    # For the sake of backwards compatibility we put empty\n    # string values for path if there are any defined values\n    # beyond the path in the URL.\n    # TODO: Remove this when we break backwards compatibility.\n    path = uri_ref.path\n    if not path:\n        if (uri_ref.query is not None\n                or uri_ref.fragment is not None):\n            path = \"\"\n        else:\n            path = None\n\n    # Ensure that each part of the URL is a `str` for\n    # backwards compatibility.\n    def to_input_type(x):\n        if x is None:\n            return None\n        elif not is_string and not isinstance(x, six.binary_type):\n            return x.encode('utf-8')\n        return x\n\n    return Url(\n        scheme=to_input_type(uri_ref.scheme),\n        auth=to_input_type(uri_ref.userinfo),\n        host=to_input_type(uri_ref.host),\n        port=int(uri_ref.port) if uri_ref.port is not None else None,\n        path=to_input_type(path),\n        query=to_input_type(uri_ref.query),\n        fragment=to_input_type(uri_ref.fragment)\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_many(self, data, timeout=DEFAULT_TIMEOUT, version=None):\n        safe_data = {}\n        original_keys = {}\n        for key, value in data.items():\n            safe_key = self.make_key(key, version=version)\n            self.validate_key(safe_key)\n            safe_data[safe_key] = value\n            original_keys[safe_key] = key\n        failed_keys = self._cache.set_multi(safe_data, self.get_backend_timeout(timeout))\n        return [original_keys[k] for k in failed_keys]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def get(self, request: web.Request) -> web.Response:\n        \"\"\"Return a list of persons if request comes from a local IP.\"\"\"\n        return self.json_message(\n            message=\"Not local\",\n            status_code=HTTPStatus.BAD_REQUEST,\n            message_code=\"not_local\",\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_file_transfer_type(sql: str) -> FileTransferType | None:\n        \"\"\"Decide whether a SQL is a file transfer and return its type.\n\n        None is returned if the SQL isn't a file transfer so that this function can be\n        used in an if-statement.\n        \"\"\"\n        return get_file_transfer_type(sql)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def explain(self, using, format=None, **options):\n        q = self.clone()\n        for option_name in options:\n            if (\n                not EXPLAIN_OPTIONS_PATTERN.fullmatch(option_name)\n                or \"--\" in option_name\n            ):\n                raise ValueError(f\"Invalid option name: {option_name!r}.\")\n        q.explain_info = ExplainInfo(format, options)\n        compiler = q.get_compiler(using=using)\n        return \"\\n\".join(compiler.explain_query())\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _generate_kwarg_handlers(context: Context, sig: FunctionSignature) -> List[Any]:\n    # generate kwarg handlers.\n    # since they might come in thru calldata or be default,\n    # allocate them in memory and then fill it in based on calldata or default,\n    # depending on the signature\n    # a kwarg handler looks like\n    # (if (eq _method_id <method_id>)\n    #    copy calldata args to memory\n    #    write default args to memory\n    #    goto external_function_common_ir\n\n    def handler_for(calldata_kwargs, default_kwargs):\n        calldata_args = sig.base_args + calldata_kwargs\n        # create a fake type so that get_element_ptr works\n        calldata_args_t = TupleT(list(arg.typ for arg in calldata_args))\n\n        abi_sig = sig.abi_signature_for_kwargs(calldata_kwargs)\n        method_id = _annotated_method_id(abi_sig)\n\n        calldata_kwargs_ofst = IRnode(\n            4, location=CALLDATA, typ=calldata_args_t, encoding=Encoding.ABI\n        )\n\n        # a sequence of statements to strictify kwargs into memory\n        ret = [\"seq\"]\n\n        # ensure calldata is at least of minimum length\n        args_abi_t = calldata_args_t.abi_type\n        calldata_min_size = args_abi_t.min_size() + 4\n        ret.append([\"assert\", [\"ge\", \"calldatasize\", calldata_min_size]])\n\n        # TODO optimize make_setter by using\n        # TupleT(list(arg.typ for arg in calldata_kwargs + default_kwargs))\n        # (must ensure memory area is contiguous)\n\n        n_base_args = len(sig.base_args)\n\n        for i, arg_meta in enumerate(calldata_kwargs):\n            k = n_base_args + i\n\n            dst = context.lookup_var(arg_meta.name).pos\n\n            lhs = IRnode(dst, location=MEMORY, typ=arg_meta.typ)\n\n            rhs = get_element_ptr(calldata_kwargs_ofst, k, array_bounds_check=False)\n\n            copy_arg = make_setter(lhs, rhs)\n            copy_arg.source_pos = getpos(arg_meta.ast_source)\n            ret.append(copy_arg)\n\n        for x in default_kwargs:\n            dst = context.lookup_var(x.name).pos\n            lhs = IRnode(dst, location=MEMORY, typ=x.typ)\n            lhs.source_pos = getpos(x.ast_source)\n            kw_ast_val = sig.default_values[x.name]  # e.g. `3` in x: int = 3\n            rhs = Expr(kw_ast_val, context).ir_node\n\n            copy_arg = make_setter(lhs, rhs)\n            copy_arg.source_pos = getpos(x.ast_source)\n            ret.append(copy_arg)\n\n        ret.append([\"goto\", sig.external_function_base_entry_label])\n\n        method_id_check = [\"eq\", \"_calldata_method_id\", method_id]\n\n        # if there is a function whose selector is 0, it won't be distinguished\n        # from the case where nil calldata is supplied, b/c calldataload loads\n        # 0s past the end of physical calldata (cf. yellow paper).\n        # since supplying 0 calldata is expected to trigger the fallback fn,\n        # we check that calldatasize > 0, which distinguishes the 0 selector\n        # from the fallback function \"selector\"\n        # (equiv. to \"all selectors not in the selector table\").\n\n        # note: cases where not enough calldata is supplied (besides\n        # calldatasize==0) are not addressed here b/c a calldatasize\n        # well-formedness check is already present in the function body\n        # as part of abi validation\n        if method_id.value == 0:\n            method_id_check = [\"and\", [\"gt\", \"calldatasize\", 0], method_id_check]\n\n        ret = [\"if\", method_id_check, ret]\n        return ret\n\n    ret = [\"seq\"]\n\n    keyword_args = sig.default_args\n\n    # allocate variable slots in memory\n    for arg in keyword_args:\n        context.new_variable(arg.name, arg.typ, is_mutable=False)\n\n    for i, _ in enumerate(keyword_args):\n        calldata_kwargs = keyword_args[:i]\n        default_kwargs = keyword_args[i:]\n\n        ret.append(handler_for(calldata_kwargs, default_kwargs))\n\n    ret.append(handler_for(keyword_args, []))\n\n    return ret\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _connect_tls_proxy(self, hostname, conn):\n        \"\"\"\n        Establish a TLS connection to the proxy using the provided SSL context.\n        \"\"\"\n        proxy_config = self.proxy_config\n        ssl_context = proxy_config.ssl_context\n        if ssl_context:\n            # If the user provided a proxy context, we assume CA and client\n            # certificates have already been set\n            return ssl_wrap_socket(\n                sock=conn,\n                server_hostname=hostname,\n                ssl_context=ssl_context,\n            )\n\n        ssl_context = create_proxy_ssl_context(\n            self.ssl_version,\n            self.cert_reqs,\n            self.ca_certs,\n            self.ca_cert_dir,\n            self.ca_cert_data,\n        )\n        # By default urllib3's SSLContext disables `check_hostname` and uses\n        # a custom check. For proxies we're good with relying on the default\n        # verification.\n        ssl_context.check_hostname = True\n\n        # If no cert was provided, use only the default options for server\n        # certificate validation\n        return ssl_wrap_socket(\n            sock=conn,\n            ca_certs=self.ca_certs,\n            ca_cert_dir=self.ca_cert_dir,\n            ca_cert_data=self.ca_cert_data,\n            server_hostname=hostname,\n            ssl_context=ssl_context,\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static Status ValidateSavedTensors(const GraphDef& graph_def) {\n  for (const auto& node : graph_def.node()) {\n    TF_RETURN_IF_ERROR(ValidateNode(node));\n  }\n\n  if (graph_def.has_library()) {\n    const FunctionDefLibrary& library = graph_def.library();\n    for (const auto& function : library.function()) {\n      for (const auto& node : function.node_def()) {\n        TF_RETURN_IF_ERROR(ValidateNode(node));\n      }\n    }\n  }\n\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& data = ctx->input(0);\n    OP_REQUIRES(ctx, data.dims() <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", data.dims()));\n\n    const Tensor& size_t = ctx->input(1);\n    const Tensor& weights = ctx->input(2);\n\n    OP_REQUIRES(ctx, size_t.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_t.dims()));\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (data.dims() == 1) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                     ctx, data.flat<Tidx>(), weights.flat<T>(), out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, data.flat<Tidx>(), weights.flat<T>(), out, size));\n      }\n    } else if (data.dims() == 2) {\n      const int64_t num_rows = data.dim_size(0);\n      auto weight_matrix =\n          (weights.NumElements() == 0)\n              ? weights.shaped<T, 2>(gtl::InlinedVector<int64_t, 2>(2, 0))\n              : weights.matrix<T>();\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      if (binary_output_) {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountReduceFunctor<Device, Tidx, T, true>::Compute(\n                     ctx, data.matrix<Tidx>(), weight_matrix, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx,\n            functor::BincountReduceFunctor<Device, Tidx, T, false>::Compute(\n                ctx, data.matrix<Tidx>(), weight_matrix, out, size));\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "            self.confirmChangePassword = function () {\n                if (!CONFIG_ACCESS_CONTROL) return;\n\n                self.updatePassword(\n                    self.currentUser().name,\n                    self.editor.password(),\n                    self.editor.currentPassword()\n                )\n                    .done(function () {\n                        // close dialog\n                        self.currentUser(undefined);\n                        self.changePasswordDialog.modal(\"hide\");\n                    })\n                    .fail(function (xhr) {\n                        if (xhr.status === 403) {\n                            self.currentPasswordMismatch(true);\n                        }\n                    });\n            };\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def clear(self):\n        dag_id = request.form.get('dag_id')\n        task_id = request.form.get('task_id')\n        origin = get_safe_url(request.form.get('origin'))\n        dag = current_app.dag_bag.get_dag(dag_id)\n\n        execution_date = request.form.get('execution_date')\n        execution_date = timezone.parse(execution_date)\n        confirmed = request.form.get('confirmed') == \"true\"\n        upstream = request.form.get('upstream') == \"true\"\n        downstream = request.form.get('downstream') == \"true\"\n        future = request.form.get('future') == \"true\"\n        past = request.form.get('past') == \"true\"\n        recursive = request.form.get('recursive') == \"true\"\n        only_failed = request.form.get('only_failed') == \"true\"\n\n        dag = dag.sub_dag(\n            task_regex=r\"^{0}$\".format(task_id),\n            include_downstream=downstream,\n            include_upstream=upstream)\n\n        end_date = execution_date if not future else None\n        start_date = execution_date if not past else None\n\n        return self._clear_dag_tis(dag, start_date, end_date, origin,\n                                   recursive=recursive, confirmed=confirmed, only_failed=only_failed)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def authenticate(template, title):\n    \"\"\"\n    Authentication management method.\n    :param template: the template name that should be rendered\n    :returns: rendered final page or a page with error message\n    \"\"\"\n    global AUTH_ISSUERS, SAML_SUPPORT, AUTH_TYPE\n    cookie = []\n    valid_token_dict = validate_webui_token()\n    if not valid_token_dict:\n        cookie.append({'key': 'rucio-requested-path', 'value': request.environ.get('REQUEST_URI')})\n    else:\n        return access_granted(valid_token_dict, template, title)\n\n    # login without any known server config\n    if not AUTH_TYPE:\n        return add_cookies(make_response(render_template(\"select_login_method.html\", oidc_issuers=AUTH_ISSUERS, saml_support=SAML_SUPPORT)), cookie)\n    # for AUTH_TYPE predefined by the server continue\n    else:\n        if AUTH_TYPE == 'userpass':\n            return redirect('/login', code=303)\n        elif AUTH_TYPE == 'x509':\n            return x509token_auth(None)\n        elif AUTH_TYPE == 'x509_userpass':\n            if request.environ.get('SSL_CLIENT_VERIFY') == 'SUCCESS':\n                return x509token_auth(None)\n            return render_template(\"no_certificate.html\")\n        elif AUTH_TYPE == 'oidc':\n            return oidc_auth(None, AUTH_ISSUER_WEBUI)\n        elif AUTH_TYPE == 'saml':\n            return saml_auth(\"GET\")\n        return render_template('problem.html', msg='Invalid auth type')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def validate_absolute_path(self, root: str, absolute_path: str) -> Optional[str]:\n        \"\"\"Validate and return the absolute path.\n\n        ``root`` is the configured path for the `StaticFileHandler`,\n        and ``path`` is the result of `get_absolute_path`\n\n        This is an instance method called during request processing,\n        so it may raise `HTTPError` or use methods like\n        `RequestHandler.redirect` (return None after redirecting to\n        halt further processing).  This is where 404 errors for missing files\n        are generated.\n\n        This method may modify the path before returning it, but note that\n        any such modifications will not be understood by `make_static_url`.\n\n        In instance methods, this method's result is available as\n        ``self.absolute_path``.\n\n        .. versionadded:: 3.1\n        \"\"\"\n        # os.path.abspath strips a trailing /.\n        # We must add it back to `root` so that we only match files\n        # in a directory named `root` instead of files starting with\n        # that prefix.\n        root = os.path.abspath(root)\n        if not root.endswith(os.path.sep):\n            # abspath always removes a trailing slash, except when\n            # root is '/'. This is an unusual case, but several projects\n            # have independently discovered this technique to disable\n            # Tornado's path validation and (hopefully) do their own,\n            # so we need to support it.\n            root += os.path.sep\n        # The trailing slash also needs to be temporarily added back\n        # the requested path so a request to root/ will match.\n        if not (absolute_path + os.path.sep).startswith(root):\n            raise HTTPError(403, \"%s is not in root static directory\", self.path)\n        if os.path.isdir(absolute_path) and self.default_filename is not None:\n            # need to look at the request.path here for when path is empty\n            # but there is some prefix to the path that was already\n            # trimmed by the routing\n            if not self.request.path.endswith(\"/\"):\n                if self.request.path.startswith(\"//\"):\n                    # A redirect with two initial slashes is a \"protocol-relative\" URL.\n                    # This means the next path segment is treated as a hostname instead\n                    # of a part of the path, making this effectively an open redirect.\n                    # Reject paths starting with two slashes to prevent this.\n                    # This is only reachable under certain configurations.\n                    raise HTTPError(\n                        403, \"cannot redirect path with two initial slashes\"\n                    )\n                self.redirect(self.request.path + \"/\", permanent=True)\n                return None\n            absolute_path = os.path.join(absolute_path, self.default_filename)\n        if not os.path.exists(absolute_path):\n            raise HTTPError(404)\n        if not os.path.isfile(absolute_path):\n            raise HTTPError(403, \"%s is not a file\", self.path)\n        return absolute_path\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _make_cmd(self, tmpfilename, info_dict):\n        cmd = [self.exe, '--location', '-o', tmpfilename, '--compressed']\n        if self.ydl.cookiejar.get_cookie_header(info_dict['url']):\n            cmd += ['--cookie-jar', self._write_cookies()]\n        if info_dict.get('http_headers') is not None:\n            for key, val in info_dict['http_headers'].items():\n                cmd += ['--header', f'{key}: {val}']\n\n        cmd += self._bool_option('--continue-at', 'continuedl', '-', '0')\n        cmd += self._valueless_option('--silent', 'noprogress')\n        cmd += self._valueless_option('--verbose', 'verbose')\n        cmd += self._option('--limit-rate', 'ratelimit')\n        retry = self._option('--retry', 'retries')\n        if len(retry) == 2:\n            if retry[1] in ('inf', 'infinite'):\n                retry[1] = '2147483647'\n            cmd += retry\n        cmd += self._option('--max-filesize', 'max_filesize')\n        cmd += self._option('--interface', 'source_address')\n        cmd += self._option('--proxy', 'proxy')\n        cmd += self._valueless_option('--insecure', 'nocheckcertificate')\n        cmd += self._configuration_args()\n        cmd += ['--', info_dict['url']]\n        return cmd\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def model_from_yaml(yaml_string, custom_objects=None):\n  \"\"\"Parses a yaml model configuration file and returns a model instance.\n\n  Note: Since TF 2.6, this method is no longer supported and will raise a\n  RuntimeError.\n\n  Args:\n      yaml_string: YAML string or open file encoding a model configuration.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n\n  Returns:\n      A Keras model instance (uncompiled).\n\n  Raises:\n      RuntimeError: announces that the method poses a security risk\n  \"\"\"\n  raise RuntimeError(\n      'Method `model_from_yaml()` has been removed due to security risk of '\n      'arbitrary code execution. Please use `Model.to_json()` and '\n      '`model_from_json()` instead.'\n  )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def handle_html_static(self, request):\n        \"\"\"Handle static html.\"\"\"\n        if not (page := request.path[1:]):\n            page = \"index.html\"\n        file = os.path.normpath(os.path.join(self.web_path, page))\n        if not file.startswith(self.web_path):\n            raise ValueError(f\"File access outside {self.web_path} not permitted.\")\n        try:\n            with open(file, encoding=\"utf-8\"):\n                return web.FileResponse(file)\n        except (FileNotFoundError, IsADirectoryError) as exc:\n            raise web.HTTPNotFound(reason=\"File not found\") from exc\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void DoImageProjectiveTransformOp(OpKernelContext* ctx,\n                                  const Interpolation& interpolation,\n                                  const Mode& fill_mode) {\n  const Tensor& images_t = ctx->input(0);\n  const Tensor& transform_t = ctx->input(1);\n  OP_REQUIRES(ctx, images_t.shape().dims() == 4,\n              errors::InvalidArgument(\"Input images must have rank 4\"));\n  OP_REQUIRES(ctx,\n              (TensorShapeUtils::IsMatrix(transform_t.shape()) &&\n               (transform_t.dim_size(0) == images_t.dim_size(0) ||\n                transform_t.dim_size(0) == 1) &&\n               transform_t.dim_size(1) == 8),\n              errors::InvalidArgument(\n                  \"Input transform should be num_images x 8 or 1 x 8\"));\n\n  int32_t out_height, out_width;\n  // Kernel is shared by legacy \"ImageProjectiveTransform\" op with 2 args.\n  if (ctx->num_inputs() >= 3) {\n    const Tensor& shape_t = ctx->input(2);\n    OP_REQUIRES(ctx, shape_t.dims() == 1,\n                errors::InvalidArgument(\"output shape must be 1-dimensional\",\n                                        shape_t.shape().DebugString()));\n    OP_REQUIRES(ctx, shape_t.NumElements() == 2,\n                errors::InvalidArgument(\"output shape must have two elements\",\n                                        shape_t.shape().DebugString()));\n    auto shape_vec = shape_t.vec<int32>();\n    out_height = shape_vec(0);\n    out_width = shape_vec(1);\n    OP_REQUIRES(ctx, out_height > 0 && out_width > 0,\n                errors::InvalidArgument(\"output dimensions must be positive\"));\n  } else {\n    // Shape is N (batch size), H (height), W (width), C (channels).\n    out_height = images_t.shape().dim_size(1);\n    out_width = images_t.shape().dim_size(2);\n  }\n\n  T fill_value(0);\n  // Kernel is shared by \"ImageProjectiveTransformV2\" with 3 args.\n  if (ctx->num_inputs() >= 4) {\n    const Tensor& fill_value_t = ctx->input(3);\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(fill_value_t.shape()),\n                errors::InvalidArgument(\"fill_value must be a scalar\",\n                                        fill_value_t.shape().DebugString()));\n    fill_value = static_cast<T>(*(fill_value_t.scalar<float>().data()));\n  }\n\n  Tensor* output_t;\n  TensorShape output_shape;\n  OP_REQUIRES_OK(\n      ctx, TensorShape::BuildTensorShape({images_t.dim_size(0), out_height,\n                                          out_width, images_t.dim_size(3)},\n                                         &output_shape));\n  OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &output_t));\n  auto output = output_t->tensor<T, 4>();\n  auto images = images_t.tensor<T, 4>();\n  auto transform = transform_t.matrix<float>();\n\n  (FillProjectiveTransform<Device, T>(interpolation))(\n      ctx->eigen_device<Device>(), &output, images, transform, fill_mode,\n      fill_value);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse_header(self, header_plus):\n        \"\"\"\n        Parses the header_plus block of text (the headers plus the\n        first line of the request).\n        \"\"\"\n        index = header_plus.find(b\"\\r\\n\")\n        if index >= 0:\n            first_line = header_plus[:index].rstrip()\n            header = header_plus[index + 2 :]\n        else:\n            raise ParsingError(\"HTTP message header invalid\")\n\n        if b\"\\r\" in first_line or b\"\\n\" in first_line:\n            raise ParsingError(\"Bare CR or LF found in HTTP message\")\n\n        self.first_line = first_line  # for testing\n\n        lines = get_header_lines(header)\n\n        headers = self.headers\n        for line in lines:\n            index = line.find(b\":\")\n            if index > 0:\n                key = line[:index]\n                if b\"_\" in key:\n                    continue\n                value = line[index + 1 :].strip()\n                key1 = tostr(key.upper().replace(b\"-\", b\"_\"))\n                # If a header already exists, we append subsequent values\n                # seperated by a comma. Applications already need to handle\n                # the comma seperated values, as HTTP front ends might do\n                # the concatenation for you (behavior specified in RFC2616).\n                try:\n                    headers[key1] += tostr(b\", \" + value)\n                except KeyError:\n                    headers[key1] = tostr(value)\n            # else there's garbage in the headers?\n\n        # command, uri, version will be bytes\n        command, uri, version = crack_first_line(first_line)\n        version = tostr(version)\n        command = tostr(command)\n        self.command = command\n        self.version = version\n        (\n            self.proxy_scheme,\n            self.proxy_netloc,\n            self.path,\n            self.query,\n            self.fragment,\n        ) = split_uri(uri)\n        self.url_scheme = self.adj.url_scheme\n        connection = headers.get(\"CONNECTION\", \"\")\n\n        if version == \"1.0\":\n            if connection.lower() != \"keep-alive\":\n                self.connection_close = True\n\n        if version == \"1.1\":\n            # since the server buffers data from chunked transfers and clients\n            # never need to deal with chunked requests, downstream clients\n            # should not see the HTTP_TRANSFER_ENCODING header; we pop it\n            # here\n            te = headers.pop(\"TRANSFER_ENCODING\", \"\")\n            if te.lower() == \"chunked\":\n                self.chunked = True\n                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n                self.body_rcv = ChunkedReceiver(buf)\n            expect = headers.get(\"EXPECT\", \"\").lower()\n            self.expect_continue = expect == \"100-continue\"\n            if connection.lower() == \"close\":\n                self.connection_close = True\n\n        if not self.chunked:\n            try:\n                cl = int(headers.get(\"CONTENT_LENGTH\", 0))\n            except ValueError:\n                cl = 0\n            self.content_length = cl\n            if cl > 0:\n                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n                self.body_rcv = FixedStreamReceiver(cl, buf)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_session_id_by_provider_id(self, provider_id, user):\n        \"\"\"\n        Return a tuple of logged-in session IDs by provider_id\n        \"\"\"\n        candidates = self._ss.get_user_sessions(user)\n\n        session_ids = []\n        for c in candidates:\n            key = c.keys()[0]\n            if c[key].get('provider_id') == provider_id:\n                samlsession = self._data_to_samlsession(key, c[key])\n                session_ids.append(samlsession.session_id.encode('utf-8'))\n\n        return tuple(session_ids)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _keyify(key):\n    key = escape(key.lower(), quote=True)\n    return _key_pattern.sub(' ', key)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def events(request):\n   \"\"\"\n   This page presents a list of all the events and links to the event log viewer.\n   \"\"\"\n   if not test_user_authenticated(request): return login(request, next=\"/cobbler_web/events\")\n   events = remote.get_events()\n\n   events2 = []\n   for id in events.keys():\n      (ttime, name, state, read_by) = events[id]\n      events2.append([id,time.asctime(time.localtime(ttime)),name,state])\n\n   def sorter(a,b):\n      return cmp(a[0],b[0])\n   events2.sort(sorter)\n\n   t = get_template('events.tmpl')\n   html = t.render(RequestContext(request,{\n       'results'  : events2,\n       'version'  : remote.version(request.session['token']),\n       'username' : username\n   }))\n   return HttpResponse(html)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _build_write_cert_pair(self, participant, base_name, ctx):\n        subject = participant.subject\n        if self.persistent_state and subject in self.persistent_state:\n            cert = x509.load_pem_x509_certificate(\n                self.persistent_state[subject][\"cert\"].encode(\"ascii\"), default_backend()\n            )\n            pri_key = serialization.load_pem_private_key(\n                self.persistent_state[subject][\"pri_key\"].encode(\"ascii\"), password=None, backend=default_backend()\n            )\n        else:\n            pri_key, cert = self.get_pri_key_cert(participant)\n            self.persistent_state[subject] = dict(\n                cert=serialize_cert(cert).decode(\"ascii\"), pri_key=serialize_pri_key(pri_key).decode(\"ascii\")\n            )\n        dest_dir = self.get_kit_dir(participant, ctx)\n        with open(os.path.join(dest_dir, f\"{base_name}.crt\"), \"wb\") as f:\n            f.write(serialize_cert(cert))\n        with open(os.path.join(dest_dir, f\"{base_name}.key\"), \"wb\") as f:\n            f.write(serialize_pri_key(pri_key))\n        pkcs12 = serialization.pkcs12.serialize_key_and_certificates(\n            subject.encode(\"ascii\"), pri_key, cert, None, serialization.BestAvailableEncryption(subject.encode(\"ascii\"))\n        )\n        with open(os.path.join(dest_dir, f\"{base_name}.pfx\"), \"wb\") as f:\n            f.write(pkcs12)\n        with open(os.path.join(dest_dir, \"rootCA.pem\"), \"wb\") as f:\n            f.write(self.serialized_cert)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& indices = context->input(0);\n    const Tensor& values = context->input(1);\n    const Tensor& shape = context->input(2);\n    const Tensor& weights = context->input(3);\n    bool use_weights = weights.NumElements() > 0;\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(indices.shape()),\n                errors::InvalidArgument(\n                    \"Input indices must be a 2-dimensional tensor. Got: \",\n                    indices.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(values.shape()),\n                errors::InvalidArgument(\"Input values must be a vector. Got: \",\n                                        values.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(shape.shape()),\n                errors::InvalidArgument(\"Input shape must be a vector. Got: \",\n                                        shape.shape().DebugString()));\n    OP_REQUIRES(context,\n                values.shape().dim_size(0) == indices.shape().dim_size(0),\n                errors::InvalidArgument(\n                    \"Number of values must match first dimension of indices.\",\n                    \"Got \", values.shape().dim_size(0),\n                    \" values, indices shape: \", indices.shape().DebugString()));\n    OP_REQUIRES(\n        context, shape.shape().dim_size(0) == indices.shape().dim_size(1),\n        errors::InvalidArgument(\n            \"Number of dimensions must match second dimension of indices.\",\n            \"Got \", shape.shape().dim_size(0),\n            \" dimensions, indices shape: \", indices.shape().DebugString()));\n    OP_REQUIRES(context, shape.NumElements() > 0,\n                errors::InvalidArgument(\n                    \"The shape argument requires at least one element.\"));\n    // Validate indices: each index must be valid for the corresponding\n    // dimension. This could be possibly done better.\n    const auto indices_values = indices.matrix<int64_t>();\n    const auto shape_vector = shape.vec<int64_t>();\n    int num_values = values.NumElements();  // same as first dim of indices\n    int rank = indices.shape().dim_size(1);\n    for (int i = 0; i < num_values; ++i) {\n      for (int j = 0; j < rank; ++j) {\n        OP_REQUIRES(\n            context,\n            indices_values(i, j) >= 0 && indices_values(i, j) < shape_vector(j),\n            errors::InvalidArgument(\n                \"Invalid index value at \", i, \": dimension \", j, \" has value \",\n                indices_values(i, j), \" which is not in [0, \", shape_vector(j),\n                \") (as given by dense shape \", shape.DebugString()));\n      }\n    }\n\n    if (use_weights) {\n      OP_REQUIRES(\n          context, weights.shape() == values.shape(),\n          errors::InvalidArgument(\n              \"Weights and values must have the same shape. Weight shape: \",\n              weights.shape().DebugString(),\n              \"; values shape: \", values.shape().DebugString()));\n    }\n\n    bool is_1d = shape.NumElements() == 1;\n    int num_batches = is_1d ? 1 : shape_vector(0);\n\n    const auto values_values = values.flat<T>();\n    const auto weight_values = weights.flat<W>();\n\n    auto per_batch_counts = BatchedMap<W>(num_batches);\n\n    T max_value = 0;\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      int batch = is_1d ? 0 : indices_values(idx, 0);\n      if (batch >= num_batches) {\n        OP_REQUIRES(context, batch < num_batches,\n                    errors::InvalidArgument(\n                        \"Indices value along the first dimension must be \",\n                        \"lower than the first index of the shape.\", \"Got \",\n                        batch, \" as batch and \", num_batches,\n                        \" as the first dimension of the shape.\"));\n      }\n      const auto& value = values_values(idx);\n      if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {\n        if (binary_output_) {\n          per_batch_counts[batch][value] = 1;\n        } else if (use_weights) {\n          per_batch_counts[batch][value] += weight_values(idx);\n        } else {\n          per_batch_counts[batch][value]++;\n        }\n        if (value > max_value) {\n          max_value = value;\n        }\n      }\n    }\n\n    int num_output_values = GetOutputSize(max_value, maxlength_, minlength_);\n    OP_REQUIRES_OK(context, OutputSparse<W>(per_batch_counts, num_output_values,\n                                            is_1d, context));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def setup(self, task_metadata: Dict) -> None:\n        \"\"\"\n        Invokes Terraform to provision supporting resources for the instance\n\n        \"\"\"\n\n        state_file = self._get_tf_statefile_path(task_metadata)\n\n        # Init Terraform (doing this in a blocking manner to avoid race conditions during init, better way would to be use asyncio\n        # locks or to ensure that terraform init is run just once)\n        subprocess.run([\"terraform init\"], cwd=self._TF_DIR, shell=True, check=True)\n\n        # await self._run_async_subprocess([\"terraform\", \"init\"], cwd=self._TF_DIR, log_output=True)\n\n        # Apply Terraform Plan\n        base_cmd = [\n            \"terraform\",\n            \"apply\",\n            \"-auto-approve\",\n            f\"-state={state_file}\",\n        ]\n\n        self.infra_vars = [\n            \"-var=name=covalent-task-{dispatch_id}-{node_id}\".format(\n                dispatch_id=task_metadata[\"dispatch_id\"],\n                node_id=task_metadata[\"node_id\"],\n            ),\n            f\"-var=instance_type={self.instance_type}\",\n            f\"-var=disk_size={self.volume_size}\",\n            f\"-var=key_file={self.ssh_key_file}\",\n            f\"-var=key_name={self.key_name}\",\n        ]\n\n        if self.region:\n            self.infra_vars += [\n                f\"-var=aws_region={self.region}\",\n            ]\n\n        if self.profile:\n            self.infra_vars += [f\"-var=aws_profile={self.profile}\"]\n\n        if self.credentials_file:\n            self.infra_vars += [f\"-var=aws_credentials={self.credentials_file}\"]\n\n        if self.vpc:\n            self.infra_vars += [f\"-var=vpc_id={self.vpc}\"]\n        if self.subnet:\n            self.infra_vars += [f\"-var=subnet_id={self.subnet}\"]\n\n        cmd = base_cmd + self.infra_vars\n\n        app_log.debug(f\"Running Terraform setup command: {cmd}\")\n\n        await self._run_async_subprocess(cmd, cwd=self._TF_DIR, log_output=True)\n\n        # Get Hostname from Terraform Output\n        self.hostname = await self._get_tf_output(\"hostname\", state_file)\n\n        # Get Username from Terraform Output\n        self.username = await self._get_tf_output(\"username\", state_file)\n\n        # Get Remote Cache from Terraform Output\n        self.remote_cache = await self._get_tf_output(\"remote_cache\", state_file)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def post(self, request, *args, **kwargs):\n        \"\"\"\n        Handles POST request.\n        \"\"\"\n        return_url = self.get_return_to(self.request.POST)\n        terms_ids = request.POST.getlist(\"terms\")\n\n        if not terms_ids:  # pragma: nocover\n            return HttpResponseRedirect(return_url)\n\n        if request.user.is_authenticated:\n            user = request.user\n        else:\n            # Get user out of saved pipeline from django-socialauth\n            if \"partial_pipeline\" in request.session:\n                user_pk = request.session[\"partial_pipeline\"][\"kwargs\"][\"user\"][\"pk\"]\n                user = User.objects.get(id=user_pk)\n            else:\n                return HttpResponseRedirect(\"/\")\n\n        store_ip_address = getattr(settings, \"TERMS_STORE_IP_ADDRESS\", True)\n        if store_ip_address:\n            ip_address = request.META.get(\n                getattr(settings, \"TERMS_IP_HEADER_NAME\", DEFAULT_TERMS_IP_HEADER_NAME)\n            )\n            if \",\" in ip_address:\n                ip_address = ip_address.split(\",\")[0].strip()\n        else:\n            ip_address = \"\"\n\n        for terms_id in terms_ids:\n            try:\n                new_user_terms = UserTermsAndConditions(\n                    user=user,\n                    terms=TermsAndConditions.objects.get(pk=int(terms_id)),\n                    ip_address=ip_address,\n                )\n                new_user_terms.save()\n            except IntegrityError:  # pragma: nocover\n                pass\n\n        return HttpResponseRedirect(return_url)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, url):\n        Error.__init__(self, 302, \"redirect\")\n        self.url = protect_redirect_url(unicode2bytes(url))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    auto x = ctx->input(0);\n    auto i = ctx->input(1);\n    auto v = ctx->input(2);\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(i.shape()),\n                errors::InvalidArgument(\"i must be a vector. \",\n                                        i.shape().DebugString()));\n    OP_REQUIRES(ctx, x.dims() == v.dims(),\n                errors::InvalidArgument(\n                    \"x and v shape doesn't match (ranks differ): \",\n                    x.shape().DebugString(), \" vs. \", v.shape().DebugString()));\n    for (int i = 1; i < x.dims(); ++i) {\n      OP_REQUIRES(\n          ctx, x.dim_size(i) == v.dim_size(i),\n          errors::InvalidArgument(\"x and v shape doesn't match at index \", i,\n                                  \" : \", x.shape().DebugString(), \" vs. \",\n                                  v.shape().DebugString()));\n    }\n    OP_REQUIRES(ctx, i.dim_size(0) == v.dim_size(0),\n                errors::InvalidArgument(\n                    \"i and x shape doesn't match at index 0: \",\n                    i.shape().DebugString(), \" vs. \", v.shape().DebugString()));\n\n    Tensor y = x;  // This creates an alias intentionally.\n    // Skip processing if tensors are empty.\n    if (x.NumElements() > 0 && v.NumElements() > 0) {\n      OP_REQUIRES_OK(ctx, DoCompute(ctx, i, v, &y));\n    }\n    ctx->set_output(0, y);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self):\n        if not self.should_run():\n            print(\"bower dependencies up to date\")\n            return\n        \n        if self.should_run_npm():\n            print(\"installing build dependencies with npm\")\n            run(['npm', 'install'], cwd=repo_root)\n            os.utime(self.node_modules, None)\n        \n        env = os.environ.copy()\n        env['PATH'] = npm_path\n        \n        try:\n            run(\n                ['bower', 'install', '--allow-root', '--config.interactive=false'],\n                cwd=repo_root,\n                env=env\n            )\n        except OSError as e:\n            print(\"Failed to run bower: %s\" % e, file=sys.stderr)\n            print(\"You can install js dependencies with `npm install`\", file=sys.stderr)\n            raise\n        # self.npm_components()\n        if not os.path.exists(self.sanitizer_dir):\n            run(['npm', 'run', 'build:webpack'], cwd=repo_root, env=env)\n        os.utime(self.bower_dir, None)\n        # update package data in case this created new files\n        update_package_data(self.distribution)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def complete_reg(request):\n    \"\"\"Completes the registeration, called by API\"\"\"\n    try:\n        if not \"fido_state\" in request.session:\n            return JsonResponse({'status': 'ERR', \"message\": \"FIDO Status can't be found, please try again\"})\n        data = cbor.decode(request.body)\n\n        client_data = CollectedClientData(data['clientDataJSON'])\n        att_obj = AttestationObject((data['attestationObject']))\n        server = getServer()\n        auth_data = server.register_complete(\n            request.session.pop['fido_state'],\n            client_data,\n            att_obj\n        )\n        encoded = websafe_encode(auth_data.credential_data)\n        uk = User_Keys()\n        uk.username = request.user.username\n        uk.properties = {\"device\": encoded, \"type\": att_obj.fmt, }\n        uk.owned_by_enterprise = getattr(settings, \"MFA_OWNED_BY_ENTERPRISE\", False)\n        uk.key_type = \"FIDO2\"\n        uk.save()\n        return HttpResponse(simplejson.dumps({'status': 'OK'}))\n    except Exception as exp:\n        import traceback\n        print(traceback.format_exc())\n        try:\n            from raven.contrib.django.raven_compat.models import client\n            client.captureException()\n        except:\n            pass\n        return JsonResponse({'status': 'ERR', \"message\": \"Error on server, please try again later\"})\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def oauth_authorized(self, provider: str) -> Response:\n        log.debug(\"Authorized init\")\n        if provider not in self.appbuilder.sm.oauth_remotes:\n            flash(u\"Provider not supported.\", \"warning\")\n            log.warning(\"OAuth authorized got an unknown provider %s\", provider)\n            return redirect(self.appbuilder.get_url_for_login)\n        resp = self.appbuilder.sm.oauth_remotes[provider].authorize_access_token()\n        if resp is None:\n            flash(u\"You denied the request to sign in.\", \"warning\")\n            return redirect(self.appbuilder.get_url_for_login)\n        log.debug(\"OAUTH Authorized resp: {0}\".format(resp))\n        # Retrieves specific user info from the provider\n        try:\n            self.appbuilder.sm.set_oauth_session(provider, resp)\n            userinfo = self.appbuilder.sm.oauth_user_info(provider, resp)\n        except Exception as e:\n            log.error(\"Error returning OAuth user info: {0}\".format(e))\n            user = None\n        else:\n            log.debug(\"User info retrieved from {0}: {1}\".format(provider, userinfo))\n            # User email is not whitelisted\n            if provider in self.appbuilder.sm.oauth_whitelists:\n                whitelist = self.appbuilder.sm.oauth_whitelists[provider]\n                allow = False\n                for e in whitelist:\n                    if re.search(e, userinfo[\"email\"]):\n                        allow = True\n                        break\n                if not allow:\n                    flash(u\"You are not authorized.\", \"warning\")\n                    return redirect(self.appbuilder.get_url_for_login)\n            else:\n                log.debug(\"No whitelist for OAuth provider\")\n            user = self.appbuilder.sm.auth_user_oauth(userinfo)\n\n        if user is None:\n            flash(as_unicode(self.invalid_login_message), \"warning\")\n            return redirect(self.appbuilder.get_url_for_login)\n        else:\n            login_user(user)\n            try:\n                state = jwt.decode(\n                    request.args[\"state\"],\n                    self.appbuilder.app.config[\"SECRET_KEY\"],\n                    algorithms=[\"HS256\"],\n                )\n            except jwt.InvalidTokenError:\n                raise Exception(\"State signature is not valid!\")\n\n            next_url = self.appbuilder.get_url_for_index\n            # Check if there is a next url on state\n            if \"next\" in state and len(state[\"next\"]) > 0:\n                parsed_uri = urlparse(state[\"next\"][0])\n                if parsed_uri.netloc != request.host:\n                    log.warning(\"Got an invalid next URL: %s\", parsed_uri.netloc)\n                else:\n                    next_url = state[\"next\"][0]\n            return redirect(next_url)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void RestoreTensor(OpKernelContext* context,\n                   checkpoint::TensorSliceReader::OpenTableFunction open_func,\n                   int preferred_shard, bool restore_slice, int restore_index) {\n  const Tensor& file_pattern_t = context->input(0);\n  {\n    const int64_t size = file_pattern_t.NumElements();\n    OP_REQUIRES(\n        context, size == 1,\n        errors::InvalidArgument(\n            \"Input 0 (file_pattern) must be a string scalar; got a tensor of \",\n            size, \" elements\"));\n  }\n  const string& file_pattern = file_pattern_t.flat<tstring>()(0);\n\n  const Tensor& tensor_name_t = context->input(1);\n  {\n    const int64_t size = tensor_name_t.NumElements();\n    OP_REQUIRES(context, size > restore_index,\n                errors::InvalidArgument(\n                    \"Input 1 (file_pattern) must be a have at least \",\n                    restore_index + 1, \" elements\"));\n  }\n  const string& tensor_name = tensor_name_t.flat<tstring>()(restore_index);\n\n  // If we cannot find a cached reader we will allocate our own.\n  std::unique_ptr<checkpoint::TensorSliceReader> allocated_reader;\n\n  const checkpoint::TensorSliceReader* reader = nullptr;\n\n  if (context->slice_reader_cache()) {\n    reader = context->slice_reader_cache()->GetReader(file_pattern, open_func,\n                                                      preferred_shard);\n  }\n  if (!reader) {\n    allocated_reader.reset(new checkpoint::TensorSliceReader(\n        file_pattern, open_func, preferred_shard));\n    reader = allocated_reader.get();\n  }\n  OP_REQUIRES_OK(context, CHECK_NOTNULL(reader)->status());\n\n  // Get the shape and type from the save file.\n  DataType type;\n  TensorShape saved_shape;\n  OP_REQUIRES(\n      context, reader->HasTensor(tensor_name, &saved_shape, &type),\n      errors::NotFound(\"Tensor name \\\"\", tensor_name,\n                       \"\\\" not found in checkpoint files \", file_pattern));\n  OP_REQUIRES(\n      context, type == context->expected_output_dtype(restore_index),\n      errors::InvalidArgument(\"Expected to restore a tensor of type \",\n                              DataTypeString(context->expected_output_dtype(0)),\n                              \", got a tensor of type \", DataTypeString(type),\n                              \" instead: tensor_name = \", tensor_name));\n\n  // Shape of the output and slice to load.\n  TensorShape output_shape(saved_shape);\n  TensorSlice slice_to_load(saved_shape.dims());\n  if (restore_slice) {\n    const tstring& shape_spec =\n        context->input(2).flat<tstring>()(restore_index);\n    if (!shape_spec.empty()) {\n      TensorShape parsed_shape;\n      OP_REQUIRES_OK(context, checkpoint::ParseShapeAndSlice(\n                                  shape_spec, &parsed_shape, &slice_to_load,\n                                  &output_shape));\n      OP_REQUIRES(\n          context, parsed_shape.IsSameSize(saved_shape),\n          errors::InvalidArgument(\n              \"Shape in shape_and_slice spec does not match the shape in the \"\n              \"save file: \",\n              parsed_shape.DebugString(),\n              \", save file shape: \", saved_shape.DebugString()));\n    }\n  }\n\n  Tensor* t = nullptr;\n  OP_REQUIRES_OK(context,\n                 context->allocate_output(restore_index, output_shape, &t));\n\n  if (output_shape.num_elements() == 0) return;\n\n#define READER_COPY(T)                                                \\\n  case DataTypeToEnum<T>::value:                                      \\\n    OP_REQUIRES(context,                                              \\\n                reader->CopySliceData(tensor_name, slice_to_load,     \\\n                                      t->flat<T>().data()),           \\\n                errors::InvalidArgument(\"Error copying slice data\")); \\\n    break;\n\n  switch (type) {\n    TF_CALL_SAVE_RESTORE_TYPES(READER_COPY)\n    default:\n      context->SetStatus(errors::Unimplemented(\n          \"Restoring data type \", DataTypeString(type), \" not yet supported\"));\n  }\n#undef READER_COPY\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def validate_node_sign(signature_node, elem, cert=None, fingerprint=None, fingerprintalg='sha1', validatecert=False, debug=False):\n        \"\"\"\n        Validates a signature node.\n\n        :param signature_node: The signature node\n        :type: Node\n\n        :param xml: The element we should validate\n        :type: Document\n\n        :param cert: The public cert\n        :type: string\n\n        :param fingerprint: The fingerprint of the public cert\n        :type: string\n\n        :param fingerprintalg: The algorithm used to build the fingerprint\n        :type: string\n\n        :param validatecert: If true, will verify the signature and if the cert is valid.\n        :type: bool\n\n        :param debug: Activate the xmlsec debug\n        :type: bool\n\n        :param raise_exceptions: Whether to return false on failure or raise an exception\n        :type raise_exceptions: Boolean\n        \"\"\"\n        error_callback_method = None\n        if debug:\n            error_callback_method = print_xmlsec_errors\n        xmlsec.set_error_callback(error_callback_method)\n\n        xmlsec.addIDs(elem, [\"ID\"])\n\n        if (cert is None or cert == '') and fingerprint:\n            x509_certificate_nodes = OneLogin_Saml2_Utils.query(signature_node, '//ds:Signature/ds:KeyInfo/ds:X509Data/ds:X509Certificate')\n            if len(x509_certificate_nodes) > 0:\n                x509_certificate_node = x509_certificate_nodes[0]\n                x509_cert_value = OneLogin_Saml2_Utils.element_text(x509_certificate_node)\n                x509_fingerprint_value = OneLogin_Saml2_Utils.calculate_x509_fingerprint(x509_cert_value, fingerprintalg)\n                if fingerprint == x509_fingerprint_value:\n                    cert = OneLogin_Saml2_Utils.format_cert(x509_cert_value)\n\n        # Check if Reference URI is empty\n        # reference_elem = OneLogin_Saml2_Utils.query(signature_node, '//ds:Reference')\n        # if len(reference_elem) > 0:\n        #    if reference_elem[0].get('URI') == '':\n        #        reference_elem[0].set('URI', '#%s' % signature_node.getparent().get('ID'))\n\n        if cert is None or cert == '':\n            raise OneLogin_Saml2_Error(\n                'Could not validate node signature: No certificate provided.',\n                OneLogin_Saml2_Error.CERT_NOT_FOUND\n            )\n\n        file_cert = OneLogin_Saml2_Utils.write_temp_file(cert)\n\n        if validatecert:\n            mngr = xmlsec.KeysMngr()\n            mngr.loadCert(file_cert.name, xmlsec.KeyDataFormatCertPem, xmlsec.KeyDataTypeTrusted)\n            dsig_ctx = xmlsec.DSigCtx(mngr)\n        else:\n            dsig_ctx = xmlsec.DSigCtx()\n            dsig_ctx.signKey = xmlsec.Key.load(file_cert.name, xmlsec.KeyDataFormatCertPem, None)\n\n        file_cert.close()\n\n        dsig_ctx.setEnabledKeyData([xmlsec.KeyDataX509])\n\n        try:\n            dsig_ctx.verify(signature_node)\n        except Exception as err:\n            raise OneLogin_Saml2_ValidationError(\n                'Signature validation failed. SAML Response rejected. %s',\n                OneLogin_Saml2_ValidationError.INVALID_SIGNATURE,\n                err.__str__()\n            )\n\n        return True\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def snake_case(value: str) -> str:\n    return stringcase.snakecase(group_title(_sanitize(value)))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status TensorSliceWriter::SaveData(const T* data, int64_t num_elements,\n                                   SavedSlice* ss) {\n  size_t max_bytes_per_element =\n      MaxBytesPerElementOrZero(DataTypeToEnum<T>::value);\n  if (max_bytes_per_element == 0) {\n    return errors::InvalidArgument(\n        \"Tensor slice serialization not implemented for dtype \",\n        DataTypeToEnum<T>::value);\n  }\n  size_t size_bound = ss->ByteSize() + kTensorProtoHeaderBytes +\n                      (max_bytes_per_element * num_elements);\n  if (size_bound > kMaxMessageBytes) {\n    return errors::InvalidArgument(\n        \"Tensor slice is too large to serialize (conservative estimate: \",\n        size_bound, \" bytes)\");\n  }\n  Fill(data, num_elements, ss->mutable_data());\n  DCHECK_GE(ss->ByteSize(), 0);\n  DCHECK_LE(ss->ByteSize(), size_bound);\n  return OkStatus();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_valid_filename(name):\n    \"\"\"\n    Return the given string converted to a string that can be used for a clean\n    filename. Remove leading and trailing spaces; convert other spaces to\n    underscores; and remove anything that is not an alphanumeric, dash,\n    underscore, or dot.\n    >>> get_valid_filename(\"john's portrait in 2004.jpg\")\n    'johns_portrait_in_2004.jpg'\n    \"\"\"\n    s = str(name).strip().replace(' ', '_')\n    s = re.sub(r'(?u)[^-\\w.]', '', s)\n    if s in {'', '.', '..'}:\n        raise SuspiciousFileOperation(\"Could not derive file name from '%s'\" % name)\n    return s\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  static void launch(OpKernelContext* context, const PoolParameters& params,\n                     const Tensor& grad_in, const Tensor& argmax,\n                     Tensor* grad_out, const bool include_batch_in_index) {\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n\n    auto shard = [&grad_in, &argmax, &grad_out, include_batch_in_index](\n                     int64 start, int64 limit) {\n      const int64 batch_size =\n          GetTensorDim(grad_out->shape(), FORMAT_NHWC, 'N');\n      const int64 output_size_per_batch = grad_out->NumElements() / batch_size;\n      const int64 input_size_per_batch = grad_in.NumElements() / batch_size;\n\n      {\n        auto grad_out_flat = grad_out->flat<T>();\n        auto argmax_flat = argmax.flat<int64>();\n        auto grad_in_flat = grad_in.flat<T>();\n\n        const int64 output_start = start * output_size_per_batch;\n        const int64 output_end = limit * output_size_per_batch;\n        EigenMatrixMap inputShard(grad_out_flat.data() + output_start, 1,\n                                  output_end - output_start);\n        inputShard.setConstant(T(0));\n\n        const int input_start = start * input_size_per_batch;\n        const int input_end = limit * input_size_per_batch;\n        for (int64 index = input_start; index < input_end; index++) {\n          if (index >= argmax.NumElements()) {\n            break;\n          }\n          int64 grad_out_index = argmax_flat(index);\n          if (!include_batch_in_index) {\n            const int64 cur_batch = index / input_size_per_batch;\n            grad_out_index += cur_batch * output_size_per_batch;\n          }\n          CHECK(grad_out_index >= output_start && grad_out_index < output_end)\n              << \"Invalid output gradient index: \" << grad_out_index << \", \"\n              << output_start << \", \" << output_end;\n          grad_out_flat(grad_out_index) += grad_in_flat(index);\n        }\n      }\n    };\n\n    const int64 batch_size = GetTensorDim(grad_out->shape(), FORMAT_NHWC, 'N');\n    const int64 shard_cost = grad_out->NumElements() / batch_size;\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          shard_cost, shard);\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def wget_sync(self, repo):\n\n        \"\"\"\n        Handle mirroring of directories using wget\n        \"\"\"\n\n        repo_mirror = repo.mirror.strip()\n\n        if repo.rpm_list != \"\" and repo.rpm_list != []:\n            self.logger.warning(\"--rpm-list is not supported for wget'd repositories\")\n\n        # FIXME: don't hardcode\n        dest_path = os.path.join(self.settings.webdir + \"/repo_mirror\", repo.name)\n\n        # FIXME: wrapper for subprocess that logs to logger\n        cmd = \"wget -N -np -r -l inf -nd -P %s %s\" % (pipes.quote(dest_path), pipes.quote(repo_mirror))\n        rc = utils.subprocess_call(self.logger, cmd)\n\n        if rc != 0:\n            utils.die(self.logger, \"cobbler reposync failed\")\n        os.path.walk(dest_path, self.createrepo_walker, repo)\n        self.create_local_file(dest_path, repo)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n        key = self.make_key(key, version=version)\n        self.validate_key(key)\n        if not self._cache.set(key, value, self.get_backend_timeout(timeout)):\n            # make sure the key doesn't keep its old value in case of failure to set (memcached's 1MB limit)\n            self._cache.delete(key)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def url_for(  # type: ignore[override]\n        self,\n        *,\n        filename: PathLike,\n        append_version: Optional[bool] = None,\n    ) -> URL:\n        if append_version is None:\n            append_version = self._append_version\n        filename = str(filename).lstrip(\"/\")\n\n        url = URL.build(path=self._prefix, encoded=True)\n        # filename is not encoded\n        if YARL_VERSION < (1, 6):\n            url = url / filename.replace(\"%\", \"%25\")\n        else:\n            url = url / filename\n\n        if append_version:\n            unresolved_path = self._directory.joinpath(filename)\n            try:\n                if self._follow_symlinks:\n                    normalized_path = Path(os.path.normpath(unresolved_path))\n                    normalized_path.relative_to(self._directory)\n                    filepath = normalized_path.resolve()\n                else:\n                    filepath = unresolved_path.resolve()\n                    filepath.relative_to(self._directory)\n            except (ValueError, FileNotFoundError):\n                # ValueError for case when path point to symlink\n                # with follow_symlinks is False\n                return url  # relatively safe\n            if filepath.is_file():\n                # TODO cache file content\n                # with file watcher for cache invalidation\n                with filepath.open(\"rb\") as f:\n                    file_bytes = f.read()\n                h = self._get_file_hash(file_bytes)\n                url = url.with_query({self.VERSION_KEY: h})\n                return url\n        return url\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def make_nonce(cls):\n        \"\"\"Generate pseudorandom number.\"\"\"\n        return str(random.SystemRandom().randint(0, 100000000))\n "}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def main():\n    cli_args = create_parser().parse_args()\n\n    if cli_args.quiet:\n        logger.setLevel(logging.ERROR)\n    elif cli_args.debug:\n        logger.setLevel(logging.DEBUG)\n    elif cli_args.verbose:\n        logger.setLevel(logging.INFO)\n\n    # parse extra options\n    extra_input_options = _split_options(cli_args.extra_input_options)\n    extra_output_options = _split_options(cli_args.extra_output_options)\n\n    ffmpeg_normalize = FFmpegNormalize(\n        normalization_type=cli_args.normalization_type,\n        target_level=cli_args.target_level,\n        print_stats=cli_args.print_stats,\n        loudness_range_target=cli_args.loudness_range_target,\n        # threshold=cli_args.threshold,\n        true_peak=cli_args.true_peak,\n        offset=cli_args.offset,\n        dual_mono=cli_args.dual_mono,\n        dynamic=cli_args.dynamic,\n        audio_codec=cli_args.audio_codec,\n        audio_bitrate=cli_args.audio_bitrate,\n        sample_rate=cli_args.sample_rate,\n        keep_original_audio=cli_args.keep_original_audio,\n        pre_filter=cli_args.pre_filter,\n        post_filter=cli_args.post_filter,\n        video_codec=cli_args.video_codec,\n        video_disable=cli_args.video_disable,\n        subtitle_disable=cli_args.subtitle_disable,\n        metadata_disable=cli_args.metadata_disable,\n        chapters_disable=cli_args.chapters_disable,\n        extra_input_options=extra_input_options,\n        extra_output_options=extra_output_options,\n        output_format=cli_args.output_format,\n        dry_run=cli_args.dry_run,\n        progress=cli_args.progress,\n    )\n\n    if (\n        cli_args.output is not None\n        and len(cli_args.output) > 0\n        and len(cli_args.input) > len(cli_args.output)\n    ):\n        logger.warning(\n            \"There are more input files than output file names given. \"\n            \"Please specify one output file name per input file using -o <output1> <output2> ... \"\n            \"Will apply default file naming for the remaining ones.\"\n        )\n\n    for index, input_file in enumerate(cli_args.input):\n        if cli_args.output is not None and index < len(cli_args.output):\n            if cli_args.output_folder and cli_args.output_folder != \"normalized\":\n                logger.warning(\n                    \"Output folder {} is ignored for input file {}\".format(\n                        cli_args.output_folder, input_file\n                    )\n                )\n            output_file = cli_args.output[index]\n            output_dir = os.path.dirname(output_file)\n            if output_dir != \"\" and not os.path.isdir(output_dir):\n                raise FFmpegNormalizeError(\n                    f\"Output file path {output_dir} does not exist\"\n                )\n        else:\n            output_file = os.path.join(\n                cli_args.output_folder,\n                os.path.splitext(os.path.basename(input_file))[0]\n                + \".\"\n                + cli_args.extension,\n            )\n            if not os.path.isdir(cli_args.output_folder) and not cli_args.dry_run:\n                logger.warning(\n                    \"Output directory '{}' does not exist, will create\".format(\n                        cli_args.output_folder\n                    )\n                )\n                os.makedirs(cli_args.output_folder, exist_ok=True)\n\n        if os.path.exists(output_file) and not cli_args.force:\n            logger.error(\n                \"Output file {} already exists, skipping. Use -f to force overwriting.\".format(\n                    output_file\n                )\n            )\n        else:\n            ffmpeg_normalize.add_media_file(input_file, output_file)\n\n    ffmpeg_normalize.run_normalization()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_http_response_with_retries(\n    method,\n    url,\n    max_retries,\n    backoff_factor,\n    backoff_jitter,\n    retry_codes,\n    raise_on_status=True,\n    allow_redirects=None,\n    **kwargs,\n):\n    \"\"\"\n    Performs an HTTP request using Python's `requests` module with an automatic retry policy.\n\n    :param method: a string indicating the method to use, e.g. \"GET\", \"POST\", \"PUT\".\n    :param url: the target URL address for the HTTP request.\n    :param max_retries: Maximum total number of retries.\n    :param backoff_factor: a time factor for exponential backoff. e.g. value 5 means the HTTP\n      request will be retried with interval 5, 10, 20... seconds. A value of 0 turns off the\n      exponential backoff.\n    :param backoff_jitter: A random jitter to add to the backoff interval.\n    :param retry_codes: a list of HTTP response error codes that qualifies for retry.\n    :param raise_on_status: whether to raise an exception, or return a response, if status falls\n      in retry_codes range and retries have been exhausted.\n    :param kwargs: Additional keyword arguments to pass to `requests.Session.request()`\n\n    :return: requests.Response object.\n    \"\"\"\n    session = _get_request_session(\n        max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status\n    )\n\n    # the environment variable is hardcoded here to avoid importing mlflow.\n    # however, documentation is available in environment_variables.py\n    env_value = os.getenv(\"MLFLOW_ALLOW_HTTP_REDIRECTS\", \"true\").lower() in [\"true\", \"1\"]\n    allow_redirects = env_value if allow_redirects is None else allow_redirects\n\n    return session.request(method, url, allow_redirects=allow_redirects, **kwargs)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "inline void Conv3DTranspose(\n    const Conv3DTransposeParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& bias_shape,\n    const float* bias_data, const RuntimeShape& output_shape,\n    float* output_data) {\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int stride_depth = params.stride_depth;\n  const int pad_width = params.padding_values.width;\n  const int pad_height = params.padding_values.height;\n  const int pad_depth = params.padding_values.depth;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_num_channels = MatchingDim(input_shape, 4, filter_shape, 4);\n  const int output_num_channels = output_shape.Dims(4);\n  const int input_depth = input_shape.Dims(1);\n  const int input_height = input_shape.Dims(2);\n  const int input_width = input_shape.Dims(3);\n  const int filter_depth = filter_shape.Dims(0);\n  const int filter_height = filter_shape.Dims(1);\n  const int filter_width = filter_shape.Dims(2);\n  const int output_depth = output_shape.Dims(1);\n  const int output_height = output_shape.Dims(2);\n  const int output_width = output_shape.Dims(3);\n  if (bias_data) {\n    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_num_channels);\n  }\n\n  // Initializes the output array to zero.\n  const int num_elements = output_shape.FlatSize();\n  for (int i = 0; i < num_elements; i++) {\n    output_data[i] = 0.0f;\n  }\n\n  // Loop through input elements one at a time.\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int in_d = 0; in_d < input_depth; ++in_d) {\n      for (int in_y = 0; in_y < input_height; ++in_y) {\n        for (int in_x = 0; in_x < input_width; ++in_x) {\n          for (int in_channel = 0; in_channel < input_num_channels;\n               ++in_channel) {\n            // Loop through the output elements it will influence.\n            const int out_x_origin = (in_x * stride_width) - pad_width;\n            const int out_y_origin = (in_y * stride_height) - pad_height;\n            const int out_d_origin = (in_d * stride_depth) - pad_depth;\n            for (int filter_d = 0; filter_d < filter_depth; ++filter_d) {\n              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {\n                  for (int out_channel = 0; out_channel < output_num_channels;\n                       ++out_channel) {\n                    // Compute output element location.\n                    const int out_x =\n                        out_x_origin + params.dilation_width * filter_x;\n                    const int out_y =\n                        out_y_origin + params.dilation_height * filter_y;\n                    const int out_d =\n                        out_d_origin + params.dilation_depth * filter_d;\n                    // We cannot accumulate out of bounds.\n                    if ((out_x >= 0) && (out_x < output_width) &&\n                        (out_y >= 0) && (out_y < output_height) &&\n                        (out_d >= 0) && (out_d < output_depth)) {\n                      float input_value = input_data[Offset(\n                          input_shape, batch, in_d, in_y, in_x, in_channel)];\n                      float filter_value = filter_data[Offset(\n                          filter_shape, filter_d, filter_y, filter_x,\n                          out_channel, in_channel)];\n                      output_data[Offset(output_shape, batch, out_d, out_y,\n                                         out_x, out_channel)] +=\n                          input_value * filter_value;\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n\n  const float float_activation_min = params.float_activation_min;\n  const float float_activation_max = params.float_activation_max;\n  float* data_ptr = output_data;\n  if (bias_data) {\n    const int outer_size =\n        batches * output_depth * output_height * output_width;\n    for (int n = 0; n < outer_size; ++n) {\n      for (int c = 0; c < output_num_channels; ++c) {\n        data_ptr[c] = ActivationFunctionWithMinMax(data_ptr[c] + bias_data[c],\n                                                   float_activation_min,\n                                                   float_activation_max);\n      }\n      data_ptr += output_num_channels;\n    }\n  } else {\n    const int flat_size = output_shape.FlatSize();\n    for (int i = 0; i < flat_size; ++i) {\n      data_ptr[i] = ActivationFunctionWithMinMax(\n          data_ptr[i], float_activation_min, float_activation_max);\n    }\n  }\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def get_file(\n        self,\n        url: str,\n        output_stream: BinaryIO,\n        max_size: Optional[int] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:\n        \"\"\"GETs a file from a given URL\n        Args:\n            url: The URL to GET\n            output_stream: File to write the response body to.\n            headers: A map from header name to a list of values for that header\n        Returns:\n            A tuple of the file length, dict of the response\n            headers, absolute URI of the response and HTTP response code.\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            SynapseError: if the response is not a 2xx, the remote file is too large, or\n               another exception happens during the download.\n        \"\"\"\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", url, headers=Headers(actual_headers))\n\n        resp_headers = dict(response.headers.getAllRawHeaders())\n\n        if (\n            b\"Content-Length\" in resp_headers\n            and max_size\n            and int(resp_headers[b\"Content-Length\"][0]) > max_size\n        ):\n            logger.warning(\"Requested URL is too large > %r bytes\" % (max_size,))\n            raise SynapseError(\n                502,\n                \"Requested file is too large > %r bytes\" % (max_size,),\n                Codes.TOO_LARGE,\n            )\n\n        if response.code > 299:\n            logger.warning(\"Got %d when downloading %s\" % (response.code, url))\n            raise SynapseError(502, \"Got error %d\" % (response.code,), Codes.UNKNOWN)\n\n        # TODO: if our Content-Type is HTML or something, just read the first\n        # N bytes into RAM rather than saving it all to disk only to read it\n        # straight back in again\n\n        try:\n            length = await make_deferred_yieldable(\n                read_body_with_max_size(response, output_stream, max_size)\n            )\n        except BodyExceededMaxSize:\n            SynapseError(\n                502,\n                \"Requested file is too large > %r bytes\" % (max_size,),\n                Codes.TOO_LARGE,\n            )\n        except Exception as e:\n            raise SynapseError(502, (\"Failed to download remote body: %s\" % e)) from e\n\n        return (\n            length,\n            resp_headers,\n            response.request.absoluteURI.decode(\"ascii\"),\n            response.code,\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, source, *args, **kwargs):\n        self.source = source\n        self.args = args\n        if len(args) == 2 and isinstance(args[1], (string_types, tuple, list)):\n            self.rmatch = args[0]\n            self.vmatch = args[1]\n            self.vdict = None\n            self.attr = None\n        elif len(args) == 2 and isinstance(args[1], dict):\n            self.rmatch = args[0]\n            self.vmatch = None\n            self.vdict = args[1]\n            self.attr = None\n        elif len(args) == 3:\n            self.rmatch = args[0]\n            self.vmatch = args[1]\n            self.vdict = None\n            self.attr = args[2]\n        else:\n            assert False, 'bad parameters'\n        self.missing = kwargs.get('missing', None)\n        self.user_parser = kwargs.get('parser', None)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void CalculateOutputIndexRowSplit(\n      OpKernelContext* context, const RowPartitionTensor& row_split,\n      const vector<INDEX_TYPE>& parent_output_index,\n      INDEX_TYPE output_index_multiplier, INDEX_TYPE output_size,\n      vector<INDEX_TYPE>* result) {\n    INDEX_TYPE row_split_size = row_split.size();\n    if (row_split_size > 0) {\n      result->reserve(row_split(row_split_size - 1));\n    }\n    for (INDEX_TYPE i = 0; i < row_split_size - 1; ++i) {\n      INDEX_TYPE row_length = row_split(i + 1) - row_split(i);\n      INDEX_TYPE real_length = std::min(output_size, row_length);\n      INDEX_TYPE parent_output_index_current = parent_output_index[i];\n\n      if (parent_output_index_current == -1) {\n        real_length = 0;\n      }\n      for (INDEX_TYPE j = 0; j < real_length; ++j) {\n        result->push_back(parent_output_index_current);\n        parent_output_index_current += output_index_multiplier;\n      }\n      for (INDEX_TYPE j = 0; j < row_length - real_length; ++j) {\n        result->push_back(-1);\n      }\n    }\n    if (row_split_size > 0) {\n      OP_REQUIRES(context, result->size() == row_split(row_split_size - 1),\n                  errors::InvalidArgument(\"Invalid row split size.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def send(self, request: Request, *args, **kwargs) -> Response:\n        \"\"\"\n        Send a request with automatic retry behavior for the following status codes:\n\n        - 403 Forbidden, if the request failed due to CSRF protection\n        - 408 Request Timeout\n        - 429 CloudFlare-style rate limiting\n        - 502 Bad Gateway\n        - 503 Service unavailable\n        - Any additional status codes provided in `PREFECT_CLIENT_RETRY_EXTRA_CODES`\n        \"\"\"\n\n        super_send = super().send\n        response = await self._send_with_retry(\n            request=request,\n            send=super_send,\n            send_args=args,\n            send_kwargs=kwargs,\n            retry_codes={\n                status.HTTP_429_TOO_MANY_REQUESTS,\n                status.HTTP_503_SERVICE_UNAVAILABLE,\n                status.HTTP_502_BAD_GATEWAY,\n                status.HTTP_408_REQUEST_TIMEOUT,\n                *PREFECT_CLIENT_RETRY_EXTRA_CODES.value(),\n            },\n            retry_exceptions=(\n                httpx.ReadTimeout,\n                httpx.PoolTimeout,\n                httpx.ConnectTimeout,\n                # `ConnectionResetError` when reading socket raises as a `ReadError`\n                httpx.ReadError,\n                # Sockets can be closed during writes resulting in a `WriteError`\n                httpx.WriteError,\n                # Uvicorn bug, see https://github.com/PrefectHQ/prefect/issues/7512\n                httpx.RemoteProtocolError,\n                # HTTP2 bug, see https://github.com/PrefectHQ/prefect/issues/7442\n                httpx.LocalProtocolError,\n            ),\n        )\n\n        # Convert to a Prefect response to add nicer errors messages\n        response = PrefectResponse.from_httpx_response(response)\n\n        # Always raise bad responses\n        # NOTE: We may want to remove this and handle responses per route in the\n        #       `PrefectClient`\n        response.raise_for_status()\n\n        return response\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, **kwargs):\n        self.basic_auth = get_anymail_setting('webhook_secret', default=[],\n                                              kwargs=kwargs)  # no esp_name -- auth is shared between ESPs\n        if not self.basic_auth:\n            # Temporarily allow deprecated WEBHOOK_AUTHORIZATION setting\n            self.basic_auth = get_anymail_setting('webhook_authorization', default=[], kwargs=kwargs)\n\n        # Allow a single string:\n        if isinstance(self.basic_auth, six.string_types):\n            self.basic_auth = [self.basic_auth]\n        if self.warn_if_no_basic_auth and len(self.basic_auth) < 1:\n            warnings.warn(\n                \"Your Anymail webhooks are insecure and open to anyone on the web. \"\n                \"You should set WEBHOOK_SECRET in your ANYMAIL settings. \"\n                \"See 'Securing webhooks' in the Anymail docs.\",\n                AnymailInsecureWebhookWarning)\n        # noinspection PyArgumentList\n        super(AnymailBasicAuthMixin, self).__init__(**kwargs)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& in0 = ctx->input(0);\n    const Tensor& in1 = ctx->input(1);\n\n    ValidateInputTensors(ctx, in0, in1);\n    if (!ctx->status().ok()) return;\n\n    MatMulBCast bcast(in0.shape().dim_sizes(), in1.shape().dim_sizes());\n    OP_REQUIRES(\n        ctx, bcast.IsValid(),\n        errors::InvalidArgument(\n            \"In[0] and In[1] must have compatible batch dimensions: \",\n            in0.shape().DebugString(), \" vs. \", in1.shape().DebugString()));\n\n    TensorShape out_shape = bcast.output_batch_shape();\n    auto batch_size = bcast.output_batch_size();\n    auto d0 = in0.dim_size(in0.dims() - 2);  // Band size.\n    auto d1 = in0.dim_size(in0.dims() - 1);\n    Tensor in0_reshaped;\n    OP_REQUIRES(\n        ctx,\n        in0_reshaped.CopyFrom(in0, TensorShape({bcast.x_batch_size(), d0, d1})),\n        errors::Internal(\"Failed to reshape In[0] from \",\n                         in0.shape().DebugString()));\n    auto d2 = in1.dim_size(in1.dims() - 2);\n    auto d3 = in1.dim_size(in1.dims() - 1);\n    Tensor in1_reshaped;\n    OP_REQUIRES(\n        ctx,\n        in1_reshaped.CopyFrom(in1, TensorShape({bcast.y_batch_size(), d2, d3})),\n        errors::Internal(\"Failed to reshape In[1] from \",\n                         in1.shape().DebugString()));\n    OP_REQUIRES(ctx, d1 == d2,\n                errors::InvalidArgument(\n                    \"In[0] mismatch In[1] shape: \", d1, \" vs. \", d2, \": \",\n                    in0.shape().DebugString(), \" \", in1.shape().DebugString(),\n                    \" \", lower_, \" \", adjoint_));\n    out_shape.AddDim(d1);\n    out_shape.AddDim(d3);\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));\n    if (out->NumElements() == 0) {\n      return;\n    }\n    Tensor out_reshaped;\n    OP_REQUIRES(ctx,\n                out_reshaped.CopyFrom(*out, TensorShape({batch_size, d1, d3})),\n                errors::Internal(\"Failed to reshape output from \",\n                                 out->shape().DebugString()));\n    LaunchBatchBandedTriangularSolve<Scalar>::Launch(\n        ctx, in0_reshaped, in1_reshaped, adjoint_, lower_, bcast,\n        &out_reshaped);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def LoadSettingsFile(filename=SETTINGS_FILE):\n    \"\"\"Loads settings file in yaml format given file name.\n\n    :param filename: path for settings file. 'settings.yaml' by default.\n    :type filename: str.\n    :raises: SettingsError\n    \"\"\"\n    try:\n        with open(filename) as stream:\n            data = load(stream, Loader=SafeLoader)\n    except (YAMLError, OSError) as e:\n        raise SettingsError(e)\n    return data\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status Examples::Initialize(OpKernelContext* const context,\n                            const ModelWeights& weights,\n                            const int num_sparse_features,\n                            const int num_sparse_features_with_values,\n                            const int num_dense_features) {\n  num_features_ = num_sparse_features + num_dense_features;\n\n  OpInputList sparse_example_indices_inputs;\n  TF_RETURN_IF_ERROR(context->input_list(\"sparse_example_indices\",\n                                         &sparse_example_indices_inputs));\n  if (sparse_example_indices_inputs.size() != num_sparse_features)\n    return errors::InvalidArgument(\n        \"Expected \", num_sparse_features,\n        \" tensors in sparse_example_indices but got \",\n        sparse_example_indices_inputs.size());\n  OpInputList sparse_feature_indices_inputs;\n  TF_RETURN_IF_ERROR(context->input_list(\"sparse_feature_indices\",\n                                         &sparse_feature_indices_inputs));\n  if (sparse_feature_indices_inputs.size() != num_sparse_features)\n    return errors::InvalidArgument(\n        \"Expected \", num_sparse_features,\n        \" tensors in sparse_feature_indices but got \",\n        sparse_feature_indices_inputs.size());\n  OpInputList sparse_feature_values_inputs;\n  if (num_sparse_features_with_values > 0) {\n    TF_RETURN_IF_ERROR(context->input_list(\"sparse_feature_values\",\n                                           &sparse_feature_values_inputs));\n    if (sparse_feature_values_inputs.size() != num_sparse_features_with_values)\n      return errors::InvalidArgument(\n          \"Expected \", num_sparse_features_with_values,\n          \" tensors in sparse_feature_values but got \",\n          sparse_feature_values_inputs.size());\n  }\n\n  const Tensor* example_weights_t;\n  TF_RETURN_IF_ERROR(context->input(\"example_weights\", &example_weights_t));\n  auto example_weights = example_weights_t->flat<float>();\n\n  if (example_weights.size() >= std::numeric_limits<int>::max()) {\n    return errors::InvalidArgument(strings::Printf(\n        \"Too many examples in a mini-batch: %zu > %d\", example_weights.size(),\n        std::numeric_limits<int>::max()));\n  }\n\n  // The static_cast here is safe since num_examples can be at max an int.\n  const int num_examples = static_cast<int>(example_weights.size());\n  const Tensor* example_labels_t;\n  TF_RETURN_IF_ERROR(context->input(\"example_labels\", &example_labels_t));\n  auto example_labels = example_labels_t->flat<float>();\n  if (example_labels.size() != num_examples) {\n    return errors::InvalidArgument(\"Expected \", num_examples,\n                                   \" example labels but got \",\n                                   example_labels.size());\n  }\n\n  OpInputList dense_features_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"dense_features\", &dense_features_inputs));\n  for (int i = 0; i < dense_features_inputs.size(); ++i) {\n    if (!TensorShapeUtils::IsMatrix(dense_features_inputs[i].shape())) {\n      return errors::InvalidArgument(\"Dense features at index \", i,\n                                     \" must be rank 2 but is rank \",\n                                     dense_features_inputs[i].dims());\n    }\n  }\n\n  examples_.clear();\n  examples_.resize(num_examples);\n  probabilities_.resize(num_examples);\n  sampled_index_.resize(num_examples);\n  sampled_count_.resize(num_examples);\n  for (int example_id = 0; example_id < num_examples; ++example_id) {\n    Example* const example = &examples_[example_id];\n    example->sparse_features_.resize(num_sparse_features);\n    example->dense_vectors_.resize(num_dense_features);\n    example->example_weight_ = example_weights(example_id);\n    example->example_label_ = example_labels(example_id);\n  }\n  const DeviceBase::CpuWorkerThreads& worker_threads =\n      *context->device()->tensorflow_cpu_worker_threads();\n  TF_RETURN_IF_ERROR(CreateSparseFeatureRepresentation(\n      worker_threads, num_examples, num_sparse_features, weights,\n      sparse_example_indices_inputs, sparse_feature_indices_inputs,\n      sparse_feature_values_inputs, &examples_));\n  TF_RETURN_IF_ERROR(CreateDenseFeatureRepresentation(\n      worker_threads, num_examples, num_dense_features, weights,\n      dense_features_inputs, &examples_));\n  TF_RETURN_IF_ERROR(ComputeSquaredNormPerExample(\n      worker_threads, num_examples, num_sparse_features, num_dense_features,\n      &examples_));\n  return OkStatus();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render(self):\n        if len(self.params) == 1:\n            # Retrieve the user portrait\n            user = decleanId(self.params[0])\n            portrait = self.portal_membership.getPersonalPortrait(user)\n        elif len(self.params) == 0:\n            current_user_id = self.portal_membership.getAuthenticatedMember().getId()\n            portrait = self.portal_membership.getPersonalPortrait(current_user_id)\n        else:\n            raise Exception(\n                \"Must supply exactly zero (own portrait) or one parameter \" \"(user id)\"\n            )\n        # User uploaded portraits have a meta_type of \"Image\"\n        if not portrait or isDefaultPortrait(portrait):\n            self.request.response.setStatus(404)\n            return None\n\n        if self._should_force_download(portrait):\n            # We need a filename, even a dummy one if needed.\n            ext = portrait.content_type.split(\"/\")[-1].split(\"+\")[0]\n            filename = f\"{portrait.getId()}.{ext}\"\n            filename = quote(filename.encode(\"utf8\"))\n            self.request.response.setHeader(\n                \"Content-Disposition\", f\"attachment; filename*=UTF-8''{filename}\"\n            )\n\n        self.request.response.setStatus(200)\n        self.request.response.setHeader(\"Content-Type\", portrait.content_type)\n\n        return stream_data(portrait)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def parse_url(url):\n    \"\"\"\n    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is\n    performed to parse incomplete urls. Fields not provided will be None.\n    This parser is RFC 3986 compliant.\n\n    The parser logic and helper functions are based heavily on\n    work done in the ``rfc3986`` module.\n\n    :param str url: URL to parse into a :class:`.Url` namedtuple.\n\n    Partly backwards-compatible with :mod:`urlparse`.\n\n    Example::\n\n        >>> parse_url('http://google.com/mail/')\n        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)\n        >>> parse_url('google.com:80')\n        Url(scheme=None, host='google.com', port=80, path=None, ...)\n        >>> parse_url('/foo?bar')\n        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)\n    \"\"\"\n    if not url:\n        # Empty\n        return Url()\n\n    source_url = url\n    if not SCHEME_RE.search(url):\n        url = \"//\" + url\n\n    try:\n        scheme, authority, path, query, fragment = URI_RE.match(url).groups()\n        normalize_uri = scheme is None or scheme.lower() in NORMALIZABLE_SCHEMES\n\n        if scheme:\n            scheme = scheme.lower()\n\n        if authority:\n            auth, _, host_port = authority.rpartition(\"@\")\n            auth = auth or None\n            host, port = _HOST_PORT_RE.match(host_port).groups()\n            if auth and normalize_uri:\n                auth = _encode_invalid_chars(auth, USERINFO_CHARS)\n            if port == \"\":\n                port = None\n        else:\n            auth, host, port = None, None, None\n\n        if port is not None:\n            port = int(port)\n            if not (0 <= port <= 65535):\n                raise LocationParseError(url)\n\n        host = _normalize_host(host, scheme)\n\n        if normalize_uri and path:\n            path = _remove_path_dot_segments(path)\n            path = _encode_invalid_chars(path, PATH_CHARS)\n        if normalize_uri and query:\n            query = _encode_invalid_chars(query, QUERY_CHARS)\n        if normalize_uri and fragment:\n            fragment = _encode_invalid_chars(fragment, FRAGMENT_CHARS)\n\n    except (ValueError, AttributeError):\n        return six.raise_from(LocationParseError(source_url), None)\n\n    # For the sake of backwards compatibility we put empty\n    # string values for path if there are any defined values\n    # beyond the path in the URL.\n    # TODO: Remove this when we break backwards compatibility.\n    if not path:\n        if query is not None or fragment is not None:\n            path = \"\"\n        else:\n            path = None\n\n    # Ensure that each part of the URL is a `str` for\n    # backwards compatibility.\n    if isinstance(url, six.text_type):\n        ensure_func = six.ensure_text\n    else:\n        ensure_func = six.ensure_str\n\n    def ensure_type(x):\n        return x if x is None else ensure_func(x)\n\n    return Url(\n        scheme=ensure_type(scheme),\n        auth=ensure_type(auth),\n        host=ensure_type(host),\n        port=port,\n        path=ensure_type(path),\n        query=ensure_type(query),\n        fragment=ensure_type(fragment),\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_url(url):\n    urls = []\n    for _url in url.splitlines():\n        if not _url:\n            continue\n\n        urls.append((_validate(_url), _url))\n    \n    return urls\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def cancel(self, request_id):\n        method, okey, seq = request_id\n        if request_id in self._futures:\n            if okey in self._pending:\n                for seq_item in self._pending[okey]:\n                    if seq_item.method == method and seq_item.seq == seq:\n                        self._pending[okey].remove(seq_item)\n                if self._pending[okey]:\n                    heapq.heapify(self._pending[okey])\n                else:\n                    del self._pending[okey]\n            job = self._jobs.get(request_id, None)\n            if job:\n                # aiojobs cancels the internal task upon job closing.\n                await job.close()\n        else:\n            self._log.warning('cancellation of unknown or '\n                              'not sent yet request: %r', request_id)\n        self.cleanup(request_id)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def to_dot_graph(\n        self,\n        priorities=True,\n        addnodes=[],\n        savefn=None,\n        savelayout=\"plain\",\n        saveformat=None,\n        color_edges=True,\n    ):\n        \"\"\"\n        Converts this transform graph to the graphviz_ DOT format.\n\n        Optionally saves it (requires `graphviz`_ be installed and on your path).\n\n        .. _graphviz: http://www.graphviz.org/\n\n        Parameters\n        ----------\n        priorities : bool\n            If `True`, show the priority values for each transform.  Otherwise,\n            the will not be included in the graph.\n        addnodes : sequence of str\n            Additional coordinate systems to add (this can include systems\n            already in the transform graph, but they will only appear once).\n        savefn : None or str\n            The file name to save this graph to or `None` to not save\n            to a file.\n        savelayout : {\"plain\", \"dot\", \"neato\", \"fdp\", \"sfdp\", \"circo\", \"twopi\", \"nop\", \"nop2\", \"osage\", \"patchwork\"}\n            The graphviz program to use to layout the graph (see\n            graphviz_ for details) or 'plain' to just save the DOT graph\n            content. Ignored if ``savefn`` is `None`.\n        saveformat : str\n            The graphviz output format. (e.g. the ``-Txxx`` option for\n            the command line program - see graphviz docs for details).\n            Ignored if ``savefn`` is `None`.\n        color_edges : bool\n            Color the edges between two nodes (frames) based on the type of\n            transform. ``FunctionTransform``: red, ``StaticMatrixTransform``:\n            blue, ``DynamicMatrixTransform``: green.\n\n        Returns\n        -------\n        dotgraph : str\n            A string with the DOT format graph.\n        \"\"\"\n        # We store nodes as `dict` keys because differently from `list` uniqueness is\n        # guaranteed and differently from `set` insertion order is preserved.\n        nodes = {}\n        for node, node_graph in self._graph.items():\n            nodes[node] = None\n            nodes |= {node: None for node in node_graph}\n        nodes |= {node: None for node in addnodes}\n        nodenames = []\n        invclsaliases = {\n            f: [k for k, v in self._cached_names.items() if v == f]\n            for f in self.frame_set\n        }\n        for n in nodes:\n            if n in invclsaliases:\n                aliases = \"`\\\\n`\".join(invclsaliases[n])\n                nodenames.append(\n                    '{0} [shape=oval label=\"{0}\\\\n`{1}`\"]'.format(n.__name__, aliases)\n                )\n            else:\n                nodenames.append(n.__name__ + \"[ shape=oval ]\")\n\n        edgenames = []\n        # Now the edges\n        for a, agraph in self._graph.items():\n            for b, transform in agraph.items():\n                pri = getattr(transform, \"priority\", 1)\n                color = trans_to_color[transform.__class__] if color_edges else \"black\"\n                edgenames.append((a.__name__, b.__name__, pri, color))\n\n        # generate simple dot format graph\n        lines = [\"digraph AstropyCoordinateTransformGraph {\"]\n        lines.append(\"graph [rankdir=LR]\")\n        lines.append(\"; \".join(nodenames) + \";\")\n        for enm1, enm2, weights, color in edgenames:\n            labelstr_fmt = \"[ {0} {1} ]\"\n            priority_part = f'label = \"{weights}\"' if priorities else \"\"\n            color_part = f'color = \"{color}\"'\n            labelstr = labelstr_fmt.format(priority_part, color_part)\n            lines.append(f\"{enm1} -> {enm2}{labelstr};\")\n        lines.append(\"\")\n        lines.append(\"overlap=false\")\n        lines.append(\"}\")\n        dotgraph = \"\\n\".join(lines)\n\n        if savefn is not None:\n            if savelayout == \"plain\":\n                with open(savefn, \"w\") as f:\n                    f.write(dotgraph)\n            # Options from https://graphviz.org/docs/layouts/\n            elif savelayout in (\n                \"dot\",\n                \"neato\",\n                \"fdp\",\n                \"sfdp\",\n                \"circo\",\n                \"twopi\",\n                \"nop\",\n                \"nop2\",\n                \"osage\",\n                \"patchwork\",\n            ):\n                args = [savelayout]\n                if saveformat is not None:\n                    args.append(\"-T\" + saveformat)\n                proc = subprocess.Popen(\n                    args,\n                    stdin=subprocess.PIPE,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                )\n                stdout, stderr = proc.communicate(dotgraph)\n                if proc.returncode != 0:\n                    raise OSError(\"problem running graphviz: \\n\" + stderr)\n\n                with open(savefn, \"w\") as f:\n                    f.write(stdout)\n            else:\n                raise NotImplementedError(f'savelayout=\"{savelayout}\" is not supported')\n\n        return dotgraph\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def value_from_datadict(self, data, files, name):\n        \"File widgets take data from FILES, not POST\"\n        getter = files.get\n        if self.allow_multiple_selected:\n            try:\n                getter = files.getlist\n            except AttributeError:\n                pass\n        return getter(name)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "uint8* Decode(const void* srcdata, int datasize,\n              const std::function<uint8*(int, int, int, int)>& allocate_output,\n              string* error_string, bool expand_animations) {\n  int error_code = D_GIF_SUCCEEDED;\n  InputBufferInfo info = {reinterpret_cast<const uint8*>(srcdata), datasize};\n  GifFileType* gif_file =\n      DGifOpen(static_cast<void*>(&info), &input_callback, &error_code);\n  const auto cleanup = gtl::MakeCleanup([gif_file]() {\n    int error_code = D_GIF_SUCCEEDED;\n    if (gif_file && DGifCloseFile(gif_file, &error_code) != GIF_OK) {\n      LOG(WARNING) << \"Fail to close gif file, reason: \"\n                   << GifErrorStringNonNull(error_code);\n    }\n  });\n  if (error_code != D_GIF_SUCCEEDED) {\n    *error_string = absl::StrCat(\"failed to open gif file: \",\n                                 GifErrorStringNonNull(error_code));\n    return nullptr;\n  }\n  if (DGifSlurp(gif_file) != GIF_OK) {\n    *error_string = absl::StrCat(\"failed to slurp gif file: \",\n                                 GifErrorStringNonNull(gif_file->Error));\n    return nullptr;\n  }\n  if (gif_file->ImageCount <= 0) {\n    *error_string = \"gif file does not contain any image\";\n    return nullptr;\n  }\n\n  int target_num_frames = gif_file->ImageCount;\n\n  // Don't request more memory than needed for each frame, preventing OOM\n  int max_frame_width = 0;\n  int max_frame_height = 0;\n  for (int k = 0; k < target_num_frames; k++) {\n    SavedImage* si = &gif_file->SavedImages[k];\n    if (max_frame_height < si->ImageDesc.Height)\n      max_frame_height = si->ImageDesc.Height;\n    if (max_frame_width < si->ImageDesc.Width)\n      max_frame_width = si->ImageDesc.Width;\n  }\n\n  const int width = max_frame_width;\n  const int height = max_frame_height;\n  const int channel = 3;\n  if (!expand_animations) target_num_frames = 1;\n\n  uint8* const dstdata =\n      allocate_output(target_num_frames, width, height, channel);\n  if (!dstdata) return nullptr;\n  for (ptrdiff_t k = 0; k < target_num_frames; k++) {\n    uint8* this_dst = dstdata + k * width * channel * height;\n\n    SavedImage* this_image = &gif_file->SavedImages[k];\n    GifImageDesc* img_desc = &this_image->ImageDesc;\n\n    // The Graphics Control Block tells us which index in the color map\n    // correspond to \"transparent color\", i.e. no need to update the pixel\n    // on the canvas. The \"transparent color index\" is specific to each\n    // sub-frame.\n    GraphicsControlBlock gcb;\n    DGifSavedExtensionToGCB(gif_file, k, &gcb);\n\n    int imgLeft = img_desc->Left;\n    int imgTop = img_desc->Top;\n    int imgRight = img_desc->Left + img_desc->Width;\n    int imgBottom = img_desc->Top + img_desc->Height;\n\n    if (k > 0) {\n      uint8* last_dst = dstdata + (k - 1) * width * channel * height;\n      for (ptrdiff_t i = 0; i < height; ++i) {\n        uint8* p_dst = this_dst + i * width * channel;\n        uint8* l_dst = last_dst + i * width * channel;\n        for (ptrdiff_t j = 0; j < width; ++j) {\n          p_dst[j * channel + 0] = l_dst[j * channel + 0];\n          p_dst[j * channel + 1] = l_dst[j * channel + 1];\n          p_dst[j * channel + 2] = l_dst[j * channel + 2];\n        }\n      }\n    }\n\n    if (img_desc->Left != 0 || img_desc->Top != 0 || img_desc->Width != width ||\n        img_desc->Height != height) {\n      // If the first frame does not fill the entire canvas then fill the\n      // unoccupied canvas with zeros (black).\n      if (k == 0) {\n        for (ptrdiff_t i = 0; i < height; ++i) {\n          uint8* p_dst = this_dst + i * width * channel;\n          for (ptrdiff_t j = 0; j < width; ++j) {\n            p_dst[j * channel + 0] = 0;\n            p_dst[j * channel + 1] = 0;\n            p_dst[j * channel + 2] = 0;\n          }\n        }\n      }\n\n      imgLeft = std::max(imgLeft, 0);\n      imgTop = std::max(imgTop, 0);\n      imgRight = std::min(imgRight, width);\n      imgBottom = std::min(imgBottom, height);\n    }\n\n    ColorMapObject* color_map = this_image->ImageDesc.ColorMap\n                                    ? this_image->ImageDesc.ColorMap\n                                    : gif_file->SColorMap;\n    if (color_map == nullptr) {\n      *error_string = absl::StrCat(\"missing color map for frame \", k);\n      return nullptr;\n    }\n\n    for (ptrdiff_t i = imgTop; i < imgBottom; ++i) {\n      uint8* p_dst = this_dst + i * width * channel;\n      for (ptrdiff_t j = imgLeft; j < imgRight; ++j) {\n        GifByteType color_index =\n            this_image->RasterBits[(i - img_desc->Top) * (img_desc->Width) +\n                                   (j - img_desc->Left)];\n\n        if (color_index >= color_map->ColorCount) {\n          *error_string = absl::StrCat(\"found color index \", color_index,\n                                       \" outside of color map range \",\n                                       color_map->ColorCount);\n          return nullptr;\n        }\n\n        if (color_index == gcb.TransparentColor) {\n          // Use the pixel from the previous frame. In other words, no need to\n          // update our canvas for this pixel.\n          continue;\n        }\n\n        const GifColorType& gif_color = color_map->Colors[color_index];\n        p_dst[j * channel + 0] = gif_color.Red;\n        p_dst[j * channel + 1] = gif_color.Green;\n        p_dst[j * channel + 2] = gif_color.Blue;\n      }\n    }\n  }\n\n  return dstdata;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def authorize_and_redirect(request: HttpRequest) -> HttpResponse:\n    if not request.GET.get(\"redirect\"):\n        return HttpResponse(\"You need to pass a url to ?redirect=\", status=401)\n    if not request.META.get(\"HTTP_REFERER\"):\n        return HttpResponse('You need to make a request that includes the \"Referer\" header.', status=400)\n\n    current_team = cast(User, request.user).team\n    referer_url = urlparse(request.META[\"HTTP_REFERER\"])\n    redirect_url = urlparse(request.GET[\"redirect\"])\n\n    if not current_team or not hostname_in_app_urls(current_team, redirect_url.hostname):\n        return HttpResponse(f\"Can only redirect to a permitted domain.\", status=400)\n\n    if referer_url.hostname != redirect_url.hostname:\n        return HttpResponse(f\"Can only redirect to the same domain as the referer: {referer_url.hostname}\", status=400)\n\n    if referer_url.scheme != redirect_url.scheme:\n        return HttpResponse(f\"Can only redirect to the same scheme as the referer: {referer_url.scheme}\", status=400)\n\n    if referer_url.port != redirect_url.port:\n        return HttpResponse(\n            f\"Can only redirect to the same port as the referer: {referer_url.port or 'no port in URL'}\", status=400\n        )\n\n    return render_template(\n        \"authorize_and_redirect.html\",\n        request=request,\n        context={\"domain\": redirect_url.hostname, \"redirect_url\": request.GET[\"redirect\"]},\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def include_is_allowed(filepath):\n    filepath = os.path.abspath(filepath)\n    for root in settings.ALLOWED_INCLUDE_ROOTS:\n        if filepath.startswith(root):\n            return True\n    return False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def urlize(text, trim_url_limit=None, nofollow=False, autoescape=False):\n    \"\"\"\n    Converts any URLs in text into clickable links.\n\n    Works on http://, https://, www. links, and also on links ending in one of\n    the original seven gTLDs (.com, .edu, .gov, .int, .mil, .net, and .org).\n    Links can have trailing punctuation (periods, commas, close-parens) and\n    leading punctuation (opening parens) and it'll still do the right thing.\n\n    If trim_url_limit is not None, the URLs in the link text longer than this\n    limit will be truncated to trim_url_limit-3 characters and appended with\n    an ellipsis.\n\n    If nofollow is True, the links will get a rel=\"nofollow\" attribute.\n\n    If autoescape is True, the link text and URLs will be autoescaped.\n    \"\"\"\n    safe_input = isinstance(text, SafeData)\n\n    def trim_url(x, limit=trim_url_limit):\n        if limit is None or len(x) <= limit:\n            return x\n        return '%s...' % x[:max(0, limit - 3)]\n\n    def unescape(text, trail):\n        \"\"\"\n        If input URL is HTML-escaped, unescape it so as we can safely feed it to\n        smart_urlquote. For example:\n        http://example.com?x=1&amp;y=&lt;2&gt; => http://example.com?x=1&y=<2>\n        \"\"\"\n        unescaped = (text + trail).replace(\n            '&amp;', '&').replace('&lt;', '<').replace(\n            '&gt;', '>').replace('&quot;', '\"').replace('&#39;', \"'\")\n        if trail and unescaped.endswith(trail):\n            # Remove trail for unescaped if it was not consumed by unescape\n            unescaped = unescaped[:-len(trail)]\n        elif trail == ';':\n            # Trail was consumed by unescape (as end-of-entity marker), move it to text\n            text += trail\n            trail = ''\n        return text, unescaped, trail\n\n    def trim_punctuation(lead, middle, trail):\n        \"\"\"\n        Trim trailing and wrapping punctuation from `middle`. Return the items\n        of the new state.\n        \"\"\"\n        # Continue trimming until middle remains unchanged.\n        trimmed_something = True\n        while trimmed_something:\n            trimmed_something = False\n\n            # Trim trailing punctuation.\n            stripped = middle.rstrip(TRAILING_PUNCTUATION_CHARS)\n            if middle != stripped:\n                trail = middle[len(stripped):] + trail\n                middle = stripped\n                trimmed_something = True\n\n            # Trim wrapping punctuation.\n            for opening, closing in WRAPPING_PUNCTUATION:\n                if middle.startswith(opening):\n                    middle = middle[len(opening):]\n                    lead += opening\n                    trimmed_something = True\n                # Keep parentheses at the end only if they're balanced.\n                if (middle.endswith(closing) and\n                        middle.count(closing) == middle.count(opening) + 1):\n                    middle = middle[:-len(closing)]\n                    trail = closing + trail\n                    trimmed_something = True\n        return lead, middle, trail\n\n    def is_email_simple(value):\n        \"\"\"Return True if value looks like an email address.\"\"\"\n        # An @ must be in the middle of the value.\n        if '@' not in value or value.startswith('@') or value.endswith('@'):\n            return False\n        try:\n            p1, p2 = value.split('@')\n        except ValueError:\n            # value contains more than one @.\n            return False\n        # Dot must be in p2 (e.g. example.com)\n        if '.' not in p2 or p2.startswith('.'):\n            return False\n        return True\n\n    words = word_split_re.split(force_text(text))\n    for i, word in enumerate(words):\n        if '.' in word or '@' in word or ':' in word:\n            # lead: Current punctuation trimmed from the beginning of the word.\n            # middle: Current state of the word.\n            # trail: Current punctuation trimmed from the end of the word.\n            lead, middle, trail = '', word, ''\n            # Deal with punctuation.\n            lead, middle, trail = trim_punctuation(lead, middle, trail)\n\n            # Make URL we want to point to.\n            url = None\n            nofollow_attr = ' rel=\"nofollow\"' if nofollow else ''\n            if simple_url_re.match(middle):\n                middle, middle_unescaped, trail = unescape(middle, trail)\n                url = smart_urlquote(middle_unescaped)\n            elif simple_url_2_re.match(middle):\n                middle, middle_unescaped, trail = unescape(middle, trail)\n                url = smart_urlquote('http://%s' % middle_unescaped)\n            elif ':' not in middle and is_email_simple(middle):\n                local, domain = middle.rsplit('@', 1)\n                try:\n                    domain = domain.encode('idna').decode('ascii')\n                except UnicodeError:\n                    continue\n                url = 'mailto:%s@%s' % (local, domain)\n                nofollow_attr = ''\n\n            # Make link.\n            if url:\n                trimmed = trim_url(middle)\n                if autoescape and not safe_input:\n                    lead, trail = escape(lead), escape(trail)\n                    trimmed = escape(trimmed)\n                middle = '<a href=\"%s\"%s>%s</a>' % (escape(url), nofollow_attr, trimmed)\n                words[i] = mark_safe('%s%s%s' % (lead, middle, trail))\n            else:\n                if safe_input:\n                    words[i] = mark_safe(word)\n                elif autoescape:\n                    words[i] = escape(word)\n        elif safe_input:\n            words[i] = mark_safe(word)\n        elif autoescape:\n            words[i] = escape(word)\n    return ''.join(words)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n        key = self.make_key(key, version=version)\n        self.validate_key(key)\n        return self._cache.add(key, value, self.get_backend_timeout(timeout))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _redirect_safe(self, url, default=None):\n        \"\"\"Redirect if url is on our PATH\n\n        Full-domain redirects are allowed if they pass our CORS origin checks.\n\n        Otherwise use default (self.base_url if unspecified).\n        \"\"\"\n        if default is None:\n            default = self.base_url\n        # protect chrome users from mishandling unescaped backslashes.\n        # \\ is not valid in urls, but some browsers treat it as /\n        # instead of %5C, causing `\\\\` to behave as `//`\n        url = url.replace(\"\\\\\", \"%5C\")\n        parsed = urlparse(url)\n        if parsed.netloc or not (parsed.path + '/').startswith(self.base_url):\n            # require that next_url be absolute path within our path\n            allow = False\n            # OR pass our cross-origin check\n            if parsed.netloc:\n                # if full URL, run our cross-origin check:\n                origin = '%s://%s' % (parsed.scheme, parsed.netloc)\n                origin = origin.lower()\n                if self.allow_origin:\n                    allow = self.allow_origin == origin\n                elif self.allow_origin_pat:\n                    allow = bool(self.allow_origin_pat.match(origin))\n            if not allow:\n                # not allowed, use default\n                self.log.warning(\"Not allowing login redirect to %r\" % url)\n                url = default\n        self.redirect(url)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_columns(self) -> list[str]:\n        \"\"\"A list of columns to show in the Add form.\n\n        This dynamically calculates additional fields from providers and add\n        them to the backing list. This calculation is done exactly once (by\n        checking we're referencing the class-level variable instead of the\n        instance-level), and only after we enter the request context (to skip\n        superfuluous checks done by Flask-Appbuilder on startup).\n        \"\"\"\n        if self._add_columns is type(self)._add_columns and has_request_context():\n            self._add_columns = [\n                *self._add_columns,\n                *(k for k, _, _ in self._iter_extra_field_names_and_sensitivity()),\n            ]\n        return self._add_columns\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _check_signature(self, decoded_xml, item, node_name=NODE_NAME, origdoc=None, id_attr='', must=False, only_valid_cert=False, issuer=None):\n        try:\n            _issuer = item.issuer.text.strip()\n        except AttributeError:\n            _issuer = None\n\n        if _issuer is None:\n            try:\n                _issuer = issuer.text.strip()\n            except AttributeError:\n                _issuer = None\n\n        # More trust in certs from metadata then certs in the XML document\n        if self.metadata:\n            try:\n                _certs = self.metadata.certs(_issuer, 'any', 'signing')\n            except KeyError:\n                _certs = []\n            certs = []\n\n            for cert in _certs:\n                if isinstance(cert, six.string_types):\n                    content = pem_format(cert)\n                    tmp = make_temp(content,\n                                    suffix=\".pem\",\n                                    decode=False,\n                                    delete_tmpfiles=self.delete_tmpfiles)\n                    certs.append(tmp)\n                else:\n                    certs.append(cert)\n        else:\n            certs = []\n\n        if not certs and not self.only_use_keys_in_metadata:\n            logger.debug('==== Certs from instance ====')\n            certs = [\n                make_temp(content=pem_format(cert),\n                          suffix=\".pem\",\n                          decode=False,\n                          delete_tmpfiles=self.delete_tmpfiles)\n                for cert in cert_from_instance(item)\n            ]\n        else:\n            logger.debug('==== Certs from metadata ==== %s: %s ====', _issuer, certs)\n\n        if not certs:\n            raise MissingKey(_issuer)\n\n        # saml-core section \"5.4 XML Signature Profile\" defines constrains on the\n        # xmldsig-core facilities. It explicitly dictates that enveloped signatures\n        # are the only signatures allowed. This mean that:\n        # * Assertion/RequestType/ResponseType elements must have an ID attribute\n        # * signatures must have a single Reference element\n        # * the Reference element must have a URI attribute\n        # * the URI attribute contains an anchor\n        # * the anchor points to the enclosing element's ID attribute\n        references = item.signature.signed_info.reference\n        signatures_must_have_a_single_reference_element = len(references) == 1\n        the_Reference_element_must_have_a_URI_attribute = (\n            signatures_must_have_a_single_reference_element\n            and hasattr(references[0], \"uri\")\n        )\n        the_URI_attribute_contains_an_anchor = (\n            the_Reference_element_must_have_a_URI_attribute\n            and references[0].uri.startswith(\"#\")\n            and len(references[0].uri) > 1\n        )\n        the_anchor_points_to_the_enclosing_element_ID_attribute = (\n            the_URI_attribute_contains_an_anchor\n            and references[0].uri == \"#{id}\".format(id=item.id)\n        )\n        validators = {\n            \"signatures must have a single reference element\": (\n                signatures_must_have_a_single_reference_element\n            ),\n            \"the Reference element must have a URI attribute\": (\n                the_Reference_element_must_have_a_URI_attribute\n            ),\n            \"the URI attribute contains an anchor\": (\n                the_URI_attribute_contains_an_anchor\n            ),\n            \"the anchor points to the enclosing element ID attribute\": (\n                the_anchor_points_to_the_enclosing_element_ID_attribute\n            ),\n        }\n        if not all(validators.values()):\n            error_context = {\n                \"message\": \"Signature failed to meet constraints on xmldsig\",\n                \"validators\": validators,\n                \"item ID\": item.id,\n                \"reference URI\": item.signature.signed_info.reference[0].uri,\n                \"issuer\": _issuer,\n                \"node name\": node_name,\n                \"xml document\": decoded_xml,\n            }\n            raise SignatureError(error_context)\n\n        verified = False\n        last_pem_file = None\n\n        for pem_fd in certs:\n            try:\n                last_pem_file = pem_fd.name\n                if self.verify_signature(\n                        decoded_xml,\n                        pem_fd.name,\n                        node_name=node_name,\n                        node_id=item.id,\n                        id_attr=id_attr):\n                    verified = True\n                    break\n            except XmlsecError as exc:\n                logger.error('check_sig: %s', exc)\n                pass\n            except Exception as exc:\n                logger.error('check_sig: %s', exc)\n                raise\n\n        if verified or only_valid_cert:\n            if not self.cert_handler.verify_cert(last_pem_file):\n                raise CertificateError('Invalid certificate!')\n        else:\n            raise SignatureError('Failed to verify signature')\n\n        return item\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "function notify_update() {\n  jsonWrapper(URL_NOTIFY_GET_NEW+notify_latest_id+'/', function (data) {\n    if (data.success) {\n      $('.notification-cnt').html(data.total_count);\n      if (data.objects.length> 0) {\n        $('.notifications-empty').hide();\n      }\n      if (data.total_count > 0) {\n        $('.notification-cnt').addClass('badge-important');\n      } else {\n        $('.notification-cnt').removeClass('badge-important');\n      }\n      for (var i=data.objects.length-1; i >=0 ; i--) {\n        n = data.objects[i];\n        notify_latest_id = n.pk>notify_latest_id ? n.pk:notify_latest_id;\n        notify_oldest_id = (n.pk<notify_oldest_id || notify_oldest_id==0) ? n.pk:notify_oldest_id;\n        var element_outer_div = $(\"<div />\");\n        var element_a = $(\"<a />\", {href: URL_NOTIFY_GOTO + n.pk + \"/\"});\n        var element_message_div;\n        var element_since_div;\n        if (n.occurrences > 1) {\n          element_message_div = $(\"<div />\", {text: n.message});\n          element_since_div = $(\"<div />\", {text: n.occurrences_msg+' - ' + n.since, class: \"since\"});\n        } else {\n          element_message_div = $(\"<div />\", {text: n.message});\n          element_since_div = $(\"<div />\", {text: n.since, class: \"since\"});\n        }\n        element_a.append(element_message_div).append(element_since_div);\n        element = element_outer_div.append(element_a);\n        element.addClass('dropdown-item notification-item');\n        element.insertAfter('.notification-before-list');\n      }\n    }\n  });\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_object_element(dataset, seq_no, rel_path, download_link):\n    \"\"\"If rel_path and download_link are not None, we are called from scope.\n    Otherwise we are called from ID and need to run SQL query to fetch these attrs.\"\"\"\n\n    if rel_path is None:\n        query = \"SELECT rel_path, download_link FROM \" + \\\n        dataset + \\\n        \" WHERE sequence_no = %s\"\n\n        cnx = mysql.connector.connect(user=DB_USER,\n                                    password=DB_PASSWORD,\n                                    host=DB_HOST,\n                                    database=DB_DBNAME,\n                                    port=DB_PORT)\n        cursor = cnx.cursor()\n        cursor.execute(query, (seq_no,))\n\n        row = cursor.fetchone()\n\n        if not row:\n            return None\n\n        rel_path, download_link = row[0], row[1]\n\n    if LOCAL_OBJ_URI:\n        src_uri = 'file://' + _get_obj_absolute_path(dataset, rel_path)\n    else:\n        src_uri = url_for('.get_object_src_http', dataset=dataset, rel_path=rel_path)\n\n    return '<object id={} src={} hyperfind.external-link={} />' \\\n        .format(\n        quoteattr(url_for('.get_object_id', dataset=dataset, seq_no=seq_no)),\n        quoteattr(src_uri),\n        quoteattr(download_link))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static void SpatialMaxPoolWithArgMaxHelper(\n    OpKernelContext* context, Tensor* output, Tensor* output_arg_max,\n    Tensor* input_backprop, const Tensor& tensor_in, const Tensor& out_backprop,\n    const PoolParameters& params, const bool include_batch_in_index) {\n  if (input_backprop != nullptr) {\n    OP_REQUIRES(\n        context, include_batch_in_index,\n        errors::Internal(\n            \"SpatialMaxPoolWithArgMaxHelper requires include_batch_in_index \"\n            \"to be True when input_backprop != nullptr\"));\n    OP_REQUIRES(\n        context, (std::is_same<Targmax, int64>::value),\n        errors::Internal(\"SpatialMaxPoolWithArgMaxHelper requires Targmax \"\n                         \"to be int64 when input_backprop != nullptr\"));\n  }\n  if (tensor_in.NumElements() == 0 || output->NumElements() == 0) return;\n\n  typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n      ConstEigenMatrixMap;\n  typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n      EigenMatrixMap;\n  typedef Eigen::Map<Eigen::Matrix<Targmax, Eigen::Dynamic, Eigen::Dynamic>>\n      EigenIndexMatrixMap;\n\n  ConstEigenMatrixMap in_mat(\n      tensor_in.flat<T>().data(), params.depth,\n      params.tensor_in_cols * params.tensor_in_rows * params.tensor_in_batch);\n  EigenMatrixMap out_mat(\n      output->flat<T>().data(), params.depth,\n      params.out_width * params.out_height * params.tensor_in_batch);\n  EigenIndexMatrixMap out_arg_max_mat(\n      output_arg_max->flat<Targmax>().data(), params.depth,\n      params.out_width * params.out_height * params.tensor_in_batch);\n\n  const DeviceBase::CpuWorkerThreads& worker_threads =\n      *(context->device()->tensorflow_cpu_worker_threads());\n\n  // The following code basically does the following:\n  // 1. Flattens the input and output tensors into two dimensional arrays.\n  //    tensor_in_as_matrix:\n  //      depth by (tensor_in_cols * tensor_in_rows * tensor_in_batch)\n  //    output_as_matrix:\n  //      depth by (out_width * out_height * tensor_in_batch)\n  //\n  // 2. Walks through the set of columns in the flattened tensor_in_as_matrix,\n  //    and updates the corresponding column(s) in output_as_matrix with the\n  //    max value.\n  auto shard = [&params, &in_mat, &out_mat, &out_arg_max_mat, &input_backprop,\n                &output_arg_max, &out_backprop,\n                include_batch_in_index](int64_t start, int64_t limit) {\n    const int32_t depth = params.depth;\n    const int32_t in_rows = params.tensor_in_rows;\n    const int32_t in_cols = params.tensor_in_cols;\n    const int32_t pad_top = params.pad_top;\n    const int32_t pad_left = params.pad_left;\n    const int32_t window_rows = params.window_rows;\n    const int32_t window_cols = params.window_cols;\n    const int32_t row_stride = params.row_stride;\n    const int32_t col_stride = params.col_stride;\n    const int32_t out_height = params.out_height;\n    const int32_t out_width = params.out_width;\n\n    {\n      // Initializes the output tensor with MIN<T>.\n      const int32_t output_image_size = out_height * out_width * depth;\n      EigenMatrixMap out_shard(out_mat.data() + start * output_image_size, 1,\n                               (limit - start) * output_image_size);\n      out_shard.setConstant(Eigen::NumTraits<T>::lowest());\n      EigenIndexMatrixMap out_arg_max_shard(\n          out_arg_max_mat.data() + start * output_image_size, 1,\n          (limit - start) * output_image_size);\n      out_arg_max_shard.setConstant(kInvalidMaxPoolingIndex);\n    }\n\n    for (int64_t b = start; b < limit; ++b) {\n      for (int h = 0; h < in_rows; ++h) {\n        for (int w = 0; w < in_cols; ++w) {\n          // (h_start, h_end) * (w_start, w_end) is the range that the input\n          // vector projects to.\n          const int hpad = h + pad_top;\n          const int wpad = w + pad_left;\n          const int h_start =\n              (hpad < window_rows) ? 0 : (hpad - window_rows) / row_stride + 1;\n          const int h_end = std::min(hpad / row_stride + 1, out_height);\n          const int w_start =\n              (wpad < window_cols) ? 0 : (wpad - window_cols) / col_stride + 1;\n          const int w_end = std::min(wpad / col_stride + 1, out_width);\n          // compute elementwise max\n          const int64_t in_index = (b * in_rows + h) * in_cols + w;\n          for (int ph = h_start; ph < h_end; ++ph) {\n            const int64_t out_index_base = (b * out_height + ph) * out_width;\n            for (int pw = w_start; pw < w_end; ++pw) {\n              const int64_t out_index = out_index_base + pw;\n              /// NOTES(zhengxq): not using the eigen matrix operation for\n              /// now.\n              for (int d = 0; d < depth; ++d) {\n                const T& input_ref = in_mat.coeffRef(d, in_index);\n                T& output_ref = out_mat.coeffRef(d, out_index);\n                Targmax& out_arg_max_ref =\n                    out_arg_max_mat.coeffRef(d, out_index);\n                if (output_ref < input_ref ||\n                    out_arg_max_ref == kInvalidMaxPoolingIndex) {\n                  output_ref = input_ref;\n                  if (include_batch_in_index) {\n                    out_arg_max_ref = in_index * depth + d;\n                  } else {\n                    out_arg_max_ref = (h * in_cols + w) * depth + d;\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n\n    if (input_backprop != nullptr) {\n      auto input_backprop_flat = input_backprop->flat<T>();\n      auto out_arg_max_flat = output_arg_max->flat<int64>();\n      auto out_backprop_flat = out_backprop.flat<T>();\n\n      // Initialize output to 0.\n      const int64_t in_size = in_rows * in_cols * depth;\n      const int64_t in_start = start * in_size;\n      const int64_t in_end = limit * in_size;\n      EigenMatrixMap in_shard(input_backprop_flat.data() + in_start, 1,\n                              in_end - in_start);\n      in_shard.setConstant(T(0));\n\n      // Backpropagate.\n      const int out_size = out_height * out_width * depth;\n      const int out_start = start * out_size;\n      const int out_end = limit * out_size;\n      for (int index = out_start; index < out_end; ++index) {\n        int input_backprop_index = out_arg_max_flat(index);\n        // Although this check is in the inner loop, it is worth its value\n        // so we don't end up with memory corruptions. Our benchmark shows that\n        // the performance impact is quite small\n        // CHECK(input_backprop_index >= in_start && input_backprop_index <\n        // in_end)\n        FastBoundsCheck(input_backprop_index - in_start, in_end - in_start);\n        if (index < out_backprop.NumElements()) {\n          input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n        }\n      }\n    }\n  };\n\n  const int64_t shard_cost = params.tensor_in_rows * params.tensor_in_cols *\n                             params.depth * params.window_rows *\n                             params.window_cols;\n  Shard(worker_threads.num_threads, worker_threads.workers,\n        params.tensor_in_batch, shard_cost, shard);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _notify(self, assoc, attempt):\n        \"\"\"\n        Sends data about a new association (and, if necessary, the associated invites)\n        to the associated MXID's homeserver.\n\n        :param assoc: The association to send down to the homeserver.\n        :type assoc: dict[str, any]\n        :param attempt: The number of previous attempts to send this association.\n        :type attempt: int\n        \"\"\"\n        mxid = assoc[\"mxid\"]\n        mxid_parts = mxid.split(\":\", 1)\n\n        if len(mxid_parts) != 2:\n            logger.error(\n                \"Can't notify on bind for unparseable mxid %s. Not retrying.\",\n                assoc[\"mxid\"],\n            )\n            return\n\n        matrix_server = mxid_parts[1]\n\n        if not is_valid_hostname(matrix_server):\n            logger.error(\n                \"MXID server part '%s' not a valid hostname. Not retrying.\",\n                matrix_server,\n            )\n            return\n\n        post_url = \"matrix://%s/_matrix/federation/v1/3pid/onbind\" % (\n            matrix_server,\n        )\n\n        logger.info(\"Making bind callback to: %s\", post_url)\n\n        # Make a POST to the chosen Synapse server\n        http_client = FederationHttpClient(self.sydent)\n        try:\n            response = yield http_client.post_json_get_nothing(post_url, assoc, {})\n        except Exception as e:\n            self._notifyErrback(assoc, attempt, e)\n            return\n\n        # If the request failed, try again with exponential backoff\n        if response.code != 200:\n            self._notifyErrback(\n                assoc, attempt, \"Non-OK error code received (%d)\" % response.code\n            )\n        else:\n            logger.info(\"Successfully notified on bind for %s\" % (mxid,))\n\n            # Skip the deletion step if instructed so by the config.\n            if not self.sydent.delete_tokens_on_bind:\n                return\n\n            # Only remove sent tokens when they've been successfully sent.\n            try:\n                joinTokenStore = JoinTokenStore(self.sydent)\n                joinTokenStore.deleteTokens(assoc[\"medium\"], assoc[\"address\"])\n                logger.info(\n                    \"Successfully deleted invite for %s from the store\",\n                    assoc[\"address\"],\n                )\n            except Exception as e:\n                logger.exception(\n                    \"Couldn't remove invite for %s from the store\",\n                    assoc[\"address\"],\n                )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _truncate_html(self, length, truncate, text, truncate_len, words):\n        \"\"\"\n        Truncate HTML to a certain number of chars (not counting tags and\n        comments), or, if words is True, then to a certain number of words.\n        Close opened tags if they were correctly closed in the given HTML.\n\n        Preserve newlines in the HTML.\n        \"\"\"\n        if words and length <= 0:\n            return ''\n\n        size_limited = False\n        if len(text) > self.MAX_LENGTH_HTML:\n            text = text[: self.MAX_LENGTH_HTML]\n            size_limited = True\n\n        html4_singlets = (\n            'br', 'col', 'link', 'base', 'img',\n            'param', 'area', 'hr', 'input'\n        )\n\n        # Count non-HTML chars/words and keep note of open tags\n        pos = 0\n        end_text_pos = 0\n        current_len = 0\n        open_tags = []\n\n        regex = re_words if words else re_chars\n\n        while current_len <= length:\n            m = regex.search(text, pos)\n            if not m:\n                # Checked through whole string\n                break\n            pos = m.end(0)\n            if m[1]:\n                # It's an actual non-HTML word or char\n                current_len += 1\n                if current_len == truncate_len:\n                    end_text_pos = pos\n                continue\n            # Check for tag\n            tag = re_tag.match(m[0])\n            if not tag or current_len >= truncate_len:\n                # Don't worry about non tags or tags after our truncate point\n                continue\n            closing_tag, tagname, self_closing = tag.groups()\n            # Element names are always case-insensitive\n            tagname = tagname.lower()\n            if self_closing or tagname in html4_singlets:\n                pass\n            elif closing_tag:\n                # Check for match in open tags list\n                try:\n                    i = open_tags.index(tagname)\n                except ValueError:\n                    pass\n                else:\n                    # SGML: An end tag closes, back to the matching start tag,\n                    # all unclosed intervening start tags with omitted end tags\n                    open_tags = open_tags[i + 1:]\n            else:\n                # Add it to the start of the open tags list\n                open_tags.insert(0, tagname)\n\n        truncate_text = self.add_truncation_text(\"\", truncate)\n\n        if current_len <= length:\n            if size_limited and truncate_text:\n                text += truncate_text\n            return text\n\n        out = text[:end_text_pos]\n        if truncate_text:\n            out += truncate_text\n        # Close any tags still open\n        for tag in open_tags:\n            out += '</%s>' % tag\n        # Return string\n        return out\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    Tensor* output_tensor;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(c, c->allocate_output(0, {}, &output_tensor, attr));\n    PartialTensorShape element_shape;\n    OP_REQUIRES(\n        c, !TensorShapeUtils::IsMatrixOrHigher(c->input(1).shape()),\n        errors::InvalidArgument(\n            \"TensorListFromTensor: element_shape must be at most rank 1 but \",\n            \"has the shape of \", c->input(1).shape().DebugString()));\n    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(1), &element_shape));\n    TensorList output_list;\n    const Tensor& t = c->input(0);\n    output_list.element_dtype = t.dtype();\n    OP_REQUIRES(c, TensorShapeUtils::IsVectorOrHigher(t.shape()),\n                errors::InvalidArgument(\n                    \"Tensor must be at least a vector, but saw shape: \",\n                    t.shape().DebugString()));\n    TensorShape output_shape(t.shape());\n    output_shape.RemoveDim(0);\n    OP_REQUIRES(c, element_shape.IsCompatibleWith(output_shape),\n                errors::InvalidArgument(\n                    \"Specified a list with shape \", element_shape.DebugString(),\n                    \" from a tensor with shape \", output_shape.DebugString()));\n    output_list.element_shape = element_shape;\n    output_list.tensors().reserve(t.shape().dim_size(0));\n\n    const auto copy_tensor = IsPluggableDevice(c)\n                                 ? &CopyTensorPluggableDevice<T>\n                                 : &CopyTensor<Device, T>;\n\n    for (int i = 0; i < t.shape().dim_size(0); ++i) {\n      Tensor tmp = t.Slice(i, i + 1);\n      TensorShape tmp_shape = tmp.shape();\n      tmp_shape.RemoveDim(0);\n      OP_REQUIRES(c, tmp.CopyFrom(tmp, tmp_shape),\n                  errors::Unknown(\"Unexpected shape error.\"));\n      // TODO(apassos) maybe not always align; but weird compiler bugs seem to\n      // prevent this.\n      Tensor aligned;\n      OP_REQUIRES_OK(c, c->allocate_temp(tmp.dtype(), tmp.shape(), &aligned));\n      copy_tensor(c, tmp, aligned);\n      output_list.tensors().push_back(aligned);\n    }\n    output_tensor->scalar<Variant>()() = std::move(output_list);\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def load_pkcs12(buffer, passphrase=None):\n    \"\"\"\n    Load a PKCS12 object from a buffer\n\n    :param buffer: The buffer the certificate is stored in\n    :param passphrase: (Optional) The password to decrypt the PKCS12 lump\n    :returns: The PKCS12 object\n    \"\"\"\n    passphrase = _text_to_bytes_and_warn(\"passphrase\", passphrase)\n\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode(\"ascii\")\n\n    bio = _new_mem_buf(buffer)\n\n    # Use null passphrase if passphrase is None or empty string. With PKCS#12\n    # password based encryption no password and a zero length password are two\n    # different things, but OpenSSL implementation will try both to figure out\n    # which one works.\n    if not passphrase:\n        passphrase = _ffi.NULL\n\n    p12 = _lib.d2i_PKCS12_bio(bio, _ffi.NULL)\n    if p12 == _ffi.NULL:\n        _raise_current_error()\n    p12 = _ffi.gc(p12, _lib.PKCS12_free)\n\n    pkey = _ffi.new(\"EVP_PKEY**\")\n    cert = _ffi.new(\"X509**\")\n    cacerts = _ffi.new(\"Cryptography_STACK_OF_X509**\")\n\n    parse_result = _lib.PKCS12_parse(p12, passphrase, pkey, cert, cacerts)\n    if not parse_result:\n        _raise_current_error()\n\n    cacerts = _ffi.gc(cacerts[0], _lib.sk_X509_free)\n\n    # openssl 1.0.0 sometimes leaves an X509_check_private_key error in the\n    # queue for no particular reason.  This error isn't interesting to anyone\n    # outside this function.  It's not even interesting to us.  Get rid of it.\n    try:\n        _raise_current_error()\n    except Error:\n        pass\n\n    if pkey[0] == _ffi.NULL:\n        pykey = None\n    else:\n        pykey = PKey.__new__(PKey)\n        pykey._pkey = _ffi.gc(pkey[0], _lib.EVP_PKEY_free)\n\n    if cert[0] == _ffi.NULL:\n        pycert = None\n        friendlyname = None\n    else:\n        pycert = X509._from_raw_x509_ptr(cert[0])\n\n        friendlyname_length = _ffi.new(\"int*\")\n        friendlyname_buffer = _lib.X509_alias_get0(\n            cert[0], friendlyname_length\n        )\n        friendlyname = _ffi.buffer(\n            friendlyname_buffer, friendlyname_length[0]\n        )[:]\n        if friendlyname_buffer == _ffi.NULL:\n            friendlyname = None\n\n    pycacerts = []\n    for i in range(_lib.sk_X509_num(cacerts)):\n        x509 = _lib.sk_X509_value(cacerts, i)\n        pycacert = X509._from_raw_x509_ptr(x509)\n        pycacerts.append(pycacert)\n    if not pycacerts:\n        pycacerts = None\n\n    pkcs12 = PKCS12.__new__(PKCS12)\n    pkcs12._pkey = pykey\n    pkcs12._cert = pycert\n    pkcs12._cacerts = pycacerts\n    pkcs12._friendlyname = friendlyname\n    return pkcs12\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& x = context->input(0);\n    const Tensor& y = context->input(1);\n    const float min_x = context->input(2).flat<float>()(0);\n    const float max_x = context->input(3).flat<float>()(0);\n    const float min_y = context->input(4).flat<float>()(0);\n    const float max_y = context->input(5).flat<float>()(0);\n\n    BCast bcast(BCast::FromShape(x.shape()), BCast::FromShape(y.shape()));\n    if (!bcast.IsValid()) {\n      context->SetStatus(errors::InvalidArgument(\n          \"Incompatible shapes: \", x.shape().DebugString(), \" vs. \",\n          y.shape().DebugString()));\n      return;\n    }\n    Tensor* z;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0, BCast::ToShape(bcast.output_shape()), &z));\n\n    // Make sure that we have valid quantization ranges for the input buffers.\n    // If the difference between the min and max is negative or zero, it makes\n    // it hard to do meaningful intermediate operations on the values.\n    OP_REQUIRES(context, (max_x > min_x),\n                errors::InvalidArgument(\"max_x must be larger than min_a.\"));\n    OP_REQUIRES(context, (max_y > min_y),\n                errors::InvalidArgument(\"max_x must be larger than min_b.\"));\n    const int32 offset_x = FloatToQuantizedUnclamped<T>(0.0f, min_x, max_x);\n    const int32 offset_y = FloatToQuantizedUnclamped<T>(0.0f, min_y, max_y);\n    const T* x_data = x.flat<T>().data();\n    const T* y_data = y.flat<T>().data();\n    Toutput* z_data = z->flat<Toutput>().data();\n\n    const int ndims = bcast.x_reshape().size();\n    if (ndims <= 1) {\n      if (x.NumElements() == 1) {\n        ScalarMultiply<T, Toutput>(context, y_data, offset_y, y.NumElements(),\n                                   x_data[0], offset_x, z_data);\n      } else if (y.NumElements() == 1) {\n        ScalarMultiply<T, Toutput>(context, x_data, offset_x, x.NumElements(),\n                                   y_data[0], offset_y, z_data);\n      } else {\n        VectorMultiply<T, Toutput>(context, x_data, offset_x, y_data, offset_y,\n                                   x.NumElements(), z_data);\n      }\n    } else if (ndims == 2) {\n      const T* vector_data;\n      int64 vector_num_elements;\n      int32 vector_offset;\n      const T* tensor_data;\n      int64 tensor_num_elements;\n      int32 tensor_offset;\n      if (x.NumElements() < y.NumElements()) {\n        vector_data = x_data;\n        vector_num_elements = x.NumElements();\n        vector_offset = offset_x;\n        tensor_data = y_data;\n        tensor_num_elements = y.NumElements();\n        tensor_offset = offset_y;\n      } else {\n        vector_data = y_data;\n        vector_num_elements = y.NumElements();\n        vector_offset = offset_y;\n        tensor_data = x_data;\n        tensor_num_elements = x.NumElements();\n        tensor_offset = offset_x;\n      }\n      if (vector_num_elements == 0) {\n        context->SetStatus(\n            errors::InvalidArgument(\"vector must have at least 1 element\"));\n        return;\n      }\n      VectorTensorMultiply<T, Toutput>(\n          vector_data, vector_offset, vector_num_elements, tensor_data,\n          tensor_offset, tensor_num_elements, z_data);\n    } else {\n      LOG(INFO) << \"ndims=\" << ndims;\n      LOG(INFO) << \"bcast.x_reshape()=\"\n                << TensorShape(bcast.x_reshape()).DebugString();\n      LOG(INFO) << \"bcast.y_reshape()=\"\n                << TensorShape(bcast.y_reshape()).DebugString();\n      LOG(INFO) << \"bcast.x_bcast()=\"\n                << TensorShape(bcast.x_bcast()).DebugString();\n      LOG(INFO) << \"bcast.y_bcast()=\"\n                << TensorShape(bcast.y_bcast()).DebugString();\n\n      context->SetStatus(errors::Unimplemented(\n          \"Broadcast between \", context->input(0).shape().DebugString(),\n          \" and \", context->input(1).shape().DebugString(),\n          \" is not supported yet.\"));\n      return;\n    }\n\n    float min_z_value;\n    float max_z_value;\n    QuantizationRangeForMultiplication<T, T, Toutput>(\n        min_x, max_x, min_y, max_y, &min_z_value, &max_z_value);\n    Tensor* z_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &z_min));\n    z_min->flat<float>()(0) = min_z_value;\n\n    Tensor* z_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &z_max));\n    z_max->flat<float>()(0) = max_z_value;\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def value_from_datadict(self, data, files, name):\n        \"File widgets take data from FILES, not POST\"\n        getter = files.get\n        if self.allow_multiple_selected:\n            try:\n                getter = files.getlist\n            except AttributeError:\n                pass\n        return getter(name)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def as_sql(self, compiler, connection):\n        if not connection.ops.extract_trunc_lookup_pattern.fullmatch(self.lookup_name):\n            raise ValueError(\"Invalid lookup_name: %s\" % self.lookup_name)\n        sql, params = compiler.compile(self.lhs)\n        lhs_output_field = self.lhs.output_field\n        if isinstance(lhs_output_field, DateTimeField):\n            tzname = self.get_tzname()\n            sql = connection.ops.datetime_extract_sql(self.lookup_name, sql, tzname)\n        elif self.tzinfo is not None:\n            raise ValueError('tzinfo can only be used with DateTimeField.')\n        elif isinstance(lhs_output_field, DateField):\n            sql = connection.ops.date_extract_sql(self.lookup_name, sql)\n        elif isinstance(lhs_output_field, TimeField):\n            sql = connection.ops.time_extract_sql(self.lookup_name, sql)\n        elif isinstance(lhs_output_field, DurationField):\n            if not connection.features.has_native_duration_field:\n                raise ValueError('Extract requires native DurationField database support.')\n            sql = connection.ops.time_extract_sql(self.lookup_name, sql)\n        else:\n            # resolve_expression has already validated the output_field so this\n            # assert should never be hit.\n            assert False, \"Tried to Extract from an invalid type.\"\n        return sql, params\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def handle(self, scope, send, body):\n        \"\"\"\n        Synchronous message processing.\n        \"\"\"\n        # Set script prefix from message root_path, turning None into empty string\n        script_prefix = scope.get(\"root_path\", \"\") or \"\"\n        if settings.FORCE_SCRIPT_NAME:\n            script_prefix = settings.FORCE_SCRIPT_NAME\n        set_script_prefix(script_prefix)\n        signals.request_started.send(sender=self.__class__, scope=scope)\n        # Run request through view system\n        try:\n            request = self.request_class(scope, body)\n        except UnicodeDecodeError:\n            logger.warning(\n                \"Bad Request (UnicodeDecodeError)\",\n                exc_info=sys.exc_info(),\n                extra={\"status_code\": 400},\n            )\n            response = http.HttpResponseBadRequest()\n        except RequestTimeout:\n            # Parsing the request failed, so the response is a Request Timeout error\n            response = HttpResponse(\"408 Request Timeout (upload too slow)\", status=408)\n        except RequestAborted:\n            # Client closed connection on us mid request. Abort!\n            return\n        except RequestDataTooBig:\n            response = HttpResponse(\"413 Payload too large\", status=413)\n        else:\n            response = self.get_response(request)\n            # Fix chunk size on file responses\n            if isinstance(response, FileResponse):\n                response.block_size = 1024 * 512\n        # Transform response into messages, which we yield back to caller\n        for response_message in self.encode_response(response):\n            send(response_message)\n        # Close the response now we're done with it\n        response.close()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _construct_forms(self):\n        # instantiate all the forms and put them in self.forms\n        self.forms = []\n        for i in xrange(min(self.total_form_count(), self.absolute_max)):\n            self.forms.append(self._construct_form(i))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def send(self, conn: \"Connection\") -> \"ClientResponse\":\n        # Specify request target:\n        # - CONNECT request must send authority form URI\n        # - not CONNECT proxy must send absolute form URI\n        # - most common is origin form URI\n        if self.method == hdrs.METH_CONNECT:\n            connect_host = self.url.raw_host\n            assert connect_host is not None\n            if helpers.is_ipv6_address(connect_host):\n                connect_host = f\"[{connect_host}]\"\n            path = f\"{connect_host}:{self.url.port}\"\n        elif self.proxy and not self.is_ssl():\n            path = str(self.url)\n        else:\n            path = self.url.raw_path\n            if self.url.raw_query_string:\n                path += \"?\" + self.url.raw_query_string\n\n        protocol = conn.protocol\n        assert protocol is not None\n        writer = StreamWriter(\n            protocol,\n            self.loop,\n            on_chunk_sent=functools.partial(\n                self._on_chunk_request_sent, self.method, self.url\n            ),\n            on_headers_sent=functools.partial(\n                self._on_headers_request_sent, self.method, self.url\n            ),\n        )\n\n        if self.compress:\n            writer.enable_compression(self.compress)\n\n        if self.chunked is not None:\n            writer.enable_chunking()\n\n        # set default content-type\n        if (\n            self.method in self.POST_METHODS\n            and hdrs.CONTENT_TYPE not in self.skip_auto_headers\n            and hdrs.CONTENT_TYPE not in self.headers\n        ):\n            self.headers[hdrs.CONTENT_TYPE] = \"application/octet-stream\"\n\n        # set the connection header\n        connection = self.headers.get(hdrs.CONNECTION)\n        if not connection:\n            if self.keep_alive():\n                if self.version == HttpVersion10:\n                    connection = \"keep-alive\"\n            else:\n                if self.version == HttpVersion11:\n                    connection = \"close\"\n\n        if connection is not None:\n            self.headers[hdrs.CONNECTION] = connection\n\n        # status + headers\n        status_line = \"{0} {1} HTTP/{v.major}.{v.minor}\".format(\n            self.method, path, v=self.version\n        )\n        await writer.write_headers(status_line, self.headers)\n\n        self._writer = self.loop.create_task(self.write_bytes(writer, conn))\n\n        response_class = self.response_class\n        assert response_class is not None\n        self.response = response_class(\n            self.method,\n            self.original_url,\n            writer=self._writer,\n            continue100=self._continue,\n            timer=self._timer,\n            request_info=self.request_info,\n            traces=self._traces,\n            loop=self.loop,\n            session=self._session,\n        )\n        return self.response\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def on_PUT(self, origin, content, query, room_id):\n        content = await self.handler.on_exchange_third_party_invite_request(content)\n        return 200, content\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def save_template(cls, db: Session, zip_file: ZipFile) -> None:\n        \"\"\"\n        Extracts and validates the contents of a zip file containing a\n        custom connector template, registers the template, and saves it to the database.\n        \"\"\"\n\n        # verify the zip file before we use it\n        verify_zip(zip_file)\n\n        config_contents = None\n        dataset_contents = None\n        icon_contents = None\n        function_contents = None\n\n        for info in zip_file.infolist():\n            try:\n                file_contents = zip_file.read(info).decode()\n            except UnicodeDecodeError:\n                # skip any hidden metadata files that can't be decoded with UTF-8\n                logger.debug(f\"Unable to decode the file: {info.filename}\")\n                continue\n\n            if info.filename.endswith(\"config.yml\"):\n                if not config_contents:\n                    config_contents = file_contents\n                else:\n                    raise ValidationError(\n                        \"Multiple files ending with config.yml found, only one is allowed.\"\n                    )\n            elif info.filename.endswith(\"dataset.yml\"):\n                if not dataset_contents:\n                    dataset_contents = file_contents\n                else:\n                    raise ValidationError(\n                        \"Multiple files ending with dataset.yml found, only one is allowed.\"\n                    )\n            elif info.filename.endswith(\".svg\"):\n                if not icon_contents:\n                    icon_contents = str_to_b64_str(file_contents)\n                else:\n                    raise ValidationError(\n                        \"Multiple svg files found, only one is allowed.\"\n                    )\n            elif info.filename.endswith(\".py\"):\n                if not function_contents:\n                    function_contents = file_contents\n                else:\n                    raise ValidationError(\n                        \"Multiple Python (.py) files found, only one is allowed.\"\n                    )\n\n        if not config_contents:\n            raise ValidationError(\"Zip file does not contain a config.yml file.\")\n\n        if not dataset_contents:\n            raise ValidationError(\"Zip file does not contain a dataset.yml file.\")\n\n        # early validation of SaaS config and dataset\n        saas_config = SaaSConfig(**load_config_from_string(config_contents))\n        Dataset(**load_dataset_from_string(dataset_contents))\n\n        # extract connector_type, human_readable, and replaceable values from the SaaS config\n        connector_type = saas_config.type\n        human_readable = saas_config.name\n        replaceable = saas_config.replaceable\n\n        # if the incoming connector is flagged as replaceable we will update the version to match\n        # that of the existing connector template this way the custom connector template can be\n        # removed once a newer version is bundled with Fides\n        if replaceable:\n            existing_connector = (\n                FileConnectorTemplateLoader.get_connector_templates().get(\n                    connector_type\n                )\n            )\n            if existing_connector:\n                existing_config = SaaSConfig(\n                    **load_config_from_string(existing_connector.config)\n                )\n                config_contents = replace_version(\n                    config_contents, existing_config.version\n                )\n\n        template = CustomConnectorTemplate(\n            key=connector_type,\n            name=human_readable,\n            config=config_contents,\n            dataset=dataset_contents,\n            icon=icon_contents,\n            functions=function_contents,\n            replaceable=replaceable,\n        )\n\n        # attempt to register the template, raises an exception if validation fails\n        CustomConnectorTemplateLoader._register_template(template)\n\n        # save the custom connector to the database if it passed validation\n        CustomConnectorTemplate.create_or_update(\n            db=db,\n            data={\n                \"key\": connector_type,\n                \"name\": human_readable,\n                \"config\": config_contents,\n                \"dataset\": dataset_contents,\n                \"icon\": icon_contents,\n                \"functions\": function_contents,\n                \"replaceable\": replaceable,\n            },\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_safe_url(url, host=None):\n    \"\"\"\n    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to\n    a different host and uses a safe scheme).\n\n    Always returns ``False`` on an empty url.\n    \"\"\"\n    if not url:\n        return False\n    url_info = urlparse.urlparse(url)\n    return (not url_info[1] or url_info[1] == host) and \\\n        (not url_info[0] or url_info[0] in ['http', 'https'])\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_session_id_by_provider_id(self, provider_id, user):\n        \"\"\"\n        Return a tuple of logged-in session IDs by provider_id\n        \"\"\"\n        candidates = self._ss.get_user_sessions(user)\n\n        session_ids = []\n        for c in candidates:\n            key = c.keys()[0]\n            if c[key].get('provider_id') == provider_id:\n                samlsession = self._data_to_samlsession(key, c[key])\n                session_ids.append(samlsession.session_id.encode('utf-8'))\n\n        return tuple(session_ids)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void ProcessMedianKernel(const Context& dev_ctx,\n                         const DenseTensor& x,\n                         DenseTensor* out,\n                         DenseTensor* median_index) {\n  const T* x_data = x.data<T>();\n  T* out_data = dev_ctx.template Alloc<T>(out);\n  int64_t* m_data = dev_ctx.template Alloc<int64_t>(median_index);\n\n  int64_t numel = x.numel();\n  auto x_dim = x.dims();\n  int64_t x_rank = x_dim.size();\n  int64_t stride = x_dim[x_rank - 1];\n\n  PADDLE_ENFORCE_NE(\n      stride,\n      0,\n      phi::errors::InvalidArgument(\"The input Tensor x's shape[-1] should not \"\n                                   \"be 0, but shape is %s now.\",\n                                   x_dim));\n\n  int64_t pre_dim = numel / stride;\n  int64_t i = 0;\n\n  int64_t max_valid_num = 0;\n  std::vector<int64_t> nan_counts;\n  bool ignore_nan = true;\n  if (ignore_nan) {\n    int64_t total_nan_num = 0;\n    std::vector<T> col_vec;\n    col_vec.reserve(stride);\n    col_vec.resize(stride);\n    nan_counts.clear();\n    nan_counts.reserve(pre_dim);\n    nan_counts.resize(pre_dim);\n    for (int64_t i = 0; i < pre_dim; i++) {\n      col_vec.clear();\n      col_vec.insert(\n          col_vec.begin(), x_data + i * stride, x_data + (i + 1) * stride);\n      nan_counts[i] =\n          std::count_if(col_vec.begin(), col_vec.end(), [&](const T& val) {\n            return std::isnan(static_cast<float>(val));\n          });\n      total_nan_num += nan_counts[i];\n      if (stride - nan_counts[i] > max_valid_num)\n        max_valid_num = stride - nan_counts[i];\n    }\n    // all elems are nan\n    if (total_nan_num == numel) {\n      for (i = 0; i < pre_dim; i++) {\n        out_data[i] = std::numeric_limits<T>::quiet_NaN();\n        m_data[2 * i] = -1;\n        m_data[2 * i + 1] = -1;\n      }\n      return;\n    }\n    ignore_nan = total_nan_num > 0;\n  }\n\n  int64_t sort_k = ignore_nan ? max_valid_num : ((stride >> 1) + 1);\n  CalcMedianFunc<T, Context>(dev_ctx,\n                             x,\n                             nan_counts,\n                             ignore_nan,\n                             sort_k,\n                             stride,\n                             pre_dim,\n                             out_data,\n                             m_data);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def decorated(*args, **kwargs):\n            __tracebackhide__ = True  # Hide from pytest traceback.\n\n            appbuilder = current_app.appbuilder\n\n            dag_id = (\n                kwargs.get(\"dag_id\")\n                or request.args.get(\"dag_id\")\n                or request.form.get(\"dag_id\")\n                or (request.is_json and request.json.get(\"dag_id\"))\n                or None\n            )\n            if appbuilder.sm.check_authorization(permissions, dag_id):\n                return func(*args, **kwargs)\n            elif not g.user.is_anonymous and not g.user.perms:\n                return (\n                    render_template(\n                        \"airflow/no_roles_permissions.html\",\n                        hostname=get_hostname()\n                        if conf.getboolean(\"webserver\", \"EXPOSE_HOSTNAME\")\n                        else \"redact\",\n                        logout_url=appbuilder.get_url_for_logout,\n                    ),\n                    403,\n                )\n            else:\n                access_denied = \"Access is Denied\"\n                flash(access_denied, \"danger\")\n            return redirect(\n                url_for(\n                    appbuilder.sm.auth_view.__class__.__name__ + \".login\",\n                    next=request.url,\n                )\n            )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_many(self, data, timeout=DEFAULT_TIMEOUT, version=None):\n        safe_data = {}\n        original_keys = {}\n        for key, value in data.items():\n            safe_key = self.make_key(key, version=version)\n            self.validate_key(safe_key)\n            safe_data[safe_key] = value\n            original_keys[safe_key] = key\n        failed_keys = self._cache.set_multi(safe_data, self.get_backend_timeout(timeout))\n        return [original_keys[k] for k in failed_keys]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def creatorForNetloc(self, hostname, port):\n        \"\"\"\n        Create a L{client connection creator\n        <twisted.internet.interfaces.IOpenSSLClientConnectionCreator>} for a\n        given network location.\n\n        @param tls: The TLS protocol to create a connection for.\n        @type tls: L{twisted.protocols.tls.TLSMemoryBIOProtocol}\n\n        @param hostname: The hostname part of the URI.\n        @type hostname: L{bytes}\n\n        @param port: The port part of the URI.\n        @type port: L{int}\n\n        @return: a connection creator with appropriate verification\n            restrictions set\n        @rtype: L{client connection creator\n            <twisted.internet.interfaces.IOpenSSLClientConnectionCreator>}\n        \"\"\"\n        return optionsForClientTLS(hostname.decode(\"ascii\"),\n                                   trustRoot=self._trustRoot)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def gen_response_objects(self):\n        for o in self.event_json['Event']['Object']:\n            self.response += self.conn.object_to_entity(o)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def default(self, path=b\"\", **kwargs):\r\n        # Check permissions on path/repo\r\n        repo, path = self.app.store.get_repo_path(path)\r\n        # Check if path exists with fstats\r\n        path_obj = repo.fstat(path)\r\n        # Check user's permissions\r\n        is_maintainer()\r\n\r\n        # validate form\r\n        form = DeleteRepoForm()\r\n        form.expected_confirm = path_obj.display_name\r\n        if form.is_submitted():\r\n            if form.validate():\r\n                cherrypy.engine.publish('schedule_task', repo.delete, path)\r\n                # Redirect to parent folder or to root if repo get deleted\r\n                if path_obj.isroot:\r\n                    raise cherrypy.HTTPRedirect(url_for('/'))\r\n                else:\r\n                    parent_path = repo.fstat(os.path.dirname(path_obj.path))\r\n                    raise cherrypy.HTTPRedirect(url_for('browse', repo, parent_path))\r\n            else:\r\n                raise cherrypy.HTTPError(400, form.error_message)\r\n        else:\r\n            raise cherrypy.HTTPError(405)\r\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def load_djpeg(self):\n\n        # ALTERNATIVE: handle JPEGs via the IJG command line utilities\n\n        import tempfile, os\n        f, path = tempfile.mkstemp()\n        os.close(f)\n        if os.path.exists(self.filename):\n            os.system(\"djpeg '%s' >'%s'\" % (self.filename, path))\n        else:\n            raise ValueError(\"Invalid Filename\")\n\n        try:\n            self.im = Image.core.open_ppm(path)\n        finally:\n            try: os.unlink(path)\n            except: pass\n\n        self.mode = self.im.mode\n        self.size = self.im.size\n\n        self.tile = []\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_value(section: str, option: str) -> Response:\n    serializer = {\n        \"text/plain\": _config_to_text,\n        \"application/json\": _config_to_json,\n    }\n    return_type = request.accept_mimetypes.best_match(serializer.keys())\n    if conf.get(\"webserver\", \"expose_config\").lower() == \"non-sensitive-only\":\n        expose_config = True\n    else:\n        expose_config = conf.getboolean(\"webserver\", \"expose_config\")\n\n    if return_type not in serializer:\n        return Response(status=HTTPStatus.NOT_ACCEPTABLE)\n    elif expose_config:\n        if not conf.has_option(section, option):\n            raise NotFound(\n                \"Config not found.\", detail=f\"The option [{section}/{option}] is not found in config.\"\n            )\n\n        print(conf.sensitive_config_values)\n        if (section, option) in conf.sensitive_config_values:\n            value = \"< hidden >\"\n        else:\n            value = conf.get(section, option)\n\n        config = Config(\n            sections=[ConfigSection(name=section, options=[ConfigOption(key=option, value=value)])]\n        )\n        config_text = serializer[return_type](config)\n        return Response(config_text, headers={\"Content-Type\": return_type})\n    else:\n        raise PermissionDenied(\n            detail=(\n                \"Your Airflow administrator chose not to expose the configuration, most likely for security\"\n                \" reasons.\"\n            )\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _write(self, contents: dict):\n        LOGGER.debug(f'Writing to {self.path}')\n\n        def opener(path, flags):\n            return os.open(path, flags, self.permissions)\n\n        with open(self.path, 'w', opener=opener) as fp:\n            fp.write(json.dumps(contents))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, value_type: VyperType, length: int):\n        if not 0 < length < 2**256:\n            raise InvalidType(\"Array length is invalid\")\n\n        if length >= 2**64:\n            warnings.warn(\"Use of large arrays can be unsafe!\")\n\n        super().__init__(UINT256_T, value_type)\n        self.length = length\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _load_from(self, data: bytes) -> None:\n        if data.strip() == b'':\n            data = XMP_EMPTY  # on some platforms lxml chokes on empty documents\n\n        def basic_parser(xml):\n            return parse_xml(BytesIO(xml))\n\n        def strip_illegal_bytes_parser(xml):\n            return parse_xml(BytesIO(re_xml_illegal_bytes.sub(b'', xml)))\n\n        def recovery_parser(xml):\n            return parse_xml(BytesIO(xml), recover=True)\n\n        def replace_with_empty_xmp(_xml=None):\n            log.warning(\"Error occurred parsing XMP, replacing with empty XMP.\")\n            return basic_parser(XMP_EMPTY)\n\n        if self.overwrite_invalid_xml:\n            parsers: Iterable[Callable] = [\n                basic_parser,\n                strip_illegal_bytes_parser,\n                recovery_parser,\n                replace_with_empty_xmp,\n            ]\n        else:\n            parsers = [basic_parser]\n\n        for parser in parsers:\n            try:\n                self._xmp = parser(data)\n            except (XMLSyntaxError if self.overwrite_invalid_xml else NeverRaise) as e:\n                if str(e).startswith(\"Start tag expected, '<' not found\") or str(\n                    e\n                ).startswith(\"Document is empty\"):\n                    self._xmp = replace_with_empty_xmp()\n                    break\n            else:\n                break\n\n        try:\n            pis = self._xmp.xpath('/processing-instruction()')\n            for pi in pis:\n                etree.strip_tags(self._xmp, pi.tag)\n            self._get_rdf_root()\n        except (Exception if self.overwrite_invalid_xml else NeverRaise) as e:\n            log.warning(\"Error occurred parsing XMP\", exc_info=e)\n            self._xmp = replace_with_empty_xmp()\n        return\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    const TensorList* input_list = nullptr;\n    OP_REQUIRES_OK(c, GetInputList(c, 0, &input_list));\n    OP_REQUIRES(c, TensorShapeUtils::IsScalar(c->input(1).shape()),\n                errors::InvalidArgument(\"size must be a scalar\"));\n    int32_t size = c->input(1).scalar<int32>()();\n    OP_REQUIRES(\n        c, size >= 0,\n        errors::InvalidArgument(\n            \"TensorListSlice expects size to be non-negative. Got: \", size));\n\n    std::unique_ptr<Tensor> maybe_result =\n        c->forward_input(0, 0, DT_VARIANT, TensorShape{},\n                         c->input_memory_type(0), AllocatorAttributes());\n    if (maybe_result != nullptr) {\n      TensorList* out = maybe_result->scalar<Variant>()().get<TensorList>();\n      if (out->RefCountIsOne()) {\n        // We are able to forward the input.\n        out->tensors().resize(size, Tensor(DT_INVALID));\n        c->set_output(0, *maybe_result);\n        return;\n      }\n    }\n\n    // We were not able to forward the input.  Will have to resize from scratch.\n    Tensor* result;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape{}, &result, attr));\n    TensorList output_list;\n    output_list.element_shape = input_list->element_shape;\n    output_list.element_dtype = input_list->element_dtype;\n    output_list.max_num_elements = input_list->max_num_elements;\n    if (size > input_list->tensors().size()) {\n      output_list.tensors().insert(output_list.tensors().begin(),\n                                   input_list->tensors().begin(),\n                                   input_list->tensors().end());\n      // Add DT_INVALID tensors to the end of the list if the requested size\n      // is larger than the list length.\n      output_list.tensors().resize(size, Tensor(DT_INVALID));\n    } else {\n      output_list.tensors().insert(output_list.tensors().begin(),\n                                   input_list->tensors().begin(),\n                                   input_list->tensors().begin() + size);\n    }\n    result->scalar<Variant>()() = std::move(output_list);\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def process_request(self, request, spider):\n        if 'splash' not in request.meta:\n            return\n        splash_options = request.meta['splash']\n\n        if request.method not in {'GET', 'POST'}:\n            logger.error(\n                \"Currently only GET and POST requests are supported by \"\n                \"SplashMiddleware; %(request)s is dropped\",\n                {'request': request},\n                extra={'spider': spider}\n            )\n            self.crawler.stats.inc_value('splash/dropped/method/{}'.format(\n                request.method))\n            raise IgnoreRequest(\"SplashRequest doesn't support \"\n                                \"HTTP {} method\".format(request.method))\n\n        if request.meta.get(\"_splash_processed\"):\n            # don't process the same request more than once\n            return\n\n        request.meta['_splash_processed'] = True\n\n        slot_policy = splash_options.get('slot_policy', self.slot_policy)\n        self._set_download_slot(request, request.meta, slot_policy)\n\n        args = splash_options.setdefault('args', {})\n\n        if '_replaced_args' in splash_options:\n            # restore arguments before sending request to the downloader\n            load_args = {}\n            save_args = []\n            local_arg_fingerprints = {}\n            for name in splash_options['_replaced_args']:\n                fp = args[name]\n                # Use remote Splash argument cache: if Splash key\n                # for a value is known then don't send the value to Splash;\n                # if it is unknown then try to save the value on server using\n                # ``save_args``.\n                if fp in self._remote_keys:\n                    load_args[name] = self._remote_keys[fp]\n                    del args[name]\n                else:\n                    save_args.append(name)\n                    args[name] = self._argument_values[fp]\n\n                local_arg_fingerprints[name] = fp\n\n            if load_args:\n                args['load_args'] = load_args\n            if save_args:\n                args['save_args'] = save_args\n            splash_options['_local_arg_fingerprints'] = local_arg_fingerprints\n\n            del splash_options['_replaced_args']  # ??\n\n        args.setdefault('url', request.url)\n        if request.method == 'POST':\n            args.setdefault('http_method', request.method)\n            # XXX: non-UTF8 request bodies are not supported now\n            args.setdefault('body', request.body.decode('utf8'))\n\n        if not splash_options.get('dont_send_headers'):\n            headers = scrapy_headers_to_unicode_dict(request.headers)\n            if headers:\n                # Headers set by HttpAuthMiddleware should be used for Splash,\n                # not for the remote website (backwards compatibility).\n                if _http_auth_enabled(spider):\n                    headers.pop('Authorization', None)\n                args.setdefault('headers', headers)\n\n        body = json.dumps(args, ensure_ascii=False, sort_keys=True, indent=4)\n        # print(body)\n\n        if 'timeout' in args:\n            # User requested a Splash timeout explicitly.\n            #\n            # We can't catch a case when user requested `download_timeout`\n            # explicitly because a default value for `download_timeout`\n            # is set by DownloadTimeoutMiddleware.\n            #\n            # As user requested Splash timeout explicitly, we shouldn't change\n            # it. Another reason not to change the requested Splash timeout is\n            # because it may cause a validation error on the remote end.\n            #\n            # But we can change Scrapy `download_timeout`: increase\n            # it when it's too small. Decreasing `download_timeout` is not\n            # safe.\n\n            timeout_requested = float(args['timeout'])\n            timeout_expected = timeout_requested + self.splash_extra_timeout\n\n            # no timeout means infinite timeout\n            timeout_current = request.meta.get('download_timeout', 1e6)\n\n            if timeout_expected > timeout_current:\n                request.meta['download_timeout'] = timeout_expected\n\n        endpoint = splash_options.setdefault('endpoint', self.default_endpoint)\n        splash_base_url = splash_options.get('splash_url', self.splash_base_url)\n        splash_url = urljoin(splash_base_url, endpoint)\n\n        headers = Headers({'Content-Type': 'application/json'})\n        if self.auth is not None:\n            headers['Authorization'] = self.auth\n        headers.update(splash_options.get('splash_headers', {}))\n        new_request = request.replace(\n            url=splash_url,\n            method='POST',\n            body=body,\n            headers=headers,\n            priority=request.priority + self.rescheduling_priority_adjust\n        )\n        new_request.meta['dont_obey_robotstxt'] = True\n        self.crawler.stats.inc_value('splash/%s/request_count' % endpoint)\n        return new_request\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& handle = ctx->input(0);\n    const string& name = handle.scalar<tstring>()();\n    Tensor val;\n    auto session_state = ctx->session_state();\n    OP_REQUIRES(ctx, session_state != nullptr,\n                errors::FailedPrecondition(\n                    \"GetSessionTensor called on null session state\"));\n    OP_REQUIRES_OK(ctx, session_state->GetTensor(name, &val));\n    ctx->set_output(0, val);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def decrypt(crypto: bytes, priv_key: key.PrivateKey) -> bytes:\n    r\"\"\"Decrypts the given message using PKCS#1 v1.5\n\n    The decryption is considered 'failed' when the resulting cleartext doesn't\n    start with the bytes 00 02, or when the 00 byte between the padding and\n    the message cannot be found.\n\n    :param crypto: the crypto text as returned by :py:func:`rsa.encrypt`\n    :param priv_key: the :py:class:`rsa.PrivateKey` to decrypt with.\n    :raise DecryptionError: when the decryption fails. No details are given as\n        to why the code thinks the decryption fails, as this would leak\n        information about the private key.\n\n\n    >>> import rsa\n    >>> (pub_key, priv_key) = rsa.newkeys(256)\n\n    It works with strings:\n\n    >>> crypto = encrypt(b'hello', pub_key)\n    >>> decrypt(crypto, priv_key)\n    b'hello'\n\n    And with binary data:\n\n    >>> crypto = encrypt(b'\\x00\\x00\\x00\\x00\\x01', pub_key)\n    >>> decrypt(crypto, priv_key)\n    b'\\x00\\x00\\x00\\x00\\x01'\n\n    Altering the encrypted information will *likely* cause a\n    :py:class:`rsa.pkcs1.DecryptionError`. If you want to be *sure*, use\n    :py:func:`rsa.sign`.\n\n\n    .. warning::\n\n        Never display the stack trace of a\n        :py:class:`rsa.pkcs1.DecryptionError` exception. It shows where in the\n        code the exception occurred, and thus leaks information about the key.\n        It's only a tiny bit of information, but every bit makes cracking the\n        keys easier.\n\n    >>> crypto = encrypt(b'hello', pub_key)\n    >>> crypto = crypto[0:5] + b'X' + crypto[6:] # change a byte\n    >>> decrypt(crypto, priv_key)\n    Traceback (most recent call last):\n    ...\n    rsa.pkcs1.DecryptionError: Decryption failed\n\n    \"\"\"\n\n    blocksize = common.byte_size(priv_key.n)\n    encrypted = transform.bytes2int(crypto)\n    decrypted = priv_key.blinded_decrypt(encrypted)\n    cleartext = transform.int2bytes(decrypted, blocksize)\n\n    # Detect leading zeroes in the crypto. These are not reflected in the\n    # encrypted value (as leading zeroes do not influence the value of an\n    # integer). This fixes CVE-2020-13757.\n    crypto_len_bad = len(crypto) > blocksize\n\n    # If we can't find the cleartext marker, decryption failed.\n    cleartext_marker_bad = not compare_digest(cleartext[:2], b'\\x00\\x02')\n\n    # Find the 00 separator between the padding and the message\n    try:\n        sep_idx = cleartext.index(b'\\x00', 2)\n    except ValueError:\n        sep_idx = -1\n    sep_idx_bad = sep_idx < 0\n\n    anything_bad = crypto_len_bad | cleartext_marker_bad | sep_idx_bad\n    if anything_bad:\n        raise DecryptionError('Decryption failed')\n\n    return cleartext[sep_idx + 1:]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void pointZZ_pAdd(PointZZ_p * rop, const PointZZ_p * op1, const PointZZ_p * op2, const CurveZZ_p * curve) {\n    // handle identity element cases\n    if(pointZZ_pIsIdentityElement(op1) && pointZZ_pIsIdentityElement(op2)) {\n        return pointZZ_pSetToIdentityElement(rop);\n    } else if(pointZZ_pIsIdentityElement(op1)) {\n        mpz_set(rop->x, op2->x);\n        mpz_set(rop->y, op2->y);\n        return;\n    } else if(pointZZ_pIsIdentityElement(op2)) {\n        mpz_set(rop->x, op1->x);\n        mpz_set(rop->y, op1->y);\n        return;\n    }\n\n    // use doubling algorithm if points are equal\n    if(pointZZ_pEqual(op1, op2)) {\n        pointZZ_pDouble(rop, op1, curve);\n        return;\n    }\n\n    // check if points sum to identity element\n    mpz_t negy;\n    mpz_init(negy);\n    mpz_sub(negy, curve->p, op2->y);\n    if(mpz_cmp(op1->x, op2->x) == 0 && mpz_cmp(op1->y, negy) == 0) {\n        mpz_clear(negy);\n        return pointZZ_pSetToIdentityElement(rop);\n    }\n\n\n    mpz_t xdiff, ydiff, lambda;\n    mpz_inits(xdiff, ydiff, lambda, NULL);\n\n    // calculate lambda\n    mpz_sub(ydiff, op2->y, op1->y);\n    mpz_sub(xdiff, op2->x, op1->x);\n    mpz_invert(xdiff, xdiff, curve->p);  // TODO check status\n    mpz_mul(lambda, ydiff, xdiff);\n    mpz_mod(lambda, lambda, curve->p);\n\n    // calculate resulting x coord\n    mpz_mul(rop->x, lambda, lambda);\n    mpz_sub(rop->x, rop->x, op1->x);\n    mpz_sub(rop->x, rop->x, op2->x);\n    mpz_mod(rop->x, rop->x, curve->p);\n\n    //calculate resulting y coord\n    mpz_sub(rop->y, op1->x, rop->x);\n    mpz_mul(rop->y, lambda, rop->y);\n    mpz_sub(rop->y, rop->y, op1->y);\n    mpz_mod(rop->y, rop->y, curve->p);\n\n    mpz_clears(negy, xdiff, ydiff, lambda, NULL);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "async def indieauth_done(request, datasette):\n    from datasette.utils.asgi import Response\n\n    state = request.args.get(\"state\") or \"\"\n    code = request.args.get(\"code\")\n    try:\n        state_bits = datasette.unsign(state, DATASETTE_INDIEAUTH_STATE)\n    except itsdangerous.BadSignature:\n        return await indieauth_page(\n            request, datasette, error=\"Invalid state\", status=400\n        )\n    authorization_endpoint = state_bits[\"a\"]\n\n    urls = Urls(request, datasette)\n\n    # code_verifier should be in a signed cookie\n    code_verifier = None\n    original_me = None\n    if \"ds_indieauth\" in request.cookies:\n        try:\n            cookie_bits = datasette.unsign(\n                request.cookies[\"ds_indieauth\"], DATASETTE_INDIEAUTH_COOKIE\n            )\n            code_verifier = cookie_bits[\"v\"]\n            original_me = cookie_bits[\"m\"]\n        except (itsdangerous.BadSignature, KeyError):\n            pass\n    if not code_verifier or not original_me:\n        return await indieauth_page(\n            request, datasette, error=\"Invalid ds_indieauth cookie\"\n        )\n\n    data = {\n        \"grant_type\": \"authorization_code\",\n        \"code\": code,\n        \"client_id\": urls.client_id,\n        \"redirect_uri\": urls.redirect_uri,\n        \"code_verifier\": code_verifier,\n    }\n    async with httpx.AsyncClient() as client:\n        response = await client.post(authorization_endpoint, data=data)\n\n    if response.status_code == 200:\n        body = response.text\n        try:\n            info = json.loads(body)\n        except ValueError:\n            info = dict(urllib.parse.parse_qsl(body))\n        if \"me\" not in info:\n            return await indieauth_page(\n                request,\n                datasette,\n                error=\"Invalid authorization_code response from authorization server\",\n            )\n        me = info[\"me\"]\n\n        # Returned me must be on the same domain as original_me\n        if not verify_same_domain(me, original_me):\n            return await indieauth_page(\n                request,\n                datasette,\n                error='\"me\" value returned by authorization server had a domain that did not match the initial URL',\n            )\n\n        actor = {\n            \"me\": me,\n            \"display\": display_url(me),\n        }\n        if \"scope\" in info:\n            actor[\"indieauth_scope\"] = info[\"scope\"]\n\n        if \"profile\" in info and isinstance(info[\"profile\"], dict):\n            actor.update(info[\"profile\"])\n        response = Response.redirect(datasette.urls.instance())\n        response.set_cookie(\n            \"ds_actor\",\n            datasette.sign(\n                {\"a\": actor},\n                \"actor\",\n            ),\n        )\n        return response\n    else:\n        return await indieauth_page(\n            request,\n            datasette,\n            error=\"Invalid response from authorization server\",\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def encrypt_item(table_name, aws_cmk_id, meta_table_name, material_name):\n    \"\"\"Demonstrate use of EncryptedTable to transparently encrypt an item.\"\"\"\n    index_key = {\"partition_attribute\": \"is this\", \"sort_attribute\": 55}\n    plaintext_item = {\n        \"example\": \"data\",\n        \"some numbers\": 99,\n        \"and some binary\": Binary(b\"\\x00\\x01\\x02\"),\n        \"leave me\": \"alone\",  # We want to ignore this attribute\n    }\n    # Collect all of the attributes that will be encrypted (used later).\n    encrypted_attributes = set(plaintext_item.keys())\n    encrypted_attributes.remove(\"leave me\")\n    # Collect all of the attributes that will not be encrypted (used later).\n    unencrypted_attributes = set(index_key.keys())\n    unencrypted_attributes.add(\"leave me\")\n    # Add the index pairs to the item.\n    plaintext_item.update(index_key)\n\n    # Create a normal table resource for the meta store.\n    meta_table = boto3.resource(\"dynamodb\").Table(meta_table_name)  # generated code confuse pylint: disable=no-member\n    # Create a crypto materials provider for the meta store using the specified AWS KMS key.\n    aws_kms_cmp = AwsKmsCryptographicMaterialsProvider(key_id=aws_cmk_id)\n    # Create a meta store using the AWS KMS crypto materials provider.\n    meta_store = MetaStore(table=meta_table, materials_provider=aws_kms_cmp)\n    # Create a most recent provider using the meta store.\n    most_recent_cmp = CachingMostRecentProvider(\n        provider_store=meta_store,\n        material_name=material_name,\n        version_ttl=600.0,  # Check for a new material version every five minutes.\n    )\n    # Create a normal table resource.\n    table = boto3.resource(\"dynamodb\").Table(table_name)  # generated code confuse pylint: disable=no-member\n    # Create attribute actions that tells the encrypted table to encrypt all attributes except one.\n    actions = AttributeActions(\n        default_action=CryptoAction.ENCRYPT_AND_SIGN, attribute_actions={\"leave me\": CryptoAction.DO_NOTHING}\n    )\n    # Use these objects to create an encrypted table resource.\n    encrypted_table = EncryptedTable(table=table, materials_provider=most_recent_cmp, attribute_actions=actions)\n\n    # Put the item to the table, using the encrypted table resource to transparently encrypt it.\n    encrypted_table.put_item(Item=plaintext_item)\n\n    # Get the encrypted item using the standard table resource.\n    encrypted_item = table.get_item(Key=index_key)[\"Item\"]\n\n    # Get the item using the encrypted table resource, transparently decyrpting it.\n    decrypted_item = encrypted_table.get_item(Key=index_key)[\"Item\"]\n\n    # Verify that all of the attributes are different in the encrypted item\n    for name in encrypted_attributes:\n        assert encrypted_item[name] != plaintext_item[name]\n        assert decrypted_item[name] == plaintext_item[name]\n\n    # Verify that all of the attributes that should not be encrypted were not.\n    for name in unencrypted_attributes:\n        assert decrypted_item[name] == encrypted_item[name] == plaintext_item[name]\n\n    # Clean up the item\n    encrypted_table.delete_item(Key=index_key)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_email(value: Union[str]) -> Tuple[str, str]:\n    \"\"\"\n    Email address validation using https://pypi.org/project/email-validator/\n    Notes:\n    * raw ip address (literal) domain parts are not allowed.\n    * \"John Doe <local_part@domain.com>\" style \"pretty\" email addresses are processed\n    * spaces are striped from the beginning and end of addresses but no error is raised\n    \"\"\"\n    if email_validator is None:\n        import_email_validator()\n\n    if len(value) > MAX_EMAIL_LENGTH:\n        raise errors.EmailError()\n\n    m = pretty_email_regex.fullmatch(value)\n    name: Union[str, None] = None\n    if m:\n        name, value = m.groups()\n    email = value.strip()\n    try:\n        parts = email_validator.validate_email(email, check_deliverability=False)\n    except email_validator.EmailNotValidError as e:\n        raise errors.EmailError from e\n\n    if hasattr(parts, 'normalized'):\n        # email-validator >= 2\n        email = parts.normalized\n        assert email is not None\n        name = name or parts.local_part\n        return name, email\n    else:\n        # email-validator >1, <2\n        at_index = email.index('@')\n        local_part = email[:at_index]  # RFC 5321, local part must be case-sensitive.\n        global_part = email[at_index:].lower()\n\n        return name or local_part, local_part + global_part\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def login():\n    form = forms.UserForm()\n    if form.validate_on_submit():\n        db = get_db()\n        user = db.search(\n            (Query().username == form.username.data) & (Query().type == \"user\")\n        )\n\n        if user and check_password_hash(user[0][\"hashed_password\"], form.password.data):\n            user = User.from_db(user[0])\n            login_user(user, remember=True)\n            flash(\"Login successful!\", \"success\")\n\n            next_url = request.args.get(\"next\")\n            if next_url and is_safe_redirect_url(next_url):\n                return redirect(next_url)\n            else:\n                return redirect(\"/\")\n\n        flash(\"Invalid credentials\", \"error\")\n        return redirect(\"/login\")\n    return render_template(\"users/login.html\", form=form, title=\"Login\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_request_kwargs(cls, form, url=None, **kwargs):\n        \"\"\"Extract input data from the form.\"\"\"\n        method = str(form.get(\"method\", \"get\"))\n        action = form.get(\"action\")\n        url = urllib.parse.urljoin(url, action)\n        if url is None:  # This happens when both `action` and `url` are None.\n            raise ValueError('no URL to submit to')\n\n        # read https://www.w3.org/TR/html52/sec-forms.html\n        if method.lower() == \"get\":\n            data = kwargs.pop(\"params\", dict())\n        else:\n            data = kwargs.pop(\"data\", dict())\n        files = kwargs.pop(\"files\", dict())\n\n        # Use a list of 2-tuples to better reflect the behavior of browser QSL.\n        # Requests also retains order when encoding form data in 2-tuple lists.\n        data = [(k, v) for k, v in data.items()]\n\n        multipart = form.get(\"enctype\", \"\") == \"multipart/form-data\"\n\n        # Process form tags in the order that they appear on the page,\n        # skipping those tags that do not have a name-attribute.\n        selector = \",\".join(f\"{tag}[name]\" for tag in\n                            (\"input\", \"button\", \"textarea\", \"select\"))\n        for tag in form.select(selector):\n            name = tag.get(\"name\")  # name-attribute of tag\n\n            # Skip disabled elements, since they should not be submitted.\n            if tag.has_attr('disabled'):\n                continue\n\n            if tag.name == \"input\":\n                if tag.get(\"type\", \"\").lower() in (\"radio\", \"checkbox\"):\n                    if \"checked\" not in tag.attrs:\n                        continue\n                    value = tag.get(\"value\", \"on\")\n                else:\n                    # browsers use empty string for inputs with missing values\n                    value = tag.get(\"value\", \"\")\n\n                # If the enctype is not multipart, the filename is put in\n                # the form as a text input and the file is not sent.\n                if is_multipart_file_upload(form, tag):\n                    if isinstance(value, io.IOBase):\n                        content = value\n                        filename = os.path.basename(getattr(value, \"name\", \"\"))\n                    else:\n                        content = \"\"\n                        filename = os.path.basename(value)\n                    # If content is the empty string, we still pass it\n                    # for consistency with browsers (see\n                    # https://github.com/MechanicalSoup/MechanicalSoup/issues/250).\n                    files[name] = (filename, content)\n                else:\n                    if isinstance(value, io.IOBase):\n                        value = os.path.basename(getattr(value, \"name\", \"\"))\n                    data.append((name, value))\n\n            elif tag.name == \"button\":\n                if tag.get(\"type\", \"\").lower() in (\"button\", \"reset\"):\n                    continue\n                else:\n                    data.append((name, tag.get(\"value\", \"\")))\n\n            elif tag.name == \"textarea\":\n                data.append((name, tag.text))\n\n            elif tag.name == \"select\":\n                # If the value attribute is not specified, the content will\n                # be passed as a value instead.\n                options = tag.select(\"option\")\n                selected_values = [i.get(\"value\", i.text) for i in options\n                                   if \"selected\" in i.attrs]\n                if \"multiple\" in tag.attrs:\n                    for value in selected_values:\n                        data.append((name, value))\n                elif selected_values:\n                    # A standard select element only allows one option to be\n                    # selected, but browsers pick last if somehow multiple.\n                    data.append((name, selected_values[-1]))\n                elif options:\n                    # Selects the first option if none are selected\n                    first_value = options[0].get(\"value\", options[0].text)\n                    data.append((name, first_value))\n\n        if method.lower() == \"get\":\n            kwargs[\"params\"] = data\n        else:\n            kwargs[\"data\"] = data\n\n        # The following part of the function is here to respect the\n        # enctype specified by the form, i.e. force sending multipart\n        # content. Since Requests doesn't have yet a feature to choose\n        # enctype, we have to use tricks to make it behave as we want\n        # This code will be updated if Requests implements it.\n        if multipart and not files:\n            # Requests will switch to \"multipart/form-data\" only if\n            # files pass the `if files:` test, so in this case we use\n            # a modified dict that passes the if test even if empty.\n            class DictThatReturnsTrue(dict):\n                def __bool__(self):\n                    return True\n                __nonzero__ = __bool__\n\n            files = DictThatReturnsTrue()\n\n        return cls._get_request_kwargs(method, url, files=files, **kwargs)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(\n        self, manifest: Optional[dict] = None, manifest_url: Optional[str] = None\n    ):\n        import yaml\n\n        if manifest and manifest_url:\n            raise ValueError(\"You cannot provide both a manifest and a manifest_url\")\n        elif manifest:\n            pass\n        elif manifest_url:\n            response = requests.get(manifest_url).text\n            manifest = yaml.safe_load(response)\n        else:\n            raise ValueError(\"You must provide either a manifest or a manifest_url\")\n\n        if manifest[\"api\"][\"type\"] != \"openapi\":\n            raise ValueError(\n                f'API type must be \"openapi\", not \"{manifest[\"api\"][\"type\"]}\"'\n            )\n\n        if manifest[\"auth\"][\"type\"] != \"none\":\n            raise ValueError(\"Authentication cannot be supported for ChatGPT plugins\")\n\n        self.openapi = OpenAPIToolSpec(url=manifest[\"api\"][\"url\"])\n\n        self.plugin_description = f\"\"\"\n            'human_description': {manifest['description_for_human']}\n            'model_description': {manifest['description_for_model']}\n        \"\"\"\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_argparser():\n    \"\"\"Get the argparse parser.\"\"\"\n    parser = argparse.ArgumentParser(prog='qutebrowser',\n                                     description=qutebrowser.__description__)\n    parser.add_argument('-B', '--basedir', help=\"Base directory for all \"\n                        \"storage.\")\n    parser.add_argument('-C', '--config-py', help=\"Path to config.py.\",\n                        metavar='CONFIG')\n    parser.add_argument('-V', '--version', help=\"Show version and quit.\",\n                        action='store_true')\n    parser.add_argument('-s', '--set', help=\"Set a temporary setting for \"\n                        \"this session.\", nargs=2, action='append',\n                        dest='temp_settings', default=[],\n                        metavar=('OPTION', 'VALUE'))\n    parser.add_argument('-r', '--restore', help=\"Restore a named session.\",\n                        dest='session')\n    parser.add_argument('-R', '--override-restore', help=\"Don't restore a \"\n                        \"session even if one would be restored.\",\n                        action='store_true')\n    parser.add_argument('--target', choices=['auto', 'tab', 'tab-bg',\n                                             'tab-silent', 'tab-bg-silent',\n                                             'window', 'private-window'],\n                        help=\"How URLs should be opened if there is already a \"\n                             \"qutebrowser instance running.\")\n    parser.add_argument('--backend', choices=['webkit', 'webengine'],\n                        help=\"Which backend to use.\")\n    parser.add_argument('--desktop-file-name',\n                        default=\"org.qutebrowser.qutebrowser\",\n                        help=\"Set the base name of the desktop entry for this \"\n                        \"application. Used to set the app_id under Wayland. See \"\n                        \"https://doc.qt.io/qt-5/qguiapplication.html#desktopFileName-prop\")\n    parser.add_argument('--untrusted-args',\n                        action='store_true',\n                        help=\"Mark all following arguments as untrusted, which \"\n                        \"enforces that they are URLs/search terms (and not flags or \"\n                        \"commands)\")\n\n    parser.add_argument('--json-args', help=argparse.SUPPRESS)\n    parser.add_argument('--temp-basedir-restarted',\n                        help=argparse.SUPPRESS,\n                        action='store_true')\n\n    # WORKAROUND to be able to restart from older qutebrowser versions into this one.\n    # Should be removed at some point.\n    parser.add_argument('--enable-webengine-inspector',\n                        help=argparse.SUPPRESS,\n                        action='store_true')\n\n    debug = parser.add_argument_group('debug arguments')\n    debug.add_argument('-l', '--loglevel', dest='loglevel',\n                       help=\"Override the configured console loglevel\",\n                       choices=['critical', 'error', 'warning', 'info',\n                                'debug', 'vdebug'])\n    debug.add_argument('--logfilter', type=logfilter_error,\n                       help=\"Comma-separated list of things to be logged \"\n                       \"to the debug log on stdout.\")\n    debug.add_argument('--loglines',\n                       help=\"How many lines of the debug log to keep in RAM \"\n                       \"(-1: unlimited).\",\n                       default=2000, type=int)\n    debug.add_argument('-d', '--debug', help=\"Turn on debugging options.\",\n                       action='store_true')\n    debug.add_argument('--json-logging', action='store_true', help=\"Output log\"\n                       \" lines in JSON format (one object per line).\")\n    debug.add_argument('--nocolor', help=\"Turn off colored logging.\",\n                       action='store_false', dest='color')\n    debug.add_argument('--force-color', help=\"Force colored logging\",\n                       action='store_true')\n    debug.add_argument('--nowindow', action='store_true', help=\"Don't show \"\n                       \"the main window.\")\n    debug.add_argument('-T', '--temp-basedir', action='store_true', help=\"Use \"\n                       \"a temporary basedir.\")\n    debug.add_argument('--no-err-windows', action='store_true', help=\"Don't \"\n                       \"show any error windows (used for tests/smoke.py).\")\n    debug.add_argument('--qt-arg', help=\"Pass an argument with a value to Qt. \"\n                       \"For example, you can do \"\n                       \"`--qt-arg geometry 650x555+200+300` to set the window \"\n                       \"geometry.\", nargs=2, metavar=('NAME', 'VALUE'),\n                       action='append')\n    debug.add_argument('--qt-flag', help=\"Pass an argument to Qt as flag.\",\n                       nargs=1, action='append')\n    debug.add_argument('-D', '--debug-flag', type=debug_flag_error,\n                       default=[], help=\"Pass name of debugging feature to be\"\n                       \" turned on.\", action='append', dest='debug_flags')\n    parser.add_argument('command', nargs='*', help=\"Commands to execute on \"\n                        \"startup.\", metavar=':command')\n    # URLs will actually be in command\n    parser.add_argument('url', nargs='*', help=\"URLs to open on startup \"\n                        \"(empty as a window separator).\")\n    return parser\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def check_xsrf_cookie(self):\n        \"\"\"Verifies that the ``_xsrf`` cookie matches the ``_xsrf`` argument.\n\n        To prevent cross-site request forgery, we set an ``_xsrf``\n        cookie and include the same value as a non-cookie\n        field with all ``POST`` requests. If the two do not match, we\n        reject the form submission as a potential forgery.\n\n        The ``_xsrf`` value may be set as either a form field named ``_xsrf``\n        or in a custom HTTP header named ``X-XSRFToken`` or ``X-CSRFToken``\n        (the latter is accepted for compatibility with Django).\n\n        See http://en.wikipedia.org/wiki/Cross-site_request_forgery\n\n        Prior to release 1.1.1, this check was ignored if the HTTP header\n        ``X-Requested-With: XMLHTTPRequest`` was present.  This exception\n        has been shown to be insecure and has been removed.  For more\n        information please see\n        http://www.djangoproject.com/weblog/2011/feb/08/security/\n        http://weblog.rubyonrails.org/2011/2/8/csrf-protection-bypass-in-ruby-on-rails\n        \"\"\"\n        token = (self.get_argument(\"_xsrf\", None) or\n                 self.request.headers.get(\"X-Xsrftoken\") or\n                 self.request.headers.get(\"X-Csrftoken\"))\n        if not token:\n            raise HTTPError(403, \"'_xsrf' argument missing from POST\")\n        _, token, _ = self._decode_xsrf_token(token)\n        _, expected_token, _ = self._get_raw_xsrf_token()\n        if not _time_independent_equals(utf8(token), utf8(expected_token)):\n            raise HTTPError(403, \"XSRF cookie does not match POST argument\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    OpInputList indices_list_in;\n    OP_REQUIRES_OK(context, context->input_list(\"indices\", &indices_list_in));\n    OpInputList values_list_in;\n    OP_REQUIRES_OK(context, context->input_list(\"values\", &values_list_in));\n    OpInputList shapes_list_in;\n    OP_REQUIRES_OK(context, context->input_list(\"shapes\", &shapes_list_in));\n    OpInputList dense_list_in;\n    OP_REQUIRES_OK(context,\n                   context->input_list(\"dense_inputs\", &dense_list_in));\n\n    // Set internal_type to invalid_type so that the check will be ignored.\n    DataType internal_type = DT_INVALID;\n    OP_REQUIRES_OK(\n        context, ValidateInput(indices_list_in, values_list_in, shapes_list_in,\n                               dense_list_in, internal_type));\n\n    const Tensor* num_buckets_t;\n    OP_REQUIRES_OK(context, context->input(\"num_buckets\", &num_buckets_t));\n    const int64 num_buckets = num_buckets_t->scalar<int64>()();\n\n    const Tensor* strong_hash_t;\n    OP_REQUIRES_OK(context, context->input(\"strong_hash\", &strong_hash_t));\n    const bool strong_hash = strong_hash_t->scalar<bool>()();\n\n    const Tensor* salt_t;\n    OP_REQUIRES_OK(context, context->input(\"salt\", &salt_t));\n    const auto salt = salt_t->flat<int64>();\n    std::vector<int64> key_{salt(0), salt(1)};\n\n    std::vector<std::unique_ptr<ColumnInterface<int64>>> columns =\n        GenerateKeyedColumnsFromInput<int64>(indices_list_in, values_list_in,\n                                             shapes_list_in, dense_list_in,\n                                             key_);\n    Tensor* indices_out;\n    Tensor* values_out;\n    Tensor* shape_out;\n    const int64 batch_size = CalculateBatchSize(shapes_list_in, dense_list_in);\n    std::vector<int64> output_start_indices(batch_size);\n    OP_REQUIRES_OK(\n        context,\n        CreateOutputTensors(columns, batch_size, context, &indices_out,\n                            &values_out, &shape_out, &output_start_indices));\n    const tstring unused_sep;\n    HashCrosserV2 crosser(columns, num_buckets, 0, unused_sep);\n    OutputUpdater<int64> updater(output_start_indices, indices_out, values_out);\n    auto do_work = [&columns, crosser, updater, strong_hash](int64 begin,\n                                                             int64 end) {\n      for (int b = begin; b < end; b++) {\n        ProductIterator<int64> product_iterator(columns, b);\n        int64 cross_count = 0;\n        while (product_iterator.HasNext()) {\n          const auto permutation = product_iterator.Next();\n          updater.Update(b, cross_count,\n                         crosser.Generate(b, permutation, strong_hash));\n          cross_count++;\n        }\n      }\n    };\n\n    auto* worker_threads = context->device()->tensorflow_cpu_worker_threads();\n    // TODO(zakaria): optimize kCostPerUnit\n    const int kCostPerUnit = 5000 * indices_list_in.size();\n    Shard(worker_threads->num_threads, worker_threads->workers, batch_size,\n          kCostPerUnit, do_work);\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def archive(request, bookmark_id: int):\n    try:\n        bookmark = Bookmark.objects.get(pk=bookmark_id, owner=request.user)\n    except Bookmark.DoesNotExist:\n        raise Http404('Bookmark does not exist')\n\n    archive_bookmark(bookmark)\n    return_url = get_safe_return_url(request.GET.get('return_url'), reverse('bookmarks:index'))\n    return HttpResponseRedirect(return_url)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _expand_user_properties(self, template):\n        # Make sure username and servername match the restrictions for DNS labels\n        # Note: '-' is not in safe_chars, as it is being used as escape character\n        safe_chars = set(string.ascii_lowercase + string.digits)\n\n        raw_servername = self.name or ''\n        safe_servername = escapism.escape(raw_servername, safe=safe_chars, escape_char='-').lower()\n\n        legacy_escaped_username = ''.join([s if s in safe_chars else '-' for s in self.user.name.lower()])\n        safe_username = escapism.escape(self.user.name, safe=safe_chars, escape_char='-').lower()\n        rendered = template.format(\n            userid=self.user.id,\n            username=safe_username,\n            unescaped_username=self.user.name,\n            legacy_escape_username=legacy_escaped_username,\n            servername=safe_servername,\n            unescaped_servername=raw_servername,\n        )\n        # strip trailing - delimiter in case of empty servername.\n        # k8s object names cannot have trailing -\n        return rendered.rstrip(\"-\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteDepthToSpaceParams*>(node->builtin_data);\n\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n\n  auto data_type = output->type;\n  TF_LITE_ENSURE(context,\n                 data_type == kTfLiteFloat32 || data_type == kTfLiteInt8);\n  TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n\n  const int block_size = params->block_size;\n  TF_LITE_ENSURE(context, block_size > 0);\n  const int input_height = input->dims->data[kHeightRank];\n  const int input_width = input->dims->data[kWidthRank];\n  const int input_channels = input->dims->data[kDepthRank];\n  int output_height = input_height * block_size;\n  int output_width = input_width * block_size;\n  int output_channels = input_channels / block_size / block_size;\n\n  TF_LITE_ENSURE_EQ(context, input_height, output_height / block_size);\n  TF_LITE_ENSURE_EQ(context, input_width, output_width / block_size);\n  TF_LITE_ENSURE_EQ(context, input_channels,\n                    output_channels * block_size * block_size);\n\n  // We must update the output tensor dimensions.\n  // The dims storage is expected to be the same area in memory\n  // for both TfLiteTensor and TfLiteEvalTensor.  This is important\n  // because TfLiteTensor in the MicroInterpreter is a temporary\n  // allocation.  For the KernelRunner interpreter, TfLiteEvalTensor\n  // is a temporary allocation.  We must therefore relocate the dims\n  // from the FlatBuffer to the persistant storage arena.\n  TfLiteEvalTensor* output_eval =\n      tflite::micro::GetEvalOutput(context, node, kOutputTensor);\n  TF_LITE_ENSURE_OK(context, tflite::micro::CreateWritableTensorDimsWithCopy(\n                                 context, output, output_eval));\n  output->dims->data[kBatchRank] = input->dims->data[kBatchRank];\n  output->dims->data[kHeightRank] = output_height;\n  output->dims->data[kWidthRank] = output_width;\n  output->dims->data[kDepthRank] = output_channels;\n\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void TensorSliceReader::LoadShard(int shard) const {\n  CHECK_LT(shard, sss_.size());\n  if (sss_[shard] || !status_.ok()) {\n    return;  // Already loaded, or invalid.\n  }\n  string value;\n  SavedTensorSlices sts;\n  const string fname = fnames_[shard];\n  VLOG(1) << \"Reading meta data from file \" << fname << \"...\";\n  Table* table;\n  Status s = open_function_(fname, &table);\n  if (!s.ok()) {\n    status_ = errors::DataLoss(\"Unable to open table file \", fname, \": \",\n                               s.ToString());\n    return;\n  }\n  sss_[shard].reset(table);\n  if (!(table->Get(kSavedTensorSlicesKey, &value) &&\n        ParseProtoUnlimited(&sts, value))) {\n    status_ = errors::Internal(\n        \"Failed to find the saved tensor slices at the beginning of the \"\n        \"checkpoint file: \",\n        fname);\n    return;\n  }\n  status_ = CheckVersions(sts.meta().versions(), TF_CHECKPOINT_VERSION,\n                          TF_CHECKPOINT_VERSION_MIN_PRODUCER, \"Checkpoint\",\n                          \"checkpoint\");\n  if (!status_.ok()) return;\n  for (const SavedSliceMeta& ssm : sts.meta().tensor()) {\n    TensorShape ssm_shape;\n    status_ = TensorShape::BuildTensorShapeBase(ssm.shape(), &ssm_shape);\n    if (!status_.ok()) return;\n    for (const TensorSliceProto& tsp : ssm.slice()) {\n      TensorSlice ss_slice(tsp);\n      status_ = RegisterTensorSlice(ssm.name(), ssm_shape, ssm.type(), fname,\n                                    ss_slice, &tensors_);\n      if (!status_.ok()) return;\n    }\n  }\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, **kwargs):\n        # The default maximum length of an email is 320 characters per RFC 3696\n        # section 3.\n        kwargs.setdefault(\"max_length\", 320)\n        super().__init__(strip=True, **kwargs)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_base_command(self, cwd, use_dash_c):\n        if use_dash_c:\n            return ['git', '-c', 'core.fsmonitor=', '-C', cwd]\n\n        while cwd and cwd != os.sep:\n            gitdir = os.path.join(cwd, '.git')\n\n            if os.path.isdir(gitdir):\n                return ['git', '-c', 'core.fsmonitor=', '--git-dir=%s' % gitdir, '--work-tree=%s' % cwd]\n\n            cwd = os.path.dirname(cwd)\n\n        return None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def authenticate(self, context, auth=None):\n        \"\"\"Authenticate credentials and return a token.\n\n        Accept auth as a dict that looks like::\n\n            {\n                \"auth\":{\n                    \"passwordCredentials\":{\n                        \"username\":\"test_user\",\n                        \"password\":\"mypass\"\n                    },\n                    \"tenantName\":\"customer-x\"\n                }\n            }\n\n        In this case, tenant is optional, if not provided the token will be\n        considered \"unscoped\" and can later be used to get a scoped token.\n\n        Alternatively, this call accepts auth with only a token and tenant\n        that will return a token that is scoped to that tenant.\n        \"\"\"\n\n        token_id = uuid.uuid4().hex\n        if 'passwordCredentials' in auth:\n            user_id = auth['passwordCredentials'].get('userId', None)\n            username = auth['passwordCredentials'].get('username', '')\n            password = auth['passwordCredentials'].get('password', '')\n            tenant_name = auth.get('tenantName', None)\n\n            if username:\n                try:\n                    user_ref = self.identity_api.get_user_by_name(\n                        context=context, user_name=username)\n                    user_id = user_ref['id']\n                except exception.UserNotFound:\n                    raise exception.Unauthorized()\n\n            # more compat\n            tenant_id = auth.get('tenantId', None)\n            if tenant_name:\n                try:\n                    tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                    tenant_id = tenant_ref['id']\n                except exception.TenantNotFound:\n                    raise exception.Unauthorized()\n\n            try:\n                auth_info = self.identity_api.authenticate(context=context,\n                                                           user_id=user_id,\n                                                           password=password,\n                                                           tenant_id=tenant_id)\n                (user_ref, tenant_ref, metadata_ref) = auth_info\n\n                # If the user is disabled don't allow them to authenticate\n                if not user_ref.get('enabled', True):\n                    LOG.warning('User %s is disabled' % user_id)\n                    raise exception.Unauthorized()\n\n                # If the tenant is disabled don't allow them to authenticate\n                if tenant_ref and not tenant_ref.get('enabled', True):\n                    LOG.warning('Tenant %s is disabled' % tenant_id)\n                    raise exception.Unauthorized()\n            except AssertionError as e:\n                raise exception.Unauthorized(e.message)\n\n            token_ref = self.token_api.create_token(\n                context,\n                token_id,\n                dict(id=token_id,\n                     user=user_ref,\n                     tenant=tenant_ref,\n                     metadata=metadata_ref))\n            if tenant_ref:\n                catalog_ref = self.catalog_api.get_catalog(\n                    context=context,\n                    user_id=user_ref['id'],\n                    tenant_id=tenant_ref['id'],\n                    metadata=metadata_ref)\n            else:\n                catalog_ref = {}\n\n        elif 'token' in auth:\n            token = auth['token'].get('id', None)\n\n            tenant_name = auth.get('tenantName')\n\n            # more compat\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                    context=context, tenant_name=tenant_name)\n                tenant_id = tenant_ref['id']\n            else:\n                tenant_id = auth.get('tenantId', None)\n\n            try:\n                old_token_ref = self.token_api.get_token(context=context,\n                                                         token_id=token)\n            except exception.NotFound:\n                raise exception.Unauthorized()\n\n            user_ref = old_token_ref['user']\n\n            # If the user is disabled don't allow them to authenticate\n            current_user_ref = self.identity_api.get_user(\n                context=context,\n                user_id=user_ref['id'])\n            if not current_user_ref.get('enabled', True):\n                LOG.warning('User %s is disabled' % user_ref['id'])\n                raise exception.Unauthorized()\n\n            tenants = self.identity_api.get_tenants_for_user(context,\n                                                             user_ref['id'])\n            if tenant_id and tenant_id not in tenants:\n                raise exception.Unauthorized()\n\n            try:\n                tenant_ref = self.identity_api.get_tenant(\n                    context=context,\n                    tenant_id=tenant_id)\n                metadata_ref = self.identity_api.get_metadata(\n                    context=context,\n                    user_id=user_ref['id'],\n                    tenant_id=tenant_ref['id'])\n                catalog_ref = self.catalog_api.get_catalog(\n                    context=context,\n                    user_id=user_ref['id'],\n                    tenant_id=tenant_ref['id'],\n                    metadata=metadata_ref)\n            except exception.TenantNotFound:\n                tenant_ref = None\n                metadata_ref = {}\n                catalog_ref = {}\n            except exception.MetadataNotFound:\n                metadata_ref = {}\n                catalog_ref = {}\n\n            # If the tenant is disabled don't allow them to authenticate\n            if tenant_ref and not tenant_ref.get('enabled', True):\n                LOG.warning('Tenant %s is disabled' % tenant_id)\n                raise exception.Unauthorized()\n\n            token_ref = self.token_api.create_token(\n                context, token_id, dict(id=token_id,\n                                        user=user_ref,\n                                        tenant=tenant_ref,\n                                        metadata=metadata_ref,\n                                        expires=old_token_ref['expires']))\n\n        # TODO(termie): optimize this call at some point and put it into the\n        #               the return for metadata\n        # fill out the roles in the metadata\n        roles_ref = []\n        for role_id in metadata_ref.get('roles', []):\n            roles_ref.append(self.identity_api.get_role(context, role_id))\n        logging.debug('TOKEN_REF %s', token_ref)\n        return self._format_authenticate(token_ref, roles_ref, catalog_ref)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, redirect_to):\n        super(HttpResponseRedirectBase, self).__init__()\n        parsed = urlparse(redirect_to)\n        if parsed.scheme and parsed.scheme not in self.allowed_schemes:\n            raise SuspiciousOperation(\"Unsafe redirect to URL with scheme '%s'\" % parsed.scheme)\n        self['Location'] = iri_to_uri(redirect_to)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _process_wrapping_key_provider_configs(  # noqa: C901\n    raw_keys,  # type: List[RAW_CONFIG]\n    action,  # type: str\n):\n    # type: (...) -> List[MASTER_KEY_PROVIDER_CONFIG]\n    \"\"\"Applied additional processing to prepare the wrapping key provider configuration.\n\n    :param list raw_keys: List of wrapping key provider configurations\n    :param str action: Action defined in CLI input\n    :returns: List of processed wrapping key provider configurations\n    :rtype: list of dicts\n    :raises ParameterParseError: if exactly one provider value is not provided\n    :raises ParameterParseError: if no key values are provided\n    \"\"\"\n    if raw_keys is None:\n        raise ParameterParseError(\"No wrapping key provider configuration found\")\n\n    processed_configs = []  # type: List[MASTER_KEY_PROVIDER_CONFIG]\n    for raw_config in raw_keys:\n        parsed_args = {}  # type: Dict[str, Union[str, List[str], Dict[str, Union[str, List[str]]]]]\n        parsed_args.update(_parse_kwargs(raw_config))\n\n        provider = parsed_args.get(\"provider\", [DEFAULT_MASTER_KEY_PROVIDER])  # If no provider is defined, use aws-kms\n        if len(provider) != 1:\n            raise ParameterParseError(\n                'You must provide exactly one \"provider\" for each wrapping key provider configuration. '\n                \"{} provided\".format(len(provider))\n            )\n        parsed_args[\"provider\"] = provider[0]  # type: ignore\n\n        provider_is_kms = parsed_args[\"provider\"] in (\"aws-kms\", DEFAULT_MASTER_KEY_PROVIDER)\n\n        if not provider_is_kms:\n            processed_configs.append(_process_non_kms_key_config(parsed_args))\n        else:\n            processed_configs.append(_process_kms_key_config(parsed_args, action))  # type: ignore\n    return processed_configs\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_annotation(self, annotation, alias, is_summary=False, select=True):\n        \"\"\"Add a single annotation expression to the Query.\"\"\"\n        self.check_alias(alias)\n        annotation = annotation.resolve_expression(\n            self, allow_joins=True, reuse=None, summarize=is_summary\n        )\n        if select:\n            self.append_annotation_mask([alias])\n        else:\n            self.set_annotation_mask(set(self.annotation_select).difference({alias}))\n        self.annotations[alias] = annotation\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_field_options(self, field):\n        options = {}\n        options['label'] = field.label\n        if getattr(settings, 'WAGTAILFORMS_HELP_TEXT_ALLOW_HTML', False):\n            options['help_text'] = field.help_text\n        else:\n            options['help_text'] = conditional_escape(field.help_text)\n        options['required'] = field.required\n        options['initial'] = field.default_value\n        return options\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def interceptRequest(self, info):\n        \"\"\"Handle the given request.\n\n        Reimplementing this virtual function and setting the interceptor on a\n        profile makes it possible to intercept URL requests. This function is\n        executed on the IO thread, and therefore running long tasks here will\n        block networking.\n\n        info contains the information about the URL request and will track\n        internally whether its members have been altered.\n\n        Args:\n            info: QWebEngineUrlRequestInfo &info\n        \"\"\"\n        if 'log-requests' in self._args.debug_flags:\n            resource_type = debug.qenum_key(QWebEngineUrlRequestInfo,\n                                            info.resourceType())\n            navigation_type = debug.qenum_key(QWebEngineUrlRequestInfo,\n                                              info.navigationType())\n            log.webview.debug(\"{} {}, first-party {}, resource {}, \"\n                              \"navigation {}\".format(\n                                  bytes(info.requestMethod()).decode('ascii'),\n                                  info.requestUrl().toDisplayString(),\n                                  info.firstPartyUrl().toDisplayString(),\n                                  resource_type, navigation_type))\n\n        url = info.requestUrl()\n        firstparty = info.firstPartyUrl()\n\n        if ((url.scheme(), url.host(), url.path()) ==\n                ('qute', 'settings', '/set')):\n            if (firstparty != QUrl('qute://settings/') or\n                    info.resourceType() !=\n                    QWebEngineUrlRequestInfo.ResourceTypeXhr):\n                log.webview.warning(\"Blocking malicious request from {} to {}\"\n                                    .format(firstparty.toDisplayString(),\n                                            url.toDisplayString()))\n                info.block(True)\n                return\n\n        # FIXME:qtwebengine only block ads for NavigationTypeOther?\n        if self._host_blocker.is_blocked(url):\n            log.webview.info(\"Request to {} blocked by host blocker.\".format(\n                url.host()))\n            info.block(True)\n\n        for header, value in shared.custom_headers(url=url):\n            info.setHttpHeader(header, value)\n\n        user_agent = config.instance.get('content.headers.user_agent', url=url)\n        if user_agent is not None:\n            info.setHttpHeader(b'User-Agent', user_agent.encode('ascii'))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def login():\n    from flask_login import current_user\n\n    default_redirect_url = request.script_root + url_for(\"index\")\n    redirect_url = request.args.get(\"redirect\", default_redirect_url)\n    allowed_paths = [url_for(\"index\"), url_for(\"recovery\")]\n\n    if not validate_local_redirect(redirect_url, allowed_paths):\n        _logger.warning(\n            f\"Got an invalid redirect URL with the login attempt, misconfiguration or attack attempt: {redirect_url}\"\n        )\n        redirect_url = default_redirect_url\n\n    permissions = sorted(\n        filter(\n            lambda x: x is not None and isinstance(x, OctoPrintPermission),\n            map(\n                lambda x: getattr(Permissions, x.strip()),\n                request.args.get(\"permissions\", \"\").split(\",\"),\n            ),\n        ),\n        key=lambda x: x.get_name(),\n    )\n    if not permissions:\n        permissions = [Permissions.STATUS, Permissions.SETTINGS_READ]\n\n    user_id = request.args.get(\"user_id\", \"\")\n\n    if (not user_id or current_user.get_id() == user_id) and has_permissions(\n        *permissions\n    ):\n        return redirect(redirect_url)\n\n    render_kwargs = {\n        \"theming\": [],\n        \"redirect_url\": redirect_url,\n        \"permission_names\": map(lambda x: x.get_name(), permissions),\n        \"user_id\": user_id,\n        \"logged_in\": not current_user.is_anonymous,\n    }\n\n    try:\n        additional_assets = _add_additional_assets(\"octoprint.theming.login\")\n\n        # backwards compatibility to forcelogin & loginui plugins which were replaced by this built-in dialog\n        additional_assets += _add_additional_assets(\"octoprint.plugin.forcelogin.theming\")\n        additional_assets += _add_additional_assets(\"octoprint.plugin.loginui.theming\")\n\n        render_kwargs.update({\"theming\": additional_assets})\n    except Exception:\n        _logger.exception(\"Error processing theming CSS, ignoring\")\n\n    return render_template(\"login.jinja2\", **render_kwargs)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _dump(self, file=None, format=None):\n        import tempfile, os\n        if not file:\n            f, file = tempfile.mkstemp(format or '')\n            os.close(f)\n            \n        self.load()\n        if not format or format == \"PPM\":\n            self.im.save_ppm(file)\n        else:\n            if file.endswith(format):\n                file = file + \".\" + format\n            self.save(file, format)\n        return file\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _parse_returned_data(self, res):\n        try:\n            filtered_output, warnings = _filter_non_json_lines(res.get('stdout', u''))\n            for w in warnings:\n                display.warning(w)\n            data = json.loads(filtered_output)\n            data['_ansible_parsed'] = True\n            if 'ansible_facts' in data and isinstance(data['ansible_facts'], dict):\n                remove_keys = set()\n                fact_keys = set(data['ansible_facts'].keys())\n                # first we add all of our magic variable names to the set of\n                # keys we want to remove from facts\n                for magic_var in MAGIC_VARIABLE_MAPPING:\n                    remove_keys.update(fact_keys.intersection(MAGIC_VARIABLE_MAPPING[magic_var]))\n                # next we remove any connection plugin specific vars\n                for conn_path in self._shared_loader_obj.connection_loader.all(path_only=True):\n                    try:\n                        conn_name = os.path.splitext(os.path.basename(conn_path))[0]\n                        re_key = re.compile('^ansible_%s_' % conn_name)\n                        for fact_key in fact_keys:\n                            if re_key.match(fact_key):\n                                remove_keys.add(fact_key)\n                    except AttributeError:\n                        pass\n                # finally, we search for interpreter keys to remove\n                re_interp = re.compile('^ansible_.*_interpreter$')\n                for fact_key in fact_keys:\n                    if re_interp.match(fact_key):\n                        remove_keys.add(fact_key)\n                # then we remove them (except for ssh host keys)\n                for r_key in remove_keys:\n                    if not r_key.startswith('ansible_ssh_host_key_'):\n                        del data['ansible_facts'][r_key]\n        except ValueError:\n            # not valid json, lets try to capture error\n            data = dict(failed=True, _ansible_parsed=False)\n            data['msg'] = \"MODULE FAILURE\"\n            data['module_stdout'] = res.get('stdout', u'')\n            if 'stderr' in res:\n                data['module_stderr'] = res['stderr']\n                if res['stderr'].startswith(u'Traceback'):\n                    data['exception'] = res['stderr']\n        return data\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_context_data(self, **kwargs):\n        template = self.kwargs['template']\n        templates = []\n        try:\n            default_engine = Engine.get_default()\n        except ImproperlyConfigured:\n            # Non-trivial TEMPLATES settings aren't supported (#24125).\n            pass\n        else:\n            # This doesn't account for template loaders (#24128).\n            for index, directory in enumerate(default_engine.dirs):\n                template_file = Path(safe_join(directory, template))\n                if template_file.exists():\n                    template_contents = template_file.read_text()\n                else:\n                    template_contents = ''\n                templates.append({\n                    'file': template_file,\n                    'exists': template_file.exists(),\n                    'contents': template_contents,\n                    'order': index,\n                })\n        return super().get_context_data(**{\n            **kwargs,\n            'name': template,\n            'templates': templates,\n        })\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def _resolve_orders(orders):\n            requester = get_user_or_app_from_context(info.context)\n            if not requester.has_perm(OrderPermissions.MANAGE_ORDERS):\n                # allow fetch requestor orders (except drafts)\n                if root == info.context.user:\n                    orders = list(\n                        filter(lambda order: order.status != OrderStatus.DRAFT, orders)\n                    )\n                else:\n                    raise PermissionDenied()\n\n            return create_connection_slice(\n                orders, info, kwargs, OrderCountableConnection\n            )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_min.NumElements() == depth,\n        errors::InvalidArgument(\"input_min must have the same number of \"\n                                \"elements as input_max, got \",\n                                input_min.NumElements(), \" and \", depth));\n    OP_REQUIRES(ctx, input.NumElements() > 0,\n                errors::InvalidArgument(\"input must not be empty\"));\n    OP_REQUIRES(ctx, input.dims() == 4,\n                errors::InvalidArgument(\"input must be in NHWC format\"));\n    OP_REQUIRES(\n        ctx, input.dim_size(3) == depth,\n        errors::InvalidArgument(\n            \"input must have same number of channels as length of input_min: \",\n            input.dim_size(3), \" vs \", depth));\n\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n\n    // TODO: verify performance of not transposing and finding the min max\n    // directly from input_matrix vs the one presented below of transposing and\n    // using the transposed matrix as the transposing operation in itself might\n    // be more costly.\n    // Note that this operation is a calibration step for quantization and will\n    // cease to exist in the final inference graph(will exist as a const node).\n    auto transposed_input = input_matrix.shuffle(shuffling);\n\n    // Find the ranges of each channel in parallel.\n    float out_min_max = std::numeric_limits<float>::min();\n\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  // ENABLE_ONEDNN_OPENMP\n    // TODO: Add eigen parallel_for\n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] =\n          scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\n      if (min_per_channel < 0) is_non_negative = false;\n\n      // Thread-local out_min_max.\n      out_min_max = std::max(out_min_max, ranges[i]);\n    }\n\n    // All local out_min_max gets max-reduced into one global out_min_max at\n    // the end of the loop by specifying reduction(max:out_min_max) along with\n    // omp parallel for.\n\n    // Fixing max to clip_value_max_ (example 6.0 to support relu6)\n    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n\n    Tensor* output_min = nullptr;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\n    output_max->flat<float>()(0) = out_min_max;\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def docker_monitor(\n        self,\n        cidfile: str,\n        tmpdir_prefix: str,\n        cleanup_cidfile: bool,\n        docker_exe: str,\n        process: \"subprocess.Popen[str]\",\n    ) -> None:\n        \"\"\"Record memory usage of the running Docker container.\"\"\"\n        # Todo: consider switching to `docker create` / `docker start`\n        # instead of `docker run` as `docker create` outputs the container ID\n        # to stdout, but the container is frozen, thus allowing us to start the\n        # monitoring process without dealing with the cidfile or too-fast\n        # container execution\n        cid: Optional[str] = None\n        while cid is None:\n            time.sleep(1)\n            # This is needed to avoid a race condition where the job\n            # was so fast that it already finished when it arrives here\n            if process.returncode is None:\n                process.poll()\n            if process.returncode is not None:\n                if cleanup_cidfile:\n                    try:\n                        os.remove(cidfile)\n                    except OSError as exc:\n                        _logger.warning(\"Ignored error cleaning up %s cidfile: %s\", docker_exe, exc)\n                    return\n            try:\n                with open(cidfile) as cidhandle:\n                    cid = cidhandle.readline().strip()\n            except OSError:\n                cid = None\n        max_mem = psutil.virtual_memory().total\n        tmp_dir, tmp_prefix = os.path.split(tmpdir_prefix)\n        stats_file = tempfile.NamedTemporaryFile(prefix=tmp_prefix, dir=tmp_dir)\n        stats_file_name = stats_file.name\n        try:\n            with open(stats_file_name, mode=\"w\") as stats_file_handle:\n                cmds = [docker_exe, \"stats\"]\n                if \"podman\" not in docker_exe:\n                    cmds.append(\"--no-trunc\")\n                cmds.extend([\"--format\", \"{{.MemPerc}}\", cid])\n                stats_proc = subprocess.Popen(  # nosec\n                    cmds,\n                    stdout=stats_file_handle,\n                    stderr=subprocess.DEVNULL,\n                )\n                process.wait()\n                stats_proc.kill()\n        except OSError as exc:\n            _logger.warning(\"Ignored error with %s stats: %s\", docker_exe, exc)\n            return\n        max_mem_percent: float = 0.0\n        mem_percent: float = 0.0\n        with open(stats_file_name) as stats:\n            while True:\n                line = stats.readline()\n                if not line:\n                    break\n                try:\n                    mem_percent = float(re.sub(CONTROL_CODE_RE, \"\", line).replace(\"%\", \"\"))\n                    if mem_percent > max_mem_percent:\n                        max_mem_percent = mem_percent\n                except ValueError as exc:\n                    _logger.debug(\"%s stats parsing error in line %s: %s\", docker_exe, line, exc)\n        _logger.info(\n            \"[job %s] Max memory used: %iMiB\",\n            self.name,\n            int((max_mem_percent / 100 * max_mem) / (2**20)),\n        )\n        if cleanup_cidfile:\n            os.remove(cidfile)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_in_or_equal(path_1: str | Path, path_2: str | Path):\n    \"\"\"\n    True if path_1 is a descendant (i.e. located within) path_2 or if the paths are the\n    same, returns False otherwise.\n    Parameters:\n        path_1: str or Path (should be a file)\n        path_2: str or Path (can be a file or directory)\n    \"\"\"\n    path_1, path_2 = abspath(path_1), abspath(path_2)\n    try:\n        if \"..\" in str(path_1.relative_to(path_2)):  # prevent path traversal\n            return False\n    except ValueError:\n        return False\n    return True\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get(self, id):\n        \"\"\"Get organization\n        ---\n        description: >-\n          Returns the organization specified by the id\\n\n\n          ### Permission Table\\n\n          |Rule name|Scope|Operation|Assigned to node|Assigned to container|\n          Description|\\n\n          |--|--|--|--|--|--|\\n\n          |Organization|Global|View|\u274c|\u274c|View all organizations|\\n\n          |Organization|Collaboration|View|\u2705|\u2705|View a list of organizations\n          within the scope of the collaboration|\\n\n          |Organization|Organization|View|\u2705|\u2705|View a list of organizations\n          that the user is part of|\\n\n\n          Accessible to users.\n\n        parameters:\n          - in: path\n            name: id\n            schema:\n              type: integer\n            description: Organization id\n            required: true\n\n        responses:\n          200:\n            description: Ok\n          404:\n            description: Organization not found\n          401:\n            description: Unauthorized\n\n        security:\n          - bearerAuth: []\n\n        tags: [\"Organization\"]\n        \"\"\"\n\n        # obtain organization of authenticated\n        auth_org = self.obtain_auth_organization()\n\n        # retrieve requested organization\n        req_org = db.Organization.get(id)\n        if not req_org:\n            return {'msg': f'Organization id={id} not found!'}, \\\n                HTTPStatus.NOT_FOUND\n\n        # Check if auth has enough permissions\n        if not self.r.can_by_org(P.VIEW, id, auth_org):\n            return {'msg': 'You do not have permission to do that!'}, \\\n                HTTPStatus.UNAUTHORIZED\n\n        return org_schema.dump(req_org, many=False).data, HTTPStatus.OK\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def read_from_stream(\n        stream: StreamType,\n        pdf: Any,  # PdfReader\n        forced_encoding: Union[None, str, List[str], Dict[int, str]] = None,\n    ) -> \"DictionaryObject\":\n        def get_next_obj_pos(\n            p: int, p1: int, rem_gens: List[int], pdf: Any\n        ) -> int:  # PdfReader\n            l = pdf.xref[rem_gens[0]]\n            for o in l:\n                if p1 > l[o] and p < l[o]:\n                    p1 = l[o]\n            if len(rem_gens) == 1:\n                return p1\n            else:\n                return get_next_obj_pos(p, p1, rem_gens[1:], pdf)\n\n        def read_unsized_from_steam(stream: StreamType, pdf: Any) -> bytes:  # PdfReader\n            # we are just pointing at beginning of the stream\n            eon = get_next_obj_pos(stream.tell(), 2**32, list(pdf.xref), pdf) - 1\n            curr = stream.tell()\n            rw = stream.read(eon - stream.tell())\n            p = rw.find(b\"endstream\")\n            if p < 0:\n                raise PdfReadError(\n                    f\"Unable to find 'endstream' marker for obj starting at {curr}.\"\n                )\n            stream.seek(curr + p + 9)\n            return rw[: p - 1]\n\n        tmp = stream.read(2)\n        if tmp != b\"<<\":\n            raise PdfReadError(\n                f\"Dictionary read error at byte {hex_str(stream.tell())}: \"\n                \"stream must begin with '<<'\"\n            )\n        data: Dict[Any, Any] = {}\n        while True:\n            tok = read_non_whitespace(stream)\n            if tok == b\"\\x00\":\n                continue\n            elif tok == b\"%\":\n                stream.seek(-1, 1)\n                skip_over_comment(stream)\n                continue\n            if not tok:\n                raise PdfStreamError(STREAM_TRUNCATED_PREMATURELY)\n\n            if tok == b\">\":\n                stream.read(1)\n                break\n            stream.seek(-1, 1)\n            try:\n                key = read_object(stream, pdf)\n                tok = read_non_whitespace(stream)\n                stream.seek(-1, 1)\n                value = read_object(stream, pdf, forced_encoding)\n            except Exception as exc:\n                if pdf is not None and pdf.strict:\n                    raise PdfReadError(exc.__repr__())\n                logger_warning(exc.__repr__(), __name__)\n                retval = DictionaryObject()\n                retval.update(data)\n                return retval  # return partial data\n\n            if not data.get(key):\n                data[key] = value\n            else:\n                # multiple definitions of key not permitted\n                msg = (\n                    f\"Multiple definitions in dictionary at byte \"\n                    f\"{hex_str(stream.tell())} for key {key}\"\n                )\n                if pdf is not None and pdf.strict:\n                    raise PdfReadError(msg)\n                logger_warning(msg, __name__)\n\n        pos = stream.tell()\n        s = read_non_whitespace(stream)\n        if s == b\"s\" and stream.read(5) == b\"tream\":\n            eol = stream.read(1)\n            # odd PDF file output has spaces after 'stream' keyword but before EOL.\n            # patch provided by Danial Sandler\n            while eol == b\" \":\n                eol = stream.read(1)\n            if eol not in (b\"\\n\", b\"\\r\"):\n                raise PdfStreamError(\"Stream data must be followed by a newline\")\n            if eol == b\"\\r\":\n                # read \\n after\n                if stream.read(1) != b\"\\n\":\n                    stream.seek(-1, 1)\n            # this is a stream object, not a dictionary\n            if SA.LENGTH not in data:\n                raise PdfStreamError(\"Stream length not defined\")\n            length = data[SA.LENGTH]\n            if isinstance(length, IndirectObject):\n                t = stream.tell()\n                length = pdf.get_object(length)\n                stream.seek(t, 0)\n            pstart = stream.tell()\n            data[\"__streamdata__\"] = stream.read(length)\n            e = read_non_whitespace(stream)\n            ndstream = stream.read(8)\n            if (e + ndstream) != b\"endstream\":\n                # (sigh) - the odd PDF file has a length that is too long, so\n                # we need to read backwards to find the \"endstream\" ending.\n                # ReportLab (unknown version) generates files with this bug,\n                # and Python users into PDF files tend to be our audience.\n                # we need to do this to correct the streamdata and chop off\n                # an extra character.\n                pos = stream.tell()\n                stream.seek(-10, 1)\n                end = stream.read(9)\n                if end == b\"endstream\":\n                    # we found it by looking back one character further.\n                    data[\"__streamdata__\"] = data[\"__streamdata__\"][:-1]\n                elif not pdf.strict:\n                    stream.seek(pstart, 0)\n                    data[\"__streamdata__\"] = read_unsized_from_steam(stream, pdf)\n                    pos = stream.tell()\n                else:\n                    stream.seek(pos, 0)\n                    raise PdfReadError(\n                        \"Unable to find 'endstream' marker after stream at byte \"\n                        f\"{hex_str(stream.tell())} (nd='{ndstream!r}', end='{end!r}').\"\n                    )\n        else:\n            stream.seek(pos, 0)\n        if \"__streamdata__\" in data:\n            return StreamObject.initialize_from_dictionary(data)\n        else:\n            retval = DictionaryObject()\n            retval.update(data)\n            return retval\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def call(__self, __context, __obj, *args, **kwargs):\n        \"\"\"Call an object from sandboxed code.\"\"\"\n        fmt = inspect_format_method(__obj)\n        if fmt is not None:\n            return __self.format_string(fmt, args, kwargs)\n\n        # the double prefixes are to avoid double keyword argument\n        # errors when proxying the call.\n        if not __self.is_safe_callable(__obj):\n            raise SecurityError('%r is not safely callable' % (__obj,))\n        return __context.call(__obj, *args, **kwargs)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def ask_passwords():\n        ''' prompt for connection and become passwords if needed '''\n\n        op = context.CLIARGS\n        sshpass = None\n        becomepass = None\n        become_prompt = ''\n\n        become_prompt_method = \"BECOME\" if C.AGNOSTIC_BECOME_PROMPT else op['become_method'].upper()\n\n        try:\n            if op['ask_pass']:\n                sshpass = getpass.getpass(prompt=\"SSH password: \")\n                become_prompt = \"%s password[defaults to SSH password]: \" % become_prompt_method\n                if sshpass:\n                    sshpass = to_bytes(sshpass, errors='strict', nonstring='simplerepr')\n            else:\n                become_prompt = \"%s password: \" % become_prompt_method\n\n            if op['become_ask_pass']:\n                becomepass = getpass.getpass(prompt=become_prompt)\n                if op['ask_pass'] and becomepass == '':\n                    becomepass = sshpass\n                if becomepass:\n                    becomepass = to_bytes(becomepass)\n        except EOFError:\n            pass\n\n        # we 'wrap' the passwords to prevent templating as\n        # they can contain special chars and trigger it incorrectly\n        if sshpass:\n            sshpass = AnsibleUnsafeBytes(sshpass)\n        if becomepass:\n            becomepass = AnsibleUnsafeBytes(becomepass)\n\n        return (sshpass, becomepass)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Subgraph::AddNodeWithParameters(\n    const std::vector<int>& inputs, const std::vector<int>& outputs,\n    const std::vector<int>& intermediates, const char* init_data,\n    size_t init_data_size, void* builtin_data,\n    const TfLiteRegistration* registration, int* node_index) {\n  std::unique_ptr<void, decltype(free)*> builtin_data_deleter(builtin_data,\n                                                              free);\n  if (state_ == kStateInvokableAndImmutable) {\n    ReportError(\"AddNodeWithParameters is disallowed when graph is immutable.\");\n    return kTfLiteError;\n  }\n  state_ = kStateUninvokable;\n\n  TF_LITE_ENSURE_OK(&context_, CheckTensorIndices(\"node inputs\", inputs.data(),\n                                                  inputs.size()));\n  TF_LITE_ENSURE_OK(\n      &context_,\n      CheckTensorIndices(\"node outputs\", outputs.data(), outputs.size()));\n\n  // For builtin ops, inputs and outputs must not overlap. Custom ops must do\n  // this check by themselves if they don't support overlapping tensors. This\n  // distinction is to allow custom ops to just forward a tensor, reusing it as\n  // both input and output.\n  if (builtin_data != nullptr) {\n    TF_LITE_ENSURE_OK(&context_, CheckInputAndOutputForOverlap(\n                                     inputs.data(), inputs.size(),\n                                     outputs.data(), outputs.size()));\n  }\n\n  int new_node_index = nodes_and_registration_.size();\n  if (node_index) *node_index = new_node_index;\n  nodes_and_registration_.resize(nodes_and_registration_.size() + 1);\n  auto& node_and_reg = nodes_and_registration_.back();\n  TfLiteNode& node = node_and_reg.first;\n  if (node.inputs) TfLiteIntArrayFree(node.inputs);\n  if (node.outputs) TfLiteIntArrayFree(node.outputs);\n  if (node.intermediates) TfLiteIntArrayFree(node.intermediates);\n  if (node.temporaries) TfLiteIntArrayFree(node.temporaries);\n\n  // NOTE, here we are not using move semantics yet, since our internal\n  // representation isn't std::vector, but in the future we would like to avoid\n  // copies, so we want the interface to take r-value references now.\n  node.inputs = ConvertVectorToTfLiteIntArray(inputs);\n  node.outputs = ConvertVectorToTfLiteIntArray(outputs);\n  node.intermediates = ConvertVectorToTfLiteIntArray(intermediates);\n  node.temporaries = TfLiteIntArrayCreate(0);\n  if (init_data) {\n    node.user_data = OpInit(*registration, init_data, init_data_size);\n  } else {\n    node.user_data = OpInit(\n        *registration, static_cast<const char*>(builtin_data_deleter.get()), 0);\n  }\n\n  node.builtin_data = builtin_data_deleter.release();\n  // TODO(ycling): Filling `custom_initial_data` and `custom_initial_data_size`\n  // properly for nodes generated by ReplaceNodeSubsetsWithDelegateKernels.\n\n  if (registration->builtin_code == BuiltinOperator_CUSTOM) {\n    // When it's a CUSTOM op, the `custom_options` field in the Flatbuffer\n    // `Operator` table is passed in.\n    node.custom_initial_data = init_data;\n    node.custom_initial_data_size = init_data_size;\n  } else {\n    node.custom_initial_data = nullptr;\n    node.custom_initial_data_size = 0;\n  }\n\n  node.delegate = nullptr;\n  // Copying of registration is required to support unresolved custom ops.\n  node_and_reg.second = *registration;\n  execution_plan_.push_back(new_node_index);\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input_sizes = context->input(0);\n    const Tensor& filter = context->input(1);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(input_sizes.shape()),\n        errors::InvalidArgument(\n            \"Conv2DBackpropInput: input_sizes input must be 1-dim, not \",\n            input_sizes.dims()));\n    TensorShape input_shape;\n    const int32* in_sizes_data = input_sizes.template flat<int32>().data();\n\n    for (int i = 0; i < input_sizes.NumElements(); ++i) {\n      OP_REQUIRES(context, in_sizes_data[i] >= 0,\n                  errors::InvalidArgument(\"Dimension \", i,\n                                          \" of input_sizes must be >= 0\"));\n      OP_REQUIRES_OK(context, input_shape.AddDimWithStatus(in_sizes_data[i]));\n    }\n    const TensorShape& filter_shape = filter.shape();\n    EXTRACT_AND_VERIFY_DIMENSIONS(\"DepthwiseConv2DBackpropInput\");\n\n    Tensor* in_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, input_shape, &in_backprop));\n\n    // If there is nothing to compute, return.\n    if (input_shape.num_elements() == 0) {\n      return;\n    }\n\n    // If in_depth==1, this operation is just a standard convolution.\n    // Depthwise convolution is a special case of cuDNN's grouped convolution.\n    bool use_cudnn = std::is_same<Device, GPUDevice>::value &&\n                     (in_depth == 1 ||\n                      (use_cudnn_grouped_conv_ &&\n                       IsCudnnSupportedFilterSize(/*filter_rows=*/filter_rows,\n                                                  /*filter_cols=*/filter_cols,\n                                                  /*in_depth=*/in_depth,\n                                                  /*out_depth=*/out_depth)));\n\n    VLOG(2) << \"DepthwiseConv2dNativeBackpropInput: \"\n            << \" Input: [\" << batch << \", \" << input_rows << \", \" << input_cols\n            << \", \" << in_depth << \"]; Filter: [\" << filter_rows << \", \"\n            << filter_cols << \", \" << in_depth << \", \" << depth_multiplier\n            << \"]; Output: [\" << batch << \", \" << out_rows << \", \" << out_cols\n            << \", \" << out_depth << \"], stride = \" << stride_\n            << \", pad_rows = \" << pad_top << \", pad_cols = \" << pad_left\n            << \", Use cuDNN: \" << use_cudnn;\n\n    if (use_cudnn) {\n      // Reshape from TF depthwise filter to cuDNN grouped convolution filter:\n      //\n      //                  | TensorFlow       | cuDNN\n      // --------------------------------------------------------------------\n      // filter_out_depth | depth_multiplier | depth_multiplier * group_count\n      // filter_in_depth  | in_depth         | in_depth / group_count\n      //\n      // For depthwise convolution, we have group_count == in_depth.\n      int32_t filter_in_depth = 1;\n      TensorShape shape =\n          TensorShape{filter_rows, filter_cols, filter_in_depth, out_depth};\n      Tensor reshaped_filter(/*type=*/dtype_);\n      OP_REQUIRES(\n          context, reshaped_filter.CopyFrom(filter, shape),\n          errors::Internal(\n              \"Failed to reshape filter tensor for grouped convolution.\"));\n      // TODO(yangzihao): Send in arbitrary dilation rates after the dilated\n      // conv is supported.\n      launcher_(context, /*use_cudnn=*/true, cudnn_use_autotune_, out_backprop,\n                reshaped_filter, /*row_dilation=*/1, /*col_dilation=*/1,\n                stride_, stride_, padding_, explicit_paddings_, in_backprop,\n                data_format_);\n      return;\n    }\n\n    auto out_backprop_ptr = out_backprop.template flat<T>().data();\n    auto filter_ptr = filter.template flat<T>().data();\n    auto in_backprop_ptr = in_backprop->template flat<T>().data();\n    LaunchDepthwiseConvBackpropInputOp<Device, T>()(\n        context, args, out_backprop_ptr, filter_ptr, in_backprop_ptr,\n        data_format_);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def safer_getattr(object, name, default=None, getattr=getattr):\n    \"\"\"Getattr implementation which prevents using format on string objects.\n\n    format() is considered harmful:\n    http://lucumr.pocoo.org/2016/12/29/careful-with-str-format/\n\n    \"\"\"\n    if name in ('format', 'format_map') and (\n            isinstance(object, str) or\n            (isinstance(object, type) and issubclass(object, str))):\n        raise NotImplementedError(\n            'Using the format*() methods of `str` is not safe')\n    if name.startswith('_'):\n        raise AttributeError(\n            '\"{name}\" is an invalid attribute name because it '\n            'starts with \"_\"'.format(name=name)\n        )\n    return getattr(object, name, default)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def spider_opened(self, spider):\n        if _http_auth_enabled(spider):\n            replace_downloader_middleware(self.crawler, RobotsTxtMiddleware,\n                                          SafeRobotsTxtMiddleware)\n        if not hasattr(spider, 'state'):\n            spider.state = {}\n\n        # local fingerprint => key returned by splash\n        spider.state.setdefault(self.remote_keys_key, {})\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _load_post_and_files(self):\n        \"\"\"Populate self._post and self._files if the content-type is a form type\"\"\"\n        if self.method != \"POST\":\n            self._post, self._files = (\n                QueryDict(encoding=self._encoding),\n                MultiValueDict(),\n            )\n            return\n        if self._read_started and not hasattr(self, \"_body\"):\n            self._mark_post_parse_error()\n            return\n\n        if self.content_type == \"multipart/form-data\":\n            if hasattr(self, \"_body\"):\n                # Use already read data\n                data = BytesIO(self._body)\n            else:\n                data = self\n            try:\n                self._post, self._files = self.parse_file_upload(self.META, data)\n            except (MultiPartParserError, TooManyFilesSent):\n                # An error occurred while parsing POST data. Since when\n                # formatting the error the request handler might access\n                # self.POST, set self._post and self._file to prevent\n                # attempts to parse POST data again.\n                self._mark_post_parse_error()\n                raise\n        elif self.content_type == \"application/x-www-form-urlencoded\":\n            self._post, self._files = (\n                QueryDict(self.body, encoding=self._encoding),\n                MultiValueDict(),\n            )\n        else:\n            self._post, self._files = (\n                QueryDict(encoding=self._encoding),\n                MultiValueDict(),\n            )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == 4,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional (2)\"));\n    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n                errors::InvalidArgument(\"tensor_in must not be empty (2)\"));\n\n    PoolParameters params{context,\n                          ksize_,\n                          stride_,\n                          padding_,\n                          /*explicit_paddings=*/{},\n                          FORMAT_NHWC,\n                          tensor_in.shape()};\n    if (!context->status().ok()) {\n      return;\n    }\n\n    TensorShape out_shape({params.tensor_in_batch, params.out_height,\n                           params.out_width, params.depth});\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n    Tensor* argmax = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, out_shape, &argmax));\n\n    LaunchMaxPoolingWithArgmax<Device, T, Targmax>::launch(\n        context, params, tensor_in, output, argmax, propagate_nans_,\n        include_batch_in_index_);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_image_dimensions(file_or_path, close=False):\n    \"\"\"\n    Returns the (width, height) of an image, given an open file or a path.  Set\n    'close' to True to close the file at the end if it is initially in an open\n    state.\n    \"\"\"\n    # Try to import PIL in either of the two ways it can end up installed.\n    try:\n        from PIL import ImageFile as PILImageFile\n    except ImportError:\n        import ImageFile as PILImageFile\n\n    p = PILImageFile.Parser()\n    if hasattr(file_or_path, 'read'):\n        file = file_or_path\n        file_pos = file.tell()\n        file.seek(0)\n    else:\n        file = open(file_or_path, 'rb')\n        close = True\n    try:\n        # Most of the time PIL only needs a small chunk to parse the image and\n        # get the dimensions, but with some TIFF files PIL needs to parse the\n        # whole file.\n        chunk_size = 1024\n        while 1:\n            data = file.read(chunk_size)\n            if not data:\n                break\n            p.feed(data)\n            if p.image:\n                return p.image.size\n            chunk_size = chunk_size*2\n        return None\n    finally:\n        if close:\n            file.close()\n        else:\n            file.seek(file_pos)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def clean_identity(self):\n        v = self.cleaned_data[\"identity\"]\n        if not TokenBucket.authorize_login_email(v):\n            raise forms.ValidationError(\"Too many attempts, please try later.\")\n\n        try:\n            self.user = User.objects.get(email=v)\n        except User.DoesNotExist:\n            self.user = None\n\n        return v\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def add_cookies(response, cookie={}):\n    for int_cookie in cookie:\n        response.set_cookie(**int_cookie)\n\n    return(response)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, handler, max_frame_length=DEFAULT_MAX_FRAME_LENGTH):\n        self.handler = handler\n        self.protocol_version = None\n        self.support_legacy_versions = True\n        self.supported_protocols = []\n        self.origin_checker = None\n        self.max_frame_length = max_frame_length\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def createRequest(self, op, req, outgoing_data):\n        \"\"\"Return a new QNetworkReply object.\n\n        Args:\n             op: Operation op\n             req: const QNetworkRequest & req\n             outgoing_data: QIODevice * outgoingData\n\n        Return:\n            A QNetworkReply.\n        \"\"\"\n        proxy_factory = objreg.get('proxy-factory', None)\n        if proxy_factory is not None:\n            proxy_error = proxy_factory.get_error()\n            if proxy_error is not None:\n                return networkreply.ErrorNetworkReply(\n                    req, proxy_error, QNetworkReply.UnknownProxyError,\n                    self)\n\n        for header, value in shared.custom_headers(url=req.url()):\n            req.setRawHeader(header, value)\n\n        host_blocker = objreg.get('host-blocker')\n        if host_blocker.is_blocked(req.url()):\n            log.webview.info(\"Request to {} blocked by host blocker.\".format(\n                req.url().host()))\n            return networkreply.ErrorNetworkReply(\n                req, HOSTBLOCK_ERROR_STRING, QNetworkReply.ContentAccessDenied,\n                self)\n\n        # There are some scenarios where we can't figure out current_url:\n        # - There's a generic NetworkManager, e.g. for downloads\n        # - The download was in a tab which is now closed.\n        current_url = QUrl()\n\n        if self._tab_id is not None:\n            assert self._win_id is not None\n            try:\n                tab = objreg.get('tab', scope='tab', window=self._win_id,\n                                 tab=self._tab_id)\n                current_url = tab.url()\n            except (KeyError, RuntimeError):\n                # https://github.com/qutebrowser/qutebrowser/issues/889\n                # Catching RuntimeError because we could be in the middle of\n                # the webpage shutdown here.\n                current_url = QUrl()\n\n        if 'log-requests' in self._args.debug_flags:\n            operation = debug.qenum_key(QNetworkAccessManager, op)\n            operation = operation.replace('Operation', '').upper()\n            log.webview.debug(\"{} {}, first-party {}\".format(\n                operation,\n                req.url().toDisplayString(),\n                current_url.toDisplayString()))\n\n        scheme = req.url().scheme()\n        if scheme in self._scheme_handlers:\n            result = self._scheme_handlers[scheme](req, op, current_url)\n            if result is not None:\n                result.setParent(self)\n                return result\n\n        self.set_referer(req, current_url)\n        return super().createRequest(op, req, outgoing_data)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    auto min_input_tensor = context->input(1);\n    auto max_input_tensor = context->input(2);\n    OP_REQUIRES(\n        context, min_input_tensor.NumElements() == 1,\n        errors::InvalidArgument(\n            \"min_input must be a scalar float value, got tensor with shape \",\n            min_input_tensor.shape()));\n    OP_REQUIRES(\n        context, max_input_tensor.NumElements() == 1,\n        errors::InvalidArgument(\n            \"max_input must be a scalar float value, got tensor with shape \",\n            max_input_tensor.shape()));\n    const float min_input = context->input(1).flat<float>()(0);\n    const float max_input = context->input(2).flat<float>()(0);\n    MaxPoolingOp<Device, T>::Compute(context);\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));\n    output_min->flat<float>()(0) = min_input;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));\n    output_max->flat<float>()(0) = max_input;\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status TrySimplify(NodeDef* consumer, string* simplified_node_name) override {\n    NodeDef* producer;\n\n    if (consumer->input_size() < 1) {\n      return errors::FailedPrecondition(\"Node \", simplified_node_name,\n                                        \" lacks inputs\");\n    }\n\n    TF_RETURN_IF_ERROR(GetInputNode(consumer->input(0), &producer));\n    const bool producer_is_cast = IsCastLike(*producer);\n    const bool can_optimize =\n        !IsCheckNumerics(*producer) &&\n        ((producer_is_cast && IsValuePreserving(*consumer)) ||\n         (IsValuePreserving(*producer) && IsCastLike(*consumer)));\n    if (!can_optimize || IsControlFlow(*producer) ||\n        IsInPreserveSet(*producer) ||\n        producer->device() != consumer->device()) {\n      return Status::OK();\n    }\n\n    const NodeDef* cast_like_node = producer_is_cast ? producer : consumer;\n    const OpDef* cast_like_op_def = nullptr;\n    TF_RETURN_IF_ERROR(OpRegistry::Global()->LookUpOpDef(cast_like_node->op(),\n                                                         &cast_like_op_def));\n    DataType cast_src_type;\n    TF_RETURN_IF_ERROR(InputTypeForNode(*cast_like_node, *cast_like_op_def, 0,\n                                        &cast_src_type));\n    DataType cast_dst_type;\n    TF_RETURN_IF_ERROR(OutputTypeForNode(*cast_like_node, *cast_like_op_def, 0,\n                                         &cast_dst_type));\n    if (!IsFixedSizeType(cast_src_type) || !IsFixedSizeType(cast_dst_type)) {\n      return Status::OK();\n    } else if (producer_is_cast &&\n               DataTypeSize(cast_dst_type) <= DataTypeSize(cast_src_type)) {\n      return Status::OK();\n    } else if (!producer_is_cast &&\n               DataTypeSize(cast_dst_type) >= DataTypeSize(cast_src_type)) {\n      return Status::OK();\n    }\n\n    // Check that nodes were not already optimized.\n    const string optimized_producer_name = OptimizedNodeName(\n        ParseNodeScopeAndName(producer->name()), DataTypeString(cast_dst_type));\n    const string optimized_consumer_name = OptimizedNodeName(\n        ParseNodeScopeAndName(consumer->name()), DataTypeString(cast_src_type));\n    const bool is_already_optimized =\n        ctx().node_map->NodeExists(optimized_consumer_name) ||\n        ctx().node_map->NodeExists(optimized_producer_name);\n    if (is_already_optimized) {\n      return Status::OK();\n    }\n\n    // Add copies of consumer and producer in reverse order.\n    NodeDef* input;\n    TF_RETURN_IF_ERROR(GetInputNode(producer->input(0), &input));\n    // Create new producer node.\n    NodeDef* new_producer = AddCopyNode(optimized_consumer_name, consumer);\n    new_producer->set_input(0, producer->input(0));\n    ctx().node_map->AddOutput(input->name(), new_producer->name());\n\n    // Create new consumer node.\n    NodeDef* new_consumer = AddCopyNode(optimized_producer_name, producer);\n    new_consumer->set_input(0, new_producer->name());\n\n    NodeDef* new_value_preserving =\n        producer_is_cast ? new_producer : new_consumer;\n    const DataType new_input_type =\n        producer_is_cast ? cast_src_type : cast_dst_type;\n    // Update the input type of the value-preserving node. The input and\n    // output types of the cast-like nodes remain the same.\n    TF_RETURN_IF_ERROR(SetInputType(new_input_type, new_value_preserving));\n    // Make sure there is a kernel registered for the value preserving op\n    // with the new input type.\n    TF_RETURN_IF_ERROR(IsKernelRegisteredForNode(*new_value_preserving));\n    ctx().node_map->AddOutput(new_producer->name(), new_consumer->name());\n\n    AddToOptimizationQueue(new_producer);\n    *simplified_node_name = new_consumer->name();\n\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n        key = self.make_key(key, version=version)\n        self.validate_key(key)\n        return self._cache.add(key, value, self.get_backend_timeout(timeout))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_user_roles(self, context, user_id, tenant_id=None):\n        \"\"\"Get the roles for a user and tenant pair.\n\n        Since we're trying to ignore the idea of user-only roles we're\n        not implementing them in hopes that the idea will die off.\n\n        \"\"\"\n        self.assert_admin(context)\n        if tenant_id is None:\n            raise exception.NotImplemented(message='User roles not supported: '\n                                                   'tenant ID required')\n\n        user = self.identity_api.get_user(context, user_id)\n        if user is None:\n            raise exception.UserNotFound(user_id=user_id)\n        tenant = self.identity_api.get_tenant(context, tenant_id)\n        if tenant is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        roles = self.identity_api.get_roles_for_user_and_tenant(\n                context, user_id, tenant_id)\n        return {'roles': [self.identity_api.get_role(context, x)\n                          for x in roles]}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  OpContext op_context(context, node);\n\n  // If inputs have no element, shortcircuit.\n  if (NumElements(op_context.input1) == 0 ||\n      NumElements(op_context.input2) == 0) {\n    return kTfLiteOk;\n  }\n\n  switch (op_context.output->type) {\n    case kTfLiteFloat32:\n      TFLiteOperation<kernel_type, float, OpType>(context, node, op_context);\n      break;\n    case kTfLiteUInt8:\n      TFLiteOperation<kernel_type, uint8_t, OpType>(context, node, op_context);\n      break;\n    case kTfLiteInt8:\n      TFLiteOperation<kernel_type, int8_t, OpType>(context, node, op_context);\n      break;\n    case kTfLiteInt32:\n      TFLiteOperation<kernel_type, int32_t, OpType>(context, node, op_context);\n      break;\n    case kTfLiteInt64:\n      TFLiteOperation<kernel_type, int64_t, OpType>(context, node, op_context);\n      break;\n    case kTfLiteInt16:\n      TFLiteOperation<kernel_type, int16_t, OpType>(context, node, op_context);\n      break;\n    default:\n      context->ReportError(context,\n                           \"Type %d is currently not supported by Maximum.\",\n                           op_context.output->type);\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def new_control(self, type, name, value, **kwargs):\n        \"\"\"Add a new input element to the form.\n\n        The arguments set the attributes of the new element.\n        \"\"\"\n        # Remove existing input-like elements with the same name\n        for tag in ('input', 'textarea', 'select'):\n            for old in self.form.find_all(tag, {'name': name}):\n                old.decompose()\n        # We don't have access to the original soup object (just the\n        # Tag), so we instantiate a new BeautifulSoup() to call\n        # new_tag(). We're only building the soup object, not parsing\n        # anything, so the parser doesn't matter. Specify the one\n        # included in Python to avoid having dependency issue.\n        control = BeautifulSoup(\"\", \"html.parser\").new_tag('input')\n        control['type'] = type\n        control['name'] = name\n        control['value'] = value\n        for k, v in kwargs.items():\n            control[k] = v\n        self._assert_valid_file_upload(control, value)\n        self.form.append(control)\n        return control\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status SparseTensorFromContext(OpKernelContext* ctx, const int32_t base_index,\n                               const bool validate_indices,\n                               sparse::SparseTensor* tensor) {\n  // Assume row-major order.\n  TensorShape shape;\n  const Tensor& shape_tensor = ctx->input(base_index + 2);\n  if (shape_tensor.dims() != 1) {\n    return errors::InvalidArgument(\"Shape must be a 1D tensor.\");\n  }\n  TF_RETURN_IF_ERROR(\n      TensorShape::BuildTensorShape(shape_tensor.vec<int64_t>(), &shape));\n  CheckRankAtLeast2(ctx, shape);\n  std::vector<int64_t> order(shape.dims());\n  std::iota(order.begin(), order.end(), 0);\n\n  Status status = sparse::SparseTensor::Create(\n      ctx->input(base_index), ctx->input(base_index + 1), shape, order, tensor);\n\n  if (!validate_indices || !status.ok()) return status;\n  return tensor->IndicesValid();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __call__(self, value):\n        if not isinstance(value, str):\n            raise ValidationError(self.message, code=self.code, params={'value': value})\n        if self.unsafe_chars.intersection(value):\n            raise ValidationError(self.message, code=self.code, params={'value': value})\n        # Check if the scheme is valid.\n        scheme = value.split('://')[0].lower()\n        if scheme not in self.schemes:\n            raise ValidationError(self.message, code=self.code, params={'value': value})\n\n        # Then check full URL\n        try:\n            super().__call__(value)\n        except ValidationError as e:\n            # Trivial case failed. Try for possible IDN domain\n            if value:\n                try:\n                    scheme, netloc, path, query, fragment = urlsplit(value)\n                except ValueError:  # for example, \"Invalid IPv6 URL\"\n                    raise ValidationError(self.message, code=self.code, params={'value': value})\n                try:\n                    netloc = punycode(netloc)  # IDN -> ACE\n                except UnicodeError:  # invalid domain part\n                    raise e\n                url = urlunsplit((scheme, netloc, path, query, fragment))\n                super().__call__(url)\n            else:\n                raise\n        else:\n            # Now verify IPv6 in the netloc part\n            host_match = re.search(r'^\\[(.+)\\](?::\\d{2,5})?$', urlsplit(value).netloc)\n            if host_match:\n                potential_ip = host_match[1]\n                try:\n                    validate_ipv6_address(potential_ip)\n                except ValidationError:\n                    raise ValidationError(self.message, code=self.code, params={'value': value})\n\n        # The maximum length of a full host name is 253 characters per RFC 1034\n        # section 3.1. It's defined to be 255 bytes or less, but this includes\n        # one byte for the length of the name and one byte for the trailing dot\n        # that's used to indicate absolute names in DNS.\n        if len(urlsplit(value).hostname) > 253:\n            raise ValidationError(self.message, code=self.code, params={'value': value})\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_json(self, uri, max_size = None):\n        \"\"\"Make a GET request to an endpoint returning JSON and parse result\n\n        :param uri: The URI to make a GET request to.\n        :type uri: unicode\n\n        :param max_size: The maximum size (in bytes) to allow as a response.\n        :type max_size: int\n\n        :return: A deferred containing JSON parsed into a Python object.\n        :rtype: twisted.internet.defer.Deferred[dict[any, any]]\n        \"\"\"\n        logger.debug(\"HTTP GET %s\", uri)\n\n        response = yield self.agent.request(\n            b\"GET\",\n            uri.encode(\"utf8\"),\n        )\n        body = yield read_body_with_max_size(response, max_size)\n        try:\n            # json.loads doesn't allow bytes in Python 3.5\n            json_body = json.loads(body.decode(\"UTF-8\"))\n        except Exception as e:\n            logger.exception(\"Error parsing JSON from %s\", uri)\n            raise\n        defer.returnValue(json_body)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status GraphConstructor::MakeEdge(Node* src, int output_index, Node* dst,\n                                  int input_index) {\n  if (output_index >= src->num_outputs()) {\n    return errors::InvalidArgument(\n        \"Output \", output_index, \" of node \", src->name(),\n        \" does not exist. Node only has \", src->num_outputs(), \" outputs.\");\n  }\n  if (input_index >= dst->num_inputs()) {\n    return errors::InvalidArgument(\n        \"Input \", input_index, \" of node \", dst->name(),\n        \" does not exist. Node only has \", dst->num_inputs(), \" inputs.\");\n  }\n\n  DataType src_out = src->output_type(output_index);\n  DataType dst_in = dst->input_type(input_index);\n  if (!TypesCompatible(dst_in, src_out)) {\n    return errors::InvalidArgument(\n        \"Input \", input_index, \" of node \", dst->name(), \" was passed \",\n        DataTypeString(src_out), \" from \", src->name(), \":\", output_index,\n        \" incompatible with expected \", DataTypeString(dst_in), \".\");\n  }\n  g_->AddEdge(src, output_index, dst, input_index);\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_vault_token(vault_endpoint: str, vault_secret: str, vault_endpoint_verify: str | bool | None = True) -> str:\n    if not vault_endpoint:\n        raise ValueError('vault_endpoint is required')\n    if not vault_secret:\n        raise ValueError('vault_secret is required')\n\n    headers = {'X-Vault-Token': vault_secret}\n    response = requests.get(vault_endpoint, headers=headers, verify=vault_endpoint_verify)\n\n    if response.status_code != 200:\n        logger.info(response.text)\n        raise ValueError(\n            'Could not get vault token, response code: {}'.format(\n                response.status_code))\n\n    logger.info(\"get oauth token from %s status_code=%s\", vault_endpoint, response.status_code)\n    return response.json()['data']['token']\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def start_jupyter_job(sbatch_options: dict = None, conda_env: str = None, lab: bool = False):\n\n    sbatch_options = sbatch_options if sbatch_options is not None else {}\n    sbatch_options_merged = SETTINGS.SLURM_DEFAULT['sbatch_options']\n    sbatch_options_merged.update(SETTINGS.SBATCH_OPTIONS_TEMPLATES.JUPYTER)\n    sbatch_options_merged.update(sbatch_options)\n    # Construct sbatch options string\n    sbatch_options_str = create_slurm_options_string(sbatch_options_merged)\n\n    template = pkg_resources.resource_string(__name__, \"jupyter_template.sh\").decode(\"utf-8\")\n\n    script = template.format(\n            sbatch_options=sbatch_options_str,\n            use_conda_env=str(conda_env is not None).lower(),\n            conda_env=conda_env,\n            notebook_or_lab=\" notebook\" if not lab else \"-lab\",\n    )\n\n    path = f\"/tmp/{uuid.uuid4()}.sh\"\n    with open(path, \"w\") as f:\n        f.write(script)\n\n    output = subprocess.run(f'sbatch {path}', shell=True, check=True, capture_output=True).stdout\n    os.remove(path)\n\n    slurm_array_job_id = int(output.split(b' ')[-1])\n    logging.info(f\"Queued Jupyter instance in Slurm job with ID {slurm_array_job_id}.\")\n\n    job_output = subprocess.run(f'scontrol show job {slurm_array_job_id} -o',\n                                shell=True, check=True, capture_output=True).stdout\n    job_output_results = job_output.decode(\"utf-8\").split(\" \")\n    job_info_dict = {x.split('=')[0]: x.split('=')[1] for x in job_output_results if '=' in x}\n    log_file = job_info_dict['StdOut']\n\n    logging.info(f\"The job's log-file is '{log_file}'.\")\n    logging.info(\"Waiting for start-up to fetch the machine and port of the Jupyter instance... \"\n                 \"(ctrl-C to cancel fetching)\")\n\n    while job_info_dict['JobState'] in SlurmStates.PENDING:\n        job_output = subprocess.run(f'scontrol show job {slurm_array_job_id} -o',\n                                    shell=True, check=True, capture_output=True).stdout\n        job_output_results = job_output.decode(\"utf-8\").split(\" \")\n        job_info_dict = {x.split('=')[0]: x.split('=')[1] for x in job_output_results if '=' in x}\n        time.sleep(1)\n    if job_info_dict['JobState'] not in SlurmStates.RUNNING:\n        logging.error(f\"Slurm job failed. See log-file '{log_file}' for more information.\")\n        exit(1)\n\n    logging.info(\"Slurm job is running. Jupyter instance is starting up...\")\n    log_file_contents = \"\"\n    while \" is running at\" not in log_file_contents:\n        if os.path.exists(log_file):\n            with open(log_file, \"r\") as f:\n                log_file_contents = f.read()\n        time.sleep(0.5)\n    log_file_split = log_file_contents.split(\"\\n\")\n    url_lines = [x for x in log_file_split if \"http\" in x]\n    url = url_lines[0].split(\" \")[3]\n    url = url.replace(\"https://\", \"\")\n    url = url.replace(\"http://\", \"\")\n    url = url.rstrip('/')\n    logging.info(f\"Start-up completed. The Jupyter instance is running at '{url}'.\")\n    logging.info(f\"To stop the job, run 'scancel {slurm_array_job_id}'.\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    constexpr int tensor_in_and_out_dims = 4;\n\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n      OP_REQUIRES(\n          context, input_size[i] >= pooling_ratio_[i],\n          errors::InvalidArgument(\"Pooling ratio is higher than input \"\n                                  \"dimension size for dimension \",\n                                  i, \". Input dim size: \", input_size[i],\n                                  \" pooling ratio: \", pooling_ratio_[i]));\n    }\n    // Output size.\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n\n    // Generate pooling sequence.\n    std::vector<int64_t> row_cum_seq;\n    std::vector<int64_t> col_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    row_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                          &generator, pseudo_random_);\n    col_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                          &generator, pseudo_random_);\n\n    // Prepare output.\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_row_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\n                     1, TensorShape({static_cast<int64_t>(row_cum_seq.size())}),\n                     &output_row_seq_tensor));\n    Tensor* output_col_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\n                     2, TensorShape({static_cast<int64_t>(col_cum_seq.size())}),\n                     &output_col_seq_tensor));\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n    // out_count corresponds to number of elements in each pooling cell.\n    Eigen::Matrix<T, Eigen::Dynamic, 1> out_count(out_mat.cols());\n\n    // Initializes the output tensor and out_count with 0.\n    out_mat.setZero();\n    out_count.setZero();\n\n    auto output_row_seq_flat = output_row_seq_tensor->flat<int64_t>();\n    auto output_col_seq_flat = output_col_seq_tensor->flat<int64_t>();\n\n    // Set output tensors.\n    for (int i = 0; i < row_cum_seq.size(); ++i) {\n      output_row_seq_flat(i) = row_cum_seq[i];\n    }\n\n    for (int i = 0; i < col_cum_seq.size(); ++i) {\n      output_col_seq_flat(i) = col_cum_seq[i];\n    }\n\n    // For both input and output,\n    // 0: batch\n    // 1: row / row\n    // 2: col / col\n    // 3: depth / channel\n    const int64_t row_max = input_size[1] - 1;\n    const int64_t col_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // row sequence.\n      for (int64_t hs = 0; hs < row_cum_seq.size() - 1; ++hs) {\n        // row start and end.\n        const int64_t row_start = row_cum_seq[hs];\n        int64_t row_end =\n            overlapping_ ? row_cum_seq[hs + 1] : row_cum_seq[hs + 1] - 1;\n        row_end = std::min(row_end, row_max);\n\n        // col sequence.\n        for (int64_t ws = 0; ws < col_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // col start and end.\n          const int64_t col_start = col_cum_seq[ws];\n          int64_t col_end =\n              overlapping_ ? col_cum_seq[ws + 1] : col_cum_seq[ws + 1] - 1;\n          col_end = std::min(col_end, col_max);\n          for (int64_t h = row_start; h <= row_end; ++h) {\n            for (int64_t w = col_start; w <= col_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) += in_mat.col(in_offset);\n              out_count(out_offset)++;\n            }\n          }\n        }\n      }\n    }\n    DCHECK_GT(out_count.minCoeff(), 0);\n    out_mat.array().rowwise() /= out_count.transpose().array();\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def cookies(self):\n        \"\"\"The session cookies.\"\"\"\n        return self._cookie_jar.cookies\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _configure_session(cls, app):\n        tempdir = app.config[\"PYLOAD_API\"].get_cachedir()\n        cache_path = os.path.join(tempdir, \"flask\")\n        os.makedirs(cache_path, exist_ok=True)\n\n        app.config[\"SESSION_FILE_DIR\"] = cache_path\n        app.config[\"SESSION_TYPE\"] = \"filesystem\"\n        app.config[\"SESSION_COOKIE_NAME\"] = \"pyload_session\"\n        app.config[\"SESSION_COOKIE_SECURE\"] = app.config[\"PYLOAD_API\"].get_config_value(\"webui\", \"use_ssl\")\n        app.config[\"SESSION_PERMANENT\"] = False\n\n        session_lifetime = max(app.config[\"PYLOAD_API\"].get_config_value(\"webui\", \"session_lifetime\"), 1) * 60\n        app.config[\"PERMANENT_SESSION_LIFETIME\"] = session_lifetime\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def recovery_parser(xml):\n            return parse_xml(BytesIO(xml), recover=True)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "m3ApiRawFunction(m3_wasi_generic_fd_read)\n{\n    m3ApiReturnType  (uint32_t)\n    m3ApiGetArg      (__wasi_fd_t          , fd)\n    m3ApiGetArgMem   (wasi_iovec_t *       , wasi_iovs)\n    m3ApiGetArg      (__wasi_size_t        , iovs_len)\n    m3ApiGetArgMem   (__wasi_size_t *      , nread)\n\n    m3ApiCheckMem(wasi_iovs,    iovs_len * sizeof(wasi_iovec_t));\n    m3ApiCheckMem(nread,        sizeof(__wasi_size_t));\n\n#if defined(HAS_IOVEC)\n    struct iovec iovs[iovs_len];\n    const void* mem_check = copy_iov_to_host(runtime, _mem, iovs, wasi_iovs, iovs_len);\n    if (mem_check != m3Err_none) {\n        return mem_check;\n    }\n\n    ssize_t ret = readv(fd, iovs, iovs_len);\n    if (ret < 0) { m3ApiReturn(errno_to_wasi(errno)); }\n    m3ApiWriteMem32(nread, ret);\n    m3ApiReturn(__WASI_ERRNO_SUCCESS);\n#else\n    ssize_t res = 0;\n    for (__wasi_size_t i = 0; i < iovs_len; i++) {\n        void* addr = m3ApiOffsetToPtr(m3ApiReadMem32(&wasi_iovs[i].buf));\n        size_t len = m3ApiReadMem32(&wasi_iovs[i].buf_len);\n        if (len == 0) continue;\n        m3ApiCheckMem(addr,     len);\n        int ret = read (fd, addr, len);\n        if (ret < 0) m3ApiReturn(errno_to_wasi(errno));\n        res += ret;\n        if ((size_t)ret < len) break;\n    }\n    m3ApiWriteMem32(nread, res);\n    m3ApiReturn(__WASI_ERRNO_SUCCESS);\n#endif\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def do_transform(self, request, response, config):\n        self.request = request\n        self.response = response\n        self.config = config\n        self.response += check_update(config)\n        maltego_misp_event = request.entity\n        self.conn = MISPConnection(config, request.parameters)\n        event_id = maltego_misp_event.id\n        search_result = self.conn.misp.search(controller='events', eventid=event_id, with_attachments=False)\n        if search_result:\n            self.event_json = search_result.pop()\n        else:\n            return False\n\n        self.response += event_to_entity(self.event_json)\n        return True\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def handler(req):\n\n    \"\"\"\n    Right now, index serves everything.\n\n    Hitting this URL means we've already cleared authn/authz\n    but we still need to use the token for all remote requests.\n    \"\"\"\n\n    my_user = __get_user(req)\n    my_uri = req.uri\n    sess  = __get_session(req)\n\n    if not sess.has_key('cobbler_token'):\n       # using Kerberos instead of Python Auth handler? \n       # We need to get our own token for use with authn_passthru\n       # which should also be configured in /etc/cobbler/modules.conf\n       # if another auth mode is configured in modules.conf this will\n       # most certaintly fail.\n       try:\n           if not os.path.exists(\"/var/lib/cobbler/web.ss\"):\n               apache.log_error(\"cannot load /var/lib/cobbler/web.ss\")\n               return apache.HTTP_UNAUTHORIZED\n           fd = open(\"/var/lib/cobbler/web.ss\")\n           data = fd.read()\n           my_pw = data\n           fd.close()\n           token = xmlrpc_server.login(my_user,my_pw)\n       except Exception, e:\n           apache.log_error(str(e))\n           return apache.HTTP_UNAUTHORIZED\n       sess['cobbler_token'] = token\n    else:\n       token = sess['cobbler_token']\n\n    # needed?\n    # usage later\n    req.add_common_vars()\n \n    # process form and qs data, if any\n    fs = util.FieldStorage(req)\n    form = {}\n    for x in fs.keys():\n        form[x] = str(fs.get(x,'default'))\n\n    fd = open(\"/etc/cobbler/settings\")\n    data = fd.read()\n    fd.close()\n    ydata = yaml.safe_load(data)\n    remote_port = ydata.get(\"xmlrpc_port\", 25151)\n\n    mode = form.get('mode','index')\n\n    # instantiate a CobblerWeb object\n    cw = CobblerWeb.CobblerWeb(\n         apache   = apache,\n         token    = token, \n         base_url = \"/cobbler/web/\",\n         mode     = mode,\n         server   = \"http://127.0.0.1:%s\" % remote_port\n    )\n\n    # check for a valid path/mode\n    # handle invalid paths gracefully\n    if mode in cw.modes():\n        func = getattr( cw, mode )\n        content = func( **form )\n    else:\n        func = getattr( cw, 'error_page' )\n        content = func( \"Invalid Mode: \\\"%s\\\"\" % mode )\n\n    if content.startswith(\"# REDIRECT \"):\n        util.redirect(req, location=content[11:], permanent=False)\n    else:\n        # apache.log_error(\"%s:%s ... %s\" % (my_user, my_uri, str(form)))\n        req.content_type = \"text/html;charset=utf-8\"\n        req.write(unicode(content).encode('utf-8'))\n    \n    if not content.startswith(\"# ERROR\") and content.find(\"<!-- ERROR -->\") == -1:\n       return apache.OK\n    else:\n       # catch Cheetah errors and web errors\n       return apache.HTTP_INTERNAL_SERVER_ERROR\n "}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(\n        self,\n        targets=\"127.0.0.1\",\n        options=\"-sT\",\n        event_callback=None,\n        safe_mode=True,\n        fqp=None,\n    ):\n        \"\"\"\n        Constructor of NmapProcess class.\n\n        :param targets: hosts to be scanned. Could be a string of hosts \\\n        separated with a coma or a python list of hosts/ip.\n        :type targets: string or list\n\n        :param options: list of nmap options to be applied to scan. \\\n        These options are all documented in nmap's man pages.\n\n        :param event_callback: callable function which will be ran \\\n        each time nmap process outputs data. This function will receive \\\n        two parameters:\n\n            1. the nmap process object\n            2. the data produced by nmap process. See readme for examples.\n\n        :param safe_mode: parameter to protect unsafe options like -oN, -oG, \\\n        -iL, -oA,...\n\n        :param fqp: full qualified path, if None, nmap will be searched \\\n        in the PATH\n\n        :return: NmapProcess object\n\n        \"\"\"\n        Thread.__init__(self)\n        unsafe_opts = set(\n            [\n                \"-oG\",\n                \"-oN\",\n                \"-iL\",\n                \"-oA\",\n                \"-oS\",\n                \"-oX\",\n                \"--iflist\",\n                \"--resume\",\n                \"--stylesheet\",\n                \"--datadir\",\n            ]\n        )\n        # more reliable than just using os.name() (cygwin)\n        self.__is_windows = platform.system() == \"Windows\"\n        if fqp:\n            if os.path.isfile(fqp) and os.access(fqp, os.X_OK):\n                self.__nmap_binary = fqp\n            else:\n                raise EnvironmentError(1, \"wrong path or not executable\", fqp)\n        else:\n            nmap_binary_name = \"nmap\"\n            self.__nmap_binary = self._whereis(nmap_binary_name)\n        self.__nmap_fixed_options = \"-oX - -vvv --stats-every 1s\"\n\n        if self.__nmap_binary is None:\n            em = \"nmap is not installed or could not be found in system path\"\n            raise EnvironmentError(1, em)\n\n        if isinstance(targets, str):\n            self.__nmap_targets = targets.replace(\" \", \"\").split(\",\")\n        elif isinstance(targets, list):\n            self.__nmap_targets = targets\n        else:\n            raise Exception(\n                \"Supplied target list should be either a string or a list\"\n            )\n\n        for target in self.__nmap_targets:\n            self.__validate_target(target)\n\n        self._nmap_options = set(options.split())\n        if safe_mode and not self._nmap_options.isdisjoint(unsafe_opts):\n            raise Exception(\n                \"unsafe options activated while safe_mode is set True\"\n            )\n        self.__nmap_dynamic_options = options\n        self.__sudo_run = \"\"\n        self.__nmap_command_line = self.get_command_line()\n\n        if event_callback and callable(event_callback):\n            self.__nmap_event_callback = event_callback\n        else:\n            self.__nmap_event_callback = None\n        (\n            self.DONE,\n            self.READY,\n            self.RUNNING,\n            self.CANCELLED,\n            self.FAILED,\n        ) = range(5)\n        self._run_init()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def do_template(self, data, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, disable_lookups=False):\n        if USE_JINJA2_NATIVE and not isinstance(data, string_types):\n            return data\n\n        # For preserving the number of input newlines in the output (used\n        # later in this method)\n        data_newlines = _count_newlines_from_end(data)\n\n        if fail_on_undefined is None:\n            fail_on_undefined = self._fail_on_undefined_errors\n\n        try:\n            # allows template header overrides to change jinja2 options.\n            if overrides is None:\n                myenv = self.environment.overlay()\n            else:\n                myenv = self.environment.overlay(overrides)\n\n            # Get jinja env overrides from template\n            if hasattr(data, 'startswith') and data.startswith(JINJA2_OVERRIDE):\n                eol = data.find('\\n')\n                line = data[len(JINJA2_OVERRIDE):eol]\n                data = data[eol + 1:]\n                for pair in line.split(','):\n                    (key, val) = pair.split(':')\n                    key = key.strip()\n                    setattr(myenv, key, ast.literal_eval(val.strip()))\n\n            # Adds Ansible custom filters and tests\n            myenv.filters.update(self._get_filters())\n            for k in myenv.filters:\n                if not getattr(myenv.filters[k], '__UNROLLED__', False):\n                    myenv.filters[k] = _unroll_iterator(myenv.filters[k])\n            myenv.tests.update(self._get_tests())\n\n            if escape_backslashes:\n                # Allow users to specify backslashes in playbooks as \"\\\\\" instead of as \"\\\\\\\\\".\n                data = _escape_backslashes(data, myenv)\n\n            try:\n                t = myenv.from_string(data)\n            except TemplateSyntaxError as e:\n                raise AnsibleError(\"template error while templating string: %s. String: %s\" % (to_native(e), to_native(data)))\n            except Exception as e:\n                if 'recursion' in to_native(e):\n                    raise AnsibleError(\"recursive loop detected in template string: %s\" % to_native(data))\n                else:\n                    return data\n\n            if disable_lookups:\n                t.globals['query'] = t.globals['q'] = t.globals['lookup'] = self._fail_lookup\n\n            jvars = AnsibleJ2Vars(self, t.globals)\n\n            self.cur_context = new_context = t.new_context(jvars, shared=True)\n            rf = t.root_render_func(new_context)\n\n            try:\n                res = j2_concat(rf)\n                unsafe = getattr(new_context, 'unsafe', False)\n                if unsafe:\n                    res = wrap_var(res)\n            except TypeError as te:\n                if 'AnsibleUndefined' in to_native(te):\n                    errmsg = \"Unable to look up a name or access an attribute in template string (%s).\\n\" % to_native(data)\n                    errmsg += \"Make sure your variable name does not contain invalid characters like '-': %s\" % to_native(te)\n                    raise AnsibleUndefinedVariable(errmsg)\n                else:\n                    display.debug(\"failing because of a type error, template data is: %s\" % to_text(data))\n                    raise AnsibleError(\"Unexpected templating type error occurred on (%s): %s\" % (to_native(data), to_native(te)))\n\n            if USE_JINJA2_NATIVE and not isinstance(res, string_types):\n                return res\n\n            if preserve_trailing_newlines:\n                # The low level calls above do not preserve the newline\n                # characters at the end of the input data, so we use the\n                # calculate the difference in newlines and append them\n                # to the resulting output for parity\n                #\n                # jinja2 added a keep_trailing_newline option in 2.7 when\n                # creating an Environment.  That would let us make this code\n                # better (remove a single newline if\n                # preserve_trailing_newlines is False).  Once we can depend on\n                # that version being present, modify our code to set that when\n                # initializing self.environment and remove a single trailing\n                # newline here if preserve_newlines is False.\n                res_newlines = _count_newlines_from_end(res)\n                if data_newlines > res_newlines:\n                    res += self.environment.newline_sequence * (data_newlines - res_newlines)\n                    if unsafe:\n                        res = wrap_var(res)\n            return res\n        except (UndefinedError, AnsibleUndefinedVariable) as e:\n            if fail_on_undefined:\n                raise AnsibleUndefinedVariable(e)\n            else:\n                display.debug(\"Ignoring undefined failure: %s\" % to_text(e))\n                return data\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render(self, name, value, attrs=None):\n        html = super(AdminURLFieldWidget, self).render(name, value, attrs)\n        if value:\n            value = force_text(self._format_value(value))\n            final_attrs = {'href': smart_urlquote(value)}\n            html = format_html(\n                '<p class=\"url\">{0} <a{1}>{2}</a><br />{3} {4}</p>',\n                _('Currently:'), flatatt(final_attrs), value,\n                _('Change:'), html\n            )\n        return html\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def resolve_rpsl_object_journal(rpsl_object, info: GraphQLResolveInfo):\n    \"\"\"\n    Resolve a journal subquery on an RPSL object.\n    \"\"\"\n    database_handler = info.context['request'].app.state.database_handler\n    access_list = f\"sources.{rpsl_object['source']}.nrtm_access_list\"\n    if not is_client_permitted(info.context['request'].client.host, access_list):\n        raise GraphQLError(f\"Access to journal denied for source {rpsl_object['source']}\")\n\n    query = RPSLDatabaseJournalQuery()\n    query.sources([rpsl_object['source']]).rpsl_pk(rpsl_object['rpslPk'])\n    for row in database_handler.execute_query(query, refresh_on_error=True):\n        response = {snake_to_camel_case(k): v for k, v in row.items()}\n        response['operation'] = response['operation'].name\n        if response['origin']:\n            response['origin'] = response['origin'].name\n        if response['objectText']:\n            response['objectText'] = remove_auth_hashes(response['objectText'])\n        yield response\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def tx_svcs(self) -> bool:\n        aname = re.sub(\"[^0-9a-zA-Z]+\", \"\", self.args.name) or \"a\"\n        ep = self.host\n        host = ep.split(\":\")[0]\n        hport = ep[ep.find(\":\") :] if \":\" in ep else \"\"\n        rip = (\n            host\n            if self.args.rclone_mdns or not self.args.zm\n            else self.conn.hsrv.nm.map(self.ip) or host\n        )\n        # safer than html_escape/quotep since this avoids both XSS and shell-stuff\n        pw = re.sub(r\"[<>&$?`]\", \"_\", self.pw or \"pw\")\n        vp = re.sub(r\"[<>&$?`]\", \"_\", self.uparam[\"hc\"] or \"\").lstrip(\"/\")\n        html = self.j2s(\n            \"svcs\",\n            args=self.args,\n            accs=bool(self.asrv.acct),\n            s=\"s\" if self.is_https else \"\",\n            rip=rip,\n            ep=ep,\n            vp=vp,\n            rvp=vjoin(self.args.R, vp),\n            host=host,\n            hport=hport,\n            aname=aname,\n            pw=pw,\n        )\n        self.reply(html.encode(\"utf-8\"))\n        return True\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor* input_indices;\n    const Tensor* input_values;\n    const Tensor* input_shape;\n    SparseTensorsMap* map;\n\n    OP_REQUIRES_OK(context, context->input(\"sparse_indices\", &input_indices));\n    OP_REQUIRES_OK(context, context->input(\"sparse_values\", &input_values));\n    OP_REQUIRES_OK(context, context->input(\"sparse_shape\", &input_shape));\n    OP_REQUIRES_OK(context, GetMap(context, true /* is_writing */, &map));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_indices->shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be a matrix but received shape \",\n                    input_indices->shape().DebugString()));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_values->shape()),\n                errors::InvalidArgument(\n                    \"Input values should be a vector but received shape \",\n                    input_values->shape().DebugString()));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shape should be a vector but received shape \",\n                    input_shape->shape().DebugString()));\n\n    int rank = input_shape->NumElements();\n\n    OP_REQUIRES(\n        context, rank > 1,\n        errors::InvalidArgument(\n            \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\n\n    auto input_shape_vec = input_shape->vec<int64>();\n    int new_num_elements = 1;\n    bool overflow_ocurred = false;\n    for (int i = 0; i < input_shape_vec.size(); i++) {\n      new_num_elements =\n          MultiplyWithoutOverflow(new_num_elements, input_shape_vec(i));\n      if (new_num_elements < 0) {\n        overflow_ocurred = true;\n      }\n    }\n\n    OP_REQUIRES(\n        context, !overflow_ocurred,\n        errors::Internal(\"Encountered overflow from large input shape.\"));\n\n    TensorShape tensor_input_shape(input_shape_vec);\n    gtl::InlinedVector<int64, 8> std_order(rank);\n    std::iota(std_order.begin(), std_order.end(), 0);\n    SparseTensor input_st;\n    OP_REQUIRES_OK(context, SparseTensor::Create(*input_indices, *input_values,\n                                                 tensor_input_shape, std_order,\n                                                 &input_st));\n\n    const int64 N = input_shape_vec(0);\n\n    Tensor sparse_handles(DT_INT64, TensorShape({N}));\n    auto sparse_handles_t = sparse_handles.vec<int64>();\n\n    OP_REQUIRES_OK(context, input_st.IndicesValid());\n\n    // We can generate the output shape proto string now, for all\n    // minibatch entries.\n    TensorShape output_shape;\n    OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\n                                input_shape_vec.data() + 1,\n                                input_shape->NumElements() - 1, &output_shape));\n\n    // Get groups by minibatch dimension\n    std::unordered_set<int64> visited;\n    sparse::GroupIterable minibatch = input_st.group({0});\n    for (const auto& subset : minibatch) {\n      const int64 b = subset.group()[0];\n      visited.insert(b);\n      OP_REQUIRES(\n          context, b > -1 && b < N,\n          errors::InvalidArgument(\n              \"Received unexpected column 0 value in input SparseTensor: \", b,\n              \" < 0 or >= N (= \", N, \")\"));\n\n      const auto indices = subset.indices();\n      const auto values = subset.values<T>();\n      const int64 num_entries = values.size();\n\n      Tensor output_indices = Tensor(DT_INT64, {num_entries, rank - 1});\n      Tensor output_values = Tensor(DataTypeToEnum<T>::value, {num_entries});\n\n      auto output_indices_t = output_indices.matrix<int64>();\n      auto output_values_t = output_values.vec<T>();\n\n      for (int i = 0; i < num_entries; ++i) {\n        for (int d = 1; d < rank; ++d) {\n          output_indices_t(i, d - 1) = indices(i, d);\n        }\n        output_values_t(i) = values(i);\n      }\n\n      SparseTensor st_i;\n      OP_REQUIRES_OK(context,\n                     SparseTensor::Create(output_indices, output_values,\n                                          output_shape, &st_i));\n      int64 handle;\n      OP_REQUIRES_OK(context, map->AddSparseTensor(context, st_i, &handle));\n      sparse_handles_t(b) = handle;\n    }\n\n    // Fill in any gaps; we must provide an empty ST for batch entries\n    // the grouper didn't find.\n    if (visited.size() < N) {\n      Tensor empty_indices(DT_INT64, {0, rank - 1});\n      Tensor empty_values(DataTypeToEnum<T>::value, {0});\n      SparseTensor empty_st;\n      OP_REQUIRES_OK(context, SparseTensor::Create(empty_indices, empty_values,\n                                                   output_shape, &empty_st));\n\n      for (int64 b = 0; b < N; ++b) {\n        // We skipped this batch entry.\n        if (visited.find(b) == visited.end()) {\n          int64 handle;\n          OP_REQUIRES_OK(context,\n                         map->AddSparseTensor(context, empty_st, &handle));\n          sparse_handles_t(b) = handle;\n        }\n      }\n    }\n\n    context->set_output(0, sparse_handles);\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    // Read ragged_splits inputs.\n    OpInputList ragged_nested_splits_in;\n    OP_REQUIRES_OK(context, context->input_list(\"rt_nested_splits\",\n                                                &ragged_nested_splits_in));\n    const int ragged_nested_splits_len = ragged_nested_splits_in.size();\n    RaggedTensorVariant batched_ragged_input;\n    // Read ragged_values input.\n    batched_ragged_input.set_values(context->input(ragged_nested_splits_len));\n    batched_ragged_input.mutable_nested_splits()->reserve(\n        ragged_nested_splits_len);\n    for (int i = 0; i < ragged_nested_splits_len; i++) {\n      batched_ragged_input.append_splits(ragged_nested_splits_in[i]);\n    }\n\n    if (!batched_input_) {\n      // Encode as a Scalar Variant Tensor.\n      Tensor* encoded_scalar;\n      OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}),\n                                                       &encoded_scalar));\n      encoded_scalar->scalar<Variant>()() = std::move(batched_ragged_input);\n      return;\n    }\n\n    // Checked here instead of at input in case batched_input_ is false\n    OP_REQUIRES(context, ragged_nested_splits_len > 0,\n                errors::InvalidArgument(\n                    \"rt_nested_splits must be a list of one or more, but \"\n                    \"received rt_nested_splits of length 0.\"));\n\n    // Unbatch the Ragged Tensor and encode the components.\n    std::vector<RaggedTensorVariant> unbatched_ragged_input;\n    auto batched_splits_top_vec =\n        batched_ragged_input.splits(0).vec<SPLIT_TYPE>();\n    int num_components = batched_splits_top_vec.size() - 1;\n    OP_REQUIRES(context, num_components >= 0,\n                errors::Internal(\"Invalid split argument.\"));\n    OP_REQUIRES_OK(context, UnbatchRaggedZerothDim<VALUE_TYPE, SPLIT_TYPE>(\n                                batched_ragged_input, &unbatched_ragged_input));\n\n    // Bundle the encoded scalar Variant Tensors into a rank-1 Variant Tensor.\n    Tensor* encoded_vector;\n    int output_size = unbatched_ragged_input.size();\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({output_size}),\n                                            &encoded_vector));\n    auto encoded_vector_t = encoded_vector->vec<Variant>();\n    for (int i = 0; i < output_size; i++) {\n      encoded_vector_t(i) = unbatched_ragged_input[i];\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def error_page(self, **kwargs):\n        \"\"\"\n        Default error page shown to the user when an unexpected error occur.\n        \"\"\"\n        # Log exception.\n        logger.error(\n            'error page: %s %s\\n%s' % (kwargs.get('status', ''), kwargs.get('message', ''), kwargs.get('traceback', ''))\n        )\n\n        # Replace message by generic one for 404. Default implementation leak path info.\n        if kwargs.get('status', '') == '404 Not Found':\n            kwargs['message'] = 'Nothing matches the given URI'\n\n        # Check expected response type.\n        mtype = cherrypy.tools.accept.callable(['text/html', 'text/plain'])  # @UndefinedVariable\n        if mtype == 'text/plain':\n            return kwargs.get('message')\n\n        # Try to build a nice error page.\n        try:\n            page = Controller()\n            return page._compile_template('error_page_default.html', **kwargs)\n        except Exception:\n            # If failing, send the raw error message.\n            return kwargs.get('message')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def explain_query_prefix(self, format=None, **options):\n        extra = {}\n        # Normalize options.\n        if options:\n            options = {\n                name.upper(): \"true\" if value else \"false\"\n                for name, value in options.items()\n            }\n            for valid_option in self.explain_options:\n                value = options.pop(valid_option, None)\n                if value is not None:\n                    extra[valid_option.upper()] = value\n        prefix = super().explain_query_prefix(format, **options)\n        if format:\n            extra[\"FORMAT\"] = format\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_oauth2_authorisation_code(permission_url, redirect_uri, redirect_listen_address, username):\n        \"\"\"Submit an authorisation request to the parent app and block until it is provided (or the request fails)\"\"\"\n        token_request = {'permission_url': permission_url, 'redirect_uri': redirect_uri,\n                         'redirect_listen_address': redirect_listen_address, 'username': username, 'expired': False}\n        REQUEST_QUEUE.put(token_request)\n        wait_time = 0\n        while True:\n            try:\n                data = RESPONSE_QUEUE.get(block=True, timeout=1)\n            except queue.Empty:\n                wait_time += 1\n                if wait_time < AUTHENTICATION_TIMEOUT:\n                    continue\n                else:\n                    token_request['expired'] = True\n                    REQUEST_QUEUE.put(token_request)  # re-insert the request as expired so the parent app can remove it\n                    return False, None\n\n            if data is QUEUE_SENTINEL:  # app is closing\n                RESPONSE_QUEUE.put(QUEUE_SENTINEL)  # make sure all watchers exit\n                return False, None\n\n            elif data['permission_url'] == permission_url and data['username'] == username:  # a response meant for us\n                # to improve no-GUI mode we also support the use of a local server to receive the OAuth redirection\n                # (note: not enabled by default because no-GUI mode is typically unattended, but useful in some cases)\n                if 'expired' in data and data['expired']:  # local server auth wsgi request error or failure\n                    return False, None\n\n                elif 'local_server_auth' in data:\n                    threading.Thread(target=OAuth2Helper.start_redirection_receiver_server, args=(data,),\n                                     name='EmailOAuth2Proxy-auth-%s' % data['username'], daemon=True).start()\n\n                else:\n                    if 'response_url' in data and 'code=' in data['response_url'] and data['response_url'].startswith(\n                            token_request['redirect_uri']):\n                        authorisation_code = OAuth2Helper.oauth2_url_unescape(\n                            data['response_url'].split('code=')[1].split('&')[0])\n                        if authorisation_code:\n                            return True, authorisation_code\n                    return False, None\n\n            else:  # not for this thread - put back into queue\n                RESPONSE_QUEUE.put(data)\n                time.sleep(1)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus GatherNd(TfLiteContext* context, const TfLiteTensor* params,\n                      const TfLiteTensor* indices, TfLiteTensor* output) {\n  const TfLiteStatus status = reference_ops::GatherNd(\n      GetTensorShape(params), GetTensorData<ParamsT>(params),\n      GetTensorShape(indices), GetTensorData<IndicesT>(indices),\n      GetTensorShape(output), GetTensorData<ParamsT>(output));\n  if (status != kTfLiteOk) {\n    TF_LITE_KERNEL_LOG(context, \"gather_nd index out of bounds\");\n  }\n  return status;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def urlize_quoted_links(text, trim_url_limit=None, nofollow=True, autoescape=True):\n    \"\"\"\n    Converts any URLs in text into clickable links.\n\n    Works on http://, https://, www. links, and also on links ending in one of\n    the original seven gTLDs (.com, .edu, .gov, .int, .mil, .net, and .org).\n    Links can have trailing punctuation (periods, commas, close-parens) and\n    leading punctuation (opening parens) and it'll still do the right thing.\n\n    If trim_url_limit is not None, the URLs in link text longer than this limit\n    will truncated to trim_url_limit-3 characters and appended with an ellipsis.\n\n    If nofollow is True, the URLs in link text will get a rel=\"nofollow\"\n    attribute.\n\n    If autoescape is True, the link text and URLs will get autoescaped.\n    \"\"\"\n    def trim_url(x, limit=trim_url_limit):\n        return limit is not None and (len(x) > limit and ('%s...' % x[:max(0, limit - 3)])) or x\n\n    safe_input = isinstance(text, SafeData)\n\n    # Unfortunately, Django built-in cannot be used here, because escaping\n    # is to be performed on words, which have been forcibly coerced to text\n    def conditional_escape(text):\n        return escape(text) if autoescape and not safe_input else text\n\n    words = word_split_re.split(force_text(text))\n    for i, word in enumerate(words):\n        if '.' in word or '@' in word or ':' in word:\n            # Deal with punctuation.\n            lead, middle, trail = '', word, ''\n            for punctuation in TRAILING_PUNCTUATION:\n                if middle.endswith(punctuation):\n                    middle = middle[:-len(punctuation)]\n                    trail = punctuation + trail\n            for opening, closing in WRAPPING_PUNCTUATION:\n                if middle.startswith(opening):\n                    middle = middle[len(opening):]\n                    lead = lead + opening\n                # Keep parentheses at the end only if they're balanced.\n                if (\n                    middle.endswith(closing) and\n                    middle.count(closing) == middle.count(opening) + 1\n                ):\n                    middle = middle[:-len(closing)]\n                    trail = closing + trail\n\n            # Make URL we want to point to.\n            url = None\n            nofollow_attr = ' rel=\"nofollow\"' if nofollow else ''\n            if simple_url_re.match(middle):\n                url = smart_urlquote_wrapper(middle)\n            elif simple_url_2_re.match(middle):\n                url = smart_urlquote_wrapper('http://%s' % middle)\n            elif ':' not in middle and simple_email_re.match(middle):\n                local, domain = middle.rsplit('@', 1)\n                try:\n                    domain = domain.encode('idna').decode('ascii')\n                except UnicodeError:\n                    continue\n                url = 'mailto:%s@%s' % (local, domain)\n                nofollow_attr = ''\n\n            # Make link.\n            if url:\n                trimmed = trim_url(middle)\n                lead, trail = conditional_escape(lead), conditional_escape(trail)\n                url, trimmed = conditional_escape(url), conditional_escape(trimmed)\n                middle = '<a href=\"%s\"%s>%s</a>' % (url, nofollow_attr, trimmed)\n                words[i] = '%s%s%s' % (lead, middle, trail)\n            else:\n                words[i] = conditional_escape(word)\n        else:\n            words[i] = conditional_escape(word)\n    return mark_safe(''.join(words))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  function uploadFiles (form, fileInput, name) {\n    var url = fileInput.getAttribute('data-url')\n    fileInput.loaded = 0\n    fileInput.total = 0\n    var promises = Array.from(fileInput.files).map(function (file) {\n      form.total += file.size\n      fileInput.total += file.size\n      var s3Form = new window.FormData()\n      Array.from(fileInput.attributes).forEach(function (attr) {\n        var name = attr.name\n\n        if (name.startsWith('data-fields')) {\n          name = name.replace('data-fields-', '')\n          s3Form.append(name, attr.value)\n        }\n      })\n      s3Form.append('success_action_status', '201')\n      s3Form.append('Content-Type', file.type)\n      s3Form.append('file', file)\n      return request('POST', url, s3Form, fileInput, file, form)\n    })\n    Promise.all(promises).then(function (results) {\n      results.forEach(function (result) {\n        var hiddenFileInput = document.createElement('input')\n        hiddenFileInput.type = 'hidden'\n        hiddenFileInput.name = name\n        hiddenFileInput.value = parseURL(result)\n        form.appendChild(hiddenFileInput)\n        var hiddenSignatureInput = document.createElement('input')\n        hiddenSignatureInput.type = 'hidden'\n        hiddenSignatureInput.name = name + '-s3f-signature'\n        console.log(fileInput.dataset.s3fSignature)\n        hiddenSignatureInput.value = fileInput.dataset.s3fSignature\n        form.appendChild(hiddenSignatureInput)\n      })\n      fileInput.name = ''\n      window.uploading -= 1\n    }, function (err) {\n      console.log(err)\n      fileInput.setCustomValidity(err)\n      fileInput.reportValidity()\n    })\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def make_byte_array_copier(dst, src):\n    assert isinstance(src.typ, _BytestringT)\n    assert isinstance(dst.typ, _BytestringT)\n\n    _check_assign_bytes(dst, src)\n\n    # TODO: remove this branch, copy_bytes and get_bytearray_length should handle\n    if src.value == \"~empty\":\n        # set length word to 0.\n        return STORE(dst, 0)\n\n    with src.cache_when_complex(\"src\") as (b1, src):\n        with get_bytearray_length(src).cache_when_complex(\"len\") as (b2, len_):\n            max_bytes = src.typ.maxlen\n\n            ret = [\"seq\"]\n\n            dst_ = bytes_data_ptr(dst)\n            src_ = bytes_data_ptr(src)\n\n            ret.append(copy_bytes(dst_, src_, len_, max_bytes))\n\n            # store length\n            ret.append(STORE(dst, len_))\n\n            return b1.resolve(b2.resolve(ret))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _reverse_with_prefix(self, lookup_view, _prefix, *args, **kwargs):\n        if args and kwargs:\n            raise ValueError(\"Don't mix *args and **kwargs in call to reverse()!\")\n        text_args = [force_text(v) for v in args]\n        text_kwargs = dict((k, force_text(v)) for (k, v) in kwargs.items())\n\n        if not self._populated:\n            self._populate()\n\n        try:\n            if lookup_view in self._callback_strs:\n                lookup_view = get_callable(lookup_view, True)\n        except (ImportError, AttributeError) as e:\n            raise NoReverseMatch(\"Error importing '%s': %s.\" % (lookup_view, e))\n        possibilities = self.reverse_dict.getlist(lookup_view)\n\n        prefix_norm, prefix_args = normalize(urlquote(_prefix))[0]\n        for possibility, pattern, defaults in possibilities:\n            for result, params in possibility:\n                if args:\n                    if len(args) != len(params) + len(prefix_args):\n                        continue\n                    candidate_subs = dict(zip(prefix_args + params, text_args))\n                else:\n                    if set(kwargs.keys()) | set(defaults.keys()) != set(params) | set(defaults.keys()) | set(prefix_args):\n                        continue\n                    matches = True\n                    for k, v in defaults.items():\n                        if kwargs.get(k, v) != v:\n                            matches = False\n                            break\n                    if not matches:\n                        continue\n                    candidate_subs = text_kwargs\n                # WSGI provides decoded URLs, without %xx escapes, and the URL\n                # resolver operates on such URLs. First substitute arguments\n                # without quoting to build a decoded URL and look for a match.\n                # Then, if we have a match, redo the substitution with quoted\n                # arguments in order to return a properly encoded URL.\n                candidate_pat = prefix_norm.replace('%', '%%') + result\n                if re.search('^%s%s' % (prefix_norm, pattern), candidate_pat % candidate_subs, re.UNICODE):\n                    candidate_subs = dict((k, urlquote(v)) for (k, v) in candidate_subs.items())\n                    url = candidate_pat % candidate_subs\n                    # Don't allow construction of scheme relative urls.\n                    if url.startswith('//'):\n                        url = '/%%2F%s' % url[2:]\n                    return url\n        # lookup_view can be URL label, or dotted path, or callable, Any of\n        # these can be passed in at the top, but callables are not friendly in\n        # error messages.\n        m = getattr(lookup_view, '__module__', None)\n        n = getattr(lookup_view, '__name__', None)\n        if m is not None and n is not None:\n            lookup_view_s = \"%s.%s\" % (m, n)\n        else:\n            lookup_view_s = lookup_view\n\n        patterns = [pattern for (possibility, pattern, defaults) in possibilities]\n        raise NoReverseMatch(\"Reverse for '%s' with arguments '%s' and keyword \"\n                \"arguments '%s' not found. %d pattern(s) tried: %s\" %\n                             (lookup_view_s, args, kwargs, len(patterns), patterns))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    OP_REQUIRES_OK_ASYNC(c,\n                         FillCollectiveParams(col_params, REDUCTION_COLLECTIVE,\n                                              /*group_size*/ c->input(1),\n                                              /*group_key*/ c->input(2),\n                                              /*instance_key*/ c->input(3)),\n                         done_with_cleanup);\n    col_params->instance.shape = c->input(0).shape();\n    col_params->merge_op = merge_op_.get();\n    col_params->final_op = final_op_.get();\n    VLOG(1) << \"CollectiveReduceV2 group_size \" << col_params->group.group_size\n            << \" group_key \" << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    // Allocate the output tensor, trying to reuse the input.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(c,\n                         c->forward_input_or_allocate_output(\n                             {0}, 0, col_params->instance.shape, &output),\n                         done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def idp_initiated_logout(self):\n        \"\"\"\n        Logout all SP sessions when the logout comes from the IdP.\n\n        For the current user only.\n\n        Only use HTTP-Redirect to start the logout. This is guaranteed\n        to be supported in SAML 2.\n        \"\"\"\n        self.debug(\"IdP-initiated SAML2 logout\")\n        us = UserSession()\n        user = us.get_user()\n\n        saml_sessions = self.sessionfactory\n        # pylint: disable=unused-variable\n        (mech, session) = saml_sessions.get_next_logout(\n            logout_mechs=[lasso.SAML2_METADATA_BINDING_REDIRECT], user=us.user)\n        if session is None:\n            return\n\n        logout = self.idp.get_logout_handler()\n        logout.setSessionFromDump(session.login_session)\n        try:\n            logout.initRequest(session.provider_id)\n        except lasso.ServerProviderNotFoundError:\n            self.error(\n                'Service Provider %s not found. Trying next session' %\n                session.provider_id\n            )\n            return self.idp_initiated_logout()\n        except lasso.ProfileUnsupportedProfileError:\n            self.error(\n                'Service Provider %s does not support Single Logout. '\n                'Trying next session' %\n                session.provider_id\n            )\n            return self.idp_initiated_logout()\n        try:\n            logout.buildRequestMsg()\n        except lasso.Error, e:\n            self.error('failure to build logout request msg: %s' % e)\n            raise cherrypy.HTTPRedirect(400, 'Failed to log out user: %s '\n                                        % e)\n\n        # Add a fake session to indicate where the user should\n        # be redirected to when all SP's are logged out.\n        idpurl = self._root.instance_base_url()\n        session_id = \"_\" + uuid.uuid4().hex.upper()\n        saml_sessions.add_session(session_id, idpurl, user.name, \"\", \"\",\n                                  [lasso.SAML2_METADATA_BINDING_REDIRECT])\n        init_session = saml_sessions.get_session_by_id(session_id)\n        saml_sessions.start_logout(init_session, relaystate=idpurl)\n\n        # Add the logout request id we just created to the session to be\n        # logged out so that when it responds we can find the right\n        # session.\n        session.set_logoutstate(request_id=logout.request.id)\n        saml_sessions.start_logout(session, initial=False)\n\n        self.debug('Sending initial logout request to %s' % logout.msgUrl)\n        raise cherrypy.HTTPRedirect(logout.msgUrl)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def response_for_exception(request, exc):\n    if isinstance(exc, Http404):\n        if settings.DEBUG:\n            response = debug.technical_404_response(request, exc)\n        else:\n            response = get_exception_response(request, get_resolver(get_urlconf()), 404, exc)\n\n    elif isinstance(exc, PermissionDenied):\n        response = get_exception_response(request, get_resolver(get_urlconf()), 403, exc)\n        log_response(\n            'Forbidden (Permission denied): %s', request.path,\n            response=response,\n            request=request,\n            exc_info=sys.exc_info(),\n        )\n\n    elif isinstance(exc, MultiPartParserError):\n        response = get_exception_response(request, get_resolver(get_urlconf()), 400, exc)\n        log_response(\n            'Bad request (Unable to parse request body): %s', request.path,\n            response=response,\n            request=request,\n            exc_info=sys.exc_info(),\n        )\n\n    elif isinstance(exc, BadRequest):\n        if settings.DEBUG:\n            response = debug.technical_500_response(request, *sys.exc_info(), status_code=400)\n        else:\n            response = get_exception_response(request, get_resolver(get_urlconf()), 400, exc)\n        log_response(\n            '%s: %s', str(exc), request.path,\n            response=response,\n            request=request,\n            exc_info=sys.exc_info(),\n        )\n    elif isinstance(exc, SuspiciousOperation):\n        if isinstance(exc, (RequestDataTooBig, TooManyFieldsSent, TooManyFilesSent)):\n            # POST data can't be accessed again, otherwise the original\n            # exception would be raised.\n            request._mark_post_parse_error()\n\n        # The request logger receives events for any problematic request\n        # The security logger receives events for all SuspiciousOperations\n        security_logger = logging.getLogger('django.security.%s' % exc.__class__.__name__)\n        security_logger.error(\n            str(exc),\n            extra={'status_code': 400, 'request': request},\n        )\n        if settings.DEBUG:\n            response = debug.technical_500_response(request, *sys.exc_info(), status_code=400)\n        else:\n            response = get_exception_response(request, get_resolver(get_urlconf()), 400, exc)\n\n    elif isinstance(exc, SystemExit):\n        # Allow sys.exit() to actually exit. See tickets #1023 and #4701\n        raise\n\n    else:\n        signals.got_request_exception.send(sender=None, request=request)\n        response = handle_uncaught_exception(request, get_resolver(get_urlconf()), sys.exc_info())\n        log_response(\n            '%s: %s', response.reason_phrase, request.path,\n            response=response,\n            request=request,\n            exc_info=sys.exc_info(),\n        )\n\n    # Force a TemplateResponse to be rendered.\n    if not getattr(response, 'is_rendered', True) and callable(getattr(response, 'render', None)):\n        response = response.render()\n\n    return response\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void DecodeGifV2(OpKernelContext* context, StringPiece input) {\n    // GIF has 3 channels.\n    OP_REQUIRES(context, channels_ == 0 || channels_ == 3,\n                errors::InvalidArgument(\"channels must be 0 or 3 for GIF, got \",\n                                        channels_));\n\n    if (op_type_ == \"DecodeBmp\") {\n      // TODO(b/171060723): Only DecodeBmp as op_type_ is not acceptable here\n      // because currently `decode_(jpeg|png|gif)` ops can decode any one of\n      // jpeg, png or gif but not bmp. Similarly, `decode_bmp` cannot decode\n      // anything but bmp formats. This behavior needs to be revisited. For more\n      // details, please refer to the bug.\n      OP_REQUIRES(context, false,\n                  errors::InvalidArgument(\n                      \"Trying to decode GIF format using DecodeBmp op. Use \"\n                      \"`decode_gif` or `decode_image` instead.\"));\n    } else if (op_type_ == \"DecodeAndCropJpeg\") {\n      OP_REQUIRES(context, false,\n                  errors::InvalidArgument(\n                      \"DecodeAndCropJpeg operation can run on JPEG only, but \"\n                      \"detected GIF.\"));\n    }\n\n    // Decode GIF, allocating tensor if dtype is uint8, otherwise defer tensor\n    // allocation til after dtype conversion is done. `gif`::Decode` supports\n    // uint8 only.\n    Tensor* output = nullptr;\n    ptrdiff_t buffer_size = 0;\n    string error_string;\n    uint8* buffer = gif::Decode(\n        input.data(), input.size(),\n        [&](int num_frames, int width, int height, int channels) -> uint8* {\n          buffer_size = ptrdiff_t(num_frames) * height * width * channels;\n\n          Status status;\n          // By the existing API, we support decoding GIF with `decode_jpeg` or\n          // with `decode_png` if the GIF is a single-frame GIF (non-animated).\n          // We need to make sure to return 3-D shapes when using in this case.\n          if (op_type_ == \"DecodePng\" || op_type_ == \"DecodeJpeg\") {\n            if (num_frames == 1) {\n              status = context->allocate_output(\n                  0, TensorShape({height, width, channels}), &output);\n            } else {\n              status = errors::InvalidArgument(\n                  \"Got \", num_frames, \" frames, but animated gifs \",\n                  \"can only be decoded by tf.io.decode_gif or \",\n                  \"tf.io.decode_image\");\n            }\n          } else if (op_type_ == \"DecodeGif\" ||\n                     (op_type_ == \"DecodeImage\" && expand_animations_)) {\n            status = context->allocate_output(\n                0, TensorShape({num_frames, height, width, channels}), &output);\n          } else if (op_type_ == \"DecodeImage\" && !expand_animations_) {\n            status = context->allocate_output(\n                0, TensorShape({height, width, channels}), &output);\n          } else {\n            status = errors::InvalidArgument(\"Bad op type \", op_type_);\n          }\n          if (!status.ok()) {\n            VLOG(1) << status;\n            context->SetStatus(status);\n            return nullptr;\n          }\n\n          if (data_type_ == DataType::DT_UINT8) {\n            return output->flat<uint8>().data();\n          } else {\n            return new uint8[buffer_size];\n          }\n        },\n        &error_string, expand_animations_);\n\n    OP_REQUIRES(context, buffer,\n                errors::InvalidArgument(\"Invalid GIF data (size \", input.size(),\n                                        \"), \", error_string));\n\n    // For when desired data type is uint8, the output buffer is already\n    // allocated during the `gif::Decode` call above; return.\n    if (data_type_ == DataType::DT_UINT8) {\n      return;\n    }\n    // Make sure we don't forget to deallocate `buffer`.\n    std::unique_ptr<uint8[]> buffer_unique_ptr(buffer);\n\n    // Convert the raw uint8 buffer to desired dtype.\n    // Use eigen threadpooling to speed up the copy operation.\n    TTypes<uint8>::UnalignedConstFlat buffer_view(buffer, buffer_size);\n    const auto& device = context->eigen_device<Eigen::ThreadPoolDevice>();\n    if (data_type_ == DataType::DT_UINT16) {\n      uint16 scale = floor((std::numeric_limits<uint16>::max() + 1) /\n                           (std::numeric_limits<uint8>::max() + 1));\n      // Fill output tensor with desired dtype.\n      output->flat<uint16>().device(device) =\n          buffer_view.cast<uint16>() * scale;\n    } else if (data_type_ == DataType::DT_FLOAT) {\n      float scale = 1. / std::numeric_limits<uint8>::max();\n      // Fill output tensor with desired dtype.\n      output->flat<float>().device(device) = buffer_view.cast<float>() * scale;\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _changeform_view(self, request, object_id, form_url, extra_context):\n        to_field = request.POST.get(TO_FIELD_VAR, request.GET.get(TO_FIELD_VAR))\n        if to_field and not self.to_field_allowed(request, to_field):\n            raise DisallowedModelAdminToField(\"The field %s cannot be referenced.\" % to_field)\n\n        model = self.model\n        opts = model._meta\n\n        if request.method == 'POST' and '_saveasnew' in request.POST:\n            object_id = None\n\n        add = object_id is None\n\n        if add:\n            if not self.has_add_permission(request):\n                raise PermissionDenied\n            obj = None\n\n        else:\n            obj = self.get_object(request, unquote(object_id), to_field)\n\n            if request.method == 'POST':\n                if not self.has_change_permission(request, obj):\n                    raise PermissionDenied\n            else:\n                if not self.has_view_or_change_permission(request, obj):\n                    raise PermissionDenied\n\n            if obj is None:\n                return self._get_obj_does_not_exist_redirect(request, opts, object_id)\n\n        ModelForm = self.get_form(request, obj, change=not add)\n        if request.method == 'POST':\n            form = ModelForm(request.POST, request.FILES, instance=obj)\n            form_validated = form.is_valid()\n            if form_validated:\n                new_object = self.save_form(request, form, change=not add)\n            else:\n                new_object = form.instance\n            formsets, inline_instances = self._create_formsets(request, new_object, change=not add)\n            if all_valid(formsets) and form_validated:\n                self.save_model(request, new_object, form, not add)\n                self.save_related(request, form, formsets, not add)\n                change_message = self.construct_change_message(request, form, formsets, add)\n                if add:\n                    self.log_addition(request, new_object, change_message)\n                    return self.response_add(request, new_object)\n                else:\n                    self.log_change(request, new_object, change_message)\n                    return self.response_change(request, new_object)\n            else:\n                form_validated = False\n        else:\n            if add:\n                initial = self.get_changeform_initial_data(request)\n                form = ModelForm(initial=initial)\n                formsets, inline_instances = self._create_formsets(request, form.instance, change=False)\n            else:\n                form = ModelForm(instance=obj)\n                formsets, inline_instances = self._create_formsets(request, obj, change=True)\n\n        if not add and not self.has_change_permission(request, obj):\n            readonly_fields = flatten_fieldsets(self.get_fieldsets(request, obj))\n        else:\n            readonly_fields = self.get_readonly_fields(request, obj)\n        adminForm = helpers.AdminForm(\n            form,\n            list(self.get_fieldsets(request, obj)),\n            # Clear prepopulated fields on a view-only form to avoid a crash.\n            self.get_prepopulated_fields(request, obj) if add or self.has_change_permission(request, obj) else {},\n            readonly_fields,\n            model_admin=self)\n        media = self.media + adminForm.media\n\n        inline_formsets = self.get_inline_formsets(request, formsets, inline_instances, obj)\n        for inline_formset in inline_formsets:\n            media = media + inline_formset.media\n\n        if add:\n            title = _('Add %s')\n        elif self.has_change_permission(request, obj):\n            title = _('Change %s')\n        else:\n            title = _('View %s')\n        context = {\n            **self.admin_site.each_context(request),\n            'title': title % opts.verbose_name,\n            'adminform': adminForm,\n            'object_id': object_id,\n            'original': obj,\n            'is_popup': IS_POPUP_VAR in request.POST or IS_POPUP_VAR in request.GET,\n            'to_field': to_field,\n            'media': media,\n            'inline_admin_formsets': inline_formsets,\n            'errors': helpers.AdminErrorList(form, formsets),\n            'preserved_filters': self.get_preserved_filters(request),\n        }\n\n        # Hide the \"Save\" and \"Save and continue\" buttons if \"Save as New\" was\n        # previously chosen to prevent the interface from getting confusing.\n        if request.method == 'POST' and not form_validated and \"_saveasnew\" in request.POST:\n            context['show_save'] = False\n            context['show_save_and_continue'] = False\n            # Use the change template instead of the add template.\n            add = False\n\n        context.update(extra_context or {})\n\n        return self.render_change_form(request, context, add=add, change=not add, obj=obj, form_url=form_url)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def load_tasks(request, project):\n    \"\"\" Load tasks from different types of request.data / request.files\n    \"\"\"\n    file_upload_ids, found_formats, data_keys = [], [], set()\n    could_be_tasks_lists = False\n\n    # take tasks from request FILES\n    if len(request.FILES):\n        check_file_sizes_and_number(request.FILES)\n        for filename, file in request.FILES.items():\n            file_upload = create_file_upload(request, project, file)\n            if file_upload.format_could_be_tasks_list:\n                could_be_tasks_lists = True\n            file_upload_ids.append(file_upload.id)\n        tasks, found_formats, data_keys = FileUpload.load_tasks_from_uploaded_files(project, file_upload_ids)\n\n    # take tasks from url address\n    elif 'application/x-www-form-urlencoded' in request.content_type:\n        # empty url\n        url = request.data.get('url')\n        if not url:\n            raise ValidationError('\"url\" is not found in request data')\n\n        # try to load json with task or tasks from url as string\n        json_data = str_to_json(url)\n        if json_data:\n            file_upload = create_file_upload(request, project, SimpleUploadedFile('inplace.json', url.encode()))\n            file_upload_ids.append(file_upload.id)\n            tasks, found_formats, data_keys = FileUpload.load_tasks_from_uploaded_files(project, file_upload_ids)\n            \n        # download file using url and read tasks from it\n        else:\n            if settings.SSRF_PROTECTION_ENABLED and url_is_local(url):\n                raise ImportFromLocalIPError\n\n            if url.strip().startswith('file://'):\n                raise ValidationError('\"url\" is not valid')\n\n            data_keys, found_formats, tasks, file_upload_ids = tasks_from_url(\n                file_upload_ids, project, request, url\n            )\n\n    # take one task from request DATA\n    elif 'application/json' in request.content_type and isinstance(request.data, dict):\n        tasks = [request.data]\n\n    # take many tasks from request DATA\n    elif 'application/json' in request.content_type and isinstance(request.data, list):\n        tasks = request.data\n\n    # incorrect data source\n    else:\n        raise ValidationError('load_tasks: No data found in DATA or in FILES')\n\n    # check is data root is list\n    if not isinstance(tasks, list):\n        raise ValidationError('load_tasks: Data root must be list')\n\n    # empty tasks error\n    if not tasks:\n        raise ValidationError('load_tasks: No tasks added')\n\n    check_max_task_number(tasks)\n    return tasks, file_upload_ids, could_be_tasks_lists, found_formats, list(data_keys)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, *, openapi: GeneratorData) -> None:\n        self.openapi: GeneratorData = openapi\n        self.env: Environment = Environment(loader=PackageLoader(__package__), trim_blocks=True, lstrip_blocks=True)\n\n        self.project_name: str = self.project_name_override or f\"{utils.kebab_case(openapi.title).lower()}-client\"\n        self.project_dir: Path = Path.cwd() / self.project_name\n\n        self.package_name: str = self.package_name_override or self.project_name.replace(\"-\", \"_\")\n        self.package_dir: Path = self.project_dir / self.package_name\n        self.package_description: str = utils.remove_string_escapes(\n            f\"A client library for accessing {self.openapi.title}\"\n        )\n        self.version: str = openapi.version\n\n        self.env.filters.update(self.TEMPLATE_FILTERS)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    run_id = request_dict.get(\"run_id\") or request_dict.get(\"run_uuid\")\n    path = request_dict[\"path\"]\n    validate_path_is_safe(path)\n    run = _get_tracking_store().get_run(run_id)\n\n    if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=run.info.artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_repo = _get_artifact_repo(run)\n        artifact_path = path\n\n    return _send_artifact(artifact_repo, artifact_path)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _list_artifacts():\n    request_message = _get_request_message(\n        ListArtifacts(),\n        schema={\n            \"run_id\": [_assert_string, _assert_required],\n            \"path\": [_assert_string],\n            \"page_token\": [_assert_string],\n        },\n    )\n    response_message = ListArtifacts.Response()\n    if request_message.HasField(\"path\"):\n        path = request_message.path\n        validate_path_is_safe(path)\n    else:\n        path = None\n    run_id = request_message.run_id or request_message.run_uuid\n    run = _get_tracking_store().get_run(run_id)\n\n    if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n        artifact_entities = _list_artifacts_for_proxied_run_artifact_root(\n            proxied_artifact_root=run.info.artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_entities = _get_artifact_repo(run).list_artifacts(path)\n\n    response_message.files.extend([a.to_proto() for a in artifact_entities])\n    response_message.root_uri = run.info.artifact_uri\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def config():\n    config_disabled = (\n            app.config['CONFIG_DISABLE'] or\n            not valid_user_session(session))\n\n    name = ''\n    if 'name' in request.args:\n        name = os.path.normpath(request.args.get('name'))\n        if not re.match(r'^[A-Za-z0-9_.+-]+$', name):\n            return make_response('Invalid config name', 400)\n\n    if request.method == 'GET':\n        return json.dumps(g.user_config.__dict__)\n    elif request.method == 'PUT' and not config_disabled:\n        if name:\n            config_pkl = os.path.join(app.config['CONFIG_PATH'], name)\n            session['config'] = (pickle.load(open(config_pkl, 'rb'))\n                                 if os.path.exists(config_pkl)\n                                 else session['config'])\n            return json.dumps(session['config'])\n        else:\n            return json.dumps({})\n    elif not config_disabled:\n        config_data = request.form.to_dict()\n        if 'url' not in config_data or not config_data['url']:\n            config_data['url'] = g.user_config.url\n\n        # Save config by name to allow a user to easily load later\n        if 'name' in request.args:\n            pickle.dump(\n                config_data,\n                open(os.path.join(\n                    app.config['CONFIG_PATH'],\n                    name), 'wb'))\n\n        session['config'] = config_data\n        return redirect(config_data['url'])\n    else:\n        return redirect(url_for('.index'), code=403)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def set_storage_slots(vyper_module: vy_ast.Module) -> StorageLayout:\n    \"\"\"\n    Parse module-level Vyper AST to calculate the layout of storage variables.\n    Returns the layout as a dict of variable name -> variable info\n    \"\"\"\n    # Allocate storage slots from 0\n    # note storage is word-addressable, not byte-addressable\n    allocator = SimpleStorageAllocator()\n\n    ret: Dict[str, Dict] = {}\n\n    for node in vyper_module.get_children(vy_ast.FunctionDef):\n        type_ = node._metadata[\"type\"]\n        if type_.nonreentrant is None:\n            continue\n\n        variable_name = f\"nonreentrant.{type_.nonreentrant}\"\n\n        # a nonreentrant key can appear many times in a module but it\n        # only takes one slot. after the first time we see it, do not\n        # increment the storage slot.\n        if variable_name in ret:\n            _slot = ret[variable_name][\"slot\"]\n            type_.set_reentrancy_key_position(StorageSlot(_slot))\n            continue\n\n        # TODO use one byte - or bit - per reentrancy key\n        # requires either an extra SLOAD or caching the value of the\n        # location in memory at entrance\n        slot = allocator.allocate_slot(1, variable_name)\n\n        type_.set_reentrancy_key_position(StorageSlot(slot))\n\n        # TODO this could have better typing but leave it untyped until\n        # we nail down the format better\n        ret[variable_name] = {\"type\": \"nonreentrant lock\", \"slot\": slot}\n\n\n    for node in vyper_module.get_children(vy_ast.VariableDecl):\n        # skip non-storage variables\n        if node.is_constant or node.is_immutable:\n            continue\n\n        varinfo = node.target._metadata[\"varinfo\"]\n        type_ = varinfo.typ\n\n        # CMC 2021-07-23 note that HashMaps get assigned a slot here.\n        # I'm not sure if it's safe to avoid allocating that slot\n        # for HashMaps because downstream code might use the slot\n        # ID as a salt.\n        n_slots = type_.storage_size_in_words\n        slot = allocator.allocate_slot(n_slots, node.target.id)\n\n        varinfo.set_position(StorageSlot(slot))\n\n        # this could have better typing but leave it untyped until\n        # we understand the use case better\n        ret[node.target.id] = {\"type\": str(type_), \"slot\": slot}\n\n    return ret\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_mediastatic_content(url):\n    if url.startswith(settings.STATIC_URL):\n        local_path = settings.STATIC_ROOT / url[len(settings.STATIC_URL) :]\n    elif url.startswith(settings.MEDIA_URL):\n        local_path = settings.MEDIA_ROOT / url[len(settings.MEDIA_URL) :]\n    else:\n        raise FileNotFoundError()\n\n    # Prevent directory traversal, make sure the path is inside the media or static root\n    local_path = local_path.resolve(strict=True)\n    if not any(\n        path in local_path.parents\n        for path in (settings.MEDIA_ROOT, settings.STATIC_ROOT)\n    ):\n        raise FileNotFoundError()\n\n    with open(local_path, \"rb\") as f:\n        return f.read()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    const findHeadingElement = (cell: Cell): void => {\n      model.getCellHeadings(cell).forEach(async heading => {\n        const elementId = await getIdForHeading(\n          heading,\n          this.parser!,\n          this.sanitizer\n        );\n\n        const selector = elementId\n          ? `h${heading.level}[id=\"${CSS.escape(elementId)}\"]`\n          : `h${heading.level}`;\n\n        if (heading.outputIndex !== undefined) {\n          // Code cell\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              (heading.cellRef as CodeCell).outputArea.widgets[\n                heading.outputIndex\n              ].node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        } else {\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              heading.cellRef.node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        }\n      });\n    };\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(\n        self,\n        reactor: IReactorCore,\n        tls_client_options_factory: Optional[FederationPolicyForHTTPS],\n        user_agent: bytes,\n        ip_blacklist: IPSet,\n        _srv_resolver: Optional[SrvResolver] = None,\n        _well_known_resolver: Optional[WellKnownResolver] = None,\n    ):\n        self._reactor = reactor\n        self._clock = Clock(reactor)\n        self._pool = HTTPConnectionPool(reactor)\n        self._pool.retryAutomatically = False\n        self._pool.maxPersistentPerHost = 5\n        self._pool.cachedConnectionTimeout = 2 * 60\n\n        self._agent = Agent.usingEndpointFactory(\n            self._reactor,\n            MatrixHostnameEndpointFactory(\n                reactor, tls_client_options_factory, _srv_resolver\n            ),\n            pool=self._pool,\n        )\n        self.user_agent = user_agent\n\n        if _well_known_resolver is None:\n            # Note that the name resolver has already been wrapped in a\n            # IPBlacklistingResolver by MatrixFederationHttpClient.\n            _well_known_resolver = WellKnownResolver(\n                self._reactor,\n                agent=BlacklistingAgentWrapper(\n                    Agent(\n                        self._reactor,\n                        pool=self._pool,\n                        contextFactory=tls_client_options_factory,\n                    ),\n                    self._reactor,\n                    ip_blacklist=ip_blacklist,\n                ),\n                user_agent=self.user_agent,\n            )\n\n        self._well_known_resolver = _well_known_resolver\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    const Tensor& tag = c->input(0);\n    const Tensor& tensor = c->input(1);\n    OP_REQUIRES(c, TensorShapeUtils::IsScalar(tag.shape()),\n                errors::InvalidArgument(\"Tag must be a scalar\"));\n    OP_REQUIRES(c, tensor.dims() >= 2 && tensor.dims() <= 3,\n                errors::InvalidArgument(\"Tensor must be 3-D or 2-D, got: \",\n                                        tensor.shape().DebugString()));\n    const string& base_tag = tag.scalar<tstring>()();\n\n    float sample_rate = sample_rate_attr_;\n    if (!has_sample_rate_attr_) {\n      const Tensor& sample_rate_tensor = c->input(2);\n      OP_REQUIRES(c,\n                  sample_rate_tensor.IsAligned() &&\n                      sample_rate_tensor.NumElements() == 1,\n                  errors::InvalidArgument(\n                      \"sample_rate must be rank-0 or contain a single value\"));\n      sample_rate = sample_rate_tensor.scalar<float>()();\n    }\n    OP_REQUIRES(c, sample_rate > 0.0f,\n                errors::InvalidArgument(\"sample_rate must be > 0\"));\n\n    const int batch_size = tensor.dim_size(0);\n    const int64_t length_frames = tensor.dim_size(1);\n    const int64_t num_channels =\n        tensor.dims() == 2 ? 1 : tensor.dim_size(tensor.dims() - 1);\n\n    Summary s;\n    const int N = std::min<int>(max_outputs_, batch_size);\n    for (int i = 0; i < N; ++i) {\n      Summary::Value* v = s.add_value();\n      if (max_outputs_ > 1) {\n        v->set_tag(strings::StrCat(base_tag, \"/audio/\", i));\n      } else {\n        v->set_tag(strings::StrCat(base_tag, \"/audio\"));\n      }\n\n      Summary::Audio* sa = v->mutable_audio();\n      sa->set_sample_rate(sample_rate);\n      sa->set_num_channels(num_channels);\n      sa->set_length_frames(length_frames);\n      sa->set_content_type(\"audio/wav\");\n\n      auto values =\n          tensor.shaped<float, 3>({batch_size, length_frames, num_channels});\n      const float* data =\n          tensor.NumElements() == 0 ? nullptr : &values(i, 0, 0);\n\n      size_t sample_rate_truncated = lrintf(sample_rate);\n      if (sample_rate_truncated == 0) {\n        sample_rate_truncated = 1;\n      }\n      OP_REQUIRES_OK(c, wav::EncodeAudioAsS16LEWav(\n                            data, sample_rate_truncated, num_channels,\n                            length_frames, sa->mutable_encoded_audio_string()));\n    }\n\n    Tensor* summary_tensor = nullptr;\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape({}), &summary_tensor));\n    CHECK(SerializeToTString(s, &summary_tensor->scalar<tstring>()()));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        function set_up(container) {\n            // Find prev/next links\n            var prev = container.find('.pagination .prev');\n            var next = container.find('.pagination .next');\n\n            var prev_url = null, next_url = null;\n\n            if (prev.length)\n                prev_url = prev.attr('href');\n            if (next.length)\n                next_url = next.attr('href');\n\n            function ajax(url, handler) {\n                // URL should start with a slash, but cannot start with two slashes.\n\t\t// we cannot start with \"/\\\". Modern browsers handle backslashes as normal slashes.\n                // (Otherwise we have an XSS vulnerability.)\n                if (url[0] != '/' || url[1] == '/' || url.startsWith(\"/\\\\\")\n                    url = (''+location).replace( /[#\\?].*/, '') + url;\n\n                // Append 'xhr' to make sure all content is loaded.\n                if (url.indexOf('&xhr') < 0 && url.indexOf('?xhr') < 0)\n                    if (url.indexOf('?') < 0)\n                        url += '?xhr';\n                    else\n                        url += '&xhr';\n\n                $.ajax({\n                        type: 'GET',\n                        url: url,\n                        datatype: \"html\",\n                        cache: true,\n                        success: handler,\n                        error: function(xhr, ajaxOptions, thrownError) {\n                            handler($('<div/>').append(\n                                    $('<strong>').append(''+xhr.status + ' ' + thrownError)).html());\n                        }\n                });\n            }\n\n            function showLoader() {\n                var loader = $('.paginate-loading').eq(0);\n\n                for (var i in containers)\n                {\n                    // Keep containers height\n                    var height = containers[i].height();\n                    if (height < 50) height = 50;\n                    containers[i].css('height', height + 'px');\n\n                    // Show loader\n                    containers[i].empty();\n                    containers[i].append(loader.html());\n                }\n            }\n\n            // Replace page AJAX handler\n            function replacePage(url, receivedHtml) {\n\t\t\t\t// Set location hash\n\t\t\t\tif (setHash)\n\t\t\t\t\tlocation.hash = 'page:' + url;\n\n\n                for (var i in containers) {\n\t\t\t\t\t// Empty the paginate nodes\n                    containers[i].empty();\n                    containers[i].css('height', '');\n\n                    // Fill the paginate containers with the new content\n                    receivedHtml = $.trim(receivedHtml);\n                    $(receivedHtml).find('.paginate-container').eq(i).each(function() {\n                        containers[i].append($(this).html());\n                        set_up(containers[i]);\n                    });\n                }\n\n                // Trigger event (Only add trigger to $(document) )\n                $(document).trigger('paginatorPageReplaced', [ containers ]);\n\n                // When the URL contains a hash, scroll to that element\n                if (location.hash.substr(1).indexOf('#') > 0) {\n                    var hash = (''+location.hash).replace( /.*#/, '');\n                    scroll(hash);\n                }\n                // Otherwise, make sure that the top of the clicked paginator is visible\n                else {\n                    var paginatorTop = container.offset().top;\n\n                    if (paginatorTop < $(window).scrollTop() || paginatorTop > $(window).scrollTop() + $(window).height()) {\n                        $('html,body').animate({scrollTop: paginatorTop}, 350);\n                    }\n                }\n            }\n\n            // Process hash (link with # opened from bookmark)\n            if (processHash && location.hash.match( /^#page/)) {\n                var url = location.hash.replace( /^#page:/, '');\n                container.empty();\n                ajax(url, function(html) { replacePage(url,html); });\n            }\n            processHash = false; // Never process twice. (set_up is called recursively)\n\n            // Preload previous page\n            var previousPageHtml = null;\n            var nextPageHtml = null;\n            var clickedPrevious = false;\n            var clickedNext = false;\n\n            var preload = $('.pagination_preload').size() > 0;\n\n            if (preload) {\n\t\t\t\tif (prev_url)\n                    ajax(prev_url, function(html) {\n                        previousPageHtml = html;\n                        if (clickedPrevious) replacePage(prev_url, html);\n                    });\n                if (next_url)\n                    ajax(next_url, function(html) {\n                        nextPageHtml = html;\n                        if (clickedNext) replacePage(next_url, html);\n                    });\n\n                // Handle clicks on the prev/next buttons.\n                prev.click(function() {\n                    showLoader();\n                    if (previousPageHtml)\n                        replacePage(prev_url, previousPageHtml);\n                    else\n                        clickedPrevious = true;\n                    return false;\n                });\n\n                next.click(function() {\n                    showLoader();\n                    if (nextPageHtml)\n                        replacePage(next_url, nextPageHtml);\n                    else\n                        clickedNext = true;\n                    return false;\n                });\n            }\n\n            // The digg-paginator also contains direct links to other pages\n            // then only the prev and next page.\n            container.find('.pagination a, .pagination-helper a, a.pagination-helper').each(function() {\n\n                $(this).click(function (){\n                \tvar url = $(this).attr('href');\n                    showLoader();\n\n                    if (url[0] == '#') {\n                        var hash = url.replace( /#/, '');\n                        scroll(hash);\n                    }\n                    else\n                        ajax(url, function(html) {\n\t\t\t\t\t\t\treplacePage(url, html);\n                        });\n                    return false;\n                });\n            });\n\n            // Pagination forms are forms for which the result is displayed in the paginator\n            // They always have method=\"get\" and action=\"?\"\n            container.find('form.pagination-form').each(function() {\n                var form = $(this);\n                if (form.attr('method') == 'get' && form.attr('action').match(/.*?/)) {\n                    // Submit handler\n                    form.submit(function() {\n                        // Build submit query string\n                        var url = form.attr('action') + form.serialize();\n\n                        // AJAX request\n                        ajax(url, function(html) {\n                            replacePage(url, html);\n                        });\n                        return false;\n                    });\n                }\n            });\n        }\n    };\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "inline int ComputeOutSize(TfLitePadding padding, int image_size,\n                          int filter_size, int stride, int dilation_rate = 1) {\n  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;\n\n  // TODO(b/186448822): This uses 0 since the function has no other way to\n  // report error case\n  if (stride == 0) return 0;\n\n  switch (padding) {\n    case kTfLitePaddingSame:\n      return (image_size + stride - 1) / stride;\n    case kTfLitePaddingValid:\n      return (image_size + stride - effective_filter_size) / stride;\n    default:\n      return 0;\n  }\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def create_promptlab_run_handler():\n    def assert_arg_exists(arg_name, arg):\n        if not arg:\n            raise MlflowException(\n                message=f\"CreatePromptlabRun request must specify {arg_name}.\",\n                error_code=INVALID_PARAMETER_VALUE,\n            )\n\n    _validate_content_type(request, [\"application/json\"])\n\n    args = request.json\n    experiment_id = args.get(\"experiment_id\")\n    assert_arg_exists(\"experiment_id\", experiment_id)\n    run_name = args.get(\"run_name\", None)\n    tags = args.get(\"tags\", [])\n    prompt_template = args.get(\"prompt_template\")\n    assert_arg_exists(\"prompt_template\", prompt_template)\n    raw_prompt_parameters = args.get(\"prompt_parameters\")\n    assert_arg_exists(\"prompt_parameters\", raw_prompt_parameters)\n    prompt_parameters = [\n        Param(param.get(\"key\"), param.get(\"value\")) for param in args.get(\"prompt_parameters\")\n    ]\n    model_route = args.get(\"model_route\")\n    assert_arg_exists(\"model_route\", model_route)\n    raw_model_parameters = args.get(\"model_parameters\", [])\n    model_parameters = [\n        Param(param.get(\"key\"), param.get(\"value\")) for param in raw_model_parameters\n    ]\n    model_input = args.get(\"model_input\")\n    assert_arg_exists(\"model_input\", model_input)\n    model_output = args.get(\"model_output\", None)\n    raw_model_output_parameters = args.get(\"model_output_parameters\", [])\n    model_output_parameters = [\n        Param(param.get(\"key\"), param.get(\"value\")) for param in raw_model_output_parameters\n    ]\n    mlflow_version = args.get(\"mlflow_version\")\n    assert_arg_exists(\"mlflow_version\", mlflow_version)\n    user_id = args.get(\"user_id\", \"unknown\")\n\n    # use current time if not provided\n    start_time = args.get(\"start_time\", int(time.time() * 1000))\n\n    store = _get_tracking_store()\n\n    run = _create_promptlab_run_impl(\n        store,\n        experiment_id=experiment_id,\n        run_name=run_name,\n        tags=tags,\n        prompt_template=prompt_template,\n        prompt_parameters=prompt_parameters,\n        model_route=model_route,\n        model_parameters=model_parameters,\n        model_input=model_input,\n        model_output=model_output,\n        model_output_parameters=model_output_parameters,\n        mlflow_version=mlflow_version,\n        user_id=user_id,\n        start_time=start_time,\n    )\n    response_message = CreateRun.Response()\n    response_message.run.MergeFrom(run.to_proto())\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ReadBlock(RandomAccessFile* file, const BlockHandle& handle,\n                 BlockContents* result) {\n  result->data = StringPiece();\n  result->cachable = false;\n  result->heap_allocated = false;\n\n  // Read the block contents as well as the type/crc footer.\n  // See table_builder.cc for the code that built this structure.\n  size_t n = static_cast<size_t>(handle.size());\n\n  if (kBlockTrailerSize > std::numeric_limits<size_t>::max() - n) {\n    return errors::DataLoss(\"handle.size() too big\");\n  }\n\n  char* buf = new char[n + kBlockTrailerSize];\n  StringPiece contents;\n  Status s = file->Read(handle.offset(), n + kBlockTrailerSize, &contents, buf);\n  if (!s.ok()) {\n    delete[] buf;\n    return s;\n  }\n  if (contents.size() != n + kBlockTrailerSize) {\n    delete[] buf;\n    return errors::DataLoss(\"truncated block read\");\n  }\n\n  // Check the crc of the type and the block contents\n  const char* data = contents.data();  // Pointer to where Read put the data\n  // This checksum verification is optional.  We leave it on for now\n  const bool verify_checksum = true;\n  if (verify_checksum) {\n    const uint32 crc = crc32c::Unmask(core::DecodeFixed32(data + n + 1));\n    const uint32 actual = crc32c::Value(data, n + 1);\n    if (actual != crc) {\n      delete[] buf;\n      s = errors::DataLoss(\"block checksum mismatch\");\n      return s;\n    }\n  }\n\n  switch (data[n]) {\n    case kNoCompression:\n      if (data != buf) {\n        // File implementation gave us pointer to some other data.\n        // Use it directly under the assumption that it will be live\n        // while the file is open.\n        delete[] buf;\n        result->data = StringPiece(data, n);\n        result->heap_allocated = false;\n        result->cachable = false;  // Do not double-cache\n      } else {\n        result->data = StringPiece(buf, n);\n        result->heap_allocated = true;\n        result->cachable = true;\n      }\n\n      // Ok\n      break;\n    case kSnappyCompression: {\n      size_t ulength = 0;\n      if (!port::Snappy_GetUncompressedLength(data, n, &ulength)) {\n        delete[] buf;\n        return errors::DataLoss(\"corrupted compressed block contents\");\n      }\n      char* ubuf = new char[ulength];\n      if (!port::Snappy_Uncompress(data, n, ubuf)) {\n        delete[] buf;\n        delete[] ubuf;\n        return errors::DataLoss(\"corrupted compressed block contents\");\n      }\n      delete[] buf;\n      result->data = StringPiece(ubuf, ulength);\n      result->heap_allocated = true;\n      result->cachable = true;\n      break;\n    }\n    default:\n      delete[] buf;\n      return errors::DataLoss(\"bad block type\");\n  }\n\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  public Response render(ServeEvent serveEvent) {\n    ResponseDefinition responseDefinition = serveEvent.getResponseDefinition();\n    //    if (targetAddressProhibited(responseDefinition.getProxyUrl())) {\n    //      return response()\n    //          .status(500)\n    //          .headers(new HttpHeaders(new HttpHeader(\"Content-Type\", \"text/plain\")))\n    //          .body(\"The target proxy address is denied in WireMock's configuration.\")\n    //          .build();\n    //    }\n\n    HttpUriRequest httpRequest = getHttpRequestFor(responseDefinition);\n    addRequestHeaders(httpRequest, responseDefinition);\n\n    GlobalSettings settings = settingsStore.get();\n\n    Request originalRequest = responseDefinition.getOriginalRequest();\n    if ((originalRequest.getBody() != null && originalRequest.getBody().length > 0)\n        || originalRequest.containsHeader(CONTENT_LENGTH)) {\n      httpRequest.setEntity(buildEntityFrom(originalRequest));\n    }\n\n    CloseableHttpClient client = chooseClient(serveEvent.getRequest().isBrowserProxyRequest());\n\n    try {\n      return client.execute(\n          httpRequest,\n          httpResponse ->\n              response()\n                  .status(httpResponse.getCode())\n                  .headers(headersFrom(httpResponse, responseDefinition))\n                  .body(getEntityAsByteArray(httpResponse))\n                  .fromProxy(true)\n                  .configureDelay(\n                      settings.getFixedDelay(),\n                      settings.getDelayDistribution(),\n                      responseDefinition.getFixedDelayMilliseconds(),\n                      responseDefinition.getDelayDistribution())\n                  .chunkedDribbleDelay(responseDefinition.getChunkedDribbleDelay())\n                  .build());\n    } catch (ProhibitedNetworkAddressException e) {\n      return response()\n          .status(HTTP_INTERNAL_ERROR)\n          .headers(new HttpHeaders(new HttpHeader(\"Content-Type\", \"text/plain\")))\n          .body(\"The target proxy address is denied in WireMock's configuration.\")\n          .build();\n    } catch (SSLException e) {\n      return proxyResponseError(\"SSL\", httpRequest, e);\n    } catch (IOException e) {\n      return proxyResponseError(\"Network\", httpRequest, e);\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void CalculateOutputIndexValueRowID(\n      OpKernelContext* context, const RowPartitionTensor& value_rowids,\n      const vector<INDEX_TYPE>& parent_output_index,\n      INDEX_TYPE output_index_multiplier, INDEX_TYPE output_size,\n      vector<INDEX_TYPE>* result) {\n    const INDEX_TYPE index_size = value_rowids.size();\n    result->reserve(index_size);\n    if (index_size == 0) {\n      return;\n    }\n\n    INDEX_TYPE current_output_column = 0;\n    INDEX_TYPE current_value_rowid = value_rowids(0);\n    DCHECK_LT(current_value_rowid, parent_output_index.size());\n    INDEX_TYPE current_output_index = parent_output_index[current_value_rowid];\n    result->push_back(current_output_index);\n    for (INDEX_TYPE i = 1; i < index_size; ++i) {\n      INDEX_TYPE next_value_rowid = value_rowids(i);\n      if (next_value_rowid == current_value_rowid) {\n        if (current_output_index >= 0) {\n          ++current_output_column;\n          if (current_output_column < output_size) {\n            current_output_index += output_index_multiplier;\n          } else {\n            current_output_index = -1;\n          }\n        }\n      } else {\n        current_output_column = 0;\n        current_value_rowid = next_value_rowid;\n        DCHECK_LT(next_value_rowid, parent_output_index.size());\n        current_output_index = parent_output_index[next_value_rowid];\n      }\n      result->push_back(current_output_index);\n    }\n    OP_REQUIRES(context, result->size() == value_rowids.size(),\n                errors::InvalidArgument(\"Invalid row ids.\"));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& a = ctx->input(0);\n    const Tensor& b = ctx->input(1);\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(a.shape()),\n                errors::InvalidArgument(\"a is not a matrix\"));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(b.shape()),\n                errors::InvalidArgument(\"b is not a matrix\"));\n\n    const int m = transpose_a_ ? a.dim_size(1) : a.dim_size(0);\n    const int k = transpose_a_ ? a.dim_size(0) : a.dim_size(1);\n    const int n = transpose_b_ ? b.dim_size(0) : b.dim_size(1);\n    const int k2 = transpose_b_ ? b.dim_size(1) : b.dim_size(0);\n\n    OP_REQUIRES(ctx, k == k2,\n                errors::InvalidArgument(\n                    \"Matrix size incompatible: a: \", a.shape().DebugString(),\n                    \", b: \", b.shape().DebugString()));\n    OP_REQUIRES(ctx, m >= 0 && n >= 0 && k >= 0,\n                errors::InvalidArgument(\n                    \"Matrix dimensions cannot be negative: a: \",\n                    a.shape().DebugString(), \", b: \", b.shape().DebugString()));\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({m, n}), &output));\n\n    // Return early if at least one of the output dimension size is 0.\n    if (m == 0 || n == 0) {\n      return;\n    }\n\n    if (k == 0) {\n      // If the inner dimension k in the matrix multiplication is zero, we fill\n      // the output with zeros.\n      functor::SetZeroFunctor<CPUDevice, float> f;\n      f(ctx->eigen_device<CPUDevice>(), output->flat<float>());\n      return;\n    }\n\n    auto out = output->matrix<float>();\n\n    std::unique_ptr<Tensor> a_float;\n    std::unique_ptr<Tensor> b_float;\n    if (!a_is_sparse_ && !b_is_sparse_) {\n      auto left = &a;\n      auto right = &b;\n      // TODO(agarwal): multi-thread the conversions from bfloat16 to float.\n      if (std::is_same<TL, bfloat16>::value) {\n        a_float.reset(new Tensor(DT_FLOAT, a.shape()));\n        BFloat16ToFloat(a.flat<bfloat16>().data(),\n                        a_float->flat<float>().data(), a.NumElements());\n        left = a_float.get();\n      }\n      if (std::is_same<TR, bfloat16>::value) {\n        b_float.reset(new Tensor(DT_FLOAT, b.shape()));\n        BFloat16ToFloat(b.flat<bfloat16>().data(),\n                        b_float->flat<float>().data(), b.NumElements());\n        right = b_float.get();\n      }\n      Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n      dim_pair[0].first = transpose_a_ ? 0 : 1;\n      dim_pair[0].second = transpose_b_ ? 1 : 0;\n\n      out.device(ctx->template eigen_device<CPUDevice>()) =\n          left->matrix<float>().contract(right->matrix<float>(), dim_pair);\n      return;\n    }\n\n    auto left = &a;\n    auto right = &b;\n    bool transpose_output = false;\n    bool transpose_a = transpose_a_;\n    bool transpose_b = transpose_b_;\n    if (!a_is_sparse_) {\n      // Swap the order of multiplications using the identity:\n      // A * B = (B' *  A')'.\n      std::swap(left, right);\n      std::swap(transpose_a, transpose_b);\n      transpose_a = !transpose_a;\n      transpose_b = !transpose_b;\n      transpose_output = !transpose_output;\n    }\n\n    std::unique_ptr<Tensor> right_tr;\n    if (transpose_b) {\n      // TODO(agarwal): avoid transposing the matrix here and directly handle\n      // transpose in CreateDenseSlices.\n      OP_REQUIRES(ctx, right->dim_size(0) != 0,\n                  errors::InvalidArgument(\"b has an entry 0 in it's shape.\"));\n      OP_REQUIRES(ctx, right->dim_size(1) != 0,\n                  errors::InvalidArgument(\"b has an entry 0 in it's shape.\"));\n      right_tr.reset(\n          new Tensor(right->dtype(),\n                     TensorShape({right->dim_size(1), right->dim_size(0)})));\n\n      const auto perm = dsizes_10();\n      if (transpose_output) {\n        right_tr->matrix<TL>().device(ctx->template eigen_device<CPUDevice>()) =\n            right->matrix<TL>().shuffle(perm);\n      } else {\n        right_tr->matrix<TR>().device(ctx->template eigen_device<CPUDevice>()) =\n            right->matrix<TR>().shuffle(perm);\n      }\n      right = right_tr.get();\n    }\n\n    if (transpose_output) {\n      DoMatMul<TR, TL>::Compute(&this->cache_tr_, left->matrix<TR>(),\n                                right->matrix<TL>(), transpose_a,\n                                ctx->device()->tensorflow_cpu_worker_threads(),\n                                transpose_output, &out);\n    } else {\n      DoMatMul<TL, TR>::Compute(&this->cache_nt_, left->matrix<TL>(),\n                                right->matrix<TR>(), transpose_a,\n                                ctx->device()->tensorflow_cpu_worker_threads(),\n                                transpose_output, &out);\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def verify_mode(self, value):\n        self._ctx.set_verify(\n            _stdlib_to_openssl_verify[value], _verify_callback\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _grab_image_cache_params(args):\n    if args.cache_from_image or args.cache_from_version:\n        logger.debug(\"Image caching is requested - create build image cache params obj\")\n        vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)\n        vault_endpoint = _resolve_vault_endpoint(args)\n        vault_endpoint_verify = _resolve_property(args, 'vault_endpoint_verify', ignore_value_error=True)\n        return bigflow.build.operate.BuildImageCacheParams(\n            auth_method=args.auth_method,\n            vault_endpoint=vault_endpoint,\n            vault_secret=vault_secret,\n            cache_from_version=args.cache_from_version,\n            cache_from_image=args.cache_from_image,\n            vault_endpoint_verify=vault_endpoint_verify\n        )\n    else:\n        logger.debug(\"No caching is requested - so just disable it completly\")\n        return None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def guard(dest, ev_args):\n    depth = 1\n    try:\n        next(dest)\n        while True:\n            ev = yield\n            if ev[0] == \"start\":\n                depth += 1\n            elif ev[0] == \"end\":\n                depth -= 1\n            try:\n                dest.send(ev)\n            except StopIteration as exc:\n                return exc.value\n    finally:\n        while depth > 0:\n            ev_type, *_ = yield\n            if ev_type == \"end\":\n                depth -= 1\n            elif ev_type == \"start\":\n                depth += 1\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __new__(cls, obj, *args, **kwargs):\n        if isinstance(obj, AnsibleUnsafe):\n            # Already marked unsafe\n            return obj\n\n        # In our usage we should only receive unicode strings.\n        # This conditional and conversion exists to sanity check the values\n        # we're given but we may want to take it out for testing and sanitize\n        # our input instead.\n        # Note that this does the wrong thing if we're *intentionall* passing a byte string to this\n        # function.\n        if isinstance(obj, string_types):\n            obj = AnsibleUnsafeText(to_text(obj, errors='surrogate_or_strict'))\n        return obj\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, filename):\n        \"\"\"\n        Validate the filename by calling get_valid_name() and return a filename\n        to be passed to the save() method.\n        \"\"\"\n        filename = str(filename).replace('\\\\', '/')\n        # `filename` may include a path as returned by FileField.upload_to.\n        dirname, filename = os.path.split(filename)\n        if '..' in pathlib.PurePath(dirname).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dirname)\n        return os.path.normpath(os.path.join(dirname, self.get_valid_name(filename)))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def exit():\n    # We must test it manually, because otherwise, it creates a project \"exit\"\n    if request.method == \"GET\":\n        abort(405)\n\n    form = LogoutForm()\n    if form.validate():\n        # delete the session\n        session.clear()\n        return redirect(url_for(\".home\"))\n    else:\n        flash(\n            format_form_errors(form, _(\"Unable to logout\")),\n            category=\"danger\",\n        )\n        return redirect(request.headers.get(\"Referer\") or url_for(\".home\"))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_config(\n    extra_config_path: Optional[str] = None,\n    ignore_local_config: bool = False,\n    **kwargs,\n) -> FluffConfig:\n    \"\"\"Get a config object from kwargs.\"\"\"\n    plain_output = OutputStreamFormatter.should_produce_plain_output(kwargs[\"nocolor\"])\n    if kwargs.get(\"dialect\"):\n        try:\n            # We're just making sure it exists at this stage.\n            # It will be fetched properly in the linter.\n            dialect_selector(kwargs[\"dialect\"])\n        except SQLFluffUserError as err:\n            click.echo(\n                OutputStreamFormatter.colorize_helper(\n                    plain_output,\n                    f\"Error loading dialect '{kwargs['dialect']}': {str(err)}\",\n                    color=Color.red,\n                )\n            )\n            sys.exit(EXIT_ERROR)\n        except KeyError:\n            click.echo(\n                OutputStreamFormatter.colorize_helper(\n                    plain_output,\n                    f\"Error: Unknown dialect '{kwargs['dialect']}'\",\n                    color=Color.red,\n                )\n            )\n            sys.exit(EXIT_ERROR)\n    from_root_kwargs = {}\n    if \"require_dialect\" in kwargs:\n        from_root_kwargs[\"require_dialect\"] = kwargs.pop(\"require_dialect\")\n    library_path = kwargs.pop(\"library_path\", None)\n    # Instantiate a config object (filtering out the nulls)\n    overrides = {k: kwargs[k] for k in kwargs if kwargs[k] is not None}\n    if library_path is not None:\n        # Check for a null value\n        if library_path.lower() == \"none\":\n            library_path = None  # Set an explicit None value.\n        # Set the global override\n        overrides[\"library_path\"] = library_path\n    try:\n        return FluffConfig.from_root(\n            extra_config_path=extra_config_path,\n            ignore_local_config=ignore_local_config,\n            overrides=overrides,\n            **from_root_kwargs,\n        )\n    except SQLFluffUserError as err:  # pragma: no cover\n        click.echo(\n            OutputStreamFormatter.colorize_helper(\n                plain_output,\n                f\"Error loading config: {str(err)}\",\n                color=Color.red,\n            )\n        )\n        sys.exit(EXIT_ERROR)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "            self.updatePassword = function (username, password, current) {\n                return OctoPrint.access.users.changePassword(username, password, current);\n            };\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create_table(\n        self,\n        name: str,\n        obj: pd.DataFrame | pa.Table | ir.Table | None = None,\n        *,\n        schema: ibis.Schema | None = None,\n        database: str | None = None,\n        temp: bool | None = None,\n        overwrite: bool = False,\n        default_collate: str | None = None,\n        partition_by: str | None = None,\n        cluster_by: Iterable[str] | None = None,\n        options: Mapping[str, Any] | None = None,\n    ) -> ir.Table:\n        \"\"\"Create a table in BigQuery.\n\n        Parameters\n        ----------\n        name\n            Name of the table to create\n        obj\n            The data with which to populate the table; optional, but one of `obj`\n            or `schema` must be specified\n        schema\n            The schema of the table to create; optional, but one of `obj` or\n            `schema` must be specified\n        database\n            The BigQuery *dataset* in which to create the table; optional\n        temp\n            This parameter is not yet supported in the BigQuery backend\n        overwrite\n            If `True`, replace the table if it already exists, otherwise fail if\n            the table exists\n        default_collate\n            Default collation for string columns. See BigQuery's documentation\n            for more details: https://cloud.google.com/bigquery/docs/reference/standard-sql/collation-concepts\n        partition_by\n            Partition the table by the given expression. See BigQuery's documentation\n            for more details: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#partition_expression\n        cluster_by\n            List of columns to cluster the table by. See BigQuery's documentation\n            for more details: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#clustering_column_list\n        options\n            BigQuery-specific table options; see the BigQuery documentation for\n            details: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#table_option_list\n\n        Returns\n        -------\n        Table\n            The table that was just created\n        \"\"\"\n        if obj is None and schema is None:\n            raise com.IbisError(\"One of the `schema` or `obj` parameter is required\")\n\n        if temp:\n            # TODO: these require a BQ session; figure out how to handle that\n            raise NotImplementedError(\n                \"Temporary tables in the BigQuery backend are not yet supported\"\n            )\n\n        create_stmt = \"CREATE\"\n\n        if overwrite:\n            create_stmt += \" OR REPLACE\"\n\n        table_ref = self._fully_qualified_name(name, database)\n\n        create_stmt += f\" TABLE `{table_ref}`\"\n\n        if isinstance(obj, ir.Table) and schema is not None:\n            if not schema.equals(obj.schema()):\n                raise com.IbisTypeError(\n                    \"Provided schema and Ibis table schema are incompatible. Please \"\n                    \"align the two schemas, or provide only one of the two arguments.\"\n                )\n\n        if schema is not None:\n            schema_str = \", \".join(\n                (\n                    f\"{name} {BigQueryType.from_ibis(typ)}\"\n                    + \" NOT NULL\" * (not typ.nullable)\n                )\n                for name, typ in schema.items()\n            )\n            create_stmt += f\" ({schema_str})\"\n\n        if default_collate is not None:\n            create_stmt += f\" DEFAULT COLLATE {default_collate!r}\"\n\n        if partition_by is not None:\n            create_stmt += f\" PARTITION BY {partition_by}\"\n\n        if cluster_by is not None:\n            create_stmt += f\" CLUSTER BY {', '.join(cluster_by)}\"\n\n        if options:\n            pairs = \", \".join(f\"{k}={v!r}\" for k, v in options.items())\n            create_stmt += f\" OPTIONS({pairs})\"\n\n        if obj is not None:\n            import pyarrow as pa\n            import pyarrow_hotfix  # noqa: F401\n\n            if isinstance(obj, (pd.DataFrame, pa.Table)):\n                table = ibis.memtable(obj, schema=schema)\n            else:\n                table = obj\n\n            create_stmt += f\" AS ({self.compile(table)})\"\n\n        self.raw_sql(create_stmt)\n\n        return self.table(table_ref)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def build(self):\n        g = Grammar(self.tokens)\n\n        for level, (assoc, terms) in enumerate(self.precedence, 1):\n            for term in terms:\n                g.set_precedence(term, assoc, level)\n\n        for prod_name, syms, func, precedence in self.productions:\n            g.add_production(prod_name, syms, func, precedence)\n\n        g.set_start()\n\n        for unused_term in g.unused_terminals():\n            warnings.warn(\n                \"Token %r is unused\" % unused_term,\n                ParserGeneratorWarning,\n                stacklevel=2\n            )\n        for unused_prod in g.unused_productions():\n            warnings.warn(\n                \"Production %r is not reachable\" % unused_prod,\n                ParserGeneratorWarning,\n                stacklevel=2\n            )\n\n        g.build_lritems()\n        g.compute_first()\n        g.compute_follow()\n\n        cache_dir = AppDirs(\"rply\").user_cache_dir\n        cache_file = os.path.join(\n            cache_dir,\n            \"rply-%s-%s-%s.json\" % (\n                self.VERSION, self.cache_id, self.compute_grammar_hash(g)\n            )\n        )\n\n        table = None\n        if os.path.exists(cache_file):\n            with open(cache_file) as f:\n                data = json.load(f)\n            if self.data_is_valid(g, data):\n                table = LRTable.from_cache(g, data)\n        if table is None:\n            table = LRTable.from_grammar(g)\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n\n            with open(cache_file, \"w\") as f:\n                json.dump(self.serialize_table(table), f)\n\n        if table.sr_conflicts:\n            warnings.warn(\n                \"%d shift/reduce conflict%s\" % (\n                    len(table.sr_conflicts),\n                    \"s\" if len(table.sr_conflicts) > 1 else \"\"\n                ),\n                ParserGeneratorWarning,\n                stacklevel=2,\n            )\n        if table.rr_conflicts:\n            warnings.warn(\n                \"%d reduce/reduce conflict%s\" % (\n                    len(table.rr_conflicts),\n                    \"s\" if len(table.rr_conflicts) > 1 else \"\"\n                ),\n                ParserGeneratorWarning,\n                stacklevel=2,\n            )\n        return LRParser(table, self.error_handler)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def kick(bot, trigger):\n    \"\"\"Kick a user from the channel.\"\"\"\n    chanops = get_chanops(str(trigger.sender), bot.memory['channelmgnt']['jdcache'])\n    dodeop = False\n    if chanops:\n        if bot.channels[trigger.sender].privileges[bot.nick] < OP and trigger.account in chanops:\n            bot.say('Please wait...')\n            bot.say('op ' + trigger.sender, 'ChanServ')\n            time.sleep(1)\n            dodeop = True\n        text = trigger.group().split()\n        argc = len(text)\n        if argc < 2:\n            return\n        nick = Identifier(text[1])\n        reason = ' '.join(text[2:])\n        if ',' in str(nick):\n            return bot.reply('Unable to kick. Kicking multiple users is not allowed.') \n        if '#' in str(nick):\n            return bot.reply('Unable to kick. Use of # when kicking is not expected.')\n        if nick != bot.config.core.nick and trigger.account in chanops:\n            bot.write(['KICK', trigger.sender, nick, ':' + reason])\n            if dodeop:\n                deopbot(trigger.sender, bot)\n        else:\n            bot.reply('Access Denied. If in error, please contact the channel founder.')\n    else:\n        bot.reply(f'No ChanOps Found. Please ask for assistance in {bot.settings.channelmgnt.support_channel}')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "      void context.ready.then(async () => {\n        const file = context.model;\n        const workspace = file.toJSON() as unknown as Workspace.IWorkspace;\n        const path = context.path;\n        const id = workspace.metadata.id;\n\n        // Save the file contents as a workspace.\n        await this._workspaces.save(id, workspace);\n\n        // Save last save location for the save command.\n        await this._state.save(LAST_SAVE_ID, path);\n\n        // Navigate to new workspace.\n        const workspacesBase = URLExt.join(this._application, 'workspaces');\n        const url = URLExt.join(workspacesBase, id);\n        if (!workspacesBase.startsWith(url)) {\n          throw new Error('Can only be used for workspaces');\n        }\n        if (this._router) {\n          this._router.navigate(url, { hard: true });\n        } else {\n          document.location.href = url;\n        }\n      });\n      return dummyWidget(context);"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def fetch_file(self, in_path, out_path):\n        ''' fetch a file from zone to local '''\n\n        vvv(\"FETCH %s TO %s\" % (in_path, out_path), host=self.zone)\n\n\n        p = self._buffered_exec_command('dd if=%s bs=%s' % (in_path, BUFSIZE), None)\n\n        with open(out_path, 'wb+') as out_file:\n            try:\n                for chunk in p.stdout.read(BUFSIZE):\n                    out_file.write(chunk)\n            except:\n                traceback.print_exc()\n                raise errors.AnsibleError(\"failed to transfer file to %s\" % out_path)\n            stdout, stderr = p.communicate()\n            if p.returncode != 0:\n                raise errors.AnsibleError(\"failed to transfer file to %s:\\n%s\\n%s\" % (out_path, stdout, stderr))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    run_id = request_dict.get(\"run_id\") or request_dict.get(\"run_uuid\")\n    path = request_dict[\"path\"]\n    path = validate_path_is_safe(path)\n    run = _get_tracking_store().get_run(run_id)\n\n    if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=run.info.artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_repo = _get_artifact_repo(run)\n        artifact_path = path\n\n    return _send_artifact(artifact_repo, artifact_path)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus GatherStrings(TfLiteContext* context, const TfLiteTensor* input,\n                           const TfLiteTensor* positions,\n                           TfLiteTensor* output) {\n  DynamicBuffer buffer;\n\n  const PositionT* indexes = GetTensorData<PositionT>(positions);\n  bool indices_has_only_positive_elements = true;\n  const size_t num_indices = positions->bytes / sizeof(PositionT);\n  for (size_t i = 0; i < num_indices; i++) {\n    if (indexes[i] < 0) {\n      indices_has_only_positive_elements = false;\n      break;\n    }\n  }\n  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\n\n  const PositionT num_strings = GetStringCount(input);\n  const int num_indexes = NumElements(positions);\n\n  for (int i = 0; i < num_indexes; ++i) {\n    const PositionT pos = indexes[i];\n    TF_LITE_ENSURE(context, pos < num_strings);\n    const auto string_ref = GetString(input, pos);\n    buffer.AddString(string_ref.str, string_ref.len);\n  }\n  buffer.WriteToTensor(output, /*new_shape=*/nullptr);\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void ValidateInputTensors(OpKernelContext* ctx, const Tensor& in0,\n                            const Tensor& in1) override {\n    const auto in0_num_dims = in0.dims();\n    OP_REQUIRES(\n        ctx, in0_num_dims >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0_num_dims));\n\n    const auto in1_num_dims = in1.dims();\n    OP_REQUIRES(\n        ctx, in1_num_dims >= 2,\n        errors::InvalidArgument(\"In[1] ndims must be >= 2: \", in1_num_dims));\n\n    const auto in0_last_dim = in0.dim_size(in0_num_dims - 1);\n    const auto in0_prev_dim = in0.dim_size(in0_num_dims - 2);\n    OP_REQUIRES(ctx, in0_last_dim == in0_prev_dim,\n                errors::InvalidArgument(\n                    \"In[0] matrices in the last dimensions must be square (\",\n                    in0_last_dim, \" =/= \", in0_prev_dim, \")\"));\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, fobj):\n        \"\"\"\n        fobj is a file-like object as an icns resource\n        \"\"\"\n        # signature : (start, length)\n        self.dct = dct = {}\n        self.fobj = fobj\n        sig, filesize = nextheader(fobj)\n        if sig != b'icns':\n            raise SyntaxError('not an icns file')\n        i = HEADERSIZE\n        while i < filesize:\n            sig, blocksize = nextheader(fobj)\n            if blocksize <= 0:\n                raise SyntaxError('invalid block header')\n            i += HEADERSIZE\n            blocksize -= HEADERSIZE\n            dct[sig] = (i, blocksize)\n            fobj.seek(blocksize, 1)\n            i += blocksize\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def process_jwt(jwt):\n    \"\"\"\n    Process a JSON Web Token without verifying it.\n\n    Call this before :func:`verify_jwt` if you need access to the header or claims in the token before verifying it. For example, the claims might identify the issuer such that you can retrieve the appropriate public key.\n\n    :param jwt: The JSON Web Token to verify.\n    :type jwt: str or unicode\n\n    :rtype: tuple\n    :returns: ``(header, claims)``\n    \"\"\"\n    _check_jwt_format(jwt)\n    header, claims, _ = jwt.split('.')\n    parsed_header = json_decode(base64url_decode(header))\n    parsed_claims = json_decode(base64url_decode(claims))\n    return parsed_header, parsed_claims\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def schedule(self, request_id, scheduler, coro):\n        method, okey, seq = request_id\n        loop = asyncio.get_running_loop()\n        fut = loop.create_future()\n        self._futures[request_id] = fut\n        ev = asyncio.Event()\n        heapq.heappush(self._pending[okey], _SeqItem(method, seq, ev))\n\n        while True:\n            assert len(self._pending[okey]) > 0\n            head = self._pending[okey][0]\n            if head.seq == seq:\n                break\n            # Wait until the head item finishes.\n            await head.ev.wait()\n\n        job = await scheduler.spawn(coro)\n        job._explicit = True\n        self._jobs[request_id] = job\n\n        def cb(seq_item, rqst_id, task):\n            _, okey, _ = rqst_id\n            seq_item.ev.set()\n            if task.cancelled():\n                result = CANCELLED\n                # already removed from the pending heap\n                _resolve_future(rqst_id, fut, result, self._log)\n            else:\n                s = heapq.heappop(self._pending[okey])\n                assert s == seq_item\n                if task.exception() is not None:\n                    _resolve_future(rqst_id, fut, task.exception(), self._log)\n                else:\n                    _resolve_future(rqst_id, fut, task.result(), self._log)\n\n        job._task.add_done_callback(functools.partial(cb, head, request_id))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus ResizeOutput(TfLiteContext* context, const TfLiteTensor* input,\n                          const TfLiteTensor* axis, TfLiteTensor* output) {\n  int axis_value;\n  // Retrive all 8 bytes when axis type is kTfLiteInt64 to avoid data loss.\n  if (axis->type == kTfLiteInt64) {\n    axis_value = static_cast<int>(*GetTensorData<int64_t>(axis));\n  } else {\n    axis_value = *GetTensorData<int>(axis);\n  }\n  if (axis_value < 0) {\n    axis_value += NumDimensions(input);\n  }\n\n  TF_LITE_ENSURE(context, axis_value >= 0);\n  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\n\n  // Copy the input dimensions to output except the axis dimension.\n  TfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\n  int j = 0;\n  for (int i = 0; i < NumDimensions(input); ++i) {\n    if (i != axis_value) {\n      output_dims->data[j] = SizeOfDimension(input, i);\n      ++j;\n    }\n  }\n  return context->ResizeTensor(context, output, output_dims);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    function dismissChangeRelatedObjectPopup(win, objId, newRepr, newId) {\n        objId = html_unescape(objId);\n        newRepr = html_unescape(newRepr);\n        var id = windowname_to_id(win.name).replace(/^edit_/, '');\n        var selectsSelector = interpolate('#%s, #%s_from, #%s_to', [id, id, id]);\n        var selects = django.jQuery(selectsSelector);\n        selects.find('option').each(function() {\n            if (this.value === objId) {\n                this.textContent = newRepr;\n                this.value = newId;\n            }\n        });\n        win.close();\n    }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "inline TfLiteStatus GatherNd(const RuntimeShape& params_shape,\n                             const ParamsT* params_data,\n                             const RuntimeShape& indices_shape,\n                             const IndicesT* indices_data,\n                             const RuntimeShape& output_shape,\n                             ParamsT* output_data) {\n  ruy::profiler::ScopeLabel label(\"GatherNd\");\n\n  const GatherNdHelperResult res = GatherNdHelper(params_shape, indices_shape);\n  for (int i = 0; i < res.n_slices; ++i) {\n    int64_t from_pos = 0;\n    for (int j = 0; j < res.indices_nd; ++j) {\n      from_pos += indices_data[i * res.indices_nd + j] * res.dims_to_count[j];\n    }\n    if (from_pos < 0 || from_pos + res.slice_size > params_shape.FlatSize()) {\n      return kTfLiteError;\n    }\n    std::memcpy(output_data + i * res.slice_size, params_data + from_pos,\n                sizeof(ParamsT) * res.slice_size);\n  }\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def edit(request, bookmark_id: int):\n    try:\n        bookmark = Bookmark.objects.get(pk=bookmark_id, owner=request.user)\n    except Bookmark.DoesNotExist:\n        raise Http404('Bookmark does not exist')\n    return_url = get_safe_return_url(request.GET.get('return_url'), reverse('bookmarks:index'))\n\n    if request.method == 'POST':\n        form = BookmarkForm(request.POST, instance=bookmark)\n        if form.is_valid():\n            tag_string = convert_tag_string(form.data['tag_string'])\n            update_bookmark(form.save(commit=False), tag_string, request.user)\n            return HttpResponseRedirect(return_url)\n    else:\n        form = BookmarkForm(instance=bookmark)\n\n    form.initial['tag_string'] = build_tag_string(bookmark.tag_names, ' ')\n\n    context = {\n        'form': form,\n        'bookmark_id': bookmark_id,\n        'return_url': return_url\n    }\n\n    return render(request, 'bookmarks/edit.html', context)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_ordering(self, *ordering):\n        \"\"\"\n        Add items from the 'ordering' sequence to the query's \"order by\"\n        clause. These items are either field names (not column names) --\n        possibly with a direction prefix ('-' or '?') -- or OrderBy\n        expressions.\n\n        If 'ordering' is empty, clear all ordering from the query.\n        \"\"\"\n        errors = []\n        for item in ordering:\n            if isinstance(item, str):\n                if '.' in item and ORDER_PATTERN.match(item):\n                    warnings.warn(\n                        'Passing column raw column aliases to order_by() is '\n                        'deprecated. Wrap %r in a RawSQL expression before '\n                        'passing it to order_by().' % item,\n                        category=RemovedInDjango40Warning,\n                        stacklevel=3,\n                    )\n                    continue\n                if item == '?':\n                    continue\n                if item.startswith('-'):\n                    item = item[1:]\n                if item in self.annotations:\n                    continue\n                if self.extra and item in self.extra:\n                    continue\n                # names_to_path() validates the lookup. A descriptive\n                # FieldError will be raise if it's not.\n                self.names_to_path(item.split(LOOKUP_SEP), self.model._meta)\n            elif not hasattr(item, 'resolve_expression'):\n                errors.append(item)\n            if getattr(item, 'contains_aggregate', False):\n                raise FieldError(\n                    'Using an aggregate in order_by() without also including '\n                    'it in annotate() is not allowed: %s' % item\n                )\n        if errors:\n            raise FieldError('Invalid order_by arguments: %s' % errors)\n        if ordering:\n            self.order_by += ordering\n        else:\n            self.default_ordering = False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compile(XlaOpKernelContext* ctx) override {\n    if (ctx->num_inputs() != 3) {\n      OP_REQUIRES(\n          ctx, ctx->num_inputs() == 5,\n          errors::InvalidArgument(\"Must supply ksize and stride arguments.\"));\n      const TensorShape ksize_shape = ctx->InputShape(3);\n      // Validate input sizes.\n      OP_REQUIRES(ctx, TensorShapeUtils::IsVector(ksize_shape),\n                  errors::InvalidArgument(\"ksize must be a vector, not shape \",\n                                          ksize_shape.DebugString()));\n      OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntVector(3, &ksize_));\n\n      const TensorShape stride_shape = ctx->InputShape(4);\n      // Validate input sizes.\n      OP_REQUIRES(ctx, TensorShapeUtils::IsVector(stride_shape),\n                  errors::InvalidArgument(\"stride must be a vector, not shape \",\n                                          stride_shape.DebugString()));\n      OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntVector(4, &stride_));\n    }\n\n    OP_REQUIRES(ctx, ksize_.size() == num_dims(),\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify \",\n                                        num_dims(), \" dimensions\"));\n    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));\n    OP_REQUIRES(ctx, stride_.size() == num_dims(),\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify \",\n                                        num_dims(), \" dimensions\"));\n    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));\n\n    const TensorShape tensor_in_shape = ctx->InputShape(0);\n    const TensorShape tensor_out_shape = ctx->InputShape(1);\n    const TensorShape out_backprop_shape = ctx->InputShape(2);\n\n    // For maxpooling, tensor_in should have num_dims() dimensions.\n    OP_REQUIRES(ctx, tensor_in_shape.dims() == num_dims(),\n                errors::InvalidArgument(\"tensor_in must be \", num_dims(),\n                                        \"-dimensional\"));\n    OP_REQUIRES(ctx, tensor_out_shape.dims() == num_dims(),\n                errors::InvalidArgument(\"tensor_out must be \", num_dims(),\n                                        \"-dimensional\"));\n    // For maxpooling, out_backprop should have num_dims() dimensions.\n    OP_REQUIRES(ctx, out_backprop_shape.dims() == num_dims(),\n                errors::InvalidArgument(\"out_backprop must be \", num_dims(),\n                                        \"-dimensional\"));\n\n    // TODO(phawkins): The XLA version doesn't need tensor_out. Investigate\n    // whether this is a good time/space tradeoff.\n    auto input = ctx->Input(0);\n    auto out_backprop = ctx->Input(2);\n    // We ensured padding_ is not EXPLICIT in the constructor.\n    xla::Padding xla_padding =\n        (padding_ == VALID) ? xla::Padding::kValid : xla::Padding::kSame;\n\n    // Create a MaxPool operation to check the expected resulting shape, and\n    // then throw away the operation because we don't actually need it here.\n    TensorShape expected_out_shape;\n    auto pooling =\n        xla::MaxPool(ctx->Input(0), ksize_, stride_, xla_padding,\n                     XlaTensorFormat(data_format_, tensor_in_shape.dims() - 2));\n    auto status_or_shape = pooling.builder()->GetShape(pooling);\n    OP_REQUIRES_OK(ctx, status_or_shape.status());\n    OP_REQUIRES_OK(ctx, XLAShapeToTensorShape(status_or_shape.value(),\n                                              &expected_out_shape));\n    OP_REQUIRES(ctx, expected_out_shape == out_backprop_shape,\n                errors::Unimplemented(\"The output dimensions do not match the \"\n                                      \"other input values.\"));\n\n    xla::PrimitiveType element_type;\n    OP_REQUIRES_OK(ctx, DataTypeToPrimitiveType(input_type(2), &element_type));\n    xla::XlaOp init_value = XlaHelpers::Zero(ctx->builder(), input_type(2));\n    auto select = CreateScalarGeComputation(element_type, ctx->builder());\n    auto scatter = CreateScalarAddComputation(element_type, ctx->builder());\n    xla::XlaOp gradients =\n        xla::SelectAndScatter(input, select, ksize_, stride_, xla_padding,\n                              out_backprop, init_value, scatter);\n\n    ctx->SetOutput(0, gradients);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def unban(bot, trigger):\n    \"\"\"Unban a user from the channel. The bot must be a channel operator for this command to work.\"\"\"\n    if ',' in str(parse_host_mask(trigger.group().split())):\n        return bot.reply('Unable to ban. Banning multiple users is not allowed.')\n    if '#' in str(parse_host_mask(trigger.group().split())):\n        return bot.reply('Unable to ban. Use of # when banning is not expected.')\n    makemodechange(bot, trigger, '-b', isbqmode=True)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _ensureSnapshotsFolder(self, subdir=None):\n        \"\"\" Ensure that the appropriate snapshot folder exists.\n        \"\"\"\n        path = ['snapshots', self._snapshot_id]\n\n        if subdir is not None:\n            path.extend(subdir.split('/'))\n\n        current = self._tool\n\n        for element in path:\n\n            if element not in current.objectIds():\n                # No Unicode IDs!\n                current._setObject(str(element), Folder(element))\n                current = current._getOb(element)\n                current.manage_permission(view, ('Manager', 'Owner'), 0)\n            else:\n                current = current._getOb(element)\n\n        return current\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static const char* ConvertOneFloat(PyObject* v, T* out) {\n  if (PyErr_Occurred()) {\n    return nullptr;\n  }\n  if (TF_PREDICT_TRUE(PyFloat_Check(v))) {\n    const double as_double = PyFloat_AS_DOUBLE(v);\n    *out = static_cast<T>(as_double);\n    // Check for overflow\n    if (TF_PREDICT_FALSE(CheckForOverflow<T>(as_double, out))) {\n      return ErrorOutOfRangeDouble;\n    }\n    return nullptr;\n  }\n#if PY_MAJOR_VERSION < 3\n  if (PyInt_Check(v)) {\n    *out = static_cast<T>(PyInt_AS_LONG(v));\n    return nullptr;\n  }\n#endif\n  if (PyLong_Check(v)) {\n    *out = static_cast<T>(PyLong_AsDouble(v));\n    if (PyErr_Occurred()) return ErrorOutOfRangeDouble;\n    return nullptr;\n  }\n  if (PyIsInstance(v, &PyFloatingArrType_Type)) {  // NumPy float types\n    Safe_PyObjectPtr as_float = make_safe(PyNumber_Float(v));\n    if (PyErr_Occurred()) {\n      return nullptr;\n    }\n    return ConvertOneFloat<T>(as_float.get(), out);\n  }\n  if (PyIsInstance(v, &PyIntegerArrType_Type)) {  // NumPy integers\n#if PY_MAJOR_VERSION < 3\n    Safe_PyObjectPtr as_int = make_safe(PyNumber_Int(v));\n#else\n    Safe_PyObjectPtr as_int = make_safe(PyNumber_Long(v));\n#endif\n    if (PyErr_Occurred()) {\n      return nullptr;\n    }\n    return ConvertOneFloat<T>(as_int.get(), out);\n  }\n  return ErrorMixedTypes;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(\n        self,\n        privacy_request: PrivacyRequest,\n        dsr_data: Dict[str, Any],\n    ):\n        \"\"\"\n        Manages populating HTML templates from the given data and adding the generated\n        pages to a zip file in a way that the pages can be navigated between.\n        \"\"\"\n\n        # zip file variables\n        self.baos = BytesIO()\n\n        # we close this in the finally block of generate()\n        # pylint: disable=consider-using-with\n        self.out = zipfile.ZipFile(self.baos, \"w\")\n\n        # Jinja template environment initialization\n        def pretty_print(value: str, indent: int = 4) -> str:\n            return json.dumps(value, indent=indent, default=storage_json_encoder)\n\n        jinja2.filters.FILTERS[\"pretty_print\"] = pretty_print\n        self.template_loader = Environment(\n            loader=FileSystemLoader(DSR_DIRECTORY), autoescape=True\n        )\n\n        # to pass in custom colors in the future\n        self.template_data: Dict[str, Any] = {\n            \"text_color\": TEXT_COLOR,\n            \"header_color\": HEADER_COLOR,\n            \"border_color\": BORDER_COLOR,\n        }\n        self.main_links: Dict[str, Any] = {}  # used to track the generated pages\n\n        # report data to populate the templates\n        self.request_data = _map_privacy_request(privacy_request)\n        self.dsr_data = dsr_data\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __call__(self, args: argparse.Namespace) -> None:\n        \"\"\"Perform the update.\"\"\"\n        manifest, path = dfetch.manifest.manifest.get_manifest()\n\n        exceptions: List[str] = []\n        destinations: List[str] = [\n            os.path.realpath(project.destination) for project in manifest.projects\n        ]\n        with in_directory(os.path.dirname(path)):\n            for project in manifest.selected_projects(args.projects):\n                with catch_runtime_exceptions(exceptions) as exceptions:\n                    self._check_destination(project, destinations)\n                    dfetch.project.make(project).update(force=args.force)\n\n                    if not args.no_recommendations and os.path.isdir(\n                        project.destination\n                    ):\n                        with in_directory(project.destination):\n                            check_child_manifests(manifest, project, path)\n\n        if exceptions:\n            raise RuntimeError(\"\\n\".join(exceptions))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const Tensor& filter_sizes = context->input(1);\n    const Tensor& out_backprop = context->input(2);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(filter_sizes.shape()),\n        errors::InvalidArgument(\n            \"Conv2DCustomBackpropFilter: filter_sizes input must be 1-dim, \"\n            \"not \",\n            filter_sizes.dims()));\n    TensorShape filter_shape;\n    OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\n                                filter_sizes.vec<int32>(), &filter_shape));\n\n    ConvBackpropDimensions dims;\n    OP_REQUIRES_OK(\n        context,\n        ConvBackpropComputeDimensionsV2(\n            \"Conv2DCustomBackpropFilter\", /*num_spatial_dims=*/2, input.shape(),\n            filter_shape, out_backprop.shape(), dilations_, strides_, padding_,\n            explicit_paddings_, data_format_, &dims));\n\n    Tensor* filter_backprop;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, filter_shape, &filter_backprop));\n\n    // If there is nothing to compute, return.\n    if (filter_shape.num_elements() == 0) {\n      return;\n    }\n\n    int64 pad_top, pad_bottom;\n    int64 pad_left, pad_right;\n    if (padding_ == Padding::EXPLICIT) {\n      pad_top = explicit_paddings_[2];\n      pad_bottom = explicit_paddings_[3];\n      pad_left = explicit_paddings_[4];\n      pad_right = explicit_paddings_[5];\n    }\n    OP_REQUIRES_OK(\n        context,\n        GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\n            dims.spatial_dims[0].stride, padding_,\n            &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom));\n    OP_REQUIRES_OK(\n        context,\n        GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\n            dims.spatial_dims[1].stride, padding_,\n            &dims.spatial_dims[1].output_size, &pad_left, &pad_right));\n#if defined TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS && \\\n    defined TENSORFLOW_USE_LIBXSMM_BACKWARD_CONVOLUTIONS\n    if (pad_left == pad_right && pad_top == pad_bottom) {\n      if (LaunchXsmmBackwardFilter<Device, T>()(\n              context, context->eigen_device<Device>(), input.tensor<T, 4>(),\n              filter_backprop->tensor<T, 4>(), out_backprop.tensor<T, 4>(),\n              dims.spatial_dims[0].input_size, dims.spatial_dims[1].input_size,\n              static_cast<int>(dims.spatial_dims[0].stride),\n              static_cast<int>(dims.spatial_dims[1].stride),\n              static_cast<int>(pad_top), static_cast<int>(pad_left),\n              data_format_)) {\n        return;\n      }\n    }\n#endif\n\n    // The total dimension size of each kernel.\n    const int filter_total_size = dims.spatial_dims[0].filter_size *\n                                  dims.spatial_dims[1].filter_size *\n                                  dims.in_depth;\n    OP_REQUIRES(\n        context,\n        filter_total_size * dims.out_depth == filter_backprop->NumElements(),\n        errors::InvalidArgument(\n            \"filter_size does not have enough elements, requested \",\n            filter_total_size * dims.out_depth, \", got \",\n            filter_backprop->NumElements()));\n\n    // The output image size is the spatial size of the output.\n    const int output_image_size =\n        dims.spatial_dims[0].output_size * dims.spatial_dims[1].output_size;\n\n    // Shard 'batch' images into 'shard_size' groups of images to be fed\n    // into the parallel matmul. Calculate 'shard_size' by dividing the L3 cache\n    // size ('target_working_set_size') by the matmul size of an individual\n    // image ('work_unit_size').\n\n    // TODO(andydavis)\n    // *) Get L3 cache size from device at runtime (30MB is from ivybridge).\n    // *) Consider reducing 'target_working_set_size' if L3 is shared by\n    //    other concurrently running tensorflow ops.\n    const size_t target_working_set_size = (30LL << 20) / sizeof(T);\n\n    const size_t size_A = output_image_size * filter_total_size;\n\n    const size_t size_B = output_image_size * dims.out_depth;\n\n    const size_t size_C = filter_total_size * dims.out_depth;\n\n    const size_t work_unit_size = size_A + size_B + size_C;\n\n    OP_REQUIRES(\n        context, work_unit_size != 0,\n        errors::InvalidArgument(\n            \"Work size for convolution would be 0, which is not acceptable\"));\n\n    const size_t shard_size =\n        (target_working_set_size + work_unit_size - 1) / work_unit_size;\n\n    Tensor col_buffer;\n    OP_REQUIRES_OK(context,\n                   context->allocate_temp(\n                       DataTypeToEnum<T>::value,\n                       TensorShape({static_cast<int64>(shard_size),\n                                    static_cast<int64>(output_image_size),\n                                    static_cast<int64>(filter_total_size)}),\n                       &col_buffer));\n\n    // The input offset corresponding to a single input image.\n    const int input_offset = dims.spatial_dims[0].input_size *\n                             dims.spatial_dims[1].input_size * dims.in_depth;\n    // The output offset corresponding to a single output image.\n    const int output_offset = dims.spatial_dims[0].output_size *\n                              dims.spatial_dims[1].output_size * dims.out_depth;\n\n    const T* input_data = input.template flat<T>().data();\n    T* col_buffer_data = col_buffer.template flat<T>().data();\n    const T* out_backprop_data = out_backprop.template flat<T>().data();\n    T* filter_backprop_data = filter_backprop->template flat<T>().data();\n\n    typedef Eigen::TensorMap<Eigen::Tensor<T, 2, Eigen::RowMajor>,\n                             Eigen::Unaligned>\n        TensorMap;\n    typedef Eigen::TensorMap<Eigen::Tensor<const T, 2, Eigen::RowMajor>,\n                             Eigen::Unaligned>\n        ConstTensorMap;\n\n    TensorMap C(filter_backprop_data, filter_total_size, dims.out_depth);\n    C.setZero();\n\n    // Initialize contraction dims (we need to transpose 'A' below).\n    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> contract_dims;\n    contract_dims[0].first = 0;\n    contract_dims[0].second = 0;\n\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n\n    for (int image_id = 0; image_id < dims.batch_size; image_id += shard_size) {\n      const int shard_limit =\n          std::min(static_cast<int>(shard_size),\n                   static_cast<int>(dims.batch_size) - image_id);\n\n      auto shard = [&input_data, &col_buffer_data, &dims, &pad_top, &pad_left,\n                    &pad_bottom, &pad_right, &input_offset,\n                    &size_A](int64 start, int64 limit) {\n        for (int shard_id = start; shard_id < limit; ++shard_id) {\n          const T* input_data_shard = input_data + shard_id * input_offset;\n          T* col_data_shard = col_buffer_data + shard_id * size_A;\n\n          // When we compute the gradient with respect to the filters, we need\n          // to do im2col to allow gemm-type computation.\n          Im2col<T>(\n              input_data_shard, dims.in_depth, dims.spatial_dims[0].input_size,\n              dims.spatial_dims[1].input_size, dims.spatial_dims[0].filter_size,\n              dims.spatial_dims[1].filter_size, pad_top, pad_left, pad_bottom,\n              pad_right, dims.spatial_dims[0].stride,\n              dims.spatial_dims[1].stride, col_data_shard);\n        }\n      };\n      Shard(worker_threads.num_threads, worker_threads.workers, shard_limit,\n            size_A, shard);\n\n      ConstTensorMap A(col_buffer_data, output_image_size * shard_limit,\n                       filter_total_size);\n      ConstTensorMap B(out_backprop_data, output_image_size * shard_limit,\n                       dims.out_depth);\n\n      // Gradient with respect to filter.\n      C.device(context->eigen_cpu_device()) += A.contract(B, contract_dims);\n\n      input_data += input_offset * shard_limit;\n      out_backprop_data += output_offset * shard_limit;\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def qute_settings(url):\n    \"\"\"Handler for qute://settings. View/change qute configuration.\"\"\"\n    global csrf_token\n\n    if url.path() == '/set':\n        if url.password() != csrf_token:\n            message.error(\"Invalid CSRF token for qute://settings!\")\n            raise QuteSchemeError(\"Invalid CSRF token!\",\n                                  QNetworkReply.ContentAccessDenied)\n        return _qute_settings_set(url)\n\n    # Requests to qute://settings/set should only be allowed from\n    # qute://settings. As an additional security precaution, we generate a CSRF\n    # token to use here.\n    if secrets:\n        csrf_token = secrets.token_urlsafe()\n    else:\n        # On Python < 3.6, from secrets.py\n        token = base64.urlsafe_b64encode(os.urandom(32))\n        csrf_token = token.rstrip(b'=').decode('ascii')\n\n    src = jinja.render('settings.html', title='settings',\n                       configdata=configdata,\n                       confget=config.instance.get_str,\n                       csrf_token=csrf_token)\n    return 'text/html', src\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _clean_code(self, code: str) -> str:\n        \"\"\"\n        A method to clean the code to prevent malicious code execution\n\n        Args:\n            code(str): A python code\n\n        Returns (str): Returns a Clean Code String\n\n        \"\"\"\n\n        tree = ast.parse(code)\n\n        new_body = []\n\n        # clear recent optional dependencies\n        self._additional_dependencies = []\n\n        for node in tree.body:\n            if isinstance(node, (ast.Import, ast.ImportFrom)):\n                self._check_imports(node)\n                continue\n            if self._is_df_overwrite(node) or self._is_jailbreak(node):\n                continue\n            new_body.append(node)\n\n        new_tree = ast.Module(body=new_body)\n        return astor.to_source(new_tree, pretty_source=lambda x: \"\".join(x)).strip()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render_POST(self, request):\n        \"\"\"\n        Register with the Identity Server\n        \"\"\"\n        send_cors(request)\n\n        args = get_args(request, ('matrix_server_name', 'access_token'))\n\n        hostname = args['matrix_server_name'].lower()\n\n        if not is_valid_hostname(hostname):\n            request.setResponseCode(400)\n            return {\n                'errcode': 'M_INVALID_PARAM',\n                'error': 'matrix_server_name must be a valid hostname'\n            }\n\n        result = yield self.client.get_json(\n            \"matrix://%s/_matrix/federation/v1/openid/userinfo?access_token=%s\"\n            % (\n                hostname,\n                urllib.parse.quote(args['access_token']),\n            ),\n            1024 * 5,\n        )\n        if 'sub' not in result:\n            raise Exception(\"Invalid response from homeserver\")\n\n        user_id = result['sub']\n        tok = yield issueToken(self.sydent, user_id)\n\n        # XXX: `token` is correct for the spec, but we released with `access_token`\n        # for a substantial amount of time. Serve both to make spec-compliant clients\n        # happy.\n        defer.returnValue({\n            \"access_token\": tok,\n            \"token\": tok,\n        })\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def _wait_for_free_memory(self, alloc_size: int, timeout: Optional[float]):\n        start_time = time.perf_counter()\n        loop = asyncio.get_event_loop()\n\n        with self._enqueued_size.get_lock():\n            self._enqueued_size.value += alloc_size\n        allocated = False\n        try:\n            context_manager = async_timeout.timeout(timeout) if timeout != 0 else contextlib.AsyncExitStack()\n            # contextlib.AsyncExitStack() is used as a null context here\n            async with context_manager:\n                if timeout == 0 and self.current_size_bytes + self.enqueued_size_bytes > self.max_size_bytes:\n                    raise AllocationFailed(f\"Could not allocate {alloc_size} bytes immediately: out of memory\")\n                async with enter_asynchronously(self._lock_acquire_memory):\n                    if self.current_size_bytes + alloc_size > self.max_size_bytes:\n                        if timeout == 0:\n                            raise AllocationFailed(f\"Could not allocate {alloc_size} bytes immediately: out of memory\")\n                        elapsed_time = time.perf_counter() - start_time\n                        remaining_timeout = max(0.0, timeout - elapsed_time) if timeout is not None else None\n                        await loop.run_in_executor(None, self._wait_until_available, alloc_size, remaining_timeout)\n\n                allocated = True\n                with self._enqueued_size.get_lock():\n                    self._enqueued_size.value -= alloc_size\n                yield\n        except asyncio.TimeoutError:\n            raise AllocationFailed(f\"Could not allocate {alloc_size} within {timeout} seconds\")\n        finally:\n            if not allocated:\n                with self._enqueued_size.get_lock():\n                    self._enqueued_size.value -= alloc_size\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static int generate_key(DH *dh)\n{\n    int ok = 0;\n    int generate_new_key = 0;\n#ifndef FIPS_MODULE\n    unsigned l;\n#endif\n    BN_CTX *ctx = NULL;\n    BIGNUM *pub_key = NULL, *priv_key = NULL;\n\n    if (BN_num_bits(dh->params.p) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_LARGE);\n        return 0;\n    }\n\n    if (dh->params.q != NULL\n        && BN_num_bits(dh->params.q) > OPENSSL_DH_MAX_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_Q_TOO_LARGE);\n        return 0;\n    }\n\n    if (BN_num_bits(dh->params.p) < DH_MIN_MODULUS_BITS) {\n        ERR_raise(ERR_LIB_DH, DH_R_MODULUS_TOO_SMALL);\n        return 0;\n    }\n\n    ctx = BN_CTX_new_ex(dh->libctx);\n    if (ctx == NULL)\n        goto err;\n\n    if (dh->priv_key == NULL) {\n        priv_key = BN_secure_new();\n        if (priv_key == NULL)\n            goto err;\n        generate_new_key = 1;\n    } else {\n        priv_key = dh->priv_key;\n    }\n\n    if (dh->pub_key == NULL) {\n        pub_key = BN_new();\n        if (pub_key == NULL)\n            goto err;\n    } else {\n        pub_key = dh->pub_key;\n    }\n    if (generate_new_key) {\n        /* Is it an approved safe prime ?*/\n        if (DH_get_nid(dh) != NID_undef) {\n            int max_strength =\n                    ossl_ifc_ffc_compute_security_bits(BN_num_bits(dh->params.p));\n\n            if (dh->params.q == NULL\n                || dh->length > BN_num_bits(dh->params.q))\n                goto err;\n            /* dh->length = maximum bit length of generated private key */\n            if (!ossl_ffc_generate_private_key(ctx, &dh->params, dh->length,\n                                               max_strength, priv_key))\n                goto err;\n        } else {\n#ifdef FIPS_MODULE\n            if (dh->params.q == NULL)\n                goto err;\n#else\n            if (dh->params.q == NULL) {\n                /* secret exponent length, must satisfy 2^(l-1) <= p */\n                if (dh->length != 0\n                    && dh->length >= BN_num_bits(dh->params.p))\n                    goto err;\n                l = dh->length ? dh->length : BN_num_bits(dh->params.p) - 1;\n                if (!BN_priv_rand_ex(priv_key, l, BN_RAND_TOP_ONE,\n                                     BN_RAND_BOTTOM_ANY, 0, ctx))\n                    goto err;\n                /*\n                 * We handle just one known case where g is a quadratic non-residue:\n                 * for g = 2: p % 8 == 3\n                 */\n                if (BN_is_word(dh->params.g, DH_GENERATOR_2)\n                    && !BN_is_bit_set(dh->params.p, 2)) {\n                    /* clear bit 0, since it won't be a secret anyway */\n                    if (!BN_clear_bit(priv_key, 0))\n                        goto err;\n                }\n            } else\n#endif\n            {\n                /* Do a partial check for invalid p, q, g */\n                if (!ossl_ffc_params_simple_validate(dh->libctx, &dh->params,\n                                                     FFC_PARAM_TYPE_DH, NULL))\n                    goto err;\n                /*\n                 * For FFC FIPS 186-4 keygen\n                 * security strength s = 112,\n                 * Max Private key size N = len(q)\n                 */\n                if (!ossl_ffc_generate_private_key(ctx, &dh->params,\n                                                   BN_num_bits(dh->params.q),\n                                                   MIN_STRENGTH,\n                                                   priv_key))\n                    goto err;\n            }\n        }\n    }\n\n    if (!ossl_dh_generate_public_key(ctx, dh, priv_key, pub_key))\n        goto err;\n\n    dh->pub_key = pub_key;\n    dh->priv_key = priv_key;\n    dh->dirty_cnt++;\n    ok = 1;\n err:\n    if (ok != 1)\n        ERR_raise(ERR_LIB_DH, ERR_R_BN_LIB);\n\n    if (pub_key != dh->pub_key)\n        BN_free(pub_key);\n    if (priv_key != dh->priv_key)\n        BN_free(priv_key);\n    BN_CTX_free(ctx);\n    return ok;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def delete(self):\n        from airflow.api.common.experimental import delete_dag\n        from airflow.exceptions import DagFileExists, DagNotFound\n\n        dag_id = request.values.get('dag_id')\n        origin = get_safe_url(request.values.get('origin'))\n\n        try:\n            delete_dag.delete_dag(dag_id)\n        except DagNotFound:\n            flash(\"DAG with id {} not found. Cannot delete\".format(dag_id), 'error')\n            return redirect(request.referrer)\n        except DagFileExists:\n            flash(\"Dag id {} is still in DagBag. \"\n                  \"Remove the DAG file first.\".format(dag_id),\n                  'error')\n            return redirect(request.referrer)\n\n        flash(\"Deleting DAG with id {}. May take a couple minutes to fully\"\n              \" disappear.\".format(dag_id))\n\n        # Upon success return to origin.\n        return redirect(origin)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def download_url(url, proxies=None):\n    \"\"\"\n    Downloads a given url in a temporary file. This function is not safe to use in multiple processes. Its only use is\n    for deprecated behavior allowing to download config/models with a single url instead of using the Hub.\n\n    Args:\n        url (`str`): The url of the file to download.\n        proxies (`Dict[str, str]`, *optional*):\n            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n            'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n\n    Returns:\n        `str`: The location of the temporary file where the url was downloaded.\n    \"\"\"\n    warnings.warn(\n        f\"Using `from_pretrained` with the url of a file (here {url}) is deprecated and won't be possible anymore in\"\n        \" v5 of Transformers. You should host your file on the Hub (hf.co) instead and use the repository ID. Note\"\n        \" that this is not compatible with the caching system (your file will be downloaded at each execution) or\"\n        \" multiple processes (each process will download the file in a different temporary file).\"\n    )\n    tmp_file = tempfile.mkstemp()[1]\n    with open(tmp_file, \"wb\") as f:\n        http_get(url, f, proxies=proxies)\n    return tmp_file\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def put(self, name: str):\n        \"\"\"add new file\n        params in FormData:\n            - file\n            - original_file_name [optional]\n        \"\"\"\n\n        data = {}\n        mindsdb_file_name = name\n\n        existing_file_names = ca.file_controller.get_files_names()\n\n        def on_field(field):\n            name = field.field_name.decode()\n            value = field.value.decode()\n            data[name] = value\n\n        file_object = None\n\n        def on_file(file):\n            nonlocal file_object\n            data[\"file\"] = clear_filename(file.file_name.decode())\n            file_object = file.file_object\n\n        temp_dir_path = tempfile.mkdtemp(prefix=\"mindsdb_file_\")\n\n        if request.headers[\"Content-Type\"].startswith(\"multipart/form-data\"):\n            parser = multipart.create_form_parser(\n                headers=request.headers,\n                on_field=on_field,\n                on_file=on_file,\n                config={\n                    \"UPLOAD_DIR\": temp_dir_path.encode(),  # bytes required\n                    \"UPLOAD_KEEP_FILENAME\": True,\n                    \"UPLOAD_KEEP_EXTENSIONS\": True,\n                    \"MAX_MEMORY_FILE_SIZE\": 0,\n                },\n            )\n\n            while True:\n                chunk = request.stream.read(8192)\n                if not chunk:\n                    break\n                parser.write(chunk)\n            parser.finalize()\n            parser.close()\n\n            if file_object is not None and not file_object.closed:\n                file_object.close()\n        else:\n            data = request.json\n\n        if mindsdb_file_name in existing_file_names:\n            return http_error(\n                400,\n                \"File already exists\",\n                f\"File with name '{data['file']}' already exists\",\n            )\n\n        if data.get(\"source_type\") == \"url\":\n            url = data[\"source\"]\n            data[\"file\"] = clear_filename(data[\"name\"])\n\n            config = Config()\n            is_cloud = config.get(\"cloud\", False)\n            if is_cloud and is_private_url(url):\n                return http_error(\n                    400, f'URL is private: {url}'\n                )\n\n            if is_cloud is True and ctx.user_class != 1:\n                info = requests.head(url)\n                file_size = info.headers.get(\"Content-Length\")\n                try:\n                    file_size = int(file_size)\n                except Exception:\n                    pass\n\n                if file_size is None:\n                    return http_error(\n                        400,\n                        \"Error getting file info\",\n                        \"\u0421an't determine remote file size\",\n                    )\n                if file_size > 1024 * 1024 * 100:\n                    return http_error(\n                        400, \"File is too big\", \"Upload limit for file is 100Mb\"\n                    )\n            with requests.get(url, stream=True) as r:\n                if r.status_code != 200:\n                    return http_error(\n                        400, \"Error getting file\", f\"Got status code: {r.status_code}\"\n                    )\n                file_path = os.path.join(temp_dir_path, data[\"file\"])\n                with open(file_path, \"wb\") as f:\n                    for chunk in r.iter_content(chunk_size=8192):\n                        f.write(chunk)\n\n        original_file_name = clear_filename(data.get(\"original_file_name\"))\n\n        file_path = os.path.join(temp_dir_path, data[\"file\"])\n        lp = file_path.lower()\n        if lp.endswith((\".zip\", \".tar.gz\")):\n            if lp.endswith(\".zip\"):\n                with zipfile.ZipFile(file_path) as f:\n                    f.extractall(temp_dir_path)\n            elif lp.endswith(\".tar.gz\"):\n                with tarfile.open(file_path) as f:\n                    safe_extract(f, temp_dir_path)\n            os.remove(file_path)\n            files = os.listdir(temp_dir_path)\n            if len(files) != 1:\n                os.rmdir(temp_dir_path)\n                return http_error(\n                    400, \"Wrong content.\", \"Archive must contain only one data file.\"\n                )\n            file_path = os.path.join(temp_dir_path, files[0])\n            mindsdb_file_name = files[0]\n            if not os.path.isfile(file_path):\n                os.rmdir(temp_dir_path)\n                return http_error(\n                    400, \"Wrong content.\", \"Archive must contain data file in root.\"\n                )\n\n        ca.file_controller.save_file(\n            mindsdb_file_name, file_path, file_name=original_file_name\n        )\n\n        os.rmdir(temp_dir_path)\n\n        return \"\", 200\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "   public static File writeStreamToTempFile(InputStream inputStream, String tempFileSuffix) throws IOException\n   {\n      FileOutputStream outputStream = null;\n\n      try\n      {\n         File file = Files.createTempFile(\"mpxj\", tempFileSuffix).toFile();\n         outputStream = new FileOutputStream(file);\n         byte[] buffer = new byte[1024];\n         while (true)\n         {\n            int bytesRead = inputStream.read(buffer);\n            if (bytesRead == -1)\n            {\n               break;\n            }\n            outputStream.write(buffer, 0, bytesRead);\n         }\n         return file;\n      }\n\n      finally\n      {\n         if (outputStream != null)\n         {\n            outputStream.close();\n         }\n      }\n   }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_services(self, context):\n        self.assert_admin(context)\n        service_list = self.catalog_api.list_services(context)\n        service_refs = [self.catalog_api.get_service(context, x)\n                        for x in service_list]\n        return {'OS-KSADM:services': service_refs}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& spectrogram = context->input(0);\n    OP_REQUIRES(context, spectrogram.dims() == 3,\n                errors::InvalidArgument(\"spectrogram must be 3-dimensional\",\n                                        spectrogram.shape().DebugString()));\n    const Tensor& sample_rate_tensor = context->input(1);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(sample_rate_tensor.shape()),\n                errors::InvalidArgument(\n                    \"Input sample_rate should be a scalar tensor, got \",\n                    sample_rate_tensor.shape().DebugString(), \" instead.\"));\n    const int32_t sample_rate = sample_rate_tensor.scalar<int32>()();\n\n    const int spectrogram_channels = spectrogram.dim_size(2);\n    const int spectrogram_samples = spectrogram.dim_size(1);\n    const int audio_channels = spectrogram.dim_size(0);\n\n    Mfcc mfcc;\n    mfcc.set_upper_frequency_limit(upper_frequency_limit_);\n    mfcc.set_lower_frequency_limit(lower_frequency_limit_);\n    mfcc.set_filterbank_channel_count(filterbank_channel_count_);\n    mfcc.set_dct_coefficient_count(dct_coefficient_count_);\n    OP_REQUIRES(\n        context, mfcc.Initialize(spectrogram_channels, sample_rate),\n        errors::InvalidArgument(\"Mfcc initialization failed for channel count \",\n                                spectrogram_channels, \", sample rate \",\n                                sample_rate, \" and filterbank_channel_count \",\n                                filterbank_channel_count_));\n\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\n                       0,\n                       TensorShape({audio_channels, spectrogram_samples,\n                                    dct_coefficient_count_}),\n                       &output_tensor));\n\n    const float* spectrogram_flat = spectrogram.flat<float>().data();\n    float* output_flat = output_tensor->flat<float>().data();\n\n    for (int audio_channel = 0; audio_channel < audio_channels;\n         ++audio_channel) {\n      for (int spectrogram_sample = 0; spectrogram_sample < spectrogram_samples;\n           ++spectrogram_sample) {\n        const float* sample_data =\n            spectrogram_flat +\n            (audio_channel * spectrogram_samples * spectrogram_channels) +\n            (spectrogram_sample * spectrogram_channels);\n        std::vector<double> mfcc_input(sample_data,\n                                       sample_data + spectrogram_channels);\n        std::vector<double> mfcc_output;\n        mfcc.Compute(mfcc_input, &mfcc_output);\n        DCHECK_EQ(dct_coefficient_count_, mfcc_output.size());\n        float* output_data =\n            output_flat +\n            (audio_channel * spectrogram_samples * dct_coefficient_count_) +\n            (spectrogram_sample * dct_coefficient_count_);\n        for (int i = 0; i < dct_coefficient_count_; ++i) {\n          output_data[i] = mfcc_output[i];\n        }\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def render_markdown(value):\n    \"\"\"\n    Render text as Markdown\n\n    Example:\n        {{ text | render_markdown }}\n    \"\"\"\n    # Render Markdown\n    html = markdown(value, extensions=[\"fenced_code\", \"tables\"])\n\n    # Sanitize rendered HTML\n    html = clean_html(html)\n\n    return mark_safe(html)  # noqa: S308  # suspicious-mark-safe-usage, OK here since we sanitized the string earlier\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def ban(bot, trigger):\n    \"\"\"Ban a user from the channel. The bot must be a channel operator for this command to work.\"\"\"\n    if ',' in str(nick):\n        return bot.reply('Unable to ban. Banning multiple users is not allowed.') \n    if '#' in str(nick):\n        return bot.reply('Unable to ban. Use of # when banning is not expected.')\n    makemodechange(bot, trigger, '+b', isbqmode=True)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def authorize(request, configured_username, configured_password):\n    \"\"\"\n    Match authorization header present in the request against\n    configured username and password.\n    \"\"\"\n    # Use request.META instead of request.headers to make it\n    # compatible with Django versions below 2.2.\n    if \"HTTP_AUTHORIZATION\" not in request.META:\n        raise Unauthorized('\"HTTP_AUTHORIZATION\" is not present in the request object.')\n\n    authentication = request.META[\"HTTP_AUTHORIZATION\"]\n\n    disable_consumption = getattr(\n        settings, \"BASIC_AUTH_DISABLE_CONSUMING_AUTHORIZATION_HEADER\", False,\n    )\n    if not disable_consumption:\n        # Delete \"Authorization\" header so other authentication\n        # mechanisms do not try to use it.\n        request.META.pop(\"HTTP_AUTHORIZATION\")\n\n    authentication_tuple = authentication.split(\" \", 1)\n    if len(authentication_tuple) != 2:\n        raise Unauthorized(\"Invalid format of the authorization header.\")\n    auth_method = authentication_tuple[0]\n    auth = authentication_tuple[1]\n    if \"basic\" != auth_method.lower():\n        raise Unauthorized('\"Basic\" is not an authorization method.')\n    auth = base64.b64decode(auth.strip()).decode(\"utf-8\")\n    username, password = auth.split(\":\", 1)\n    username_valid = constant_time_compare(username, configured_username)\n    password_valid = constant_time_compare(password, configured_password)\n    if username_valid & password_valid:\n        return True\n    raise Unauthorized(\"Basic authentication credentials are invalid.\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __setitem__(self, key, value):\n        self.dict[_hkey(key)] = [_hval(value)]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _process_discovery_args(key_config):  # noqa: C901\n    \"\"\"Process rules for discovery filters on Account ID and Partition ID\n    :param key_config: The key configuration being parsed\n    :raises: ParameterParseError\n    \"\"\"\n    if \"discovery\" not in key_config:\n        if \"discovery-account\" in key_config or \"discovery-partition\" in key_config:\n            raise ParameterParseError(\n                \"Discovery-account and discovery-partition are valid only when the discovery attribute is set to true\"\n            )\n\n        key_config[\"discovery\"] = False\n        return\n\n    # Translate the raw value of 'discovery' as passed by customer into a bool we can work with\n    discovery = discovery_pseudobool(key_config.pop(\"discovery\")[0])\n    key_config[\"discovery\"] = discovery\n\n    accounts = key_config.get(\"discovery-account\", None)\n    partition = key_config.get(\"discovery-partition\", None)\n\n    if not discovery:\n        if accounts or partition:\n            raise ParameterParseError(\n                \"Discovery-account and discovery-partition are valid only when the discovery attribute is set to true\"\n            )\n    else:\n        if accounts and not partition:\n            raise ParameterParseError(\"When specifying discovery-account, you must also specify discovery-partition\")\n        if partition and not accounts:\n            raise ParameterParseError(\"When specifying discovery-partition, you must also specify discovery-account\")\n\n        if accounts and partition:\n            for account in accounts:\n                if len(account) == 0:\n                    raise ParameterParseError(\"Value passed to discovery-account cannot be empty\")\n            if len(partition) != 1:\n                raise ParameterParseError(\"You can only specify discovery-partition once\")\n            if not partition[0]:\n                raise ParameterParseError(\"Value passed to discovery-partition cannot be empty\")\n\n            key_config[\"discovery-partition\"] = partition[0]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        name = str(name).replace('\\\\', '/')\n        dir_name, file_name = os.path.split(name)\n        if '..' in pathlib.PurePath(dir_name).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n        validate_file_name(file_name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, add an underscore and a random 7\n        # character alphanumeric string (before the file extension, if one\n        # exists) to the filename until the generated filename doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n        return name\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool VerifyTensors(const Model& model, ErrorReporter* error_reporter) {\n  if (!model.subgraphs()) {\n    return true;\n  }\n  if (!model.buffers()) {\n    ReportError(error_reporter, \"Missing 'buffers' section.\");\n    return false;\n  }\n\n  for (const auto& subgraph : *model.subgraphs()) {\n    if (!subgraph->tensors()) {\n      continue;\n    }\n    for (const auto& tensor : *subgraph->tensors()) {\n      if (!tensor->buffer()) {\n        continue;\n      }\n      if (tensor->buffer() >= model.buffers()->size()) {\n        ReportError(error_reporter, \"Invalid tensor buffer index: %d\",\n                    tensor->buffer());\n        return false;\n      }\n      auto* buffer = model.buffers()->Get(tensor->buffer());\n      if (!buffer) {\n        ReportError(error_reporter, \"Tensor buffer %d not set\",\n                    tensor->buffer());\n        return false;\n      }\n\n      // Many transient tensors don't have data in the flatbuffer. Their\n      // buffers will be allocated by the interpreter at run-time.\n      if (buffer->data()) {\n        if (tensor->type() == TensorType_STRING) {\n          if (!VerifyStringTensorBuffer(*buffer, error_reporter)) {\n            return false;\n          }\n        } else {\n          if (!VerifyNumericTensorBuffer(*tensor, *buffer, error_reporter)) {\n            return false;\n          }\n        }\n      }\n    }\n  }\n  return true;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _login(server, creds_file=None, api_key=None, silent=False):\n    \"\"\"Login main function.\"\"\"\n    # fetch_api_key and check_version expect server to end in /\n    if server[-1] != \"/\":\n        server = server + \"/\"\n\n    # creds file path setup\n    if creds_file is None:\n        creds_file = os.path.expanduser(\"~/.onecodex\")\n\n    # check if the creds file exists and is readable\n    if not os.path.exists(creds_file):\n        if silent:\n            return None\n\n        creds = {}\n    elif not os.access(creds_file, os.R_OK):\n        click.echo(\"Please check the permissions on {}\".format(collapse_user(creds_file)), err=True)\n        sys.exit(1)\n    else:\n        with filelock.FileLock(\"{}.lock\".format(creds_file)):\n            # it is, so let's read it!\n            with open(creds_file, \"r\") as fp:\n                try:\n                    creds = json.load(fp)\n                except ValueError:\n                    click.echo(\n                        \"Your ~/.onecodex credentials file appears to be corrupted. \"  # noqa\n                        \"Please delete it and re-authorize.\",\n                        err=True,\n                    )\n                    sys.exit(1)\n\n            # check for updates if logged in more than one day ago\n            last_update = creds.get(\"updated_at\") or creds.get(\"saved_at\")\n            last_update = (\n                last_update if last_update else datetime.datetime.now().strftime(DATE_FORMAT)\n            )\n            diff = datetime.datetime.now() - datetime.datetime.strptime(last_update, DATE_FORMAT)\n\n            if diff.days >= 1:\n                # if creds_file is old, check for updates\n                upgrade_required, msg = check_version(__version__, server)\n                creds[\"updated_at\"] = datetime.datetime.now().strftime(DATE_FORMAT)\n\n                try:\n                    json.dump(creds, open(creds_file, \"w\"))\n                except Exception as e:\n                    if e.errno == errno.EACCES:\n                        click.echo(\n                            \"Please check the permissions on {}\".format(collapse_user(creds_file)),\n                            err=True,\n                        )\n                        sys.exit(1)\n                    else:\n                        raise\n\n                if upgrade_required:\n                    click.echo(\"\\nWARNING: {}\\n\".format(msg), err=True)\n\n            # finally, give the user back what they want (whether silent or not)\n            if silent:\n                return creds.get(\"api_key\", None)\n\n            click.echo(\n                \"Credentials file already exists ({}). Logout first.\".format(\n                    collapse_user(creds_file)\n                ),\n                err=True,\n            )\n            return creds.get(\"email\", None)\n\n    # creds_file was not found and we're not silent, so prompt user to login\n    email, api_key = login_uname_pwd(server, api_key=api_key)\n\n    if api_key is None:\n        click.echo(\n            \"We could not verify your credentials. Please check your email and password again. \"\n            \"If you have single sign-on (SSO) enabled, you must login with `onecodex --api-key $API_KEY login`. \"\n            \"Please contact help@onecodex.com if you continue to experience problems.\"\n        )\n        sys.exit(1)\n\n    creds.update(\n        {\n            \"api_key\": api_key,\n            \"saved_at\": datetime.datetime.now().strftime(DATE_FORMAT),\n            \"updated_at\": None,\n            \"email\": email,\n        }\n    )\n\n    with filelock.FileLock(\"{}.lock\".format(creds_file)):\n        try:\n            json.dump(creds, open(creds_file, \"w\"))\n        except Exception as e:\n            if e.errno == errno.EACCES:\n                click.echo(\"Please check the permissions on {}\".format(creds_file), err=True)\n                sys.exit(1)\n            else:\n                raise\n\n    click.echo(\"Your ~/.onecodex credentials file was successfully created.\", err=True)\n\n    return email\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& indices = context->input(0);\n    const Tensor& values = context->input(1);\n    const Tensor& shape = context->input(2);\n    const Tensor& weights = context->input(3);\n    bool use_weights = weights.NumElements() > 0;\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(indices.shape()),\n                errors::InvalidArgument(\n                    \"Input indices must be a 2-dimensional tensor. Got: \",\n                    indices.shape().DebugString()));\n\n    if (use_weights) {\n      OP_REQUIRES(\n          context, weights.shape() == values.shape(),\n          errors::InvalidArgument(\n              \"Weights and values must have the same shape. Weight shape: \",\n              weights.shape().DebugString(),\n              \"; values shape: \", values.shape().DebugString()));\n    }\n\n    OP_REQUIRES(context, shape.NumElements() != 0,\n                errors::InvalidArgument(\n                    \"The shape argument requires at least one element.\"));\n\n    bool is_1d = shape.NumElements() == 1;\n    int num_batches = is_1d ? 1 : shape.flat<int64>()(0);\n    int num_values = values.NumElements();\n\n    OP_REQUIRES(context, num_values == indices.shape().dim_size(0),\n                errors::InvalidArgument(\n                    \"Number of values must match first dimension of indices.\",\n                    \"Got \", num_values,\n                    \" values, indices shape: \", indices.shape().DebugString()));\n\n    const auto indices_values = indices.matrix<int64>();\n    const auto values_values = values.flat<T>();\n    const auto weight_values = weights.flat<W>();\n\n    auto per_batch_counts = BatchedMap<W>(num_batches);\n\n    T max_value = 0;\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      int batch = is_1d ? 0 : indices_values(idx, 0);\n      if (batch >= num_batches) {\n        OP_REQUIRES(context, batch < num_batches,\n                    errors::InvalidArgument(\n                        \"Indices value along the first dimension must be \",\n                        \"lower than the first index of the shape.\", \"Got \",\n                        batch, \" as batch and \", num_batches,\n                        \" as the first dimension of the shape.\"));\n      }\n      const auto& value = values_values(idx);\n      if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {\n        if (binary_output_) {\n          per_batch_counts[batch][value] = 1;\n        } else if (use_weights) {\n          per_batch_counts[batch][value] += weight_values(idx);\n        } else {\n          per_batch_counts[batch][value]++;\n        }\n        if (value > max_value) {\n          max_value = value;\n        }\n      }\n    }\n\n    int num_output_values = GetOutputSize(max_value, maxlength_, minlength_);\n    OP_REQUIRES_OK(context, OutputSparse<W>(per_batch_counts, num_output_values,\n                                            is_1d, context));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, verbose_name=None, name=None, verify_exists=False, **kwargs):\n        kwargs['max_length'] = kwargs.get('max_length', 200)\n        CharField.__init__(self, verbose_name, name, **kwargs)\n        self.validators.append(validators.URLValidator(verify_exists=verify_exists))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def from_unix_seconds(seconds: Union[int, float]) -> datetime:\n    if seconds > MAX_NUMBER:\n        return datetime.max\n    elif seconds < -MAX_NUMBER:\n        return datetime.min\n\n    while abs(seconds) > MS_WATERSHED:\n        seconds /= 1000\n    dt = EPOCH + timedelta(seconds=seconds)\n    return dt.replace(tzinfo=timezone.utc)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def savepoint(self, name: t.Optional[str] = None) -> Savepoint:\n        name = name or f\"savepoint_{self.get_savepoint_id()}\"\n        validate_savepoint_name(name)\n        await self.connection.execute(f\"SAVEPOINT {name}\")\n        return Savepoint(name=name, transaction=self)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const TensorShape& input_shape = input.shape();\n    const int32_t input_dims = input_shape.dims();\n\n    const Tensor& segment_id = context->input(1);\n    const TensorShape& segment_id_shape = segment_id.shape();\n    const int32_t segment_dims = segment_id_shape.dims();\n\n    const Tensor& num_segments_tensor = context->input(2);\n    OP_REQUIRES(context, num_segments_tensor.NumElements() != 0,\n                errors::InvalidArgument(\"Number of segments cannot be empty.\"));\n    OP_REQUIRES(context,\n                TensorShapeUtils::IsScalar(num_segments_tensor.shape()),\n                errors::InvalidArgument(\"Number of segments must be a scalar\"));\n    auto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();\n\n    OP_REQUIRES(\n        context, num_segments >= 0,\n        errors::InvalidArgument(\n            \"Number of segments must be non-negative but got \", num_segments));\n    OP_REQUIRES(context, segment_dims != 0,\n                errors::InvalidArgument(\"Segment_id cannot have rank 0\"));\n\n    OP_REQUIRES(\n        context, segment_dims <= input_dims,\n        errors::OutOfRange(\"Invalid segment_id rank \", segment_dims,\n                           \" for input with \", input_dims, \" dimension(s)\"));\n    for (auto i = 0; i < segment_dims; i++) {\n      OP_REQUIRES(\n          context, segment_id_shape.dim_size(i) == input_shape.dim_size(i),\n          errors::InvalidArgument(\n              \"Segment dimension is \", segment_id_shape.dim_size(i),\n              \" while input dimension is \", input_dims, \" in rank \", i));\n    }\n\n    // Making output tensor.\n    Tensor* output_tensor = nullptr;\n    TensorShape output_shape =\n        GetOutputShape(input_shape, segment_id_shape, num_segments);\n    OP_REQUIRES_OK(context, context->allocate_output(\"output\", output_shape,\n                                                     &output_tensor));\n\n    // Preparating flat tensors.\n    auto output_flat = output_tensor->flat<tstring>();\n    auto flat_segment_id = segment_id.flat<INDICES_TYPE>();\n    auto flat_input = input.flat<tstring>();\n\n    for (int i = 0; i < flat_segment_id.size(); i++) {\n      OP_REQUIRES(\n          context,\n          ((flat_segment_id(i) < num_segments) && (flat_segment_id(i) >= 0)),\n          errors::InvalidArgument(\n              \"segment_ids are not allowed to exceed num_segments or\"\n              \" to have negative values.\"));\n    }\n\n    int64_t big_stride;\n    int64_t small_stride;\n    std::tie(big_stride, small_stride) =\n        GetStrides<INDICES_TYPE>(input_shape, segment_id_shape);\n    auto relative_offset_set =\n        GetFlattenedRelativeOffsets<INDICES_TYPE>(small_stride, big_stride);\n    for (auto start_offset = 0; start_offset < big_stride; start_offset++) {\n      for (auto i = 0; i < relative_offset_set.size(); i++) {\n        auto output_index = start_offset + flat_segment_id(i) * big_stride;\n        auto offset = start_offset + relative_offset_set[i];\n        if (output_flat(output_index).length() != 0)\n          output_flat(output_index).append(separator_.c_str());\n        output_flat(output_index).append(flat_input(offset));\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ModelWeights::Initialize(OpKernelContext* const context) {\n  OpInputList sparse_indices_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"sparse_indices\", &sparse_indices_inputs));\n  OpInputList sparse_weights_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"sparse_weights\", &sparse_weights_inputs));\n  if (sparse_indices_inputs.size() != sparse_weights_inputs.size())\n    return errors::InvalidArgument(\n        \"sparse_indices and sparse_weights must have the same length, got \",\n        sparse_indices_inputs.size(), \" and \", sparse_weights_inputs.size());\n  OpInputList dense_weights_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"dense_weights\", &dense_weights_inputs));\n\n  OpOutputList sparse_weights_outputs;\n  TF_RETURN_IF_ERROR(context->output_list(\"out_delta_sparse_weights\",\n                                          &sparse_weights_outputs));\n  if (sparse_weights_outputs.size() != sparse_weights_inputs.size())\n    return errors::InvalidArgument(\n        \"out_delta_sparse_weights and sparse_weights must have the same \"\n        \"length, got \",\n        sparse_weights_outputs.size(), \" and \", sparse_weights_inputs.size());\n\n  OpOutputList dense_weights_outputs;\n  TF_RETURN_IF_ERROR(\n      context->output_list(\"out_delta_dense_weights\", &dense_weights_outputs));\n  if (dense_weights_outputs.size() != dense_weights_inputs.size())\n    return errors::InvalidArgument(\n        \"out_delta_dense_weights and dense_weights must have the same length, \"\n        \"got \",\n        dense_weights_outputs.size(), \" and \", dense_weights_inputs.size());\n\n  for (int i = 0; i < sparse_weights_inputs.size(); ++i) {\n    Tensor* delta_t;\n    TF_RETURN_IF_ERROR(sparse_weights_outputs.allocate(\n        i, sparse_weights_inputs[i].shape(), &delta_t));\n    // Convert the input vector to a row matrix in internal representation.\n    auto deltas = delta_t->shaped<float, 2>({1, delta_t->NumElements()});\n    deltas.setZero();\n    sparse_weights_.emplace_back(FeatureWeightsSparseStorage{\n        sparse_indices_inputs[i].flat<int64>(),\n        sparse_weights_inputs[i].shaped<float, 2>(\n            {1, sparse_weights_inputs[i].NumElements()}),\n        deltas});\n  }\n\n  // Reads in the weights, and allocates and initializes the delta weights.\n  const auto initialize_weights =\n      [&](const OpInputList& weight_inputs, OpOutputList* const weight_outputs,\n          std::vector<FeatureWeightsDenseStorage>* const feature_weights) {\n        for (int i = 0; i < weight_inputs.size(); ++i) {\n          Tensor* delta_t;\n          TF_RETURN_IF_ERROR(\n              weight_outputs->allocate(i, weight_inputs[i].shape(), &delta_t));\n          // Convert the input vector to a row matrix in internal\n          // representation.\n          auto deltas = delta_t->shaped<float, 2>({1, delta_t->NumElements()});\n          deltas.setZero();\n          feature_weights->emplace_back(FeatureWeightsDenseStorage{\n              weight_inputs[i].shaped<float, 2>(\n                  {1, weight_inputs[i].NumElements()}),\n              deltas});\n        }\n        return Status::OK();\n      };\n\n  return initialize_weights(dense_weights_inputs, &dense_weights_outputs,\n                            &dense_weights_);\n}\n"}, {"role": "assistant", "content": "False"}]}
