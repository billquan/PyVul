{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def failed(self):\n        dag_id = request.form.get('dag_id')\n        task_id = request.form.get('task_id')\n        origin = request.form.get('origin')\n        execution_date = request.form.get('execution_date')\n\n        confirmed = request.form.get('confirmed') == \"true\"\n        upstream = request.form.get('failed_upstream') == \"true\"\n        downstream = request.form.get('failed_downstream') == \"true\"\n        future = request.form.get('failed_future') == \"true\"\n        past = request.form.get('failed_past') == \"true\"\n\n        return self._mark_task_instance_state(dag_id, task_id, origin, execution_date,\n                                              confirmed, upstream, downstream,\n                                              future, past, State.FAILED)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def extract_js(response):\n    \"\"\"\n    extracts javascript from a given string\n    \"\"\"\n    return re_extract_js.findall(response)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor* input_indices;\n    const Tensor* input_values;\n    const Tensor* input_shape;\n    SparseTensorsMap* map;\n\n    OP_REQUIRES_OK(context, context->input(\"sparse_indices\", &input_indices));\n    OP_REQUIRES_OK(context, context->input(\"sparse_values\", &input_values));\n    OP_REQUIRES_OK(context, context->input(\"sparse_shape\", &input_shape));\n    OP_REQUIRES_OK(context, GetMap(context, true /* is_writing */, &map));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_indices->shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be a matrix but received shape \",\n                    input_indices->shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_values->shape()),\n                errors::InvalidArgument(\n                    \"Input values should be a vector but received shape \",\n                    input_values->shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shape should be a vector but received shape \",\n                    input_shape->shape().DebugString()));\n    OP_REQUIRES(\n        context,\n        input_values->shape().dim_size(0) == input_indices->shape().dim_size(0),\n        errors::InvalidArgument(\n            \"Number of values must match first dimension of indices. \", \"Got \",\n            input_values->shape().dim_size(0),\n            \" values, indices shape: \", input_indices->shape().DebugString()));\n    OP_REQUIRES(\n        context,\n        input_shape->shape().dim_size(0) == input_indices->shape().dim_size(1),\n        errors::InvalidArgument(\n            \"Number of dimensions must match second dimension of indices. \",\n            \"Got \", input_shape->shape().dim_size(0),\n            \" dimensions, indices shape: \",\n            input_indices->shape().DebugString()));\n\n    int rank = input_shape->NumElements();\n\n    OP_REQUIRES(\n        context, rank > 1,\n        errors::InvalidArgument(\n            \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\n\n    auto input_shape_vec = input_shape->vec<int64_t>();\n    int new_num_elements = 1;\n    bool overflow_ocurred = false;\n    for (int i = 0; i < input_shape_vec.size(); i++) {\n      new_num_elements =\n          MultiplyWithoutOverflow(new_num_elements, input_shape_vec(i));\n      if (new_num_elements < 0) {\n        overflow_ocurred = true;\n        break;\n      }\n    }\n\n    OP_REQUIRES(\n        context, !overflow_ocurred,\n        errors::Internal(\"Encountered overflow from large input shape.\"));\n\n    TensorShape tensor_input_shape(input_shape_vec);\n    gtl::InlinedVector<int64_t, 8> std_order(rank);\n    std::iota(std_order.begin(), std_order.end(), 0);\n    SparseTensor input_st;\n    OP_REQUIRES_OK(context, SparseTensor::Create(*input_indices, *input_values,\n                                                 tensor_input_shape, std_order,\n                                                 &input_st));\n\n    const int64_t N = input_shape_vec(0);\n\n    Tensor sparse_handles(DT_INT64, TensorShape({N}));\n    auto sparse_handles_t = sparse_handles.vec<int64_t>();\n\n    OP_REQUIRES_OK(context, input_st.IndicesValid());\n\n    // We can generate the output shape proto string now, for all\n    // minibatch entries.\n    TensorShape output_shape;\n    OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\n                                input_shape_vec.data() + 1,\n                                input_shape->NumElements() - 1, &output_shape));\n\n    // Get groups by minibatch dimension\n    std::unordered_set<int64_t> visited;\n    sparse::GroupIterable minibatch = input_st.group({0});\n    for (const auto& subset : minibatch) {\n      const int64_t b = subset.group()[0];\n      visited.insert(b);\n      OP_REQUIRES(\n          context, b > -1 && b < N,\n          errors::InvalidArgument(\n              \"Received unexpected column 0 value in input SparseTensor: \", b,\n              \" < 0 or >= N (= \", N, \")\"));\n\n      const auto indices = subset.indices();\n      const auto values = subset.values<T>();\n      const int64_t num_entries = values.size();\n\n      Tensor output_indices = Tensor(DT_INT64, {num_entries, rank - 1});\n      Tensor output_values = Tensor(DataTypeToEnum<T>::value, {num_entries});\n\n      auto output_indices_t = output_indices.matrix<int64_t>();\n      auto output_values_t = output_values.vec<T>();\n\n      for (int i = 0; i < num_entries; ++i) {\n        for (int d = 1; d < rank; ++d) {\n          output_indices_t(i, d - 1) = indices(i, d);\n        }\n        output_values_t(i) = values(i);\n      }\n\n      SparseTensor st_i;\n      OP_REQUIRES_OK(context,\n                     SparseTensor::Create(output_indices, output_values,\n                                          output_shape, &st_i));\n      int64_t handle;\n      OP_REQUIRES_OK(context, map->AddSparseTensor(context, st_i, &handle));\n      sparse_handles_t(b) = handle;\n    }\n\n    // Fill in any gaps; we must provide an empty ST for batch entries\n    // the grouper didn't find.\n    if (visited.size() < N) {\n      Tensor empty_indices(DT_INT64, {0, rank - 1});\n      Tensor empty_values(DataTypeToEnum<T>::value, {0});\n      SparseTensor empty_st;\n      OP_REQUIRES_OK(context, SparseTensor::Create(empty_indices, empty_values,\n                                                   output_shape, &empty_st));\n\n      for (int64_t b = 0; b < N; ++b) {\n        // We skipped this batch entry.\n        if (visited.find(b) == visited.end()) {\n          int64_t handle;\n          OP_REQUIRES_OK(context,\n                         map->AddSparseTensor(context, empty_st, &handle));\n          sparse_handles_t(b) = handle;\n        }\n      }\n    }\n\n    context->set_output(0, sparse_handles);\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def rpn(self, irc, msg, args):\n        \"\"\"<rpn math expression>\n\n        Returns the value of an RPN expression.\n        \"\"\"\n        stack = []\n        for arg in args:\n            try:\n                x = complex(arg)\n                if x == abs(x):\n                    x = abs(x)\n                stack.append(x)\n            except ValueError: # Not a float.\n                if arg in self._mathSafeEnv:\n                    f = self._mathSafeEnv[arg]\n                    if callable(f):\n                        called = False\n                        arguments = []\n                        while not called and stack:\n                            arguments.append(stack.pop())\n                            try:\n                                stack.append(f(*arguments))\n                                called = True\n                            except TypeError:\n                                pass\n                        if not called:\n                            irc.error(_('Not enough arguments for %s') % arg)\n                            return\n                    else:\n                        stack.append(f)\n                elif arg in self._rpnEnv:\n                    self._rpnEnv[arg](stack)\n                else:\n                    arg2 = stack.pop()\n                    arg1 = stack.pop()\n                    s = '%s%s%s' % (arg1, arg, arg2)\n                    try:\n                        stack.append(eval(s, self._mathSafeEnv, self._mathSafeEnv))\n                    except SyntaxError:\n                        irc.error(format(_('%q is not a defined function.'),\n                                         arg))\n                        return\n        if len(stack) == 1:\n            irc.reply(str(self._complexToString(complex(stack[0]))))\n        else:\n            s = ', '.join(map(self._complexToString, list(map(complex, stack))))\n            irc.reply(_('Stack: [%s]') % s)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def append(self, key, value):\n        self.dict.setdefault(_hkey(key), []).append(\n            value if isinstance(value, unicode) else str(value))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) final {\n    // Extract inputs and validate shapes and types.\n    const CSRSparseMatrix* input_matrix;\n    OP_REQUIRES_OK(ctx, ExtractVariantFromInput(ctx, 0, &input_matrix));\n    const Tensor& input_permutation_indices = ctx->input(1);\n\n    int64 num_rows;\n    int batch_size;\n    ValidateInputs(ctx, *input_matrix, input_permutation_indices, &batch_size,\n                   &num_rows);\n\n    // Allocate batch pointers.\n    Tensor batch_ptr(cpu_allocator(), DT_INT32, TensorShape({batch_size + 1}));\n    auto batch_ptr_vec = batch_ptr.vec<int32>();\n    batch_ptr_vec(0) = 0;\n\n    // Temporary vector of Eigen SparseMatrices to store the Sparse Cholesky\n    // factors.\n    // Note: we use column-compressed (CSC) SparseMatrix because SimplicialLLT\n    // returns the factors in column major format. Since our input should be\n    // symmetric, column major and row major is identical in storage. We just\n    // have to switch to reading the upper triangular part of the input, which\n    // corresponds to the lower triangular part in row major format.\n    std::vector<SparseMatrix> sparse_cholesky_factors(batch_size);\n\n    // TODO(anudhyan): Tune the cost per unit based on benchmarks.\n    const double nnz_per_row =\n        (input_matrix->total_nnz() / batch_size) / num_rows;\n    const int64 sparse_cholesky_cost_per_batch =\n        nnz_per_row * nnz_per_row * num_rows;\n    // Perform sparse Cholesky factorization of each batch in parallel.\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n    std::atomic<int64> invalid_input_index(-1);\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          sparse_cholesky_cost_per_batch,\n          [&](int64 batch_begin, int64 batch_end) {\n            for (int64 batch_index = batch_begin; batch_index < batch_end;\n                 ++batch_index) {\n              // Define an Eigen SparseMatrix Map to operate on the\n              // CSRSparseMatrix component without copying the data.\n              Eigen::Map<const SparseMatrix> sparse_matrix(\n                  num_rows, num_rows, input_matrix->nnz(batch_index),\n                  input_matrix->row_pointers_vec(batch_index).data(),\n                  input_matrix->col_indices_vec(batch_index).data(),\n                  input_matrix->values_vec<T>(batch_index).data());\n\n              Eigen::SimplicialLLT<SparseMatrix, Eigen::Upper,\n                                   Eigen::NaturalOrdering<int>>\n                  solver;\n              auto permutation_indices_flat =\n                  input_permutation_indices.flat<int32>().data();\n\n              // Invert the fill-in reducing ordering and apply it to the input\n              // sparse matrix.\n              Eigen::Map<\n                  Eigen::PermutationMatrix<Eigen::Dynamic, Eigen::Dynamic, int>>\n                  permutation(permutation_indices_flat + batch_index * num_rows,\n                              num_rows);\n              auto permutation_inverse = permutation.inverse();\n\n              SparseMatrix permuted_sparse_matrix;\n              permuted_sparse_matrix.template selfadjointView<Eigen::Upper>() =\n                  sparse_matrix.template selfadjointView<Eigen::Upper>()\n                      .twistedBy(permutation_inverse);\n\n              // Compute the Cholesky decomposition.\n              solver.compute(permuted_sparse_matrix);\n              if (solver.info() != Eigen::Success) {\n                invalid_input_index = batch_index;\n                return;\n              }\n\n              // Get the upper triangular factor, which would end up in the\n              // lower triangular part of the output CSRSparseMatrix when\n              // interpreted in row major format.\n              sparse_cholesky_factors[batch_index] =\n                  std::move(solver.matrixU());\n              // For now, batch_ptr contains the number of nonzeros in each\n              // batch.\n              batch_ptr_vec(batch_index + 1) =\n                  sparse_cholesky_factors[batch_index].nonZeros();\n            }\n          });\n\n    // Check for invalid input.\n    OP_REQUIRES(\n        ctx, invalid_input_index == -1,\n        errors::InvalidArgument(\n            \"Sparse Cholesky factorization failed for batch index \",\n            invalid_input_index.load(), \". The input might not be valid.\"));\n\n    // Compute a cumulative sum to obtain the batch pointers.\n    std::partial_sum(batch_ptr_vec.data(),\n                     batch_ptr_vec.data() + batch_size + 1,\n                     batch_ptr_vec.data());\n\n    // Allocate output Tensors.\n    const int64 total_nnz = batch_ptr_vec(batch_size);\n    Tensor output_row_ptr(cpu_allocator(), DT_INT32,\n                          TensorShape({(num_rows + 1) * batch_size}));\n    Tensor output_col_ind(cpu_allocator(), DT_INT32, TensorShape({total_nnz}));\n    Tensor output_values(cpu_allocator(), DataTypeToEnum<T>::value,\n                         TensorShape({total_nnz}));\n    auto output_row_ptr_ptr = output_row_ptr.flat<int32>().data();\n    auto output_col_ind_ptr = output_col_ind.flat<int32>().data();\n    auto output_values_ptr = output_values.flat<T>().data();\n\n    // Copy the output matrices from each batch into the CSRSparseMatrix\n    // Tensors.\n    // TODO(b/129906419): Factor out the copy from Eigen SparseMatrix to\n    // CSRSparseMatrix into common utils. This is also used in\n    // SparseMatrixSparseMatMul.\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          (3 * total_nnz) / batch_size /* cost per unit */,\n          [&](int64 batch_begin, int64 batch_end) {\n            for (int64 batch_index = batch_begin; batch_index < batch_end;\n                 ++batch_index) {\n              const SparseMatrix& cholesky_factor =\n                  sparse_cholesky_factors[batch_index];\n              const int64 nnz = cholesky_factor.nonZeros();\n\n              std::copy(cholesky_factor.outerIndexPtr(),\n                        cholesky_factor.outerIndexPtr() + num_rows + 1,\n                        output_row_ptr_ptr + batch_index * (num_rows + 1));\n              std::copy(cholesky_factor.innerIndexPtr(),\n                        cholesky_factor.innerIndexPtr() + nnz,\n                        output_col_ind_ptr + batch_ptr_vec(batch_index));\n              std::copy(cholesky_factor.valuePtr(),\n                        cholesky_factor.valuePtr() + nnz,\n                        output_values_ptr + batch_ptr_vec(batch_index));\n            }\n          });\n\n    // Create the CSRSparseMatrix instance from its component Tensors and\n    // prepare the Variant output Tensor.\n    CSRSparseMatrix output_csr_matrix;\n    OP_REQUIRES_OK(\n        ctx,\n        CSRSparseMatrix::CreateCSRSparseMatrix(\n            DataTypeToEnum<T>::value, input_matrix->dense_shape(), batch_ptr,\n            output_row_ptr, output_col_ind, output_values, &output_csr_matrix));\n    Tensor* output_csr_matrix_tensor;\n    AllocatorAttributes cpu_alloc;\n    cpu_alloc.set_on_host(true);\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, TensorShape({}), &output_csr_matrix_tensor,\n                                  cpu_alloc));\n    output_csr_matrix_tensor->scalar<Variant>()() =\n        std::move(output_csr_matrix);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "ALGnew(PyObject *self, PyObject *args, PyObject *kwdict)\n{\n\tunsigned char *key, *IV;\n\tALGobject * new=NULL;\n\tint keylen, IVlen=0, mode=MODE_ECB, segment_size=0;\n\tPyObject *counter = NULL;\n\tint counter_shortcut = 0;\n#ifdef PCT_ARC2_MODULE\n        int effective_keylen = 1024;    /* this is a weird default, but it's compatible with old versions of PyCrypto */\n#endif\n\t/* Set default values */\n\tif (!PyArg_ParseTupleAndKeywords(args, kwdict, \"s#|is#Oi\"\n#ifdef PCT_ARC2_MODULE\n\t\t\t\t\t \"i\"\n#endif\n\t\t\t\t\t , kwlist,\n\t\t\t\t\t &key, &keylen, &mode, &IV, &IVlen,\n\t\t\t\t\t &counter, &segment_size\n#ifdef PCT_ARC2_MODULE\n\t\t\t\t\t , &effective_keylen\n#endif\n\t\t)) \n\t{\n\t\treturn NULL;\n\t}\n\n\tif (mode<MODE_ECB || mode>MODE_CTR) \n\t{\n\t\tPyErr_Format(PyExc_ValueError, \n\t\t\t     \"Unknown cipher feedback mode %i\",\n\t\t\t     mode);\n\t\treturn NULL;\n\t}\n\tif (mode == MODE_PGP) {\n\t\tPyErr_Format(PyExc_ValueError, \n\t\t\t     \"MODE_PGP is not supported anymore\");\n\t\treturn NULL;\n\t}\n\tif (KEY_SIZE!=0 && keylen!=KEY_SIZE)\n\t{\n\t\tPyErr_Format(PyExc_ValueError,\n\t\t\t     \"Key must be %i bytes long, not %i\",\n\t\t\t     KEY_SIZE, keylen);\n\t\treturn NULL;\n\t}\n\tif (KEY_SIZE==0 && keylen==0)\n\t{\n\t\tPyErr_SetString(PyExc_ValueError,\n\t\t\t\t\"Key cannot be the null string\");\n\t\treturn NULL;\n\t}\n\tif (IVlen != BLOCK_SIZE && mode != MODE_ECB && mode != MODE_CTR)\n\t{\n\t\tPyErr_Format(PyExc_ValueError,\n\t\t\t     \"IV must be %i bytes long\", BLOCK_SIZE);\n\t\treturn NULL;\n\t}\n\n\t/* Mode-specific checks */\n\tif (mode == MODE_CFB) {\n\t\tif (segment_size == 0) segment_size = 8;\n\t\tif (segment_size < 1 || segment_size > BLOCK_SIZE*8 || ((segment_size & 7) != 0)) {\n\t\t\tPyErr_Format(PyExc_ValueError, \n\t\t\t\t     \"segment_size must be multiple of 8 (bits) \"\n\t\t\t\t     \"between 1 and %i\", BLOCK_SIZE*8);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\tif (mode == MODE_CTR) {\n\t\tif (counter == NULL) {\n\t\t\tPyErr_SetString(PyExc_TypeError,\n\t\t\t\t\t\"'counter' keyword parameter is required with CTR mode\");\n\t\t\treturn NULL;\n\t\t} else if (Py_TYPE(counter) == PCT_CounterBEType || Py_TYPE(counter) == PCT_CounterLEType) {\n\t\t\tcounter_shortcut = 1;\n\t\t} else if (!PyCallable_Check(counter)) {\n\t\t\tPyErr_SetString(PyExc_ValueError, \n\t\t\t\t\t\"'counter' parameter must be a callable object\");\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\tif (counter != NULL) {\n\t\t\tPyErr_SetString(PyExc_ValueError, \n\t\t\t\t\t\"'counter' parameter only useful with CTR mode\");\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\t/* Cipher-specific checks */\n#ifdef PCT_ARC2_MODULE\n        if (effective_keylen<0 || effective_keylen>1024) {\n\t\tPyErr_Format(PyExc_ValueError,\n\t\t\t     \"RC2: effective_keylen must be between 0 and 1024, not %i\",\n\t\t\t     effective_keylen);\n\t\treturn NULL;\n        }\n#endif\n\n\t/* Copy parameters into object */\n\tnew = newALGobject();\n\tnew->segment_size = segment_size;\n\tnew->counter = counter;\n\tPy_XINCREF(counter);\n\tnew->counter_shortcut = counter_shortcut;\n#ifdef PCT_ARC2_MODULE\n        new->st.effective_keylen = effective_keylen;\n#endif\n\n\tblock_init(&(new->st), key, keylen);\n\tif (PyErr_Occurred())\n\t{\n\t\tPy_DECREF(new);\n\t\treturn NULL;\n\t}\n\tmemset(new->IV, 0, BLOCK_SIZE);\n\tmemset(new->oldCipher, 0, BLOCK_SIZE);\n\tmemcpy(new->IV, IV, IVlen);\n\tnew->mode = mode;\n\tnew->count=BLOCK_SIZE;   /* stores how many bytes in new->oldCipher have been used */\n\treturn new;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext *ctx) override {\n    // (0) validations\n    const Tensor *a_indices, *b_indices, *a_values_t, *b_values_t, *a_shape,\n        *b_shape, *thresh_t;\n\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_indices\", &a_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_indices\", &b_indices));\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsMatrix(a_indices->shape()) &&\n                    TensorShapeUtils::IsMatrix(b_indices->shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be matrices but received shapes: \",\n                    a_indices->shape().DebugString(), \" and \",\n                    b_indices->shape().DebugString()));\n    const int64 a_nnz = a_indices->dim_size(0);\n    const int64 b_nnz = b_indices->dim_size(0);\n\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_values\", &b_values_t));\n\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(a_values_t->shape()) &&\n                    TensorShapeUtils::IsVector(b_values_t->shape()),\n                errors::InvalidArgument(\n                    \"Input values should be vectors but received shapes: \",\n                    a_values_t->shape().DebugString(), \" and \",\n                    b_values_t->shape().DebugString()));\n    auto a_values = ctx->input(1).vec<T>();\n    auto b_values = ctx->input(4).vec<T>();\n    OP_REQUIRES(\n        ctx, a_values.size() == a_nnz && b_values.size() == b_nnz,\n        errors::InvalidArgument(\"Expected \", a_nnz, \" and \", b_nnz,\n                                \" non-empty input values, got \",\n                                a_values.size(), \" and \", b_values.size()));\n\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_shape\", &a_shape));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_shape\", &b_shape));\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(a_shape->shape()) &&\n                    TensorShapeUtils::IsVector(b_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    a_shape->shape().DebugString(), \" and \",\n                    b_shape->shape().DebugString()));\n    OP_REQUIRES(\n        ctx, a_shape->IsSameSize(*b_shape),\n        errors::InvalidArgument(\n            \"Operands do not have the same ranks; got shapes: \",\n            a_shape->SummarizeValue(10), \" and \", b_shape->SummarizeValue(10)));\n    const auto a_shape_flat = a_shape->flat<int64>();\n    const auto b_shape_flat = b_shape->flat<int64>();\n    for (int i = 0; i < a_shape->NumElements(); ++i) {\n      OP_REQUIRES(ctx, a_shape_flat(i) == b_shape_flat(i),\n                  errors::InvalidArgument(\n                      \"Operands' shapes do not match: got \", a_shape_flat(i),\n                      \" and \", b_shape_flat(i), \" for dimension \", i));\n    }\n\n    OP_REQUIRES_OK(ctx, ctx->input(\"thresh\", &thresh_t));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(thresh_t->shape()),\n                errors::InvalidArgument(\n                    \"The magnitude threshold must be a scalar: got shape \",\n                    thresh_t->shape().DebugString()));\n    // std::abs() so that it works for complex{64,128} values as well\n    const Treal thresh = thresh_t->scalar<Treal>()();\n\n    // (1) do a pass over inputs, and append values and indices to vectors\n    auto a_indices_mat = a_indices->matrix<int64>();\n    auto b_indices_mat = b_indices->matrix<int64>();\n    std::vector<std::pair<bool, int64>> entries_to_copy;  // from_a?, idx\n    entries_to_copy.reserve(a_nnz + b_nnz);\n    std::vector<T> out_values;\n    const int num_dims = a_shape->dim_size(0);\n\n    OP_REQUIRES(ctx, num_dims > 0,\n                errors::InvalidArgument(\"Invalid input_a shape. Received: \",\n                                        a_shape->DebugString()));\n\n    // The input and output sparse tensors are assumed to be ordered along\n    // increasing dimension number.\n    int64 i = 0, j = 0;\n    T s;\n    while (i < a_nnz && j < b_nnz) {\n      switch (sparse::DimComparator::cmp(a_indices_mat, b_indices_mat, i, j,\n                                         num_dims)) {\n        case -1:\n          entries_to_copy.emplace_back(true, i);\n          out_values.push_back(a_values(i));\n          ++i;\n          break;\n        case 0:\n          s = a_values(i) + b_values(j);\n          if (thresh <= std::abs(s)) {\n            entries_to_copy.emplace_back(true, i);\n            out_values.push_back(s);\n          }\n          ++i;\n          ++j;\n          break;\n        case 1:\n          entries_to_copy.emplace_back(false, j);\n          out_values.push_back(b_values(j));\n          ++j;\n          break;\n      }\n    }\n\n#define HANDLE_LEFTOVERS(A_OR_B, IDX, IS_A)     \\\n  while (IDX < A_OR_B##_nnz) {                  \\\n    entries_to_copy.emplace_back(IS_A, IDX);    \\\n    out_values.push_back(A_OR_B##_values(IDX)); \\\n    ++IDX;                                      \\\n  }\n\n    // at most one of these calls appends new values\n    HANDLE_LEFTOVERS(a, i, true);\n    HANDLE_LEFTOVERS(b, j, false);\n#undef HANDLE_LEFTOVERS\n\n    // (2) allocate and fill output tensors\n    const int64 sum_nnz = out_values.size();\n    Tensor *out_indices_t, *out_values_t;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({sum_nnz, num_dims}),\n                                        &out_indices_t));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(1, TensorShape({sum_nnz}), &out_values_t));\n    auto out_indices_mat = out_indices_t->matrix<int64>();\n    auto out_values_flat = out_values_t->vec<T>();\n\n    for (i = 0; i < sum_nnz; ++i) {\n      const bool from_a = entries_to_copy[i].first;\n      const int64 idx = entries_to_copy[i].second;\n      out_indices_mat.chip<0>(i) =\n          from_a ? a_indices_mat.chip<0>(idx) : b_indices_mat.chip<0>(idx);\n    }\n    if (sum_nnz > 0) {\n      std::copy_n(out_values.begin(), sum_nnz, &out_values_flat(0));\n    }\n    ctx->set_output(2, *a_shape);\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def received_client_receipt(\n        self,\n        room_id: str,\n        receipt_type: str,\n        user_id: str,\n        event_id: str,\n        thread_id: Optional[str],\n    ) -> None:\n        \"\"\"Called when a client tells us a local user has read up to the given\n        event_id in the room.\n        \"\"\"\n        receipt = ReadReceipt(\n            room_id=room_id,\n            receipt_type=receipt_type,\n            user_id=user_id,\n            event_ids=[event_id],\n            thread_id=thread_id,\n            data={\"ts\": int(self.clock.time_msec())},\n        )\n\n        is_new = await self._handle_new_receipts([receipt])\n        if not is_new:\n            return\n\n        if self.federation_sender and receipt_type != ReceiptTypes.READ_PRIVATE:\n            await self.federation_sender.send_read_receipt(receipt)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def sendEmail(\n    sydent: \"Sydent\",\n    templateFile: str,\n    mailTo: str,\n    substitutions: Dict[str, str],\n    log_send_errors: bool = True,\n) -> None:\n    \"\"\"\n    Sends an email with the given parameters.\n\n    :param sydent: The Sydent instance to use when building the configuration to send the\n        email with.\n    :param templateFile: The filename of the template to use when building the body of the\n        email.\n    :param mailTo: The email address to send the email to.\n    :param substitutions: The substitutions to use with the template.\n    :param log_send_errors: Whether to log errors happening when sending an email.\n    \"\"\"\n    mailFrom = sydent.config.email.sender\n    myHostname = sydent.config.email.host_name\n\n    midRandom = \"\".join([random.choice(string.ascii_letters) for _ in range(16)])\n    messageid = \"<%d%s@%s>\" % (time_msec(), midRandom, myHostname)\n\n    substitutions.update(\n        {\n            \"messageid\": messageid,\n            \"date\": email.utils.formatdate(localtime=False),\n            \"to\": mailTo,\n            \"from\": mailFrom,\n        }\n    )\n\n    # use jinja for rendering if jinja templates are present\n    if templateFile.endswith(\".j2\"):\n        # We add randomize the multipart boundary to stop user input from\n        # conflicting with it.\n        substitutions[\"multipart_boundary\"] = generateAlphanumericTokenOfLength(32)\n        template = sydent.config.general.template_environment.get_template(templateFile)\n        mailString = template.render(substitutions)\n    else:\n        allSubstitutions = {}\n        for k, v in substitutions.items():\n            allSubstitutions[k] = v\n            allSubstitutions[k + \"_forhtml\"] = escape(v)\n            allSubstitutions[k + \"_forurl\"] = urllib.parse.quote(v)\n        allSubstitutions[\"multipart_boundary\"] = generateAlphanumericTokenOfLength(32)\n        with open(templateFile) as template_file:\n            mailString = template_file.read() % allSubstitutions\n\n    try:\n        check_valid_email_address(mailTo, allow_description=False)\n    except EmailAddressException:\n        logger.warning(\"Invalid email address %s\", mailTo)\n        raise\n\n    mailServer = sydent.config.email.smtp_server\n    mailPort = int(sydent.config.email.smtp_port)\n    mailUsername = sydent.config.email.smtp_username\n    mailPassword = sydent.config.email.smtp_password\n    mailTLSMode = sydent.config.email.tls_mode\n\n    logger.info(\n        \"Sending mail to %s with mail server: %s\"\n        % (\n            mailTo,\n            mailServer,\n        )\n    )\n    try:\n        smtp: smtplib.SMTP\n        if mailTLSMode == \"SSL\" or mailTLSMode == \"TLS\":\n            smtp = smtplib.SMTP_SSL(mailServer, mailPort, myHostname)\n        elif mailTLSMode == \"STARTTLS\":\n            smtp = smtplib.SMTP(mailServer, mailPort, myHostname)\n            smtp.starttls()\n        else:\n            smtp = smtplib.SMTP(mailServer, mailPort, myHostname)\n        if mailUsername != \"\":\n            smtp.login(mailUsername, mailPassword)\n\n        email_counter.inc()\n\n        # We're using the parsing above to do basic validation, but instead of\n        # failing it may munge the address it returns. So we should *not* use\n        # that parsed address, as it may not match any validation done\n        # elsewhere.\n        smtp.sendmail(mailFrom, mailTo, mailString.encode(\"utf-8\"))\n        smtp.quit()\n    except Exception as origException:\n        if log_send_errors:\n            twisted.python.log.err()\n        raise EmailSendException() from origException\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def write_data_table(self, report, report_data, has_totals=True):\n        self.data.append([c[\"title\"] for c in report.schema])\n        for datum in report_data:\n            datum = report.read_datum(datum)\n            self.data.append([format_data(data, format_iso_dates=True) for data in datum])\n\n        if has_totals:\n            for datum in report.get_totals(report_data):\n                datum = report.read_datum(datum)\n                self.data.append([format_data(data) for data in datum])\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static Status ValidateSavedTensors(const GraphDef& graph_def) {\n  for (const auto& node : graph_def.node()) {\n    const auto node_iterator = node.attr().find(\"value\");\n    if (node_iterator != node.attr().end()) {\n      AttrValue node_value = node_iterator->second;\n      if (node_value.has_tensor()) {\n        const PartialTensorShape node_shape(node_value.tensor().tensor_shape());\n        if (node_shape.num_elements() < 0) {\n          return errors::FailedPrecondition(\n              \"Saved model contains node \\\"\", node.name(), \"\\\" (op \\\"\",\n              node.op(), \"\\\") which initializes from a tensor with \",\n              node_shape.num_elements(), \" elements\");\n        }\n      }\n    }\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n    Tensor input_min_tensor;\n    Tensor input_max_tensor;\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    if (range_given_) {\n      input_min_tensor = ctx->input(1);\n      input_max_tensor = ctx->input(2);\n      if (axis_ == -1) {\n        auto min_val = input_min_tensor.scalar<T>()();\n        auto max_val = input_max_tensor.scalar<T>()();\n        OP_REQUIRES(ctx, min_val <= max_val,\n                    errors::InvalidArgument(\"Invalid range: input_min \",\n                                            min_val, \" > input_max \", max_val));\n      } else {\n        OP_REQUIRES(ctx, input_min_tensor.dim_size(0) == depth,\n                    errors::InvalidArgument(\n                        \"input_min_tensor has incorrect size, was \",\n                        input_min_tensor.dim_size(0), \" expected \", depth,\n                        \" to match dim \", axis_, \" of the input \",\n                        input_min_tensor.shape()));\n        OP_REQUIRES(ctx, input_max_tensor.dim_size(0) == depth,\n                    errors::InvalidArgument(\n                        \"input_max_tensor has incorrect size, was \",\n                        input_max_tensor.dim_size(0), \" expected \", depth,\n                        \" to match dim \", axis_, \" of the input \",\n                        input_max_tensor.shape()));\n      }\n    } else {\n      auto range_shape = (axis_ == -1) ? TensorShape({}) : TensorShape({depth});\n      OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                             range_shape, &input_min_tensor));\n      OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                             range_shape, &input_max_tensor));\n    }\n\n    if (axis_ == -1) {\n      functor::QuantizeAndDequantizeOneScaleFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(), input.flat<T>(), signed_input_, num_bits_,\n        range_given_, &input_min_tensor, &input_max_tensor, round_mode_,\n        narrow_range_, output->flat<T>());\n    } else {\n      functor::QuantizeAndDequantizePerChannelFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(),\n        input.template flat_inner_outer_dims<T, 3>(axis_ - 1), signed_input_,\n        num_bits_, range_given_, &input_min_tensor, &input_max_tensor,\n        round_mode_, narrow_range_,\n        output->template flat_inner_outer_dims<T, 3>(axis_ - 1));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def users(self, criteria=u\"\", search=u\"\", action=u\"\", **kwargs):\n\n        # If we're just showing the initial page, just do that\n        form = UserForm()\n        if action in [\"add\", \"edit\"] and not form.validate():\n            # Display the form error to user using flash\n            flash(form.error_message, level='error')\n\n        elif action in [\"add\", \"edit\"] and form.validate():\n            if action == 'add':\n                user = self.app.store.add_user(form.username.data, form.password.data)\n            else:\n                user = self.app.store.get_user(form.username.data)\n                if form.password.data:\n                    user.set_password(form.password.data, old_password=None)\n            # Don't allow the user to changes it's \"role\" state.\n            if form.username.data != self.app.currentuser.username:\n                user.role = form.role.data\n            user.email = form.email.data or ''\n            if form.user_root.data:\n                user.user_root = form.user_root.data\n                if not user.valid_user_root():\n                    flash(_(\"User's root directory %s is not accessible!\") % user.user_root, level='error')\n                    logger.warning(\"user's root directory %s is not accessible\" % user.user_root)\n            # Try to update disk quota if the human readable value changed.\n            # Report error using flash.\n            if form.disk_quota and form.disk_quota.data:\n                try:\n                    new_quota = form.disk_quota.data\n                    old_quota = humanfriendly.parse_size(humanfriendly.format_size(user.disk_quota, binary=True))\n                    if old_quota != new_quota:\n                        user.disk_quota = new_quota\n                        flash(_(\"User's quota updated\"), level='success')\n                except QuotaUnsupported as e:\n                    flash(_(\"Setting user's quota is not supported\"), level='warning')\n                except Exception as e:\n                    flash(_(\"Failed to update user's quota: %s\") % e, level='error')\n                    logger.warning(\"failed to update user's quota\", exc_info=1)\n            if action == 'add':\n                flash(_(\"User added successfully.\"))\n            else:\n                flash(_(\"User information modified successfully.\"))\n        elif action == 'delete':\n            if form.username.data == self.app.currentuser.username:\n                flash(_(\"You cannot remove your own account!\"), level='error')\n            else:\n                try:\n                    user = self.app.store.get_user(form.username.data)\n                    if user:\n                        user.delete()\n                        flash(_(\"User account removed.\"))\n                    else:\n                        flash(_(\"User doesn't exists!\"), level='warning')\n                except ValueError as e:\n                    flash(e, level='error')\n\n        params = {\n            \"form\": UserForm(formdata=None),\n            \"criteria\": criteria,\n            \"search\": search,\n            \"users\": list(self.app.store.users(search=search, criteria=criteria))}\n\n        # Build users page\n        return self._compile_template(\"admin_users.html\", **params)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_lm_corpus(datadir, dataset):\n    fn = os.path.join(datadir, \"cache.pt\")\n    fn_pickle = os.path.join(datadir, \"cache.pkl\")\n    if os.path.exists(fn):\n        logger.info(\"Loading cached dataset...\")\n        corpus = torch.load(fn_pickle)\n    elif os.path.exists(fn):\n        logger.info(\"Loading cached dataset from pickle...\")\n        with open(fn, \"rb\") as fp:\n            corpus = pickle.load(fp)\n    else:\n        logger.info(f\"Producing dataset {dataset}...\")\n        kwargs = {}\n        if dataset in [\"wt103\", \"wt2\"]:\n            kwargs[\"special\"] = [\"<eos>\"]\n            kwargs[\"lower_case\"] = False\n        elif dataset == \"ptb\":\n            kwargs[\"special\"] = [\"<eos>\"]\n            kwargs[\"lower_case\"] = True\n        elif dataset == \"lm1b\":\n            kwargs[\"special\"] = []\n            kwargs[\"lower_case\"] = False\n            kwargs[\"vocab_file\"] = os.path.join(datadir, \"1b_word_vocab.txt\")\n        elif dataset in [\"enwik8\", \"text8\"]:\n            pass\n\n        corpus = TransfoXLCorpus(datadir, dataset, **kwargs)\n        torch.save(corpus, fn)\n\n    return corpus\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def list_artifacts(self, path=None):\n        endpoint = \"/mlflow-artifacts/artifacts\"\n        url, tail = self.artifact_uri.split(endpoint, maxsplit=1)\n        root = tail.lstrip(\"/\")\n        params = {\"path\": posixpath.join(root, path) if path else root}\n        host_creds = _get_default_host_creds(url)\n        resp = http_request(host_creds, endpoint, \"GET\", params=params)\n        augmented_raise_for_status(resp)\n        file_infos = []\n        for f in resp.json().get(\"files\", []):\n            validate_path_is_safe(f[\"path\"])\n            file_info = FileInfo(\n                posixpath.join(path, f[\"path\"]) if path else f[\"path\"],\n                f[\"is_dir\"],\n                int(f[\"file_size\"]) if (\"file_size\" in f) else None,\n            )\n            file_infos.append(file_info)\n\n        return sorted(file_infos, key=lambda f: f.path)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def window():\n    target_url = request.args.get('location')\n    if target_url.startswith('gAAAAA'):\n        cipher_suite = Fernet(g.session_key)\n        target_url = cipher_suite.decrypt(target_url.encode()).decode()\n\n    content_filter = Filter(\n        g.session_key,\n        root_url=request.url_root,\n        config=g.user_config)\n    target = urlparse.urlparse(target_url)\n    host_url = f'{target.scheme}://{target.netloc}'\n\n    get_body = g.user_request.send(base_url=target_url).text\n\n    results = bsoup(get_body, 'html.parser')\n    src_attrs = ['src', 'href', 'srcset', 'data-srcset', 'data-src']\n\n    # Parse HTML response and replace relative links w/ absolute\n    for element in results.find_all():\n        for attr in src_attrs:\n            if not element.has_attr(attr) or not element[attr].startswith('/'):\n                continue\n\n            element[attr] = host_url + element[attr]\n\n    # Replace or remove javascript sources\n    for script in results.find_all('script', {'src': True}):\n        if 'nojs' in request.args:\n            script.decompose()\n        else:\n            content_filter.update_element_src(script, 'application/javascript')\n\n    # Replace all possible image attributes\n    img_sources = ['src', 'data-src', 'data-srcset', 'srcset']\n    for img in results.find_all('img'):\n        _ = [\n            content_filter.update_element_src(img, 'image/png', attr=_)\n            for _ in img_sources if img.has_attr(_)\n        ]\n\n    # Replace all stylesheet sources\n    for link in results.find_all('link', {'href': True}):\n        content_filter.update_element_src(link, 'text/css', attr='href')\n\n    # Use anonymous view for all links on page\n    for a in results.find_all('a', {'href': True}):\n        a['href'] = f'{Endpoint.window}?location=' + a['href'] + (\n            '&nojs=1' if 'nojs' in request.args else '')\n\n    # Remove all iframes -- these are commonly used inside of <noscript> tags\n    # to enforce loading Google Analytics\n    for iframe in results.find_all('iframe'):\n        iframe.decompose()\n\n    return render_template(\n        'display.html',\n        response=results,\n        translation=app.config['TRANSLATIONS'][\n            g.user_config.get_localization_lang()\n        ]\n    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def delete(request, name):\n  if 'delete' not in getPermissions(request.user):\n    return json_response( dict(error=\"Must be logged in with appropriate permissions to delete\") )\n\n  try:\n    dashboard = Dashboard.objects.get(name=name)\n  except Dashboard.DoesNotExist:\n    return json_response( dict(error=\"Dashboard '%s' does not exist. \" % name) )\n  else:\n    dashboard.delete()\n    return json_response( dict(success=True) )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, **kwargs):\n        \"\"\"Initialize HTMLSerializer.\n\n        Keyword options (default given first unless specified) include:\n\n        inject_meta_charset=True|False\n          Whether it insert a meta element to define the character set of the\n          document.\n        quote_attr_values=True|False\n          Whether to quote attribute values that don't require quoting\n          per HTML5 parsing rules.\n        quote_char=u'\"'|u\"'\"\n          Use given quote character for attribute quoting. Default is to\n          use double quote unless attribute value contains a double quote,\n          in which case single quotes are used instead.\n        escape_lt_in_attrs=False|True\n          Whether to escape < in attribute values.\n        escape_rcdata=False|True\n          Whether to escape characters that need to be escaped within normal\n          elements within rcdata elements such as style.\n        resolve_entities=True|False\n          Whether to resolve named character entities that appear in the\n          source tree. The XML predefined entities &lt; &gt; &amp; &quot; &apos;\n          are unaffected by this setting.\n        strip_whitespace=False|True\n          Whether to remove semantically meaningless whitespace. (This\n          compresses all whitespace to a single space except within pre.)\n        minimize_boolean_attributes=True|False\n          Shortens boolean attributes to give just the attribute value,\n          for example <input disabled=\"disabled\"> becomes <input disabled>.\n        use_trailing_solidus=False|True\n          Includes a close-tag slash at the end of the start tag of void\n          elements (empty elements whose end tag is forbidden). E.g. <hr/>.\n        space_before_trailing_solidus=True|False\n          Places a space immediately before the closing slash in a tag\n          using a trailing solidus. E.g. <hr />. Requires use_trailing_solidus.\n        sanitize=False|True\n          Strip all unsafe or unknown constructs from output.\n          See `html5lib user documentation`_\n        omit_optional_tags=True|False\n          Omit start/end tags that are optional.\n        alphabetical_attributes=False|True\n          Reorder attributes to be in alphabetical order.\n\n        .. _html5lib user documentation: http://code.google.com/p/html5lib/wiki/UserDocumentation\n        \"\"\"\n        if 'quote_char' in kwargs:\n            self.use_best_quote_char = False\n        for attr in self.options:\n            setattr(self, attr, kwargs.get(attr, getattr(self, attr)))\n        self.errors = []\n        self.strict = False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def reset_password(self, userid, password):\n        \"\"\"\n        Change/Reset a user's password for authdb.\n\n        Password will be hashed and saved.\n        :param userid: the user id to reset the password\n        :param password: the clear text password to reset and save hashed on the db\n        \"\"\"\n        user = self.get_user_by_id(userid)\n        user.password = generate_password_hash(password)\n        self.update_user(user)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def archive(request, bookmark_id: int):\n    bookmark = Bookmark.objects.get(pk=bookmark_id)\n    archive_bookmark(bookmark)\n    return_url = request.GET.get('return_url')\n    return_url = return_url if return_url else reverse('bookmarks:index')\n    return HttpResponseRedirect(return_url)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const auto splits = ctx->input(0).flat<int64_t>();\n    const auto values = ctx->input(1).flat<Tidx>();\n    const Tensor& size_t = ctx->input(2);\n    const auto weights = ctx->input(3).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    int num_rows = splits.size() - 1;\n    int num_values = values.size();\n    int batch_idx = 0;\n\n    OP_REQUIRES(ctx, splits(0) == 0,\n                errors::InvalidArgument(\"Splits must start with 0, not with \",\n                                        splits(0)));\n\n    OP_REQUIRES(ctx, splits(num_rows) == num_values,\n                errors::InvalidArgument(\n                    \"Splits must end with the number of values, got \",\n                    splits(num_rows), \" instead of \", num_values));\n\n    Tensor* out_t;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n    functor::SetZeroFunctor<Device, T> fill;\n    fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n    const auto out = out_t->matrix<T>();\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      while (idx >= splits(batch_idx)) {\n        batch_idx++;\n      }\n      Tidx bin = values(idx);\n      OP_REQUIRES(ctx, bin >= 0,\n                  errors::InvalidArgument(\"Input must be non-negative\"));\n      if (bin < size) {\n        if (binary_output_) {\n          out(batch_idx - 1, bin) = T(1);\n        } else {\n          T value = (weights_size > 0) ? weights(idx) : T(1);\n          out(batch_idx - 1, bin) += value;\n        }\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __call__(self, value):\n        if not isinstance(value, str):\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n        if self.unsafe_chars.intersection(value):\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n        # Check if the scheme is valid.\n        scheme = value.split(\"://\")[0].lower()\n        if scheme not in self.schemes:\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n\n        # Then check full URL\n        try:\n            splitted_url = urlsplit(value)\n        except ValueError:\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n        try:\n            super().__call__(value)\n        except ValidationError as e:\n            # Trivial case failed. Try for possible IDN domain\n            if value:\n                scheme, netloc, path, query, fragment = splitted_url\n                try:\n                    netloc = punycode(netloc)  # IDN -> ACE\n                except UnicodeError:  # invalid domain part\n                    raise e\n                url = urlunsplit((scheme, netloc, path, query, fragment))\n                super().__call__(url)\n            else:\n                raise\n        else:\n            # Now verify IPv6 in the netloc part\n            host_match = re.search(r\"^\\[(.+)\\](?::[0-9]{1,5})?$\", splitted_url.netloc)\n            if host_match:\n                potential_ip = host_match[1]\n                try:\n                    validate_ipv6_address(potential_ip)\n                except ValidationError:\n                    raise ValidationError(\n                        self.message, code=self.code, params={\"value\": value}\n                    )\n\n        # The maximum length of a full host name is 253 characters per RFC 1034\n        # section 3.1. It's defined to be 255 bytes or less, but this includes\n        # one byte for the length of the name and one byte for the trailing dot\n        # that's used to indicate absolute names in DNS.\n        if splitted_url.hostname is None or len(splitted_url.hostname) > 253:\n            raise ValidationError(self.message, code=self.code, params={\"value\": value})\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def parse_command_line(argv: List[str]) -> argparse.Namespace:  # noqa: C901.\n    defaults: Dict[str, Any] = {\n        \"json_indent\": None,\n        \"ordered\": True,\n        \"stringify\": False,\n        \"yaml_options\": {},\n    }\n\n    me = Path(argv[0]).name\n    argv0_from, argv0_to = argv0_to_format(me)\n    format_from_argv0 = argv0_to != \"\"\n\n    parser = argparse.ArgumentParser(\n        description=\"Convert between CBOR, JSON, MessagePack, TOML, and YAML.\"\n    )\n    parser.add_argument(\"-v\", \"--version\", action=\"version\", version=__version__)\n\n    input_group = parser.add_mutually_exclusive_group()\n    input_group.add_argument(\"input\", nargs=\"?\", default=\"-\", help=\"input file\")\n    input_group.add_argument(\n        \"-i\",\n        \"--input\",\n        dest=\"input_flag\",\n        metavar=\"input\",\n        default=None,\n        help=\"input file\",\n    )\n\n    if not format_from_argv0:\n        parser.add_argument(\n            \"--if\",\n            \"-if\",\n            \"--input-format\",\n            dest=\"input_format\",\n            default=\"\",\n            help=\"input format\",\n            choices=FORMATS,\n        )\n\n    if not format_from_argv0 or argv0_to == \"json\":\n        parser.add_argument(\n            \"--json-indent\",\n            \"--indent-json\",\n            dest=\"json_indent\",\n            metavar=\"n\",\n            type=int,\n            default=defaults[\"json_indent\"],\n            help=\"JSON indentation\",\n        )\n\n    if not format_from_argv0 or argv0_to in {\"json\", \"toml\"}:\n        parser.add_argument(\n            \"-k\",\n            \"--stringify\",\n            dest=\"stringify\",\n            action=\"store_true\",\n            help=(\n                \"Turn into strings boolean, date-time, and null keys for JSON \"\n                \"and TOML and null values for TOML\"\n            ),\n        )\n\n    output_group = parser.add_mutually_exclusive_group()\n    output_group.add_argument(\"output\", nargs=\"?\", default=\"-\", help=\"output file\")\n    output_group.add_argument(\n        \"-o\",\n        \"--output\",\n        dest=\"output_flag\",\n        metavar=\"output\",\n        default=None,\n        help=\"output file\",\n    )\n\n    if not format_from_argv0:\n        parser.add_argument(\n            \"--of\",\n            \"-of\",\n            \"--output-format\",\n            dest=\"output_format\",\n            default=\"\",\n            help=\"output format\",\n            choices=FORMATS,\n        )\n\n    parser.add_argument(\n        \"-p\",\n        \"--preserve-key-order\",\n        help=argparse.SUPPRESS,\n    )\n\n    if not format_from_argv0 or argv0_to in {\"json\", \"toml\", \"yaml\"}:\n        parser.add_argument(\n            \"-s\",\n            \"--sort-keys\",\n            dest=\"ordered\",\n            action=\"store_false\",\n            help=\"sort JSON, TOML, YAML keys instead of preserving key order\",\n        )\n\n    parser.add_argument(\n        \"--unwrap\",\n        dest=\"unwrap\",\n        metavar=\"key\",\n        default=None,\n        help=\"only output the data stored under the given key\",\n    )\n    parser.add_argument(\n        \"--wrap\",\n        dest=\"wrap\",\n        metavar=\"key\",\n        default=None,\n        help=\"wrap the data in a map type with the given key\",\n    )\n\n    if not format_from_argv0 or argv0_to == \"yaml\":\n        parser.add_argument(\n            \"--yaml-indent\",\n            dest=\"yaml_indent\",\n            metavar=\"n\",\n            type=int,\n            default=2,\n            help=\"YAML indentation\",\n        )\n        parser.add_argument(\n            \"--yaml-style\",\n            dest=\"yaml_style\",\n            default=None,\n            help=\"YAML formatting style\",\n            choices=[\"\", \"'\", '\"', \"|\", \">\"],\n        )\n\n        def yaml_width(value: str) -> int:\n            # This is theoretically compatible with LibYAML.\n            return (1 << 32) - 1 if value.lower() == \"inf\" else int(value)\n\n        parser.add_argument(\n            \"--yaml-width\",\n            dest=\"yaml_width\",\n            metavar=\"n\",\n            type=yaml_width,  # Allow \"inf\".\n            default=80,\n            help=\"YAML line width for long strings\",\n        )\n\n    args = parser.parse_args(args=argv[1:])\n\n    # Use the positional input and output arguments.\n    if args.input_flag is not None:\n        args.input = args.input_flag\n\n    if args.output_flag is not None:\n        args.output = args.output_flag\n\n    # Determine the implicit input and output format if possible.\n    if format_from_argv0:\n        args.input_format = argv0_from\n        args.output_format = argv0_to\n    else:\n        if args.input_format == \"\":\n            args.input_format = extension_to_format(args.input)\n            if args.input_format == \"\":\n                parser.error(\"Need an explicit input format\")\n\n        if args.output_format == \"\":\n            args.output_format = extension_to_format(args.output)\n            if args.output_format == \"\":\n                parser.error(\"Need an explicit output format\")\n\n    for key, value in defaults.items():\n        vars(args).setdefault(key, value)\n\n    # Wrap the yaml_* option.\n    if \"yaml_indent\" in vars(args):\n        vars(args)[\"yaml_options\"] = {\n            \"default_style\": args.yaml_style,\n            \"indent\": args.yaml_indent,\n            \"width\": args.yaml_width,\n        }\n        for key in [\"yaml_indent\", \"yaml_style\", \"yaml_width\"]:\n            del vars(args)[key]\n\n    return args\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, filename):\n        \"\"\"\n        Validate the filename by calling get_valid_name() and return a filename\n        to be passed to the save() method.\n        \"\"\"\n        # `filename` may include a path as returned by FileField.upload_to.\n        dirname, filename = os.path.split(filename)\n        return os.path.normpath(os.path.join(dirname, self.get_valid_name(filename)))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def serve(request, path, document_root=None, show_indexes=False):\n    \"\"\"\n    Serve static files below a given point in the directory structure.\n\n    To use, put a URL pattern such as::\n\n        (r'^(?P<path>.*)$', 'django.views.static.serve', {'document_root': '/path/to/my/files/'})\n\n    in your URLconf. You must provide the ``document_root`` param. You may\n    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index\n    of the directory.  This index view will use the template hardcoded below,\n    but if you'd like to override it, you can create a template called\n    ``static/directory_index.html``.\n    \"\"\"\n    path = posixpath.normpath(unquote(path))\n    path = path.lstrip('/')\n    newpath = ''\n    for part in path.split('/'):\n        if not part:\n            # Strip empty path components.\n            continue\n        drive, part = os.path.splitdrive(part)\n        head, part = os.path.split(part)\n        if part in (os.curdir, os.pardir):\n            # Strip '.' and '..' in path.\n            continue\n        newpath = os.path.join(newpath, part).replace('\\\\', '/')\n    if newpath and path != newpath:\n        return HttpResponseRedirect(newpath)\n    fullpath = os.path.join(document_root, newpath)\n    if os.path.isdir(fullpath):\n        if show_indexes:\n            return directory_index(newpath, fullpath)\n        raise Http404(_(\"Directory indexes are not allowed here.\"))\n    if not os.path.exists(fullpath):\n        raise Http404(_('\"%(path)s\" does not exist') % {'path': fullpath})\n    # Respect the If-Modified-Since header.\n    statobj = os.stat(fullpath)\n    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),\n                              statobj.st_mtime, statobj.st_size):\n        return HttpResponseNotModified()\n    content_type, encoding = mimetypes.guess_type(fullpath)\n    content_type = content_type or 'application/octet-stream'\n    response = StreamingHttpResponse(open(fullpath, 'rb'),\n                                     content_type=content_type)\n    response[\"Last-Modified\"] = http_date(statobj.st_mtime)\n    if stat.S_ISREG(statobj.st_mode):\n        response[\"Content-Length\"] = statobj.st_size\n    if encoding:\n        response[\"Content-Encoding\"] = encoding\n    return response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create_table(\n        self,\n        name: str,\n        obj: pd.DataFrame | pa.Table | ir.Table | None = None,\n        *,\n        schema: sch.Schema | None = None,\n        database: str | None = None,\n        temp: bool = False,\n        overwrite: bool = False,\n    ) -> ir.Table:\n        \"\"\"Create a table.\n\n        Parameters\n        ----------\n        name\n            Name of the new table.\n        obj\n            An Ibis table expression or pandas table that will be used to\n            extract the schema and the data of the new table. If not provided,\n            `schema` must be given.\n        schema\n            The schema for the new table. Only one of `schema` or `obj` can be\n            provided.\n        database\n            Name of the database where the table will be created, if not the\n            default.\n        temp\n            Should the table be temporary for the session.\n        overwrite\n            Clobber existing data\n\n        Returns\n        -------\n        Table\n            The table that was created.\n        \"\"\"\n        if obj is None and schema is None:\n            raise com.IbisError(\"The schema or obj parameter is required\")\n\n        import pandas as pd\n        import pyarrow as pa\n\n        if isinstance(obj, (pd.DataFrame, pa.Table)):\n            obj = ibis.memtable(obj)\n\n        if database == self.current_database:\n            # avoid fully qualified name\n            database = None\n\n        if database is not None:\n            raise NotImplementedError(\n                \"Creating tables from a different database is not yet implemented\"\n            )\n\n        if obj is not None and schema is not None:\n            if not obj.schema().equals(ibis.schema(schema)):\n                raise com.IbisTypeError(\n                    \"Expression schema is not equal to passed schema. \"\n                    \"Try passing the expression without the schema\"\n                )\n        if schema is None:\n            schema = obj.schema()\n\n        self._schemas[self._fully_qualified_name(name, database)] = schema\n\n        if has_expr := obj is not None:\n            # this has to happen outside the `begin` block, so that in-memory\n            # tables are visible inside the transaction created by it\n            self._run_pre_execute_hooks(obj)\n\n        table = self._table_from_schema(\n            name,\n            schema,\n            # most databases don't allow temporary tables in a specific\n            # database so let the backend decide\n            #\n            # the ones that do (e.g., snowflake) should implement their own\n            # `create_table`\n            database=None if temp else (database or self.current_database),\n            temp=temp,\n        )\n\n        if has_expr:\n            if self.supports_create_or_replace:\n                ctas = CreateTableAs(\n                    name,\n                    self.compile(obj),\n                    temp=temp,\n                    overwrite=overwrite,\n                    quote=self.compiler.translator_class._quote_table_names,\n                )\n                with self.begin() as bind:\n                    bind.execute(ctas)\n            else:\n                tmptable = self._table_from_schema(\n                    util.gen_name(\"tmp_table_insert\"),\n                    schema,\n                    # some backends don't support temporary tables\n                    temp=self.supports_temporary_tables,\n                )\n                method = self._get_insert_method(obj)\n                insert = table.insert().from_select(tmptable.columns, tmptable.select())\n\n                with self.begin() as bind:\n                    # 1. write `obj` to a unique temp table\n                    tmptable.create(bind=bind)\n\n                # try/finally here so that a successfully created tmptable gets\n                # cleaned up no matter what\n                try:\n                    with self.begin() as bind:\n                        bind.execute(method(tmptable.insert()))\n\n                        # 2. recreate the existing table\n                        if overwrite:\n                            table.drop(bind=bind, checkfirst=True)\n                        table.create(bind=bind)\n\n                        # 3. insert the temp table's data into the (re)created table\n                        bind.execute(insert)\n                finally:\n                    self._clean_up_tmp_table(tmptable)\n        else:\n            with self.begin() as bind:\n                if overwrite:\n                    table.drop(bind=bind, checkfirst=True)\n                table.create(bind=bind)\n        return self.table(name, database=database)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def message_to_binary(self, message, message_type=ua.MessageType.SecureMessage, request_id=0):\n        \"\"\"\n        Convert OPC UA secure message to binary.\n        The only supported types are SecureOpen, SecureMessage, SecureClose.\n        If message_type is SecureMessage, the AlgorithmHeader should be passed as arg.\n        \"\"\"\n        chunks = MessageChunk.message_to_chunks(\n            self.security_policy,\n            message,\n            self._max_chunk_size,\n            message_type=message_type,\n            channel_id=self.security_token.ChannelId,\n            request_id=request_id,\n            token_id=self.security_token.TokenId,\n        )\n        for chunk in chunks:\n            self._sequence_number += 1\n            if self._sequence_number >= (1 << 32):\n                logger.debug(\"Wrapping sequence number: %d -> 1\", self._sequence_number)\n                self._sequence_number = 1\n            chunk.SequenceHeader.SequenceNumber = self._sequence_number\n        return b\"\".join([chunk.to_binary() for chunk in chunks])\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def from_unix_seconds(seconds: Union[int, float]) -> datetime:\n    while abs(seconds) > MS_WATERSHED:\n        seconds /= 1000\n    dt = EPOCH + timedelta(seconds=seconds)\n    return dt.replace(tzinfo=timezone.utc)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _on_ssl_errors(self, error):\n        self._has_ssl_errors = True\n\n        url = error.url()\n        log.webview.debug(\"Certificate error: {}\".format(error))\n\n        if error.is_overridable():\n            error.ignore = shared.ignore_certificate_errors(\n                url, [error], abort_on=[self.abort_questions])\n        else:\n            log.webview.error(\"Non-overridable certificate error: \"\n                              \"{}\".format(error))\n\n        log.webview.debug(\"ignore {}, URL {}, requested {}\".format(\n            error.ignore, url, self.url(requested=True)))\n\n        # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-56207\n        show_cert_error = (\n            not qtutils.version_check('5.9') and\n            not error.ignore\n        )\n        # WORKAROUND for https://codereview.qt-project.org/c/qt/qtwebengine/+/270556\n        show_non_overr_cert_error = (\n            not error.is_overridable() and (\n                # Affected Qt versions:\n                # 5.13 before 5.13.2\n                # 5.12 before 5.12.6\n                # < 5.12\n                (qtutils.version_check('5.13') and\n                 not qtutils.version_check('5.13.2')) or\n                (qtutils.version_check('5.12') and\n                 not qtutils.version_check('5.12.6')) or\n                not qtutils.version_check('5.12')\n            )\n        )\n\n        # We can't really know when to show an error page, as the error might\n        # have happened when loading some resource.\n        # However, self.url() is not available yet and the requested URL\n        # might not match the URL we get from the error - so we just apply a\n        # heuristic here.\n        if ((show_cert_error or show_non_overr_cert_error) and\n                url.matches(self.data.last_navigation.url, QUrl.RemoveScheme)):\n            self._show_error_page(url, str(error))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "inline void ComputeInterpolationWeights(\n    const int64 out_size, const int64 in_size, const float scale,\n    const int resolution, InterpolationCache<T_SCALE>* interpolation) {\n  const Scaler scaler;\n  interpolation->lower.resize(out_size + 1);\n  interpolation->upper.resize(out_size + 1);\n  interpolation->lerp.resize(out_size + 1);\n  interpolation->ilerp.resize(out_size + 1);\n\n  interpolation->lower[out_size] = 0;\n  interpolation->upper[out_size] = 0;\n  for (int64 i = out_size - 1; i >= 0; --i) {\n    const float in = scaler(i, scale);\n    const float in_f = std::floor(in);\n    interpolation->lower[i] =\n        std::max(static_cast<int64>(in_f), static_cast<int64>(0));\n    interpolation->upper[i] =\n        std::min(static_cast<int64>(std::ceil(in)), in_size - 1);\n    interpolation->lerp[i] = in - in_f;\n    interpolation->ilerp[i] =\n        static_cast<T_SCALE>((in - in_f) * (1 << resolution));\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_attributes(self):\n        \"\"\"\n        Gets the Attributes from the AttributeStatement element.\n        EncryptedAttributes are not supported\n        \"\"\"\n        attributes = {}\n        attribute_nodes = self.__query_assertion('/saml:AttributeStatement/saml:Attribute')\n        for attribute_node in attribute_nodes:\n            attr_name = attribute_node.get('Name')\n            if attr_name in attributes.keys():\n                raise OneLogin_Saml2_ValidationError(\n                    'Found an Attribute element with duplicated Name',\n                    OneLogin_Saml2_ValidationError.DUPLICATED_ATTRIBUTE_NAME_FOUND\n                )\n\n            values = []\n            for attr in attribute_node.iterchildren('{%s}AttributeValue' % OneLogin_Saml2_Constants.NSMAP[OneLogin_Saml2_Constants.NS_PREFIX_SAML]):\n                # Remove any whitespace (which may be present where attributes are\n                # nested inside NameID children).\n                if attr.text:\n                    text = attr.text.strip()\n                    if text:\n                        values.append(text)\n\n                # Parse any nested NameID children\n                for nameid in attr.iterchildren('{%s}NameID' % OneLogin_Saml2_Constants.NSMAP[OneLogin_Saml2_Constants.NS_PREFIX_SAML]):\n                    values.append({\n                        'NameID': {\n                            'Format': nameid.get('Format'),\n                            'NameQualifier': nameid.get('NameQualifier'),\n                            'value': nameid.text\n                        }\n                    })\n\n            attributes[attr_name] = values\n        return attributes\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void MakeDataset(OpKernelContext* ctx, DatasetBase** output) override {\n    // Create a new SparseTensorSliceDatasetOp::Dataset, insert it in\n    // the step container, and return it as the output.\n    const Tensor* indices;\n    OP_REQUIRES_OK(ctx, ctx->input(\"indices\", &indices));\n    const Tensor* values;\n    OP_REQUIRES_OK(ctx, ctx->input(\"values\", &values));\n    const Tensor* dense_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"dense_shape\", &dense_shape));\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(indices->shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be a matrix but received shape \",\n                    indices->shape().DebugString()));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(values->shape()),\n                errors::InvalidArgument(\n                    \"Input values should be a vector but received shape \",\n                    indices->shape().DebugString()));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(dense_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shape should be a vector but received shape \",\n                    dense_shape->shape().DebugString()));\n\n    // We currently ensure that `sparse_tensor` is ordered in the\n    // batch dimension.\n    // TODO(mrry): Investigate ways to avoid this unconditional check\n    // if we can be sure that the sparse tensor was produced in an\n    // appropriate order (e.g. by `tf.parse_example()` or a Dataset\n    // that batches elements into rows of a SparseTensor).\n    int64_t previous_batch_index = -1;\n    for (int64_t i = 0; i < indices->dim_size(0); ++i) {\n      int64_t next_batch_index = indices->matrix<int64>()(i, 0);\n      OP_REQUIRES(\n          ctx, next_batch_index >= previous_batch_index,\n          errors::Unimplemented(\"The SparseTensor must be ordered in the batch \"\n                                \"dimension; handling arbitrarily ordered input \"\n                                \"is not currently supported.\"));\n      previous_batch_index = next_batch_index;\n    }\n    gtl::InlinedVector<int64, 8> std_order(dense_shape->NumElements(), 0);\n    sparse::SparseTensor tensor;\n    OP_REQUIRES_OK(\n        ctx, sparse::SparseTensor::Create(\n                 *indices, *values, TensorShape(dense_shape->vec<int64>()),\n                 std_order, &tensor));\n    *output = new Dataset<T>(ctx, std::move(tensor));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void TensorSliceReader::LoadShard(int shard) const {\n  CHECK_LT(shard, sss_.size());\n  if (sss_[shard] || !status_.ok()) {\n    return;  // Already loaded, or invalid.\n  }\n  string value;\n  SavedTensorSlices sts;\n  const string fname = fnames_[shard];\n  VLOG(1) << \"Reading meta data from file \" << fname << \"...\";\n  Table* table;\n  Status s = open_function_(fname, &table);\n  if (!s.ok()) {\n    status_ = errors::DataLoss(\"Unable to open table file \", fname, \": \",\n                               s.ToString());\n    return;\n  }\n  sss_[shard].reset(table);\n  if (!(table->Get(kSavedTensorSlicesKey, &value) &&\n        ParseProtoUnlimited(&sts, value))) {\n    status_ = errors::Internal(\n        \"Failed to find the saved tensor slices at the beginning of the \"\n        \"checkpoint file: \",\n        fname);\n    return;\n  }\n  status_ = CheckVersions(sts.meta().versions(), TF_CHECKPOINT_VERSION,\n                          TF_CHECKPOINT_VERSION_MIN_PRODUCER, \"Checkpoint\",\n                          \"checkpoint\");\n  if (!status_.ok()) return;\n  for (const SavedSliceMeta& ssm : sts.meta().tensor()) {\n    TensorShape ssm_shape;\n    status_ = TensorShape::BuildTensorShapeBase(ssm.shape(), &ssm_shape);\n    if (!status_.ok()) return;\n    for (const TensorSliceProto& tsp : ssm.slice()) {\n      TensorSlice ss_slice(tsp);\n      status_ = RegisterTensorSlice(ssm.name(), ssm_shape, ssm.type(), fname,\n                                    ss_slice, &tensors_);\n      if (!status_.ok()) return;\n    }\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  public ProxyResponseRenderer(\n      ProxySettings proxySettings,\n      KeyStoreSettings trustStoreSettings,\n      boolean preserveHostHeader,\n      String hostHeaderValue,\n      SettingsStore settingsStore,\n      boolean trustAllProxyTargets,\n      List<String> trustedProxyTargets,\n      boolean stubCorsEnabled,\n      NetworkAddressRules targetAddressRules,\n      int proxyTimeout) {\n    this.settingsStore = settingsStore;\n    reverseProxyClient =\n        HttpClientFactory.createClient(\n            1000,\n            proxyTimeout,\n            proxySettings,\n            trustStoreSettings,\n            true,\n            Collections.emptyList(),\n            true);\n    forwardProxyClient =\n        HttpClientFactory.createClient(\n            1000,\n            proxyTimeout,\n            proxySettings,\n            trustStoreSettings,\n            trustAllProxyTargets,\n            trustAllProxyTargets ? Collections.emptyList() : trustedProxyTargets,\n            false);\n\n    this.preserveHostHeader = preserveHostHeader;\n    this.hostHeaderValue = hostHeaderValue;\n    this.stubCorsEnabled = stubCorsEnabled;\n    this.targetAddressRules = targetAddressRules;\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def evaluate_link(self, link: Link) -> Package | None:\n        \"\"\"\n        Evaluate the link and return the package if it matches or None if it doesn't.\n        \"\"\"\n        try:\n            self.format_control.check_format(link, self.package_name)\n            self.check_yanked(link)\n            self.check_requires_python(link)\n            version: str | None = None\n            if link.is_wheel:\n                try:\n                    wheel_info = parse_wheel_filename(link.filename)\n                except (InvalidWheelFilename, InvalidVersion) as e:\n                    raise LinkMismatchError(str(e))\n                if self._canonical_name != wheel_info[0]:\n                    raise LinkMismatchError(\n                        f\"The package name doesn't match {wheel_info[0]}\"\n                    )\n                if not self.ignore_compatibility and wheel_info[3].isdisjoint(\n                    self.target_python.supported_tags()\n                ):\n                    raise LinkMismatchError(\n                        \"none of the wheel tags({}) are compatible\".format(\n                            \", \".join(sorted(str(tag) for tag in wheel_info[3]))\n                        ),\n                    )\n                version = str(wheel_info[1])\n            else:\n                if link._fragment_dict.get(\"egg\"):\n                    egg_info = strip_extras(link._fragment_dict[\"egg\"])\n                else:\n                    egg_info, ext = splitext(link.filename)\n                    if not ext:\n                        raise LinkMismatchError(f\"Not a file: {link.filename}\")\n                    if ext not in ARCHIVE_EXTENSIONS:\n                        raise LinkMismatchError(\n                            f\"Unsupported archive format: {link.filename}\"\n                        )\n                version = parse_version_from_egg_info(egg_info, self._canonical_name)\n                if version is None:\n                    raise LinkMismatchError(\n                        f\"Missing version in the filename {egg_info}\"\n                    )\n                try:\n                    Version(version)\n                except InvalidVersion:\n                    raise LinkMismatchError(\n                        f\"Invalid version in the filename {egg_info}: {version}\"\n                    )\n        except LinkMismatchError as e:\n            logger.debug(\"Skipping link %s: %s\", link, e)\n            return None\n        return Package(name=self.package_name, version=version, link=link)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def post(self, kernel_id, action):\n        \"\"\"Interrupt or restart a kernel.\"\"\"\n        km = self.kernel_manager\n        if action == \"interrupt\":\n            await ensure_async(km.interrupt_kernel(kernel_id))  # type:ignore[func-returns-value]\n            self.set_status(204)\n        if action == \"restart\":\n            try:\n                await km.restart_kernel(kernel_id)\n            except Exception as e:\n                message = \"Exception restarting kernel\"\n                self.log.error(message, exc_info=True)\n                traceback = format_tb(e.__traceback__)\n                self.write(json.dumps({\"message\": message, \"traceback\": traceback}))\n                self.set_status(500)\n            else:\n                model = await ensure_async(km.kernel_model(kernel_id))\n                self.write(json.dumps(model, default=json_default))\n        self.finish()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def compat_shlex_quote(s):\n        import re\n        return s if re.match(r'^[-_\\w./]+$', s) else '\"%s\"' % s.replace('\"', '\\\\\"')\nelse:"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "      void context.ready.then(async () => {\n        const file = context.model;\n        const workspace = (file.toJSON() as unknown) as Workspace.IWorkspace;\n        const path = context.path;\n        const id = workspace.metadata.id;\n\n        // Save the file contents as a workspace.\n        await this._workspaces.save(id, workspace);\n\n        // Save last save location for the save command.\n        await this._state.save(LAST_SAVE_ID, path);\n\n        // Navigate to new workspace.\n        const url = URLExt.join(this._application, 'workspaces', id);\n        if (this._router) {\n          this._router.navigate(url, { hard: true });\n        } else {\n          document.location.href = url;\n        }\n      });\n      return dummyWidget(context);"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def as_sql(self, compiler, connection):\n        key_transforms = [self.key_name]\n        previous = self.lhs\n        while isinstance(previous, KeyTransform):\n            key_transforms.insert(0, previous.key_name)\n            previous = previous.lhs\n        lhs, params = compiler.compile(previous)\n        if len(key_transforms) > 1:\n            return \"(%s %s %%s)\" % (lhs, self.nested_operator), [key_transforms] + params\n        try:\n            int(self.key_name)\n        except ValueError:\n            lookup = \"'%s'\" % self.key_name\n        else:\n            lookup = \"%s\" % self.key_name\n        return \"(%s %s %s)\" % (lhs, self.operator, lookup), params\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def image(\n        self,\n        tokens: Sequence[Token],\n        idx: int,\n        options: OptionsDict,\n        env: MutableMapping,\n    ) -> str:\n        token = tokens[idx]\n\n        # \"alt\" attr MUST be set, even if empty. Because it's mandatory and\n        # should be placed on proper position for tests.\n\n        assert (\n            token.attrs and \"alt\" in token.attrs\n        ), '\"image\" token\\'s attrs must contain `alt`'\n\n        # Replace content with actual value\n\n        token.attrSet(\"alt\", self.renderInlineAsText(token.children, options, env))\n\n        return self.renderToken(tokens, idx, options, env)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ValidateInput(const OpInputList& indices_list_in,\n                     const OpInputList& values_list_in,\n                     const OpInputList& shapes_list_in,\n                     const OpInputList& dense_list_in) {\n  const auto size = indices_list_in.size();\n  // Validates indices_list_in OpInputList.\n  for (int i = 0; i < size; i++) {\n    if (!TensorShapeUtils::IsMatrix(indices_list_in[i].shape())) {\n      return errors::InvalidArgument(\n          \"Input indices should be a matrix but received shape \",\n          indices_list_in[i].shape().DebugString(), \" at position \", i);\n    }\n    if (indices_list_in[i].shape().dim_size(1) != 2) {\n      return errors::InvalidArgument(\"Expected D2 of index to be 2 got \",\n                                     indices_list_in[i].shape().dim_size(1),\n                                     \" at position \", i);\n    }\n  }\n\n  // Validates values_list_in OpInputList.\n  if (values_list_in.size() != size) {\n    return errors::InvalidArgument(\"Expected \", size, \" input values, got \",\n                                   values_list_in.size());\n  }\n  for (int i = 0; i < size; i++) {\n    if (!TensorShapeUtils::IsVector(values_list_in[i].shape())) {\n      return errors::InvalidArgument(\n          \"Input values should be a vector but received shape \",\n          values_list_in[i].shape().DebugString(), \" at position \", i);\n    }\n    if (indices_list_in[i].shape().dim_size(0) !=\n        values_list_in[i].shape().dim_size(0)) {\n      return errors::InvalidArgument(\n          \"Expected size of values to be \",\n          indices_list_in[i].shape().dim_size(0), \" got \",\n          values_list_in[i].shape().dim_size(0), \" at position \", i);\n    }\n  }\n\n  // Validates shapes_list_in OpInputList\n  if (shapes_list_in.size() != size) {\n    return errors::InvalidArgument(\"Expected \", size, \" input shapes, got \",\n                                   shapes_list_in.size());\n  }\n  for (int i = 0; i < size; i++) {\n    if (!TensorShapeUtils::IsVector(shapes_list_in[i].shape())) {\n      return errors::InvalidArgument(\n          \"Input shapes should be a vector but received shape \",\n          shapes_list_in[i].shape().DebugString(), \" at position \", i);\n    }\n\n    if (shapes_list_in[i].vec<int64>().size() != 2) {\n      return errors::InvalidArgument(\"shape should imply a 2D tensor, but got \",\n                                     shapes_list_in[i].shape().DebugString(),\n                                     \" at position \", i);\n    }\n  }\n\n  // Validates dense_list_in OpInputList\n  for (int i = 0; i < dense_list_in.size(); ++i) {\n    if (!TensorShapeUtils::IsMatrix(dense_list_in[i].shape())) {\n      return errors::InvalidArgument(\n          \"Dense inputs should be a matrix but received shape \",\n          dense_list_in[i].shape().DebugString(), \" at position \", i);\n    }\n  }\n\n  // Validates batch sizes.  (Note: we do this after validating the input\n  // shapes, because CalculateBatchSize() depends on inputs having valid\n  // shapes).\n  const auto batch_size = CalculateBatchSize(shapes_list_in, dense_list_in);\n  for (int i = 0; i < size; i++) {\n    if (shapes_list_in[i].vec<int64>()(0) != batch_size) {\n      return errors::InvalidArgument(\"Expected batch size \", batch_size,\n                                     \" got \", shapes_list_in[i].vec<int64>()(0),\n                                     \" at position \", i);\n    }\n  }\n  for (int i = 0; i < dense_list_in.size(); ++i) {\n    if (dense_list_in[i].dim_size(0) != batch_size) {\n      return errors::InvalidArgument(\"Expected batch size \", batch_size,\n                                     \" got \", dense_list_in[i].dim_size(0),\n                                     \" at dense tensor \", i);\n    }\n  }\n\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void DoCompute(OpKernelContext* c) {\n    core::RefCountPtr<Var> v;\n    OP_REQUIRES_OK(c, LookupResource(c, HandleFromInput(c, 0), &v));\n    Tensor* params = v->tensor();\n    const Tensor& indices = c->input(1);\n    const Tensor& updates = c->input(2);\n\n    // Check that rank(updates.shape) = rank(indices.shape + params.shape[1:])\n    OP_REQUIRES(c,\n                updates.dims() == 0 ||\n                    updates.dims() == indices.dims() + params->dims() - 1,\n                errors::InvalidArgument(\n                    \"Must have updates.shape = indices.shape + \"\n                    \"params.shape[1:] or updates.shape = [], got \",\n                    \"updates.shape \", updates.shape().DebugString(),\n                    \", indices.shape \", indices.shape().DebugString(),\n                    \", params.shape \", params->shape().DebugString()));\n\n    // Check that we have enough index space\n    const int64_t N_big = indices.NumElements();\n    OP_REQUIRES(\n        c, N_big <= std::numeric_limits<Index>::max(),\n        errors::InvalidArgument(\"indices has too many elements for \",\n                                DataTypeString(DataTypeToEnum<Index>::v()),\n                                \" indexing: \", N_big, \" > \",\n                                std::numeric_limits<Index>::max()));\n    const Index N = static_cast<Index>(N_big);\n    OP_REQUIRES(\n        c, params->dim_size(0) <= std::numeric_limits<Index>::max(),\n        errors::InvalidArgument(\"params.shape[0] too large for \",\n                                DataTypeString(DataTypeToEnum<Index>::v()),\n                                \" indexing: \", params->dim_size(0), \" > \",\n                                std::numeric_limits<Index>::max()));\n\n    if (N > 0) {\n      auto indices_flat = indices.flat<Index>();\n      auto params_flat = params->flat_outer_dims<T>();\n      if (TensorShapeUtils::IsScalar(updates.shape())) {\n        const auto update = updates.scalar<T>();\n\n        functor::ScatterScalarFunctor<Device, T, Index, op> functor;\n        const Index bad_i = functor(c, c->template eigen_device<Device>(),\n                                    params_flat, update, indices_flat);\n        OP_REQUIRES(c, bad_i < 0,\n                    errors::InvalidArgument(\n                        \"indices\", SliceDebugString(indices.shape(), bad_i),\n                        \" = \", indices_flat(bad_i), \" is not in [0, \",\n                        params->dim_size(0), \")\"));\n      } else {\n        int64_t num_updates = updates.NumElements();\n        OP_REQUIRES(c, num_updates % N == 0,\n                    errors::InvalidArgument(\n                        \"shape of indices (\", indices.shape().DebugString(),\n                        \") is not compatible with the shape of updates (\",\n                        updates.shape().DebugString(), \")\"));\n        auto updates_flat = updates.shaped<T, 2>({N, num_updates / N});\n\n        functor::ScatterFunctor<Device, T, Index, op> functor;\n        const Index bad_i = functor(c, c->template eigen_device<Device>(),\n                                    params_flat, updates_flat, indices_flat);\n        OP_REQUIRES(c, bad_i < 0,\n                    errors::InvalidArgument(\n                        \"indices\", SliceDebugString(indices.shape(), bad_i),\n                        \" = \", indices_flat(bad_i), \" is not in [0, \",\n                        params->dim_size(0), \")\"));\n      }\n    }\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def verify(cls, message, signature, publicKey, hashfunc=sha256):\n        byteMessage = hashfunc(toBytes(message)).digest()\n        numberMessage = numberFromByteString(byteMessage)\n        curve = publicKey.curve\n        r = signature.r\n        s = signature.s\n        inv = Math.inv(s, curve.N)\n        u1 = Math.multiply(curve.G, n=(numberMessage * inv) % curve.N, N=curve.N, A=curve.A, P=curve.P)\n        u2 = Math.multiply(publicKey.point, n=(r * inv) % curve.N, N=curve.N, A=curve.A, P=curve.P)\n        add = Math.add(u1, u2, A=curve.A, P=curve.P)\n        modX = add.x % curve.N\n        return r == modX\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render_prefs_panel(self, panelid, action=None, **kwargs):  # @UnusedVariable\n        # Process the parameters.\n        profile_form = UserProfileForm(email=self.app.currentuser.email)\n        password_form = UserPasswordForm()\n        if action == \"set_profile_info\":\n            self._handle_set_profile_info(action, profile_form)\n        elif action == \"set_password\":\n            self._handle_set_password(action, password_form)\n        elif action == \"update_repos\":\n            self.app.currentuser.refresh_repos(delete=True)\n            flash(_(\"Repositories successfully updated\"), level='success')\n        elif action is None:\n            pass\n        else:\n            _logger.warning(\"unknown action: %s\", action)\n            raise cherrypy.NotFound(\"Unknown action\")\n        params = {\n            'profile_form': profile_form,\n            'password_form': password_form,\n        }\n        return \"prefs_general.html\", params\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status AutoParallel::Initialize(const GrapplerItem& item) {\n  num_gpus_ = GetNumAvailableGPUs();\n  LOG(INFO) << \"Number of GPUs: \" << num_gpus_;\n  item_ = &item;\n  graph_ = item.graph;\n  LOG(INFO) << \"Original graph size: \" << graph_.node_size();\n  if (item.fetch.empty()) {\n    return Status(error::INVALID_ARGUMENT, \"No fetch nodes provided.\");\n  }\n\n  if (item.MainVariables().empty()) {\n    return Status(error::INVALID_ARGUMENT, \"No variables provided.\");\n  }\n\n  for (const auto& init : item.init_ops) {\n    VLOG(1) << \"Init node: \" << init;\n  }\n\n  for (const auto& fetch : item.fetch) {\n    VLOG(1) << \"Fetch node: \" << fetch;\n  }\n\n  for (const auto& var : item.MainVariables()) {\n    VLOG(2) << \"Variable: \" << var->name();\n  }\n\n  const std::set<string> apply_gradients_ops = {\"ApplyGradientDescent\",\n                                                \"ApplyProximalGradientDescent\",\n                                                \"ApplyAdadelta\",\n                                                \"ApplyAdagrad\",\n                                                \"ApplyProximalAdagrad\",\n                                                \"ApplyAdagradDA\",\n                                                \"ApplyFtrl\",\n                                                \"ApplyMomentum\",\n                                                \"ApplyAdam\",\n                                                \"ApplyRMSProp\",\n                                                \"ApplyCenteredRMSProp\"};\n  for (int i = 0; i < graph_.node_size(); i++) {\n    all_nodes_.insert(\n        std::make_pair(graph_.node(i).name(), graph_.mutable_node(i)));\n    if (apply_gradients_ops.find(graph_.node(i).op()) !=\n        apply_gradients_ops.end()) {\n      apply_gradients_nodes_.insert(graph_.node(i).name());\n      VLOG(2) << \"Apply gradients node: \" << graph_.node(i).name();\n    }\n  }\n\n  auto div_const_node = AddNodeDivConst();\n  all_nodes_.insert(std::make_pair(div_const_node->name(), div_const_node));\n  std::map<string, int> gradient_pos = {{\"ApplyGradientDescent\", 2},\n                                        {\"ApplyProximalGradientDescent\", 4},\n                                        {\"ApplyAdadelta\", 6},\n                                        {\"ApplyAdagrad\", 3},\n                                        {\"ApplyProximalAdagrad\", 5},\n                                        {\"ApplyAdagradDA\", 3},\n                                        {\"ApplyFtrl\", 3},\n                                        {\"ApplyMomentum\", 3},\n                                        {\"ApplyAdam\", 9},\n                                        {\"ApplyRMSProp\", 7},\n                                        {\"ApplyCenteredRMSProp\", 8}};\n  for (const auto& apply_gradient_node_name : apply_gradients_nodes_) {\n    auto apply_gradients_op = all_nodes_[apply_gradient_node_name]->op();\n    auto apply_gradients_node = all_nodes_[apply_gradient_node_name];\n\n    auto div_node = AddNodeDiv(\n        apply_gradient_node_name,\n        apply_gradients_node->input(gradient_pos[apply_gradients_op]),\n        div_const_node->name());\n    all_nodes_.insert(std::make_pair(div_node->name(), div_node));\n    *apply_gradients_node->mutable_input(gradient_pos[apply_gradients_op]) =\n        div_node->name();\n  }\n  LOG(INFO) << \"Graph size after adding div nodes: \" << all_nodes_.size();\n\n  std::vector<const NodeDef*> train_nodes;\n  TF_RETURN_IF_ERROR(ComputeTransitiveFanin(graph_, item.fetch, &train_nodes));\n  LOG(INFO) << \"Number of training nodes: \" << train_nodes.size();\n\n  const NodeDef* dequeue_node;\n  for (const auto& train_node : train_nodes) {\n    if (IsDequeueOp(*train_node)) {\n      dequeue_node = train_node;\n      break;\n    }\n  }\n\n  std::vector<const NodeDef*> input_nodes;\n  if (dequeue_node) {\n    LOG(INFO) << \"Dequeue node: \" << dequeue_node->name();\n    TF_RETURN_IF_ERROR(ComputeTransitiveFanin(graph_, {dequeue_node->name()},\n                                              {}, &input_nodes));\n  }\n  LOG(INFO) << \"Number of input nodes: \" << input_nodes.size();\n\n  std::set<string> dont_replicate_nodes;\n  for (const auto& variable : item.MainVariables()) {\n    dont_replicate_nodes.insert(variable->name());\n  }\n\n  for (const auto& init : item.init_ops) {\n    dont_replicate_nodes.insert(NodeName(init));\n  }\n\n  // Don't replicate all input nodes, except the dequeue node.\n  for (const auto& input_node : input_nodes) {\n    if (input_node->name() != dequeue_node->name()) {\n      dont_replicate_nodes.insert(input_node->name());\n    }\n  }\n\n  for (const auto& node : train_nodes) {\n    if (dont_replicate_nodes.find(node->name()) == dont_replicate_nodes.end()) {\n      replica_nodes_.insert(node->name());\n    }\n  }\n  LOG(INFO) << \"Number of replica nodes: \" << replica_nodes_.size();\n\n  for (const auto& node : all_nodes_) {\n    if (replica_nodes_.find(node.first) == replica_nodes_.end()) {\n      shared_nodes_.insert(node.first);\n    }\n  }\n  LOG(INFO) << \"Number of shared nodes: \" << shared_nodes_.size();\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _execute_helper(\n        self,\n        query: str,\n        timeout: int = 0,\n        statement_params: dict[str, str] | None = None,\n        binding_params: tuple | dict[str, dict[str, str]] = None,\n        binding_stage: str | None = None,\n        is_internal: bool = False,\n        describe_only: bool = False,\n        _no_results: bool = False,\n        _is_put_get=None,\n        _no_retry: bool = False,\n    ):\n        del self.messages[:]\n\n        if statement_params is not None and not isinstance(statement_params, dict):\n            Error.errorhandler_wrapper(\n                self.connection,\n                self,\n                ProgrammingError,\n                {\n                    \"msg\": \"The data type of statement params is invalid. It must be dict.\",\n                    \"errno\": ER_INVALID_VALUE,\n                },\n            )\n\n        # check if current installation include arrow extension or not,\n        # if not, we set statement level query result format to be JSON\n        if not CAN_USE_ARROW_RESULT_FORMAT:\n            logger.debug(\"Cannot use arrow result format, fallback to json format\")\n            if statement_params is None:\n                statement_params = {\n                    PARAMETER_PYTHON_CONNECTOR_QUERY_RESULT_FORMAT: \"JSON\"\n                }\n            else:\n                result_format_val = statement_params.get(\n                    PARAMETER_PYTHON_CONNECTOR_QUERY_RESULT_FORMAT\n                )\n                if str(result_format_val).upper() == \"ARROW\":\n                    self.check_can_use_arrow_resultset()\n                elif result_format_val is None:\n                    statement_params[\n                        PARAMETER_PYTHON_CONNECTOR_QUERY_RESULT_FORMAT\n                    ] = \"JSON\"\n\n        self._sequence_counter = self._connection._next_sequence_counter()\n        self._request_id = uuid.uuid4()\n\n        logger.debug(f\"Request id: {self._request_id}\")\n\n        logger.debug(\"running query [%s]\", self._format_query_for_log(query))\n        if _is_put_get is not None:\n            # if told the query is PUT or GET, use the information\n            self._is_file_transfer = _is_put_get\n        else:\n            # or detect it.\n            self._is_file_transfer = self.PUT_SQL_RE.match(\n                query\n            ) or self.GET_SQL_RE.match(query)\n        logger.debug(\"is_file_transfer: %s\", self._is_file_transfer is not None)\n\n        real_timeout = (\n            timeout if timeout and timeout > 0 else self._connection.network_timeout\n        )\n\n        if real_timeout is not None:\n            self._timebomb = Timer(real_timeout, self.__cancel_query, [query])\n            self._timebomb.start()\n            logger.debug(\"started timebomb in %ss\", real_timeout)\n        else:\n            self._timebomb = None\n\n        original_sigint = signal.getsignal(signal.SIGINT)\n\n        def interrupt_handler(*_):  # pragma: no cover\n            try:\n                signal.signal(signal.SIGINT, exit_handler)\n            except (ValueError, TypeError):\n                # ignore failures\n                pass\n            try:\n                if self._timebomb is not None:\n                    self._timebomb.cancel()\n                    logger.debug(\"cancelled timebomb in finally\")\n                    self._timebomb = None\n                self.__cancel_query(query)\n            finally:\n                if original_sigint:\n                    try:\n                        signal.signal(signal.SIGINT, original_sigint)\n                    except (ValueError, TypeError):\n                        # ignore failures\n                        pass\n            raise KeyboardInterrupt\n\n        try:\n            if not original_sigint == exit_handler:\n                signal.signal(signal.SIGINT, interrupt_handler)\n        except ValueError:  # pragma: no cover\n            logger.debug(\n                \"Failed to set SIGINT handler. \" \"Not in main thread. Ignored...\"\n            )\n        ret = {\"data\": {}}\n        try:\n            ret = self._connection.cmd_query(\n                query,\n                self._sequence_counter,\n                self._request_id,\n                binding_params=binding_params,\n                binding_stage=binding_stage,\n                is_file_transfer=bool(self._is_file_transfer),\n                statement_params=statement_params,\n                is_internal=is_internal,\n                describe_only=describe_only,\n                _no_results=_no_results,\n                _no_retry=_no_retry,\n            )\n        finally:\n            try:\n                if original_sigint:\n                    signal.signal(signal.SIGINT, original_sigint)\n            except (ValueError, TypeError):  # pragma: no cover\n                logger.debug(\n                    \"Failed to reset SIGINT handler. Not in main \" \"thread. Ignored...\"\n                )\n            if self._timebomb is not None:\n                self._timebomb.cancel()\n                logger.debug(\"cancelled timebomb in finally\")\n\n        if \"data\" in ret and \"parameters\" in ret[\"data\"]:\n            parameters = ret[\"data\"][\"parameters\"]\n            # Set session parameters for cursor object\n            for kv in parameters:\n                if \"TIMESTAMP_OUTPUT_FORMAT\" in kv[\"name\"]:\n                    self._timestamp_output_format = kv[\"value\"]\n                if \"TIMESTAMP_NTZ_OUTPUT_FORMAT\" in kv[\"name\"]:\n                    self._timestamp_ntz_output_format = kv[\"value\"]\n                if \"TIMESTAMP_LTZ_OUTPUT_FORMAT\" in kv[\"name\"]:\n                    self._timestamp_ltz_output_format = kv[\"value\"]\n                if \"TIMESTAMP_TZ_OUTPUT_FORMAT\" in kv[\"name\"]:\n                    self._timestamp_tz_output_format = kv[\"value\"]\n                if \"DATE_OUTPUT_FORMAT\" in kv[\"name\"]:\n                    self._date_output_format = kv[\"value\"]\n                if \"TIME_OUTPUT_FORMAT\" in kv[\"name\"]:\n                    self._time_output_format = kv[\"value\"]\n                if \"TIMEZONE\" in kv[\"name\"]:\n                    self._timezone = kv[\"value\"]\n                if \"BINARY_OUTPUT_FORMAT\" in kv[\"name\"]:\n                    self._binary_output_format = kv[\"value\"]\n            # Set session parameters for connection object\n            self._connection._update_parameters(\n                {p[\"name\"]: p[\"value\"] for p in parameters}\n            )\n\n        self.query = query\n        self._sequence_counter = -1\n        return ret\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const int64 axis_input = context->input(0).scalar<int64>()();\n    const Tensor& input_indices = context->input(1);\n    const Tensor& input_values = context->input(2);\n    const Tensor& input_shape = context->input(3);\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_indices.shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be a matrix but received shape \",\n                    input_indices.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_values.shape()),\n                errors::InvalidArgument(\n                    \"Input values should be a vector but received shape \",\n                    input_indices.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape.shape()),\n                errors::InvalidArgument(\n                    \"Input shape should be a vector but received shape \",\n                    input_shape.shape().DebugString()));\n\n    const int64 input_rank = input_shape.vec<int64>().size();\n    const int64 axis = (axis_input < 0) ? input_rank + axis_input : axis_input;\n\n    OP_REQUIRES(\n        context, axis >= 0 && axis < input_rank,\n        errors::InvalidArgument(\"Input axis should be in range [\", -input_rank,\n                                \", \", input_rank, \"), got \", axis_input));\n\n    OP_REQUIRES(context,\n                num_split_ >= 1 && num_split_ <= input_shape.vec<int64>()(axis),\n                errors::InvalidArgument(\"Input num_split should be between 1 \"\n                                        \"and the splitting dimension size (\",\n                                        input_shape.vec<int64>()(axis),\n                                        \"), got \", num_split_));\n\n    sparse::SparseTensor sparse_tensor;\n    OP_REQUIRES_OK(context,\n                   sparse::SparseTensor::Create(\n                       input_indices, input_values,\n                       TensorShape(input_shape.vec<int64>()), &sparse_tensor));\n\n    std::vector<sparse::SparseTensor> outputs;\n    OP_REQUIRES_OK(context, sparse::SparseTensor::Split<T>(\n                                sparse_tensor, axis, num_split_, &outputs));\n\n    for (int slice_index = 0; slice_index < num_split_; ++slice_index) {\n      context->set_output(slice_index, outputs[slice_index].indices());\n      context->set_output(slice_index + num_split_,\n                          outputs[slice_index].values());\n      Tensor* shape = nullptr;\n      OP_REQUIRES_OK(context, context->allocate_output(\n                                  slice_index + 2 * num_split_,\n                                  {outputs[slice_index].dims()}, &shape));\n      auto output_shape = outputs[slice_index].shape();\n      for (int dim = 0; dim < outputs[slice_index].dims(); ++dim) {\n        shape->vec<int64>()(dim) = output_shape[dim];\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def kickban(bot, trigger):\n    \"\"\"Kick and ban a user from the channel. The bot must be a channel operator for this command to work.\"\"\"\n    chanops = get_chanops(str(trigger.sender), bot.memory['channelmgnt']['jdcache'])\n    dodeop = False\n    if chanops:\n        if bot.channels[trigger.sender].privileges[bot.nick] < OP and trigger.account in chanops:\n            bot.say('Please wait...')\n            bot.say('op ' + trigger.sender, 'ChanServ')\n            time.sleep(1)\n            dodeop = True\n        text = trigger.group().split()\n        argc = len(text)\n        if argc < 3:\n            bot.reply('Syntax is: .kickban <nick> <reason>')\n            if dodeop:\n                deopbot(trigger.sender, bot)\n            return\n        nick = Identifier(text[1])\n        mask = text[2] if any(s in text[2] for s in '!@*') else ''\n        reasonidx = 3 if mask != '' else 2\n        reason = ' '.join(text[reasonidx:])\n        mask = parse_host_mask(trigger.group().split())\n        if mask == '':\n            mask = nick + '!*@*'\n        if trigger.account in chanops:\n            bot.write(['MODE', trigger.sender, '+b', mask])\n            bot.write(['KICK', trigger.sender, nick, ':' + reason])\n            if dodeop:\n                deopbot(trigger.sender, bot)\n        else:\n            bot.reply('Access Denied. If in error, please contact the channel founder.')\n    else:\n        bot.reply(f'No ChanOps Found. Please ask for assistance in {bot.settings.channelmgnt.support_channel}')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def on_POST(\n        self, request: SynapseRequest, room_id: str, receipt_type: str, event_id: str\n    ) -> Tuple[int, JsonDict]:\n        requester = await self.auth.get_user_by_req(request)\n\n        if not RoomID.is_valid(room_id) or not event_id.startswith(EventID.SIGIL):\n            raise SynapseError(400, \"A valid room ID and event ID must be specified\")\n\n        if receipt_type not in self._known_receipt_types:\n            raise SynapseError(\n                400,\n                f\"Receipt type must be {', '.join(self._known_receipt_types)}\",\n            )\n\n        body = parse_json_object_from_request(request)\n\n        # Pull the thread ID, if one exists.\n        thread_id = None\n        if \"thread_id\" in body:\n            thread_id = body.get(\"thread_id\")\n            if not thread_id or not isinstance(thread_id, str):\n                raise SynapseError(\n                    400,\n                    \"thread_id field must be a non-empty string\",\n                    Codes.INVALID_PARAM,\n                )\n\n            if receipt_type == ReceiptTypes.FULLY_READ:\n                raise SynapseError(\n                    400,\n                    f\"thread_id is not compatible with {ReceiptTypes.FULLY_READ} receipts.\",\n                    Codes.INVALID_PARAM,\n                )\n\n            # Ensure the event ID roughly correlates to the thread ID.\n            if not await self._is_event_in_thread(event_id, thread_id):\n                raise SynapseError(\n                    400,\n                    f\"event_id {event_id} is not related to thread {thread_id}\",\n                    Codes.INVALID_PARAM,\n                )\n\n        await self.presence_handler.bump_presence_active_time(\n            requester.user, requester.device_id\n        )\n\n        if receipt_type == ReceiptTypes.FULLY_READ:\n            await self.read_marker_handler.received_client_read_marker(\n                room_id,\n                user_id=requester.user.to_string(),\n                event_id=event_id,\n            )\n        else:\n            await self.receipts_handler.received_client_receipt(\n                room_id,\n                receipt_type,\n                user_id=requester.user.to_string(),\n                event_id=event_id,\n                thread_id=thread_id,\n            )\n\n        return 200, {}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& image = context->input(0);\n    OP_REQUIRES(context, image.dims() == 3,\n                errors::InvalidArgument(\"image must be 3-dimensional\",\n                                        image.shape().DebugString()));\n    OP_REQUIRES(\n        context,\n        FastBoundsCheck(image.NumElements(), std::numeric_limits<int32>::max()),\n        errors::InvalidArgument(\"image cannot have >= int32 max elements\"));\n    const int32 height = static_cast<int32>(image.dim_size(0));\n    const int32 width = static_cast<int32>(image.dim_size(1));\n    const int32 channels = static_cast<int32>(image.dim_size(2));\n\n    // In some cases, we pass width*channels*2 to png.\n    const int32 max_row_width = std::numeric_limits<int32>::max() / 2;\n\n    OP_REQUIRES(context, FastBoundsCheck(width * channels, max_row_width),\n                errors::InvalidArgument(\"image too wide to encode\"));\n\n    OP_REQUIRES(context, channels >= 1 && channels <= 4,\n                errors::InvalidArgument(\n                    \"image must have 1, 2, 3, or 4 channels, got \", channels));\n\n    // Encode image to png string\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({}), &output));\n    if (desired_channel_bits_ == 8) {\n      OP_REQUIRES(context,\n                  png::WriteImageToBuffer(\n                      image.flat<uint8>().data(), width, height,\n                      width * channels, channels, desired_channel_bits_,\n                      compression_, &output->scalar<tstring>()(), nullptr),\n                  errors::Internal(\"PNG encoding failed\"));\n    } else {\n      OP_REQUIRES(context,\n                  png::WriteImageToBuffer(\n                      image.flat<uint16>().data(), width, height,\n                      width * channels * 2, channels, desired_channel_bits_,\n                      compression_, &output->scalar<tstring>()(), nullptr),\n                  errors::Internal(\"PNG encoding failed\"));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, sydent):\n        self.sydent = sydent\n        # The default endpoint factory in Twisted 14.0.0 (which we require) uses the\n        # BrowserLikePolicyForHTTPS context factory which will do regular cert validation\n        # 'like a browser'\n        self.agent = Agent(\n            self.sydent.reactor,\n            connectTimeout=15,\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _update_load_status(self, ok: bool) -> None:\n        \"\"\"Update the load status after a page finished loading.\n\n        Needs to be called by subclasses to trigger a load status update, e.g.\n        as a response to a loadFinished signal.\n        \"\"\"\n        if ok and not self._has_ssl_errors:\n            if self.url().scheme() == 'https':\n                self._set_load_status(usertypes.LoadStatus.success_https)\n            else:\n                self._set_load_status(usertypes.LoadStatus.success)\n        elif ok:\n            self._set_load_status(usertypes.LoadStatus.warn)\n        else:\n            self._set_load_status(usertypes.LoadStatus.error)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_incoming_transaction(self, transaction_data):\n        # keep this as early as possible to make the calculated origin ts as\n        # accurate as possible.\n        request_time = self._clock.time_msec()\n\n        transaction = Transaction(**transaction_data)\n\n        if not transaction.transaction_id:\n            raise Exception(\"Transaction missing transaction_id\")\n        if not transaction.origin:\n            raise Exception(\"Transaction missing origin\")\n\n        logger.debug(\"[%s] Got transaction\", transaction.transaction_id)\n\n        # use a linearizer to ensure that we don't process the same transaction\n        # multiple times in parallel.\n        with (yield self._transaction_linearizer.queue(\n                (transaction.origin, transaction.transaction_id),\n        )):\n            result = yield self._handle_incoming_transaction(\n                transaction, request_time,\n            )\n\n        defer.returnValue(result)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _task_executor(self):\r\n        \"\"\"Worker that executes Tasks that have satisfied dependencies.\"\"\"\r\n        # this event blocks until the TaskGraph has signaled ready to execute\r\n        self._taskgraph_started_event.wait()\r\n        while True:\r\n            # this event blocks until the task graph has signaled it wants\r\n            # the executors to read the state of the queue or a stop event\r\n            self._executor_ready_event.wait()\r\n            # this lock synchronizes changes between the queue and\r\n            # executor_ready_event\r\n            if self._terminated:\r\n                LOGGER.debug(\r\n                    \"taskgraph is terminated, ending %s\",\r\n                    threading.currentThread())\r\n                break\r\n            task = None\r\n            try:\r\n                task = self._task_ready_priority_queue.get_nowait()\r\n                self._task_waiting_count -= 1\r\n                task_name_time_tuple = (task.task_name, time.time())\r\n                self._active_task_list.append(task_name_time_tuple)\r\n            except queue.Empty:\r\n                # no tasks are waiting could be because the taskgraph is\r\n                # closed or because the queue is just empty.\r\n                if self._closed and not self._task_dependent_map:\r\n                    # the task graph is signaling executors to stop,\r\n                    # since the self._task_dependent_map is empty the\r\n                    # executor can terminate.\r\n                    LOGGER.debug(\r\n                        \"no tasks are pending and taskgraph closed, normally \"\r\n                        \"terminating executor %s.\" %\r\n                        threading.currentThread())\r\n                    break\r\n                else:\r\n                    self._executor_ready_event.clear()\r\n            if task is None:\r\n                continue\r\n            try:\r\n                task._call()\r\n                task.task_done_executing_event.set()\r\n            except Exception as e:\r\n                # An error occurred on a call, terminate the taskgraph\r\n                task.exception_object = e\r\n                LOGGER.exception(\r\n                    'A taskgraph _task_executor failed on Task '\r\n                    '%s. Terminating taskgraph.', task.task_name)\r\n                self._terminate()\r\n                break\r\n\r\n            LOGGER.debug(\r\n                \"task %s is complete, checking to see if any dependent \"\r\n                \"tasks can be executed now\", task.task_name)\r\n            self._completed_task_names.add(task.task_name)\r\n            self._active_task_list.remove(task_name_time_tuple)\r\n            for waiting_task_name in (\r\n                    self._task_dependent_map[task.task_name]):\r\n                # remove `task` from the set of tasks that\r\n                # `waiting_task` was waiting on.\r\n                self._dependent_task_map[waiting_task_name].remove(\r\n                    task.task_name)\r\n                # if there aren't any left, we can push `waiting_task`\r\n                # to the work queue\r\n                if not self._dependent_task_map[waiting_task_name]:\r\n                    # if we removed the last task we can put it to the\r\n                    # work queue\r\n                    LOGGER.debug(\r\n                        \"Task %s is ready for processing, sending to \"\r\n                        \"task_ready_priority_queue\",\r\n                        waiting_task_name)\r\n                    del self._dependent_task_map[waiting_task_name]\r\n                    self._task_ready_priority_queue.put(\r\n                        self._task_name_map[waiting_task_name])\r\n                    self._task_waiting_count += 1\r\n                    # indicate to executors there is work to do\r\n                    self._executor_ready_event.set()\r\n            del self._task_dependent_map[task.task_name]\r\n            LOGGER.debug(\"task %s done processing\", task.task_name)\r\n        LOGGER.debug(\"task executor shutting down\")\r\n\r"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self, login_url='/login/', logout_url='/logout', persistent_timeout=43200, absolute_timeout=30):\n        \"\"\"\n        A tool that verify if the session is associated to a user by tracking\n        a session key. If session is not authenticated, redirect user to login page.\n        \"\"\"\n        request = cherrypy.serving.request\n        # Skip execution of this tools when browsing the login page.\n        if request.path_info == login_url:\n            if self._is_login():\n                raise cherrypy.HTTPRedirect('/')\n            return\n\n        # Clear session when browsing /logout\n        if request.path_info == logout_url or request.path_info.startswith(logout_url):\n            self.logout()\n            raise cherrypy.HTTPRedirect('/')\n\n        # Check if login\n        if not self._is_login():\n            # Store original URL\n            self._set_redirect_url()\n            # And redirect to login page\n            raise cherrypy.HTTPRedirect(login_url)\n\n        self._update_session_timeout()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def kick(bot, trigger):\n    \"\"\"Kick a user from the channel.\"\"\"\n    chanops = get_chanops(str(trigger.sender), bot.memory['channelmgnt']['jdcache'])\n    dodeop = False\n    if chanops:\n        if bot.channels[trigger.sender].privileges[bot.nick] < OP and trigger.account in chanops:\n            bot.say('Please wait...')\n            bot.say('op ' + trigger.sender, 'ChanServ')\n            time.sleep(1)\n            dodeop = True\n        text = trigger.group().split()\n        argc = len(text)\n        if argc < 2:\n            return\n        nick = Identifier(text[1])\n        reason = ' '.join(text[2:])\n        if nick != bot.config.core.nick and trigger.account in chanops:\n            bot.write(['KICK', trigger.sender, nick, ':' + reason])\n            if dodeop:\n                deopbot(trigger.sender, bot)\n        else:\n            bot.reply('Access Denied. If in error, please contact the channel founder.')\n    else:\n        bot.reply(f'No ChanOps Found. Please ask for assistance in {bot.settings.channelmgnt.support_channel}')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def eval(expression, _dict={}, **kw):\n    \"\"\"\n    Evaluates an image expression.\n\n    :param expression: A string containing a Python-style expression.\n    :param options: Values to add to the evaluation context.  You\n                    can either use a dictionary, or one or more keyword\n                    arguments.\n    :return: The evaluated expression. This is usually an image object, but can\n             also be an integer, a floating point value, or a pixel tuple,\n             depending on the expression.\n    \"\"\"\n\n    # build execution namespace\n    args = ops.copy()\n    args.update(_dict)\n    args.update(kw)\n    for k, v in list(args.items()):\n        if hasattr(v, \"im\"):\n            args[k] = _Operand(v)\n\n    out = builtins.eval(expression, args)\n    try:\n        return out.im\n    except AttributeError:\n        return out\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def load(self, dst_path=None) -> str:\n        \"\"\"\n        Downloads the dataset source to the local filesystem.\n\n        :param dst_path: Path of the local filesystem destination directory to which to download the\n                         dataset source. If the directory does not exist, it is created. If\n                         unspecified, the dataset source is downloaded to a new uniquely-named\n                         directory on the local filesystem.\n        :return: The path to the downloaded dataset source on the local filesystem.\n        \"\"\"\n        resp = cloud_storage_http_request(\n            method=\"GET\",\n            url=self.url,\n            stream=True,\n        )\n        augmented_raise_for_status(resp)\n\n        path = urlparse(self.url).path\n        content_disposition = resp.headers.get(\"Content-Disposition\")\n        if content_disposition is not None and (\n            file_name := next(re.finditer(r\"filename=(.+)\", content_disposition), None)\n        ):\n            # NB: If the filename is quoted, unquote it\n            basename = file_name[1].strip(\"'\\\"\")\n            if _is_path(basename):\n                raise MlflowException.invalid_parameter_value(\n                    f\"Invalid filename in Content-Disposition header: {basename}. \"\n                    \"It must be a file name, not a path.\"\n                )\n        elif path is not None and len(posixpath.basename(path)) > 0:\n            basename = posixpath.basename(path)\n        else:\n            basename = \"dataset_source\"\n\n        if dst_path is None:\n            dst_path = create_tmp_dir()\n\n        dst_path = os.path.join(dst_path, basename)\n        with open(dst_path, \"wb\") as f:\n            chunk_size = 1024 * 1024  # 1 MB\n            for chunk in resp.iter_content(chunk_size=chunk_size):\n                f.write(chunk)\n\n        return dst_path\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_safe_url(url, host=None):\n    \"\"\"\n    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to\n    a different host).\n\n    Always returns ``False`` on an empty url.\n    \"\"\"\n    if not url:\n        return False\n    netloc = urllib_parse.urlparse(url)[1]\n    return not netloc or netloc == host\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def getlength(self, text, mode=\"\", direction=None, features=None, language=None):\n        \"\"\"\n        Returns length (in pixels with 1/64 precision) of given text when rendered\n        in font with provided direction, features, and language.\n\n        This is the amount by which following text should be offset.\n        Text bounding box may extend past the length in some fonts,\n        e.g. when using italics or accents.\n\n        The result is returned as a float; it is a whole number if using basic layout.\n\n        Note that the sum of two lengths may not equal the length of a concatenated\n        string due to kerning. If you need to adjust for kerning, include the following\n        character and subtract its length.\n\n        For example, instead of ::\n\n          hello = font.getlength(\"Hello\")\n          world = font.getlength(\"World\")\n          hello_world = hello + world  # not adjusted for kerning\n          assert hello_world == font.getlength(\"HelloWorld\")  # may fail\n\n        use ::\n\n          hello = font.getlength(\"HelloW\") - font.getlength(\"W\")  # adjusted for kerning\n          world = font.getlength(\"World\")\n          hello_world = hello + world  # adjusted for kerning\n          assert hello_world == font.getlength(\"HelloWorld\")  # True\n\n        or disable kerning with (requires libraqm) ::\n\n          hello = draw.textlength(\"Hello\", font, features=[\"-kern\"])\n          world = draw.textlength(\"World\", font, features=[\"-kern\"])\n          hello_world = hello + world  # kerning is disabled, no need to adjust\n          assert hello_world == draw.textlength(\"HelloWorld\", font, features=[\"-kern\"])\n\n        .. versionadded:: 8.0.0\n\n        :param text: Text to measure.\n        :param mode: Used by some graphics drivers to indicate what mode the\n                     driver prefers; if empty, the renderer may return either\n                     mode. Note that the mode is always a string, to simplify\n                     C-level implementations.\n\n        :param direction: Direction of the text. It can be 'rtl' (right to\n                          left), 'ltr' (left to right) or 'ttb' (top to bottom).\n                          Requires libraqm.\n\n        :param features: A list of OpenType font features to be used during text\n                         layout. This is usually used to turn on optional\n                         font features that are not enabled by default,\n                         for example 'dlig' or 'ss01', but can be also\n                         used to turn off default font features for\n                         example '-liga' to disable ligatures or '-kern'\n                         to disable kerning.  To get all supported\n                         features, see\n                         https://learn.microsoft.com/en-us/typography/opentype/spec/featurelist\n                         Requires libraqm.\n\n        :param language: Language of the text. Different languages may use\n                         different glyph shapes or ligatures. This parameter tells\n                         the font which language the text is in, and to apply the\n                         correct substitutions as appropriate, if available.\n                         It should be a `BCP 47 language code\n                         <https://www.w3.org/International/articles/language-tags/>`_\n                         Requires libraqm.\n\n        :return: Width for horizontal, height for vertical text.\n        \"\"\"\n        return self.font.getlength(text, mode, direction, features, language) / 64\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def secureModule(mname, *imp):\n    modsec = _moduleSecurity.get(mname, None)\n    if modsec is None:\n        if mname in _appliedModuleSecurity:\n            return sys.modules[mname]\n        return  # no MSI, no module\n\n    if imp:\n        __import__(mname, *imp)\n    del _moduleSecurity[mname]\n    module = sys.modules[mname]\n    modsec.apply(module.__dict__)\n    _appliedModuleSecurity[mname] = modsec\n    return module\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "      void context.ready.then(async () => {\n        const file = context.model;\n        const workspace = file.toJSON() as unknown as Workspace.IWorkspace;\n        const path = context.path;\n        const id = workspace.metadata.id;\n\n        // Save the file contents as a workspace.\n        await this._workspaces.save(id, workspace);\n\n        // Save last save location for the save command.\n        await this._state.save(LAST_SAVE_ID, path);\n\n        // Navigate to new workspace.\n        const url = URLExt.join(this._application, 'workspaces', id);\n        if (this._router) {\n          this._router.navigate(url, { hard: true });\n        } else {\n          document.location.href = url;\n        }\n      });\n      return dummyWidget(context);"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_ordering(self, *ordering):\n        \"\"\"\n        Add items from the 'ordering' sequence to the query's \"order by\"\n        clause. These items are either field names (not column names) --\n        possibly with a direction prefix ('-' or '?') -- or OrderBy\n        expressions.\n\n        If 'ordering' is empty, clear all ordering from the query.\n        \"\"\"\n        errors = []\n        for item in ordering:\n            if isinstance(item, str):\n                if '.' in item:\n                    warnings.warn(\n                        'Passing column raw column aliases to order_by() is '\n                        'deprecated. Wrap %r in a RawSQL expression before '\n                        'passing it to order_by().' % item,\n                        category=RemovedInDjango40Warning,\n                        stacklevel=3,\n                    )\n                    continue\n                if item == '?':\n                    continue\n                if item.startswith('-'):\n                    item = item[1:]\n                if item in self.annotations:\n                    continue\n                if self.extra and item in self.extra:\n                    continue\n                # names_to_path() validates the lookup. A descriptive\n                # FieldError will be raise if it's not.\n                self.names_to_path(item.split(LOOKUP_SEP), self.model._meta)\n            elif not hasattr(item, 'resolve_expression'):\n                errors.append(item)\n            if getattr(item, 'contains_aggregate', False):\n                raise FieldError(\n                    'Using an aggregate in order_by() without also including '\n                    'it in annotate() is not allowed: %s' % item\n                )\n        if errors:\n            raise FieldError('Invalid order_by arguments: %s' % errors)\n        if ordering:\n            self.order_by += ordering\n        else:\n            self.default_ordering = False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& handle = ctx->input(0);\n    const string& name = handle.scalar<tstring>()();\n    OP_REQUIRES_OK(ctx, ctx->session_state()->DeleteTensor(name));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def from_unix_seconds(seconds: Union[int, float]) -> datetime:\n    while abs(seconds) > MS_WATERSHED:\n        seconds /= 1000\n    dt = EPOCH + timedelta(seconds=seconds)\n    return dt.replace(tzinfo=timezone.utc)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def match(self, path):\n        match = self.regex.search(path)\n        if match:\n            # If there are any named groups, use those as kwargs, ignoring\n            # non-named groups. Otherwise, pass all non-named arguments as\n            # positional arguments.\n            kwargs = match.groupdict()\n            args = () if kwargs else match.groups()\n            kwargs = {k: v for k, v in kwargs.items() if v is not None}\n            return path[match.end():], args, kwargs\n        return None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  static Status ParseEquation(const string& equation,\n                              OperandLabels* input_labels,\n                              Labels* output_labels,\n                              std::vector<DimensionType>* label_types,\n                              OperandLabelCounts* input_label_counts,\n                              LabelCounts* output_label_counts,\n                              gtl::InlinedVector<bool, 2>* input_has_ellipsis,\n                              bool* output_has_ellipsis) {\n    gtl::InlinedVector<string, 2> input_str;\n    string output_str;\n    TF_RETURN_IF_ERROR(ParseEinsumEquation(equation, &input_str, &output_str));\n\n    // Temporary map from single character labels to (consecutive) integer\n    // labels.\n    absl::flat_hash_map<char, int> label_mapping;\n    int num_inputs = input_str.size();\n    input_labels->resize(num_inputs);\n\n    // Map from single characters to integer labels.\n    for (int i = 0; i < num_inputs; ++i) {\n      MapToLabels(input_str[i], &input_labels->at(i), &label_mapping);\n    }\n    MapToLabels(output_str, output_labels, &label_mapping);\n\n    // Compute counts for input and output labels.\n    int num_labels = label_mapping.size();\n    input_label_counts->resize(num_inputs);\n    input_has_ellipsis->resize(num_inputs);\n    for (int i = 0; i < num_inputs; ++i) {\n      input_label_counts->at(i).resize(num_labels);\n      for (const int label : input_labels->at(i)) {\n        if (label != kEllipsisLabel)\n          input_label_counts->at(i)[label] += 1;\n        else\n          input_has_ellipsis->at(i) = true;\n      }\n    }\n    output_label_counts->resize(num_labels);\n    for (const int label : *output_labels) {\n      if (label != kEllipsisLabel)\n        output_label_counts->at(label) += 1;\n      else\n        *output_has_ellipsis = true;\n    }\n\n    // Map each label to a unique DimensionType.\n    label_types->resize(num_labels);\n    for (int label = 0; label < num_labels; ++label) {\n      if (label == kEllipsisLabel) continue;\n      bool removed = (*output_label_counts)[label] == 0;\n      bool unique = num_inputs == 1 || (*input_label_counts)[0][label] == 0 ||\n                    (*input_label_counts)[1][label] == 0;\n      (*label_types)[label] = GetDimensionType(removed, unique);\n    }\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def Ghostscript(tile, size, fp, scale=1):\n    \"\"\"Render an image using Ghostscript\"\"\"\n\n    # Unpack decoder tile\n    decoder, tile, offset, data = tile[0]\n    length, bbox = data\n\n    #Hack to support hi-res rendering\n    scale = int(scale) or 1\n    orig_size = size\n    orig_bbox = bbox\n    size = (size[0] * scale, size[1] * scale)\n    bbox = [bbox[0], bbox[1], bbox[2] * scale, bbox[3] * scale]\n    #print(\"Ghostscript\", scale, size, orig_size, bbox, orig_bbox)\n\n    import tempfile, os, subprocess\n\n    file = tempfile.mktemp()\n\n    # Build ghostscript command\n    command = [\"gs\",\n               \"-q\",                    # quite mode\n               \"-g%dx%d\" % size,        # set output geometry (pixels)\n               \"-r%d\" % (72*scale),     # set input DPI (dots per inch)\n               \"-dNOPAUSE -dSAFER\",     # don't pause between pages, safe mode\n               \"-sDEVICE=ppmraw\",       # ppm driver\n               \"-sOutputFile=%s\" % file,# output file\n            ]\n\n    if gs_windows_binary is not None:\n        if gs_windows_binary is False:\n            raise WindowsError('Unable to locate Ghostscript on paths')\n        command[0] = gs_windows_binary\n\n    # push data through ghostscript\n    try:\n        gs = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n        # adjust for image origin\n        if bbox[0] != 0 or bbox[1] != 0:\n            gs.stdin.write((\"%d %d translate\\n\" % (-bbox[0], -bbox[1])).encode('ascii'))\n        fp.seek(offset)\n        while length > 0:\n            s = fp.read(8192)\n            if not s:\n                break\n            length = length - len(s)\n            gs.stdin.write(s)\n        gs.stdin.close()\n        status = gs.wait()\n        if status:\n            raise IOError(\"gs failed (status %d)\" % status)\n        im = Image.core.open_ppm(file)\n    finally:\n        try: os.unlink(file)\n        except: pass\n\n    return im\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render_POST(self, request):\n        send_cors(request)\n\n        args = get_args(request, (\"medium\", \"address\", \"room_id\", \"sender\",))\n        medium = args[\"medium\"]\n        address = args[\"address\"]\n        roomId = args[\"room_id\"]\n        sender = args[\"sender\"]\n\n        verified_sender = None\n        if self.require_auth:\n            account = authV2(self.sydent, request)\n            verified_sender = sender\n            if account.userId != sender:\n                raise MatrixRestError(403, \"M_UNAUTHORIZED\", \"'sender' doesn't match\")\n\n        globalAssocStore = GlobalAssociationStore(self.sydent)\n        mxid = globalAssocStore.getMxid(medium, address)\n        if mxid:\n            request.setResponseCode(400)\n            return {\n                \"errcode\": \"M_THREEPID_IN_USE\",\n                \"error\": \"Binding already known\",\n                \"mxid\": mxid,\n            }\n\n        if medium != \"email\":\n            request.setResponseCode(400)\n            return {\n                \"errcode\": \"M_UNRECOGNIZED\",\n                \"error\": \"Didn't understand medium '%s'\" % (medium,),\n            }\n\n        token = self._randomString(128)\n\n        tokenStore = JoinTokenStore(self.sydent)\n\n        ephemeralPrivateKey = nacl.signing.SigningKey.generate()\n        ephemeralPublicKey = ephemeralPrivateKey.verify_key\n\n        ephemeralPrivateKeyBase64 = encode_base64(ephemeralPrivateKey.encode(), True)\n        ephemeralPublicKeyBase64 = encode_base64(ephemeralPublicKey.encode(), True)\n\n        tokenStore.storeEphemeralPublicKey(ephemeralPublicKeyBase64)\n        tokenStore.storeToken(medium, address, roomId, sender, token)\n\n        # Variables to substitute in the template.\n        substitutions = {}\n        # Include all arguments sent via the request.\n        for k, v in args.items():\n            if isinstance(v, string_types):\n                substitutions[k] = v\n        substitutions[\"token\"] = token\n\n        # Substitutions that the template requires, but are optional to provide\n        # to the API.\n        extra_substitutions = [\n            'sender_display_name',\n            'token',\n            'room_name',\n            'bracketed_room_name',\n            'room_avatar_url',\n            'sender_avatar_url',\n            'guest_user_id',\n            'guest_access_token',\n        ]\n        for k in extra_substitutions:\n            substitutions.setdefault(k, '')\n\n        substitutions[\"bracketed_verified_sender\"] = \"\"\n        if verified_sender:\n            substitutions[\"bracketed_verified_sender\"] = \"(%s) \" % (verified_sender,)\n\n        substitutions[\"ephemeral_private_key\"] = ephemeralPrivateKeyBase64\n        if substitutions[\"room_name\"] != '':\n            substitutions[\"bracketed_room_name\"] = \"(%s) \" % substitutions[\"room_name\"]\n\n        substitutions[\"web_client_location\"] = self.sydent.default_web_client_location\n        if 'org.matrix.web_client_location' in substitutions:\n            substitutions[\"web_client_location\"] = substitutions.pop(\"org.matrix.web_client_location\")\n\n        subject_header = Header(self.sydent.cfg.get('email', 'email.invite.subject', raw=True) % substitutions, 'utf8')\n        substitutions[\"subject_header_value\"] = subject_header.encode()\n\n        brand = self.sydent.brand_from_request(request)\n        templateFile = self.sydent.get_branded_template(\n            brand,\n            \"invite_template.eml\",\n            ('email', 'email.invite_template'),\n        )\n\n        sendEmail(self.sydent, templateFile, address, substitutions)\n\n        pubKey = self.sydent.keyring.ed25519.verify_key\n        pubKeyBase64 = encode_base64(pubKey.encode())\n\n        baseUrl = \"%s/_matrix/identity/api/v1\" % (self.sydent.cfg.get('http', 'client_http_base'),)\n\n        keysToReturn = []\n        keysToReturn.append({\n            \"public_key\": pubKeyBase64,\n            \"key_validity_url\": baseUrl + \"/pubkey/isvalid\",\n        })\n        keysToReturn.append({\n            \"public_key\": ephemeralPublicKeyBase64,\n            \"key_validity_url\": baseUrl + \"/pubkey/ephemeral/isvalid\",\n        })\n\n        resp = {\n            \"token\": token,\n            \"public_key\": pubKeyBase64,\n            \"public_keys\": keysToReturn,\n            \"display_name\": self.redact_email_address(address),\n        }\n\n        return resp\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def load(self, loader):\n        loader.add_option(\n            \"connection_strategy\", str, \"eager\",\n            \"Determine when server connections should be established. When set to lazy, mitmproxy \"\n            \"tries to defer establishing an upstream connection as long as possible. This makes it possible to \"\n            \"use server replay while being offline. When set to eager, mitmproxy can detect protocols with \"\n            \"server-side greetings, as well as accurately mirror TLS ALPN negotiation.\",\n            choices=(\"eager\", \"lazy\")\n        )\n        loader.add_option(\n            \"stream_large_bodies\", Optional[str], None,\n            \"\"\"\n            Stream data to the client if response body exceeds the given\n            threshold. If streamed, the body will not be stored in any way.\n            Understands k/m/g suffixes, i.e. 3m for 3 megabytes.\n            \"\"\"\n        )\n        loader.add_option(\n            \"body_size_limit\", Optional[str], None,\n            \"\"\"\n            Byte size limit of HTTP request and response bodies. Understands\n            k/m/g suffixes, i.e. 3m for 3 megabytes.\n            \"\"\"\n        )\n        loader.add_option(\n            \"keep_host_header\", bool, False,\n            \"\"\"\n            Reverse Proxy: Keep the original host header instead of rewriting it\n            to the reverse proxy target.\n            \"\"\"\n        )\n        loader.add_option(\n            \"proxy_debug\", bool, False,\n            \"Enable debug logs in the proxy core.\",\n        )\n        loader.add_option(\n            \"normalize_outbound_headers\", bool, True,\n            \"\"\"\n            Normalize outgoing HTTP/2 header names, but emit a warning when doing so.\n            HTTP/2 does not allow uppercase header names. This option makes sure that HTTP/2 headers set\n            in custom scripts are lowercased before they are sent.\n            \"\"\",\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compile(XlaOpKernelContext* ctx) override {\n    int64_t output_size;\n    xla::XlaOp output_size_param = ctx->Input(\"size\");\n    StatusOr<xla::Shape> output_shape_or =\n        ctx->builder()->GetShape(output_size_param);\n    OP_REQUIRES_OK(ctx, output_shape_or.status());\n    auto output_shape_param = output_shape_or.ValueOrDie();\n    auto output_rank = output_shape_param.rank();\n    OP_REQUIRES(ctx, output_rank == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        output_rank));\n    OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntScalar(\"size\", &output_size));\n    OP_REQUIRES(ctx, output_size >= 0,\n                errors::InvalidArgument(\"size (\", output_size,\n                                        \") must be non-negative\"));\n    xla::XlaOp idx, updates, output;\n    xla::XlaOp input = ctx->Input(0);\n    auto input_xla_type = ctx->input_xla_type(0);\n    xla::PrimitiveType dtype = ctx->InputXlaType(\"weights\");\n    auto zero = xla::Zero(ctx->builder(), dtype);\n    auto one = xla::One(ctx->builder(), dtype);\n    StatusOr<xla::Shape> input_shape_or = ctx->builder()->GetShape(input);\n    OP_REQUIRES_OK(ctx, input_shape_or.status());\n    auto input_shape = input_shape_or.ValueOrDie();\n    auto size = input_shape.dimensions(0);\n\n    if (!size) {\n      output = xla::Broadcast(zero, {output_size});\n      ctx->SetOutput(0, output);\n      return;\n    }\n    auto rank = input_shape.rank();\n\n    OP_REQUIRES(ctx, rank <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", rank));\n\n    xla::XlaOp weights = ctx->Input(2);\n    StatusOr<xla::Shape> weights_shape_or = ctx->builder()->GetShape(weights);\n    OP_REQUIRES_OK(ctx, weights_shape_or.status());\n\n    auto weights_shape = weights_shape_or.ValueOrDie();\n    auto weights_size = weights_shape.dimensions(0);\n    bool has_weights = false;\n    if (weights_size) {\n      has_weights = true;\n    }\n    xla::Shape output_shape = xla::ShapeUtil::MakeShape(dtype, {output_size});\n    xla::ScatterDimensionNumbers scatter_dnums;\n    scatter_dnums.set_index_vector_dim(1);\n    scatter_dnums.add_inserted_window_dims(0);\n    scatter_dnums.add_scatter_dims_to_operand_dims(0);\n\n    if (rank == 2) {\n      output_shape = xla::ShapeUtil::MakeShape(dtype, {size, output_size});\n      scatter_dnums.add_inserted_window_dims(1);\n      scatter_dnums.add_scatter_dims_to_operand_dims(1);\n      auto i_shape =\n          xla::ShapeUtil::MakeShape(input_xla_type, {input_shape.dimensions()});\n      auto i = xla::Iota(ctx->builder(), i_shape, 0);\n      i = xla::Reshape(\n          i, {input_shape.dimensions(0) * input_shape.dimensions(1), 1});\n      auto j = xla::Reshape(\n          input, {input_shape.dimensions(0) * input_shape.dimensions(1), 1});\n      std::vector<xla::XlaOp> iotas_to_concat;\n      iotas_to_concat.push_back(i);\n      iotas_to_concat.push_back(j);\n      idx = xla::ConcatInDim(ctx->builder(), iotas_to_concat, 1);\n      updates = xla::Broadcast(\n          one, {input_shape.dimensions(0) * input_shape.dimensions(1)});\n      output = xla::Broadcast(\n          zero, {output_shape.dimensions(0), output_shape.dimensions(1)});\n      if (has_weights && !binary_output_) {\n        weights = xla::Reshape(\n            weights, {input_shape.dimensions(0) * input_shape.dimensions(1)});\n        updates = weights;\n      }\n    } else {\n      input = xla::Reshape(input, {size, 1});\n      idx = xla::Reshape(input, {size, 1});\n      updates = xla::Broadcast(one, {size});\n      output = xla::Broadcast(zero, {output_size});\n      if (has_weights && !binary_output_) {\n        updates = weights;\n      }\n    }\n\n    xla::XlaComputation assn_computation = [&] {\n      std::unique_ptr<xla::XlaBuilder> subb =\n          ctx->builder()->CreateSubBuilder(\"scatter_bincount\");\n      xla::Shape param_shape = xla::ShapeUtil::MakeShape(dtype, {});\n      auto p0 = xla::Parameter(subb.get(), 0, param_shape, \"p0\");\n      auto p1 = xla::Parameter(subb.get(), 1, param_shape, \"p1\");\n      if (!binary_output_) {\n        xla::Add(p0, p1);\n      }\n      return subb->BuildAndNoteError();\n    }();\n    output = xla::Scatter(output, idx, updates, assn_computation, scatter_dnums,\n                          false, false);\n    ctx->SetOutput(0, output);\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor x = context->input(0);\n    const Tensor& scale = context->input(1);\n    const Tensor& offset = context->input(2);\n    const Tensor& estimated_mean = context->input(3);\n    const Tensor& estimated_variance = context->input(4);\n    const Tensor* side_input = has_side_input_ ? &context->input(5) : nullptr;\n\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(context, offset.dims() == 1,\n                errors::InvalidArgument(\"offset must be 1-dimensional\",\n                                        offset.shape().DebugString()));\n    OP_REQUIRES(context, estimated_mean.dims() == 1,\n                errors::InvalidArgument(\"estimated_mean must be 1-dimensional\",\n                                        estimated_mean.shape().DebugString()));\n    OP_REQUIRES(\n        context, estimated_variance.dims() == 1,\n        errors::InvalidArgument(\"estimated_variance must be 1-dimensional\",\n                                estimated_variance.shape().DebugString()));\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64_t in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64_t in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64_t in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64_t in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64_t in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    const auto num_channels = GetTensorDim(x, tensor_format_, 'C');\n    OP_REQUIRES(\n        context, scale.NumElements() == num_channels,\n        errors::InvalidArgument(\"scale must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                scale.NumElements(), \" and \", num_channels));\n    OP_REQUIRES(\n        context, offset.NumElements() == num_channels,\n        errors::InvalidArgument(\"offset must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                offset.NumElements(), \" and \", num_channels));\n    if (estimated_mean.NumElements() != 0) {\n      OP_REQUIRES(context, estimated_mean.NumElements() == num_channels,\n                  errors::InvalidArgument(\n                      \"mean must be empty or have the same number of \"\n                      \"elements as the channels of x, got \",\n                      estimated_mean.NumElements(), \" and \", num_channels));\n    }\n    if (estimated_variance.NumElements() != 0) {\n      OP_REQUIRES(context, estimated_variance.NumElements() == num_channels,\n                  errors::InvalidArgument(\n                      \"variance must be empty or have the same number of \"\n                      \"elements as the channels of x, got \",\n                      estimated_variance.NumElements(), \" and \", num_channels));\n    }\n\n    if (has_side_input_) {\n      OP_REQUIRES(context, side_input->shape() == x.shape(),\n                  errors::InvalidArgument(\n                      \"side_input shape must be equal to input shape: \",\n                      side_input->shape().DebugString(),\n                      \" != \", x.shape().DebugString()));\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity) {\n      // NOTE(ezhulenev): This requirement is coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx.\n      OP_REQUIRES(\n          context, !is_training_ || num_channels % 4 == 0,\n          errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                  \"channel dimension to be a multiple of 4.\"));\n    }\n\n    Tensor* y = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, alloc_shape, &y));\n\n    Tensor* batch_mean = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {3}, 1, scale.shape(), &batch_mean));\n    Tensor* batch_var = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {4}, 2, scale.shape(), &batch_var));\n    Tensor* saved_mean = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(3, scale.shape(), &saved_mean));\n    Tensor* saved_maybe_inv_var = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(4, scale.shape(),\n                                                     &saved_maybe_inv_var));\n\n    if (is_training_) {\n      functor::FusedBatchNorm<Device, T, U, true>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    } else {\n      functor::FusedBatchNorm<Device, T, U, false>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, y->CopyFrom(*y, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& grad_in = context->input(1);\n    const Tensor& argmax = context->input(2);\n\n    PoolParameters params{context,\n                          ksize_,\n                          stride_,\n                          padding_,\n                          /*explicit_paddings=*/{},\n                          FORMAT_NHWC,\n                          tensor_in.shape()};\n    if (!context->status().ok()) {\n      return;\n    }\n\n    TensorShape out_shape({params.tensor_in_batch, params.tensor_in_rows,\n                           params.tensor_in_cols, params.depth});\n    Tensor* grad_out = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, out_shape, &grad_out));\n\n    LaunchMaxPoolingGradWithArgmax<Device, T>::launch(\n        context, params, grad_in, argmax, grad_out, include_batch_in_index_);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def requestStarted(self, job):\n        \"\"\"Handle a request for a qute: scheme.\n\n        This method must be reimplemented by all custom URL scheme handlers.\n        The request is asynchronous and does not need to be handled right away.\n\n        Args:\n            job: QWebEngineUrlRequestJob\n        \"\"\"\n        url = job.requestUrl()\n\n        if url.scheme() in ['chrome-error', 'chrome-extension']:\n            # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-63378\n            job.fail(QWebEngineUrlRequestJob.UrlInvalid)\n            return\n\n        assert job.requestMethod() == b'GET'\n        assert url.scheme() == 'qute'\n        log.misc.debug(\"Got request for {}\".format(url.toDisplayString()))\n        try:\n            mimetype, data = qutescheme.data_for_url(url)\n        except qutescheme.NoHandlerFound:\n            log.misc.debug(\"No handler found for {}\".format(\n                url.toDisplayString()))\n            job.fail(QWebEngineUrlRequestJob.UrlNotFound)\n        except qutescheme.QuteSchemeOSError:\n            # FIXME:qtwebengine how do we show a better error here?\n            log.misc.exception(\"OSError while handling qute://* URL\")\n            job.fail(QWebEngineUrlRequestJob.UrlNotFound)\n        except qutescheme.QuteSchemeError:\n            # FIXME:qtwebengine how do we show a better error here?\n            log.misc.exception(\"Error while handling qute://* URL\")\n            job.fail(QWebEngineUrlRequestJob.RequestFailed)\n        except qutescheme.Redirect as e:\n            qtutils.ensure_valid(e.url)\n            job.redirect(e.url)\n        else:\n            log.misc.debug(\"Returning {} data\".format(mimetype))\n\n            # We can't just use the QBuffer constructor taking a QByteArray,\n            # because that somehow segfaults...\n            # https://www.riverbankcomputing.com/pipermail/pyqt/2016-September/038075.html\n            buf = QBuffer(parent=self)\n            buf.open(QIODevice.WriteOnly)\n            buf.write(data)\n            buf.seek(0)\n            buf.close()\n            job.reply(mimetype.encode('ascii'), buf)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "$.fn.reviewsAutoComplete = function(options) {\n    return this.each(function() {\n        $(this)\n            .autocomplete({\n                formatItem: function(data) {\n                    var s = data[options.nameKey];\n\n                    if (options.descKey) {\n                        s += \" <span>(\" + data[options.descKey] + \")</span>\";\n                    }\n\n                    return s;\n                },\n                matchCase: false,\n                multiple: true,\n                parse: function(data) {\n                    var jsonData = eval(\"(\" + data + \")\");\n                    var items = jsonData[options.fieldName];\n                    var parsed = [];\n\n                    for (var i = 0; i < items.length; i++) {\n                        var value = items[i];\n\n                        parsed.push({\n                            data: value,\n                            value: value[options.nameKey],\n                            result: value[options.nameKey]\n                        });\n                    }\n\n                    return parsed;\n                },\n                url: SITE_ROOT + gReviewRequestSitePrefix + \"api/\" + options.fieldName + \"/\",\n                extraParams: options.extraParams\n            })\n            .bind(\"autocompleteshow\", function() {\n                /*\n                 * Add the footer to the bottom of the results pane the\n                 * first time it's created.\n                 *\n                 * Note that we may have multiple .ui-autocomplete-results\n                 * elements, and we don't necessarily know which is tied to\n                 * this. So, we'll look for all instances that don't contain\n                 * a footer.\n                 */\n                var resultsPane = $(\".ui-autocomplete-results:not(\" +\n                                    \":has(.ui-autocomplete-footer))\");\n\n                if (resultsPane.length > 0) {\n                    $(\"<div/>\")\n                        .addClass(\"ui-autocomplete-footer\")\n                        .text(\"Press Tab to auto-complete.\")\n                        .appendTo(resultsPane);\n                }\n            });\n    });\n};\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  public static CloseableHttpClient createClient(\n      int maxConnections,\n      int timeoutMilliseconds,\n      ProxySettings proxySettings,\n      KeyStoreSettings trustStoreSettings,\n      boolean useSystemProperties) {\n    return createClient(\n        maxConnections,\n        timeoutMilliseconds,\n        proxySettings,\n        trustStoreSettings,\n        true,\n        Collections.<String>emptyList(),\n        useSystemProperties);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def send_newkeys(self, k: bytes, h: bytes) -> None:\n        \"\"\"Finish a key exchange and send a new keys message\"\"\"\n\n        if not self._session_id:\n            first_kex = True\n            self._session_id = h\n        else:\n            first_kex = False\n\n        enc_keysize_cs, enc_ivsize_cs, enc_blocksize_cs, \\\n        mac_keysize_cs, mac_hashsize_cs, etm_cs = \\\n            get_encryption_params(self._enc_alg_cs, self._mac_alg_cs)\n\n        enc_keysize_sc, enc_ivsize_sc, enc_blocksize_sc, \\\n        mac_keysize_sc, mac_hashsize_sc, etm_sc = \\\n            get_encryption_params(self._enc_alg_sc, self._mac_alg_sc)\n\n        if mac_keysize_cs == 0:\n            self._mac_alg_cs = self._enc_alg_cs\n\n        if mac_keysize_sc == 0:\n            self._mac_alg_sc = self._enc_alg_sc\n\n        cmp_after_auth_cs = get_compression_params(self._cmp_alg_cs)\n        cmp_after_auth_sc = get_compression_params(self._cmp_alg_sc)\n\n        self.logger.debug2('  Client to server:')\n        self.logger.debug2('    Encryption alg: %s', self._enc_alg_cs)\n        self.logger.debug2('    MAC alg: %s', self._mac_alg_cs)\n        self.logger.debug2('    Compression alg: %s', self._cmp_alg_cs)\n        self.logger.debug2('  Server to client:')\n        self.logger.debug2('    Encryption alg: %s', self._enc_alg_sc)\n        self.logger.debug2('    MAC alg: %s', self._mac_alg_sc)\n        self.logger.debug2('    Compression alg: %s', self._cmp_alg_sc)\n\n        assert self._kex is not None\n\n        iv_cs = self._kex.compute_key(k, h, b'A', self._session_id,\n                                      enc_ivsize_cs)\n        iv_sc = self._kex.compute_key(k, h, b'B', self._session_id,\n                                      enc_ivsize_sc)\n        enc_key_cs = self._kex.compute_key(k, h, b'C', self._session_id,\n                                           enc_keysize_cs)\n        enc_key_sc = self._kex.compute_key(k, h, b'D', self._session_id,\n                                           enc_keysize_sc)\n        mac_key_cs = self._kex.compute_key(k, h, b'E', self._session_id,\n                                           mac_keysize_cs)\n        mac_key_sc = self._kex.compute_key(k, h, b'F', self._session_id,\n                                           mac_keysize_sc)\n        self._kex = None\n\n        next_enc_cs = get_encryption(self._enc_alg_cs, enc_key_cs, iv_cs,\n                                     self._mac_alg_cs, mac_key_cs, etm_cs)\n        next_enc_sc = get_encryption(self._enc_alg_sc, enc_key_sc, iv_sc,\n                                     self._mac_alg_sc, mac_key_sc, etm_sc)\n\n        self.send_packet(MSG_NEWKEYS)\n\n        self._extensions_to_send[b'global-requests-ok'] = b''\n\n        if self.is_client():\n            self._send_encryption = next_enc_cs\n            self._send_enchdrlen = 1 if etm_cs else 5\n            self._send_blocksize = max(8, enc_blocksize_cs)\n            self._compressor = get_compressor(self._cmp_alg_cs)\n            self._compress_after_auth = cmp_after_auth_cs\n\n            self._next_recv_encryption = next_enc_sc\n            self._next_recv_blocksize = max(8, enc_blocksize_sc)\n            self._next_recv_macsize = mac_hashsize_sc\n            self._next_decompressor = get_decompressor(self._cmp_alg_sc)\n            self._next_decompress_after_auth = cmp_after_auth_sc\n\n            self.set_extra_info(\n                send_cipher=self._enc_alg_cs.decode('ascii'),\n                send_mac=self._mac_alg_cs.decode('ascii'),\n                send_compression=self._cmp_alg_cs.decode('ascii'),\n                recv_cipher=self._enc_alg_sc.decode('ascii'),\n                recv_mac=self._mac_alg_sc.decode('ascii'),\n                recv_compression=self._cmp_alg_sc.decode('ascii'))\n\n            if first_kex:\n                if self._wait == 'kex' and self._waiter and \\\n                        not self._waiter.cancelled():\n                    self._waiter.set_result(None)\n                    self._wait = None\n                else:\n                    self.send_service_request(_USERAUTH_SERVICE)\n        else:\n            self._send_encryption = next_enc_sc\n            self._send_enchdrlen = 1 if etm_sc else 5\n            self._send_blocksize = max(8, enc_blocksize_sc)\n            self._compressor = get_compressor(self._cmp_alg_sc)\n            self._compress_after_auth = cmp_after_auth_sc\n\n            self._next_recv_encryption = next_enc_cs\n            self._next_recv_blocksize = max(8, enc_blocksize_cs)\n            self._next_recv_macsize = mac_hashsize_cs\n            self._next_decompressor = get_decompressor(self._cmp_alg_cs)\n            self._next_decompress_after_auth = cmp_after_auth_cs\n\n            self.set_extra_info(\n                send_cipher=self._enc_alg_sc.decode('ascii'),\n                send_mac=self._mac_alg_sc.decode('ascii'),\n                send_compression=self._cmp_alg_sc.decode('ascii'),\n                recv_cipher=self._enc_alg_cs.decode('ascii'),\n                recv_mac=self._mac_alg_cs.decode('ascii'),\n                recv_compression=self._cmp_alg_cs.decode('ascii'))\n\n            if first_kex:\n                self._next_service = _USERAUTH_SERVICE\n\n                self._extensions_to_send[b'server-sig-algs'] = \\\n                    b','.join(self._sig_algs)\n\n        if self._can_send_ext_info:\n            self._send_ext_info()\n            self._can_send_ext_info = False\n\n        self._kex_complete = True\n        self._send_deferred_packets()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def rebuild_proxies(self, prepared_request, proxies):\n        \"\"\"This method re-evaluates the proxy configuration by considering the\n        environment variables. If we are redirected to a URL covered by\n        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing\n        proxy keys for this URL (in case they were stripped by a previous\n        redirect).\n\n        This method also replaces the Proxy-Authorization header where\n        necessary.\n\n        :rtype: dict\n        \"\"\"\n        headers = prepared_request.headers\n        scheme = urlparse(prepared_request.url).scheme\n        new_proxies = resolve_proxies(prepared_request, proxies, self.trust_env)\n\n        if \"Proxy-Authorization\" in headers:\n            del headers[\"Proxy-Authorization\"]\n\n        try:\n            username, password = get_auth_from_url(new_proxies[scheme])\n        except KeyError:\n            username, password = None, None\n\n        if username and password:\n            headers[\"Proxy-Authorization\"] = _basic_auth_str(username, password)\n\n        return new_proxies\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_ip(ip: str) -> None:\n    \"\"\"Checks if an IP is local/private.\n\n    :param ip: IP address to be checked.\n    \"\"\"\n\n    if ip == '0.0.0.0':  # nosec\n        raise InvalidUploadUrlError\n\n    local_subnets = [\n        '127.0.0.0/8',\n        '10.0.0.0/8',\n        '172.16.0.0/12',\n        '192.168.0.0/16',\n    ]\n\n    for subnet in local_subnets:\n        if ipaddress.ip_address(ip) in ipaddress.ip_network(subnet):\n            raise InvalidUploadUrlError\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _load_prompt(config: dict) -> PromptTemplate:\n    \"\"\"Load the prompt template from config.\"\"\"\n    # Load the template from disk if necessary.\n    config = _load_template(\"template\", config)\n    config = _load_output_parser(config)\n    return PromptTemplate(**config)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  static port::StatusOr<CudnnRnnSequenceTensorDescriptor> Create(\n      GpuExecutor* parent, int max_seq_length, int batch_size, int data_size,\n      const absl::Span<const int>& seq_lengths, bool time_major,\n      cudnnDataType_t data_type) {\n    CHECK_GT(max_seq_length, 0);\n    int dims[] = {batch_size, data_size, 1};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    TensorDescriptor tensor_desc = CreateTensorDescriptor();\n    RETURN_IF_CUDNN_ERROR(cudnnSetTensorNdDescriptor(\n        /*tensorDesc=*/tensor_desc.get(), /*dataType=*/data_type,\n        /*nbDims=*/sizeof(dims) / sizeof(dims[0]), /*dimA=*/dims,\n        /*strideA=*/strides));\n    const int* seq_lengths_array = seq_lengths.data();\n    RNNDataDescriptor data_desc = CreateRNNDataDescriptor();\n    float padding_fill = 0.0f;\n    cudnnRNNDataLayout_t layout;\n    if (time_major) {\n      layout = CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED;\n    } else {\n      layout = CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED;\n    }\n    RETURN_IF_CUDNN_ERROR(cudnnSetRNNDataDescriptor(\n        /*RNNDataDesc=*/data_desc.get(), /*dataType*/ data_type,\n        /*layout=*/layout,\n        /*maxSeqLength=*/max_seq_length,\n        /*batchSize=*/batch_size, /*vectorSize=*/data_size,\n        /*seqLengthArray=*/seq_lengths_array,\n        /*paddingFill*/ (void*)&padding_fill));\n    return CudnnRnnSequenceTensorDescriptor(\n        parent, max_seq_length, batch_size, data_size, data_type,\n        std::move(data_desc), std::move(tensor_desc));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const float in_min = context->input(2).flat<float>()(0);\n    const float in_max = context->input(3).flat<float>()(0);\n\n    ImageResizerState st(align_corners_, false);\n    st.ValidateAndCreateOutput(context);\n\n    if (!context->status().ok()) return;\n\n    // Return if the output is empty.\n    if (st.output->NumElements() == 0) return;\n\n    typename TTypes<T, 4>::ConstTensor image_data(\n        context->input(0).tensor<T, 4>());\n    typename TTypes<T, 4>::Tensor output_data(st.output->tensor<T, 4>());\n\n    ResizeBilinear<T>(image_data, st.height_scale, st.width_scale, in_min,\n                      in_max, half_pixel_centers_, &output_data);\n    Tensor* out_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &out_min));\n    out_min->flat<float>()(0) = in_min;\n\n    Tensor* out_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &out_max));\n    out_max->flat<float>()(0) = in_max;\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def response_for_exception(request, exc):\n    if isinstance(exc, Http404):\n        if settings.DEBUG:\n            response = debug.technical_404_response(request, exc)\n        else:\n            response = get_exception_response(\n                request, get_resolver(get_urlconf()), 404, exc\n            )\n\n    elif isinstance(exc, PermissionDenied):\n        response = get_exception_response(\n            request, get_resolver(get_urlconf()), 403, exc\n        )\n        log_response(\n            \"Forbidden (Permission denied): %s\",\n            request.path,\n            response=response,\n            request=request,\n            exc_info=sys.exc_info(),\n        )\n\n    elif isinstance(exc, MultiPartParserError):\n        response = get_exception_response(\n            request, get_resolver(get_urlconf()), 400, exc\n        )\n        log_response(\n            \"Bad request (Unable to parse request body): %s\",\n            request.path,\n            response=response,\n            request=request,\n            exc_info=sys.exc_info(),\n        )\n\n    elif isinstance(exc, BadRequest):\n        if settings.DEBUG:\n            response = debug.technical_500_response(\n                request, *sys.exc_info(), status_code=400\n            )\n        else:\n            response = get_exception_response(\n                request, get_resolver(get_urlconf()), 400, exc\n            )\n        log_response(\n            \"%s: %s\",\n            str(exc),\n            request.path,\n            response=response,\n            request=request,\n            exc_info=sys.exc_info(),\n        )\n    elif isinstance(exc, SuspiciousOperation):\n        if isinstance(exc, (RequestDataTooBig, TooManyFieldsSent)):\n            # POST data can't be accessed again, otherwise the original\n            # exception would be raised.\n            request._mark_post_parse_error()\n\n        # The request logger receives events for any problematic request\n        # The security logger receives events for all SuspiciousOperations\n        security_logger = logging.getLogger(\n            \"django.security.%s\" % exc.__class__.__name__\n        )\n        security_logger.error(\n            str(exc),\n            extra={\"status_code\": 400, \"request\": request},\n        )\n        if settings.DEBUG:\n            response = debug.technical_500_response(\n                request, *sys.exc_info(), status_code=400\n            )\n        else:\n            response = get_exception_response(\n                request, get_resolver(get_urlconf()), 400, exc\n            )\n\n    else:\n        signals.got_request_exception.send(sender=None, request=request)\n        response = handle_uncaught_exception(\n            request, get_resolver(get_urlconf()), sys.exc_info()\n        )\n        log_response(\n            \"%s: %s\",\n            response.reason_phrase,\n            request.path,\n            response=response,\n            request=request,\n            exc_info=sys.exc_info(),\n        )\n\n    # Force a TemplateResponse to be rendered.\n    if not getattr(response, \"is_rendered\", True) and callable(\n        getattr(response, \"render\", None)\n    ):\n        response = response.render()\n\n    return response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def model_from_yaml(yaml_string, custom_objects=None):\n  \"\"\"Parses a yaml model configuration file and returns a model instance.\n\n  Usage:\n\n  >>> model = tf.keras.Sequential([\n  ...     tf.keras.layers.Dense(5, input_shape=(3,)),\n  ...     tf.keras.layers.Softmax()])\n  >>> try:\n  ...   import yaml\n  ...   config = model.to_yaml()\n  ...   loaded_model = tf.keras.models.model_from_yaml(config)\n  ... except ImportError:\n  ...   pass\n\n  Arguments:\n      yaml_string: YAML string or open file encoding a model configuration.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n\n  Returns:\n      A Keras model instance (uncompiled).\n\n  Raises:\n      ImportError: if yaml module is not found.\n  \"\"\"\n  if yaml is None:\n    raise ImportError('Requires yaml module installed (`pip install pyyaml`).')\n  # The method unsafe_load only exists in PyYAML 5.x+, so which branch of the\n  # try block is covered by tests depends on the installed version of PyYAML.\n  try:\n    # PyYAML 5.x+\n    config = yaml.unsafe_load(yaml_string)\n  except AttributeError:\n    config = yaml.load(yaml_string)\n  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\n  return deserialize(config, custom_objects=custom_objects)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def encryption_materials(self, encryption_context):\n        # type: (EncryptionContext) -> CryptographicMaterials\n        \"\"\"Return encryption materials.\n\n        :param EncryptionContext encryption_context: Encryption context for request\n        :raises AttributeError: if no encryption materials are available\n        \"\"\"\n        ttl_action = self._ttl_action()\n\n        _LOGGER.debug('TTL Action \"%s\" when getting encryption materials', ttl_action.name)\n\n        provider = None\n\n        if ttl_action is TtlActions.LIVE:\n            try:\n                _LOGGER.debug(\"Looking in cache for encryption materials provider version %d\", self._version)\n                provider = self._cache.get(self._version)\n            except KeyError:\n                _LOGGER.debug(\"Encryption materials provider not found in cache\")\n                ttl_action = TtlActions.EXPIRED\n\n        if provider is None:\n            # Just get the latest local version if we cannot acquire the lock.\n            # Otherwise, block until we can acquire the lock.\n            allow_local = bool(ttl_action is TtlActions.GRACE_PERIOD)\n\n            _LOGGER.debug(\"Getting most recent materials provider version\")\n            provider = self._get_most_recent_version(allow_local)\n\n        return provider.encryption_materials(encryption_context)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def execute_command(self, *args: Any, **kwargs: Any) -> Any:\n        # Acquire connection\n        connection = self.acquire_connection()\n\n        # Execute command\n        await connection.send_packed_command(connection.pack_command(*args), False)\n\n        # Read response\n        try:\n            return await self.parse_response(connection, args[0], **kwargs)\n        finally:\n            # Release connection\n            self._free.append(connection)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _property_resolver(arg):\n    \"\"\"\n    When arg is convertible to float, behave like operator.itemgetter(arg)\n    Otherwise, behave like Variable(arg).resolve\n\n    >>> _property_resolver(1)('abc')\n    'b'\n    >>> _property_resolver('1')('abc')\n    Traceback (most recent call last):\n    ...\n    TypeError: string indices must be integers\n    >>> class Foo:\n    ...     a = 42\n    ...     b = 3.14\n    ...     c = 'Hey!'\n    >>> _property_resolver('b')(Foo())\n    3.14\n    \"\"\"\n    try:\n        float(arg)\n    except ValueError:\n        return Variable(arg).resolve\n    else:\n        return itemgetter(arg)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def login():\n    from flask_login import current_user\n\n    redirect_url = request.args.get(\"redirect\", request.script_root + url_for(\"index\"))\n    permissions = sorted(\n        filter(\n            lambda x: x is not None and isinstance(x, OctoPrintPermission),\n            map(\n                lambda x: getattr(Permissions, x.strip()),\n                request.args.get(\"permissions\", \"\").split(\",\"),\n            ),\n        ),\n        key=lambda x: x.get_name(),\n    )\n    if not permissions:\n        permissions = [Permissions.STATUS, Permissions.SETTINGS_READ]\n\n    user_id = request.args.get(\"user_id\", \"\")\n\n    if (not user_id or current_user.get_id() == user_id) and has_permissions(\n        *permissions\n    ):\n        return redirect(redirect_url)\n\n    render_kwargs = {\n        \"theming\": [],\n        \"redirect_url\": redirect_url,\n        \"permission_names\": map(lambda x: x.get_name(), permissions),\n        \"user_id\": user_id,\n        \"logged_in\": not current_user.is_anonymous,\n    }\n\n    try:\n        additional_assets = _add_additional_assets(\"octoprint.theming.login\")\n\n        # backwards compatibility to forcelogin & loginui plugins which were replaced by this built-in dialog\n        additional_assets += _add_additional_assets(\"octoprint.plugin.forcelogin.theming\")\n        additional_assets += _add_additional_assets(\"octoprint.plugin.loginui.theming\")\n\n        render_kwargs.update({\"theming\": additional_assets})\n    except Exception:\n        _logger.exception(\"Error processing theming CSS, ignoring\")\n\n    return render_template(\"login.jinja2\", **render_kwargs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_initial_logout(self):\n        \"\"\"\n        Get the initial logout request.\n\n        Raises ValueError if no sessions in INIT_LOGOUT state.\n        \"\"\"\n        candidates = self._ss.get_user_sessions(self.user)\n\n        # FIXME: what does it mean if there are multiple in init? We\n        #        just return the first one for now. How do we know\n        #        it's the \"right\" one if multiple logouts are started\n        #        at the same time from different SPs?\n        for c in candidates:\n            key = c.keys()[0]\n            if int(c[key].get('logoutstate', 0)) == INIT_LOGOUT:\n                samlsession = self._data_to_samlsession(key, c[key])\n                return samlsession\n        raise ValueError()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def generate_jsrun_rankfile(settings, path=None):\n    \"\"\"\n    Generates rankfile to use with jsrun.\n    It splits the cores among the processes, which leads to best performance according to experiments.\n\n    Args:\n        settings: Settings for running jsrun.\n                  Note: settings.num_proc and settings.hosts must not be None.\n        path: Optional path of the rankfile.\n              Note: this file will be overwritten.\n    \"\"\"\n    cpu_per_gpu = (lsf.LSFUtils.get_num_cores() * lsf.LSFUtils.get_num_threads()) // lsf.LSFUtils.get_num_gpus()\n    host_list = (x.split(':') for x in settings.hosts.split(','))\n\n    # Verify and truncate host list if necessary\n    validated_list = []\n    remaining_slots = settings.num_proc\n    for host, slots in host_list:\n        slots = int(slots)\n        if slots > lsf.LSFUtils.get_num_gpus():\n            raise ValueError('Invalid host input, slot count for host \\'{host}:{slots}\\' is greater '\n                             'than number of GPUs per host \\'{gpus}\\'.'.format(\n                host=host, slots=slots, gpus=lsf.LSFUtils.get_num_gpus()))\n        needed_slots = min(slots, remaining_slots)\n        validated_list.append((host, needed_slots))\n        remaining_slots -= needed_slots\n        if remaining_slots == 0:\n            break\n    if remaining_slots != 0:\n        raise ValueError('Not enough slots on the hosts to fulfill the {slots} requested.'.format(\n            slots=settings.num_proc))\n\n    # Generate rankfile\n    path = tempfile.mktemp() if path is None else path\n    with open(path, 'w') as tmp:\n        tmp.write('overlapping_rs: allow\\n')\n        tmp.write('cpu_index_using: logical\\n')\n        rank = 0\n        for host, slots in validated_list:\n            cpu_val = 0\n            tmp.write('\\n')\n            for s in range(slots):\n                tmp.write('rank: {rank}: {{ hostname: {host}; cpu: {{{scpu}-{ecpu}}} ; gpu: * ; mem: * }}\\n'.format(\n                    rank=rank,\n                    host=host,\n                    scpu=cpu_val,\n                    ecpu=cpu_val + cpu_per_gpu - 1\n                ))\n                rank += 1\n                cpu_val += cpu_per_gpu\n    return path\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def lookup_internal_function(self, method_name, args_ir, ast_source):\n        # TODO is this the right module for me?\n        \"\"\"\n        Using a list of args, find the internal method to use, and\n        the kwargs which need to be filled in by the compiler\n        \"\"\"\n\n        sig = self.sigs[\"self\"].get(method_name, None)\n\n        def _check(cond, s=\"Unreachable\"):\n            if not cond:\n                raise CompilerPanic(s)\n\n        # these should have been caught during type checking; sanity check\n        _check(sig is not None)\n        _check(sig.internal)\n        _check(len(sig.base_args) <= len(args_ir) <= len(sig.args))\n        # more sanity check, that the types match\n        # _check(all(l.typ == r.typ for (l, r) in zip(args_ir, sig.args))\n\n        num_provided_kwargs = len(args_ir) - len(sig.base_args)\n        num_kwargs = len(sig.default_args)\n        kwargs_needed = num_kwargs - num_provided_kwargs\n\n        kw_vals = list(sig.default_values.values())[:kwargs_needed]\n\n        return sig, kw_vals\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, hs: \"HomeServer\"):\n        super().__init__(hs)\n\n        self.hs = hs\n\n        self.store = hs.get_datastore()\n        self.storage = hs.get_storage()\n        self.state_store = self.storage.state\n        self.federation_client = hs.get_federation_client()\n        self.state_handler = hs.get_state_handler()\n        self._state_resolution_handler = hs.get_state_resolution_handler()\n        self.server_name = hs.hostname\n        self.keyring = hs.get_keyring()\n        self.action_generator = hs.get_action_generator()\n        self.is_mine_id = hs.is_mine_id\n        self.spam_checker = hs.get_spam_checker()\n        self.event_creation_handler = hs.get_event_creation_handler()\n        self._message_handler = hs.get_message_handler()\n        self._server_notices_mxid = hs.config.server_notices_mxid\n        self.config = hs.config\n        self.http_client = hs.get_simple_http_client()\n        self._instance_name = hs.get_instance_name()\n        self._replication = hs.get_replication_data_handler()\n\n        self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)\n        self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(\n            hs\n        )\n\n        if hs.config.worker_app:\n            self._user_device_resync = ReplicationUserDevicesResyncRestServlet.make_client(\n                hs\n            )\n            self._maybe_store_room_on_outlier_membership = ReplicationStoreRoomOnOutlierMembershipRestServlet.make_client(\n                hs\n            )\n        else:\n            self._device_list_updater = hs.get_device_handler().device_list_updater\n            self._maybe_store_room_on_outlier_membership = (\n                self.store.maybe_store_room_on_outlier_membership\n            )\n\n        # When joining a room we need to queue any events for that room up.\n        # For each room, a list of (pdu, origin) tuples.\n        self.room_queues = {}  # type: Dict[str, List[Tuple[EventBase, str]]]\n        self._room_pdu_linearizer = Linearizer(\"fed_room_pdu\")\n\n        self.third_party_event_rules = hs.get_third_party_event_rules()\n\n        self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& val = ctx->input(0);\n    int64 id = ctx->session_state()->GetNewId();\n    TensorStore::TensorAndKey tk{val, id, requested_device()};\n    OP_REQUIRES_OK(ctx, ctx->tensor_store()->AddTensor(name(), tk));\n\n    Tensor* handle = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({}), &handle));\n    if (ctx->expected_output_dtype(0) == DT_RESOURCE) {\n      ResourceHandle resource_handle = MakeResourceHandle<Tensor>(\n          ctx, SessionState::kTensorHandleResourceTypeName,\n          tk.GetHandle(name()));\n      resource_handle.set_maybe_type_name(\n          SessionState::kTensorHandleResourceTypeName);\n      handle->scalar<ResourceHandle>()() = resource_handle;\n    } else {\n      // Legacy behavior in V1.\n      handle->flat<tstring>().setConstant(tk.GetHandle(name()));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def writeDataFile(self, filename, text, content_type, subdir=None):\n        \"\"\" See IExportContext.\n        \"\"\"\n        if subdir is not None:\n            filename = '/'.join((subdir, filename))\n\n        sep = filename.rfind('/')\n        if sep != -1:\n            subdir = filename[:sep]\n            filename = filename[sep+1:]\n\n        if six.PY2 and isinstance(text, six.text_type):\n            encoding = self.getEncoding() or 'utf-8'\n            text = text.encode(encoding)\n\n        folder = self._ensureSnapshotsFolder(subdir)\n\n        # MISSING: switch on content_type\n        ob = self._createObjectByType(filename, text, content_type)\n        folder._setObject(str(filename), ob)  # No Unicode IDs!\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def raise_standardErrorMessage(\n        self, client=None, REQUEST={},\n        error_type=None, error_value=None, tb=None,\n        error_tb=None, error_message='',\n        tagSearch=re.compile(r'[a-zA-Z]>').search,\n        error_log_url=''):\n\n        try:\n            if error_type  is None: error_type =sys.exc_info()[0]\n            if error_value is None: error_value=sys.exc_info()[1]\n\n            # allow for a few different traceback options\n            if tb is None and error_tb is None:\n                tb=sys.exc_info()[2]\n            if type(tb) is not str and (error_tb is None):\n                error_tb = pretty_tb(error_type, error_value, tb)\n            elif type(tb) is str and not error_tb:\n                error_tb = tb\n\n            if hasattr(self, '_v_eek'):\n                # Stop if there is recursion.\n                raise error_type, error_value, tb\n            self._v_eek = 1\n\n            if hasattr(error_type, '__name__'):\n                error_name = error_type.__name__\n            else:\n                error_name = 'Unknown'\n\n            if not error_message:\n                try:\n                    s = ustr(error_value)\n                except:\n                    s = error_value\n                try:\n                    match = tagSearch(s)\n                except TypeError:\n                    match = None\n                if match is not None:\n                    error_message=error_value\n\n            if client is None:\n                client = self\n\n            if not REQUEST:\n                REQUEST = aq_acquire(self, 'REQUEST')\n\n            try:\n                s = aq_acquire(client, 'standard_error_message')\n\n                # For backward compatibility, we pass 'error_name' as\n                # 'error_type' here as historically this has always\n                # been a string.\n                kwargs = {'error_type': error_name,\n                          'error_value': error_value,\n                          'error_tb': error_tb,\n                          'error_traceback': error_tb,\n                          'error_message': error_message,\n                          'error_log_url': error_log_url}\n\n                if getattr(aq_base(s), 'isDocTemp', 0):\n                    v = s(client, REQUEST, **kwargs)\n                elif callable(s):\n                    v = s(**kwargs)\n                else:\n                    v = HTML.__call__(s, client, REQUEST, **kwargs)\n            except:\n                logger.error(\n                    'Exception while rendering an error message',\n                    exc_info=True\n                    )\n                try:\n                    strv = repr(error_value) # quotes tainted strings\n                except:\n                    strv = ('<unprintable %s object>' % \n                            str(type(error_value).__name__))\n                v = strv + (\n                    (\" (Also, the following error occurred while attempting \"\n                     \"to render the standard error message, please see the \"\n                     \"event log for full details: %s)\")%(\n                    html_quote(sys.exc_info()[1]),\n                    ))\n\n            # If we've been asked to handle errors, just return the rendered\n            # exception and let the ZPublisher Exception Hook deal with it.\n            return error_type, v, tb\n        finally:\n            if hasattr(self, '_v_eek'): del self._v_eek\n            tb = None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def aws_kms_master_key_provider(discovery=True, **kwargs):\n    # type: (bool, **List[Union[Text, str]]) -> Union[DiscoveryAwsKmsMasterKeyProvider, StrictAwsKmsMasterKeyProvider]\n    \"\"\"Apply post-processing to transform ``KMSMasterKeyProvider``-specific values from CLI\n    arguments to valid ``KMSMasterKeyProvider`` parameters, then call ``KMSMasterKeyprovider``\n    with those parameters.\n\n    :param bool discovery: Return a DiscoveryAwsKmsMasterKeyProvider\n    :param dict kwargs: Named parameters collected from CLI arguments as prepared\n        in aws_encryption_sdk_cli.internal.master_key_parsing._parse_master_key_providers_from_args\n    :rtype: aws_encryption_sdk.key_providers.kms.BaseKMSMasterKeyProvider\n    \"\"\"\n    kwargs = copy.deepcopy(kwargs)\n    try:\n        profile_names = kwargs.pop(\"profile\")\n        if len(profile_names) != 1:\n            raise BadUserArgumentError(\n                \"Only one profile may be specified per master key provider configuration. {} provided.\".format(\n                    len(profile_names)\n                )\n            )\n        profile_name = profile_names[0]  # type: Optional[Text]\n    except KeyError:\n        profile_name = None\n\n    botocore_session = botocore.session.Session(profile=profile_name)\n    botocore_session.user_agent_extra = USER_AGENT_SUFFIX\n    kwargs[\"botocore_session\"] = botocore_session\n\n    try:\n        region_name = kwargs.pop(\"region\")\n        if len(region_name) != 1:\n            raise BadUserArgumentError(\n                \"Only one region may be specified per master key provider configuration. {} provided.\".format(\n                    len(region_name)\n                )\n            )\n        kwargs[\"region_names\"] = region_name\n    except KeyError:\n        pass\n    if discovery:\n        return DiscoveryAwsKmsMasterKeyProvider(**kwargs)\n    return StrictAwsKmsMasterKeyProvider(**kwargs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __new__(cls, obj, *args, **kwargs):\n        # In our usage we should only receive unicode strings.\n        # This conditional and conversion exists to sanity check the values\n        # we're given but we may want to take it out for testing and sanitize\n        # our input instead.\n        if isinstance(obj, string_types):\n            obj = to_text(obj, errors='surrogate_or_strict')\n            return AnsibleUnsafeText(obj)\n        return obj\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def check_db_or_user_exists(db_name, db_user, config):\n    with _create_pg_connection(config) as con:\n        with con.cursor() as cur:\n            cur.execute(\"SELECT 1 FROM pg_database WHERE datname='{}';\".format(db_name))\n            db_exists = cur.fetchone() is not None\n            cur.execute(\"SELECT 1 FROM pg_roles WHERE rolname='{}';\".format(db_user))\n            user = cur.fetchone()\n            user_exists = user is not None\n            return db_exists or user_exists\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void ReshapeSparseTensor(OpKernelContext *context,\n                         const Tensor &input_indices_in,\n                         const Tensor &input_shape_in,\n                         const Tensor &target_shape_in, int output_indices_idx,\n                         int output_shape_idx) {\n  OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_indices_in.shape()),\n              errors::InvalidArgument(\n                  \"Input indices should be a matrix but received shape \",\n                  input_indices_in.shape().DebugString()));\n  OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape_in.shape()),\n              errors::InvalidArgument(\n                  \"Input shape should be a vector but received shape \",\n                  input_shape_in.shape().DebugString()));\n  OP_REQUIRES(context, TensorShapeUtils::IsVector(target_shape_in.shape()),\n              errors::InvalidArgument(\n                  \"Target shape should be a vector but received shape \",\n                  target_shape_in.shape().DebugString()));\n\n  const int64_t output_rank = target_shape_in.NumElements();\n  const TensorShape input_shape(input_shape_in.vec<int64>());\n  const int64_t dense_size = input_shape.num_elements();\n  const int64_t nnz = input_indices_in.shape().dim_size(0);\n\n  // Compute the output shape. Determine product of specified dimensions, and\n  // find the index of the unspecified one.\n  TensorShape output_shape;\n  int64_t product = 1;\n  int unknown_index = -1;\n  auto target_shape = target_shape_in.vec<int64>();\n  for (int d = 0; d < output_rank; ++d) {\n    const int64_t size = target_shape(d);\n    if (size == -1) {\n      OP_REQUIRES(\n          context, unknown_index == -1,\n          errors::InvalidArgument(\"only one output dimension may be -1, \"\n                                  \"not both \",\n                                  unknown_index, \" and \", d));\n      unknown_index = d;\n      output_shape.AddDim(1);\n    } else {\n      OP_REQUIRES(context, size >= 0,\n                  errors::InvalidArgument(\"size \", d,\n                                          \" must be non-negative, not \", size));\n      product *= size;\n      output_shape.AddDim(size);\n    }\n  }\n  if (unknown_index != -1) {\n    OP_REQUIRES(\n        context, product > 0,\n        errors::InvalidArgument(\"reshape cannot infer the missing \"\n                                \"input size for an empty tensor unless all \"\n                                \"specified input sizes are non-zero\"));\n    const int64_t missing = dense_size / product;\n    OP_REQUIRES(\n        context, product * missing == dense_size,\n        errors::InvalidArgument(\n            \"Input to reshape is a SparseTensor with \", dense_size,\n            \" dense values, but the requested shape requires a multiple of \",\n            product, \". input_shape=\", input_shape.DebugString(),\n            \" output_shape=\", output_shape.DebugString()));\n    output_shape.set_dim(unknown_index, missing);\n  }\n\n  OP_REQUIRES(\n      context, output_shape.num_elements() == dense_size,\n      errors::InvalidArgument(\"Input to reshape is a tensor with \", dense_size,\n                              \" dense values, but the requested shape has \",\n                              output_shape.num_elements(),\n                              \". input_shape=\", input_shape.DebugString(),\n                              \" output_shape=\", output_shape.DebugString()));\n\n  // Optimize for reshaping to the same shape.\n  if (input_shape == output_shape) {\n    context->set_output(output_indices_idx, input_indices_in);\n    context->set_output(output_shape_idx, input_shape_in);\n    return;\n  }\n\n  Tensor *result_shape = nullptr;\n  OP_REQUIRES_OK(context, context->allocate_output(output_shape_idx,\n                                                   TensorShape({output_rank}),\n                                                   &result_shape));\n  auto output_shape_vec = result_shape->vec<int64>();\n  for (int j = 0; j < output_shape.dims(); ++j) {\n    output_shape_vec(j) = output_shape.dim_size(j);\n  }\n\n  Tensor *result_indices = nullptr;\n  OP_REQUIRES_OK(context,\n                 context->allocate_output(output_indices_idx,\n                                          TensorShape({nnz, output_rank}),\n                                          &result_indices));\n  if (nnz > 0) {\n    OP_REQUIRES_OK(context, functor::ReshapeSparseTensorFunctor<Device>()(\n                                context, input_shape, output_shape,\n                                input_indices_in.matrix<int64>(),\n                                result_indices->matrix<int64>()));\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def send(\n        self,\n        request_params: SaaSRequestParams,\n        ignore_errors: Optional[Union[bool, List[int]]] = False,\n    ) -> Response:\n        \"\"\"\n        Builds and executes an authenticated request.\n        Optionally ignores:\n          - all non-2xx/3xx responses if ignore_errors is set to True\n          - no non-2xx/3xx repsones if ignore_errors is set to False\n          - specific non-2xx/3xx responses if ignore_errors is set to a list of status codes\n        \"\"\"\n        rate_limit_requests = self.build_rate_limit_requests()\n        RateLimiter().limit(rate_limit_requests)\n\n        prepared_request: PreparedRequest = self.get_authenticated_request(\n            request_params\n        )\n        response = self.session.send(prepared_request)\n\n        log_request_and_response_for_debugging(\n            prepared_request, response\n        )  # Dev mode only\n\n        if not response.ok:\n            if self._should_ignore_error(\n                status_code=response.status_code,\n                errors_to_ignore=ignore_errors,\n            ):\n                logger.info(\n                    \"Ignoring errors on response with status code {} as configured.\",\n                    response.status_code,\n                )\n                return response\n            raise RequestFailureResponseException(response=response)\n        return response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def render_and_merge_yaml(root, template_name, context_name):\n    \"\"\"\n    Renders a Jinja2-templated YAML file based on a YAML context file, merge them, and return\n    result as a dictionary.\n\n    :param root: Root directory of the YAML files\n    :param template_name: Name of the template file\n    :param context_name: Name of the context file\n    :return: Data in yaml file as dictionary\n    \"\"\"\n    import jinja2\n\n    template_path = os.path.join(root, template_name)\n    context_path = os.path.join(root, context_name)\n\n    for path in (template_path, context_path):\n        if not pathlib.Path(path).is_file():\n            raise MissingConfigException(f\"Yaml file '{path}' does not exist.\")\n\n    j2_env = jinja2.Environment(\n        loader=jinja2.FileSystemLoader(root, encoding=ENCODING),\n        undefined=jinja2.StrictUndefined,\n        line_comment_prefix=\"#\",\n    )\n\n    def from_json(input_var):\n        with open(input_var, encoding=\"utf-8\") as f:\n            return json.load(f)\n\n    j2_env.filters[\"from_json\"] = from_json\n    # Compute final source of context file (e.g. my-profile.yml), applying Jinja filters\n    # like from_json as needed to load context information from files, then load into a dict\n    context_source = j2_env.get_template(context_name).render({})\n    context_dict = yaml.load(context_source, Loader=UniqueKeyLoader) or {}\n\n    # Substitute parameters from context dict into template\n    source = j2_env.get_template(template_name).render(context_dict)\n    rendered_template_dict = yaml.load(source, Loader=UniqueKeyLoader)\n    return merge_dicts(rendered_template_dict, context_dict)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_path_is_safe(path):\n    \"\"\"\n    Validates that the specified path is safe to join with a trusted prefix. This is a security\n    measure to prevent path traversal attacks.\n    \"\"\"\n    if (\n        any((s in path) for s in _OS_ALT_SEPS)\n        or \"..\" in path.split(posixpath.sep)\n        or posixpath.isabs(path)\n    ):\n        raise MlflowException(f\"Invalid path: {path}\", error_code=INVALID_PARAMETER_VALUE)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def _make_well_known_request(\n        self, server_name: bytes, retry: bool\n    ) -> Tuple[IResponse, bytes]:\n        \"\"\"Make the well known request.\n\n        This will retry the request if requested and it fails (with unable\n        to connect or receives a 5xx error).\n\n        Args:\n            server_name: name of the server, from the requested url\n            retry: Whether to retry the request if it fails.\n\n        Returns:\n            Returns the response object and body. Response may be a non-200 response.\n        \"\"\"\n        uri = b\"https://%s/.well-known/matrix/server\" % (server_name,)\n        uri_str = uri.decode(\"ascii\")\n\n        headers = {\n            b\"User-Agent\": [self.user_agent],\n        }\n\n        i = 0\n        while True:\n            i += 1\n\n            logger.info(\"Fetching %s\", uri_str)\n            try:\n                response = await make_deferred_yieldable(\n                    self._well_known_agent.request(\n                        b\"GET\", uri, headers=Headers(headers)\n                    )\n                )\n                body = await make_deferred_yieldable(readBody(response))\n\n                if 500 <= response.code < 600:\n                    raise Exception(\"Non-200 response %s\" % (response.code,))\n\n                return response, body\n            except defer.CancelledError:\n                # Bail if we've been cancelled\n                raise\n            except Exception as e:\n                if not retry or i >= WELL_KNOWN_RETRY_ATTEMPTS:\n                    logger.info(\"Error fetching %s: %s\", uri_str, e)\n                    raise _FetchWellKnownFailure(temporary=True)\n\n                logger.info(\"Error fetching %s: %s. Retrying\", uri_str, e)\n\n            # Sleep briefly in the hopes that they come back up\n            await self._clock.sleep(0.5)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def send_event(self, url, authtoken, state, result, runtime):\n        if result._task_fields['args'].get('_ansible_check_mode') is True:\n            self.ansible_check_mode = True\n\n        if result._task_fields['args'].get('_ansible_version'):\n            self.ansible_version = \\\n                result._task_fields['args'].get('_ansible_version')\n\n        if result._task._role:\n            ansible_role = str(result._task._role)\n        else:\n            ansible_role = None\n\n        data = {}\n        data['uuid'] = result._task._uuid\n        data['session'] = self.session\n        data['status'] = state\n        data['timestamp'] = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S '\n                                                       '+0000')\n        data['host'] = self.host\n        data['ip_address'] = self.ip_address\n        data['user'] = self.user\n        data['runtime'] = runtime\n        data['ansible_version'] = self.ansible_version\n        data['ansible_check_mode'] = self.ansible_check_mode\n        data['ansible_host'] = result._host.name\n        data['ansible_playbook'] = self.ansible_playbook\n        data['ansible_role'] = ansible_role\n        data['ansible_task'] = result._task_fields\n        data['ansible_result'] = result._result\n\n        # This wraps the json payload in and outer json event needed by Splunk\n        jsondata = json.dumps(data, cls=AnsibleJSONEncoder, sort_keys=True)\n        jsondata = '{\"event\":' + jsondata + \"}\"\n\n        open_url(\n            url,\n            jsondata,\n            headers={\n                'Content-type': 'application/json',\n                'Authorization': 'Splunk ' + authtoken\n            },\n            method='POST'\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __iter__(self):\n        vmatch = self.vmatch\n        vdict = self.vdict\n\n        with self.source.open('rb') as xmlf:\n\n            tree = etree.parse(xmlf)\n            if not hasattr(tree, 'iterfind'):\n                # Python 2.6 compatibility\n                tree.iterfind = tree.findall\n\n            if vmatch is not None:\n                # simple case, all value paths are the same\n                for rowelm in tree.iterfind(self.rmatch):\n                    if self.attr is None:\n                        getv = attrgetter('text')\n                    else:\n                        getv = lambda e: e.get(self.attr)\n                    if isinstance(vmatch, string_types):\n                        # match only one path\n                        velms = rowelm.findall(vmatch)\n                    else:\n                        # match multiple paths\n                        velms = itertools.chain(*[rowelm.findall(enm)\n                                                  for enm in vmatch])\n                    yield tuple(getv(velm)\n                                for velm in velms)\n\n            else:\n                # difficult case, deal with different paths for each field\n\n                # determine output header\n                flds = tuple(sorted(map(text_type, vdict.keys())))\n                yield flds\n\n                # setup value getters\n                vmatches = dict()\n                vgetters = dict()\n                for f in flds:\n                    vmatch = self.vdict[f]\n                    if isinstance(vmatch, string_types):\n                        # match element path\n                        vmatches[f] = vmatch\n                        vgetters[f] = element_text_getter(self.missing)\n                    else:\n                        # match element path and attribute name\n                        vmatches[f] = vmatch[0]\n                        attr = vmatch[1]\n                        vgetters[f] = attribute_text_getter(attr, self.missing)\n\n                # determine data rows\n                for rowelm in tree.iterfind(self.rmatch):\n                    yield tuple(vgetters[f](rowelm.findall(vmatches[f]))\n                                for f in flds)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def lu_unpack(x, y, unpack_ludata=True, unpack_pivots=True, name=None):\n    r\"\"\"\n    Unpack L U and P to single matrix tensor .\n    unpack L and U matrix from LU, unpack permutation matrix P from Pivtos .\n\n    P mat can be get by pivots:\n\n    .. code-block:: text\n\n        ones = eye(rows) #eye matrix of rank rows\n        for i in range(cols):\n            swap(ones[i], ones[pivots[i]])\n\n\n    Args:\n        x (Tensor): The LU tensor get from paddle.linalg.lu, which is combined by L and U.\n\n        y (Tensor): Pivots get from paddle.linalg.lu.\n\n        unpack_ludata (bool,optional): whether to unpack L and U from x. Default: True.\n\n        unpack_pivots (bool, optional): whether to unpack permutation matrix P from Pivtos. Default: True.\n\n        name (str, optional): Name for the operation (optional, default is None).\n            For more information, please refer to :ref:`api_guide_Name`.\n\n    Returns:\n        P (Tensor), Permutation matrix P of lu factorization.\n\n        L (Tensor), The lower triangular matrix tensor of lu factorization.\n\n        U (Tensor), The upper triangular matrix tensor of lu factorization.\n\n\n    Examples:\n        .. code-block:: python\n\n            import paddle\n\n            x = paddle.to_tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]).astype('float64')\n            lu,p,info = paddle.linalg.lu(x, get_infos=True)\n\n            # >>> lu:\n            # Tensor(shape=[3, 2], dtype=float64, place=CUDAPlace(0), stop_gradient=True,\n            #    [[5.        , 6.        ],\n            #        [0.20000000, 0.80000000],\n            #        [0.60000000, 0.50000000]])\n            # >>> p\n            # Tensor(shape=[2], dtype=int32, place=CUDAPlace(0), stop_gradient=True,\n            #    [3, 3])\n            # >>> info\n            # Tensor(shape=[], dtype=int32, place=CUDAPlace(0), stop_gradient=True,\n            #    0)\n\n            P,L,U = paddle.linalg.lu_unpack(lu,p)\n\n            # >>> P\n            # (Tensor(shape=[3, 3], dtype=float64, place=CUDAPlace(0), stop_gradient=True,\n            # [[0., 1., 0.],\n            # [0., 0., 1.],\n            # [1., 0., 0.]]),\n            # >>> L\n            # Tensor(shape=[3, 2], dtype=float64, place=CUDAPlace(0), stop_gradient=True,\n            # [[1.        , 0.        ],\n            # [0.20000000, 1.        ],\n            # [0.60000000, 0.50000000]]),\n            # >>> U\n            # Tensor(shape=[2, 2], dtype=float64, place=CUDAPlace(0), stop_gradient=True,\n            # [[5.        , 6.        ],\n            # [0.        , 0.80000000]]))\n\n            # one can verify : X = P @ L @ U ;\n    \"\"\"\n\n    if in_dynamic_mode():\n        P, L, U = _C_ops.lu_unpack(x, y, unpack_ludata, unpack_pivots)\n        return P, L, U\n    else:\n        check_variable_and_dtype(\n            x, 'dtype', ['float32', 'float64'], 'lu_unpack'\n        )\n        helper = LayerHelper('lu_unpack', **locals())\n        p = helper.create_variable_for_type_inference(dtype=x.dtype)\n        l = helper.create_variable_for_type_inference(dtype=x.dtype)\n        u = helper.create_variable_for_type_inference(dtype=x.dtype)\n\n        attrs = {}\n        attrs['unpack_ludata'] = unpack_ludata\n        attrs['unpack_pivots'] = unpack_pivots\n        helper.append_op(\n            type='lu_unpack',\n            inputs={'X': x, 'Pivots': y},\n            outputs={'Pmat': p, 'L': l, 'U': u},\n            attrs=attrs,\n        )\n        return p, l, u\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_obj_absolute_path(obj_path):\n    return os.path.join(DATAROOT, obj_path)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def put_column_description() -> Response:\n\n    @action_logging\n    def _log_put_column_description(*, table_key: str, column_name: str, description: str, source: str) -> None:\n        pass  # pragma: no cover\n\n    try:\n        args = request.get_json()\n\n        table_key = get_query_param(args, 'key')\n        table_endpoint = _get_table_endpoint()\n\n        column_name = get_query_param(args, 'column_name')\n        description = get_query_param(args, 'description')\n\n        src = get_query_param(args, 'source')\n\n        url = '{0}/{1}/column/{2}/description'.format(table_endpoint, table_key, column_name)\n        _log_put_column_description(table_key=table_key, column_name=column_name, description=description, source=src)\n\n        response = request_metadata(url=url, method='PUT', data=json.dumps({'description': description}))\n        status_code = response.status_code\n\n        if status_code == HTTPStatus.OK:\n            message = 'Success'\n        else:\n            message = 'Update column description failed'\n\n        payload = jsonify({'msg': message})\n        return make_response(payload, status_code)\n    except Exception as e:\n        payload = jsonify({'msg': 'Encountered exception: ' + str(e)})\n        return make_response(payload, HTTPStatus.INTERNAL_SERVER_ERROR)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_path_is_safe(path):\n    \"\"\"\n    Validates that the specified path is safe to join with a trusted prefix. This is a security\n    measure to prevent path traversal attacks.\n    A valid path should:\n        not contain separators other than '/'\n        not contain .. to navigate to parent dir in path\n        not be an absolute path\n    \"\"\"\n    from mlflow.utils.file_utils import local_file_uri_to_path\n\n    exc = MlflowException(f\"Invalid path: {path}\", error_code=INVALID_PARAMETER_VALUE)\n    if any((s in path) for s in (\"#\", \"%23\")):\n        raise exc\n\n    if is_file_uri(path):\n        path = local_file_uri_to_path(path)\n    if (\n        any((s in path) for s in _OS_ALT_SEPS)\n        or \"..\" in path.split(\"/\")\n        or pathlib.PureWindowsPath(path).is_absolute()\n        or pathlib.PurePosixPath(path).is_absolute()\n        or (is_windows() and len(path) >= 2 and path[1] == \":\")\n    ):\n        raise exc\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def file(path):\n    path = secure_filename(path)\n    if app.interface.encrypt and isinstance(app.interface.examples, str) and path.startswith(app.interface.examples):\n        with open(os.path.join(app.cwd, path), \"rb\") as encrypted_file:\n            encrypted_data = encrypted_file.read()\n        file_data = encryptor.decrypt(\n            app.interface.encryption_key, encrypted_data)\n        return send_file(io.BytesIO(file_data), attachment_filename=os.path.basename(path))\n    else:\n        return send_file(os.path.join(app.cwd, path))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def handle_get(self) -> bool:\n        if self.do_log:\n            logmsg = \"%-4s %s @%s\" % (self.mode, self.req, self.uname)\n\n            if \"range\" in self.headers:\n                try:\n                    rval = self.headers[\"range\"].split(\"=\", 1)[1]\n                except:\n                    rval = self.headers[\"range\"]\n\n                logmsg += \" [\\033[36m\" + rval + \"\\033[0m]\"\n\n            self.log(logmsg)\n\n        # \"embedded\" resources\n        if self.vpath.startswith(\".cpr\"):\n            if self.vpath.startswith(\".cpr/ico/\"):\n                return self.tx_ico(self.vpath.split(\"/\")[-1], exact=True)\n\n            if self.vpath.startswith(\".cpr/ssdp\"):\n                return self.conn.hsrv.ssdp.reply(self)\n\n            if self.vpath.startswith(\".cpr/dd/\") and self.args.mpmc:\n                if self.args.mpmc == \".\":\n                    raise Pebkac(404)\n\n                loc = self.args.mpmc.rstrip(\"/\") + self.vpath[self.vpath.rfind(\"/\") :]\n                h = {\"Location\": loc, \"Cache-Control\": \"max-age=39\"}\n                self.reply(b\"\", 301, headers=h)\n                return True\n\n            static_path = os.path.join(self.E.mod, \"web/\", self.vpath[5:])\n            return self.tx_file(static_path)\n\n        if \"cf_challenge\" in self.uparam:\n            self.reply(self.j2s(\"cf\").encode(\"utf-8\", \"replace\"))\n            return True\n\n        if not self.can_read and not self.can_write and not self.can_get:\n            t = \"@{} has no access to [{}]\"\n            self.log(t.format(self.uname, self.vpath))\n\n            if \"on403\" in self.vn.flags:\n                ret = self.on40x(self.vn.flags[\"on403\"], self.vn, self.rem)\n                if ret == \"true\":\n                    return True\n                elif ret == \"false\":\n                    return False\n                elif ret == \"allow\":\n                    self.log(\"plugin override; access permitted\")\n                    self.can_read = self.can_write = self.can_move = True\n                    self.can_delete = self.can_get = self.can_upget = True\n                    self.can_admin = True\n                else:\n                    return self.tx_404(True)\n            else:\n                if self.vpath:\n                    return self.tx_404(True)\n\n                self.uparam[\"h\"] = \"\"\n\n        if \"tree\" in self.uparam:\n            return self.tx_tree()\n\n        if \"scan\" in self.uparam:\n            return self.scanvol()\n\n        if self.args.getmod:\n            if \"delete\" in self.uparam:\n                return self.handle_rm([])\n\n            if \"move\" in self.uparam:\n                return self.handle_mv()\n\n        if not self.vpath:\n            if \"reload\" in self.uparam:\n                return self.handle_reload()\n\n            if \"stack\" in self.uparam:\n                return self.tx_stack()\n\n            if \"ups\" in self.uparam:\n                return self.tx_ups()\n\n            if \"k304\" in self.uparam:\n                return self.set_k304()\n\n            if \"setck\" in self.uparam:\n                return self.setck()\n\n            if \"reset\" in self.uparam:\n                return self.set_cfg_reset()\n\n            if \"hc\" in self.uparam:\n                return self.tx_svcs()\n\n        if \"h\" in self.uparam:\n            return self.tx_mounts()\n\n        # conditional redirect to single volumes\n        if self.vpath == \"\" and not self.ouparam:\n            nread = len(self.rvol)\n            nwrite = len(self.wvol)\n            if nread + nwrite == 1 or (self.rvol == self.wvol and nread == 1):\n                if nread == 1:\n                    vpath = self.rvol[0]\n                else:\n                    vpath = self.wvol[0]\n\n                if self.vpath != vpath:\n                    self.redirect(vpath, flavor=\"redirecting to\", use302=True)\n                    return True\n\n        return self.tx_browser()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _save(self, name, content):\n        full_path = self.path(name)\n\n        # Create any intermediate directories that do not exist.\n        directory = os.path.dirname(full_path)\n        try:\n            if self.directory_permissions_mode is not None:\n                # os.makedirs applies the global umask, so we reset it,\n                # for consistency with file_permissions_mode behavior.\n                old_umask = os.umask(0)\n                try:\n                    os.makedirs(directory, self.directory_permissions_mode, exist_ok=True)\n                finally:\n                    os.umask(old_umask)\n            else:\n                os.makedirs(directory, exist_ok=True)\n        except FileExistsError:\n            raise FileExistsError('%s exists and is not a directory.' % directory)\n\n        # There's a potential race condition between get_available_name and\n        # saving the file; it's possible that two threads might return the\n        # same name, at which point all sorts of fun happens. So we need to\n        # try to create the file, but if it already exists we have to go back\n        # to get_available_name() and try again.\n\n        while True:\n            try:\n                # This file has a file path that we can move.\n                if hasattr(content, 'temporary_file_path'):\n                    file_move_safe(content.temporary_file_path(), full_path)\n\n                # This is a normal uploadedfile that we can stream.\n                else:\n                    # The current umask value is masked out by os.open!\n                    fd = os.open(full_path, self.OS_OPEN_FLAGS, 0o666)\n                    _file = None\n                    try:\n                        locks.lock(fd, locks.LOCK_EX)\n                        for chunk in content.chunks():\n                            if _file is None:\n                                mode = 'wb' if isinstance(chunk, bytes) else 'wt'\n                                _file = os.fdopen(fd, mode)\n                            _file.write(chunk)\n                    finally:\n                        locks.unlock(fd)\n                        if _file is not None:\n                            _file.close()\n                        else:\n                            os.close(fd)\n            except FileExistsError:\n                # A new name is needed if the file exists.\n                name = self.get_available_name(name)\n                full_path = self.path(name)\n            else:\n                # OK, the file save worked. Break out of the loop.\n                break\n\n        if self.file_permissions_mode is not None:\n            os.chmod(full_path, self.file_permissions_mode)\n\n        # Store filenames with forward slashes, even on Windows.\n        return str(name).replace('\\\\', '/')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _add_widgets(self, package_name: str, hook_class: type, widgets: dict[str, Any]):\n        conn_type = hook_class.conn_type  # type: ignore\n        for field_identifier, field in widgets.items():\n            if field_identifier.startswith(\"extra__\"):\n                prefixed_field_name = field_identifier\n            else:\n                prefixed_field_name = f\"extra__{conn_type}__{field_identifier}\"\n            if prefixed_field_name in self._connection_form_widgets:\n                log.warning(\n                    \"The field %s from class %s has already been added by another provider. Ignoring it.\",\n                    field_identifier,\n                    hook_class.__name__,\n                )\n                # In case of inherited hooks this might be happening several times\n                continue\n            self._connection_form_widgets[prefixed_field_name] = ConnectionFormWidgetInfo(\n                hook_class.__name__, package_name, field, field_identifier\n            )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _getResourceForRequest(self, request):\n        \"\"\"(Internal) Get the appropriate resource for the given host.\n        \"\"\"\n        host = string.lower(request.getHeader('host'))\n        return self.hosts.get(host, error.NoResource())\n        "}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, fobj):\n        \"\"\"\n        fobj is a file-like object as an icns resource\n        \"\"\"\n        # signature : (start, length)\n        self.dct = dct = {}\n        self.fobj = fobj\n        sig, filesize = nextheader(fobj)\n        if sig != b'icns':\n            raise SyntaxError('not an icns file')\n        i = HEADERSIZE\n        while i < filesize:\n            sig, blocksize = nextheader(fobj)\n            i += HEADERSIZE\n            blocksize -= HEADERSIZE\n            dct[sig] = (i, blocksize)\n            fobj.seek(blocksize, 1)\n            i += blocksize\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static int expandrow2(UINT8* dest, const UINT8* src, int n, int z)\n{\n    UINT8 pixel, count;\n\n\n    for (;n > 0; n--)\n    {\n        pixel = src[1];\n        src+=2;\n        if (n == 1 && pixel != 0)\n            return n;\n        count = pixel & RLE_MAX_RUN;\n        if (!count)\n            return count;\n        if (pixel & RLE_COPY_FLAG) {\n            while(count--) {\n                memcpy(dest, src, 2);\n                src += 2;\n                dest += z * 2;\n            }\n        }\n        else {\n            while (count--) {\n                memcpy(dest, src, 2);\n                dest += z * 2;\n            }\n            src+=2;\n        }\n    }\n    return 0;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _on_ssl_errors(self, error):\n        self._has_ssl_errors = True\n\n        url = error.url()\n        log.webview.debug(\"Certificate error: {}\".format(error))\n\n        if error.is_overridable():\n            error.ignore = shared.ignore_certificate_errors(\n                url, [error], abort_on=[self.shutting_down, self.load_started])\n        else:\n            log.webview.error(\"Non-overridable certificate error: \"\n                              \"{}\".format(error))\n\n        log.webview.debug(\"ignore {}, URL {}, requested {}\".format(\n            error.ignore, url, self.url(requested=True)))\n\n        # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-56207\n        # We can't really know when to show an error page, as the error might\n        # have happened when loading some resource.\n        # However, self.url() is not available yet and the requested URL\n        # might not match the URL we get from the error - so we just apply a\n        # heuristic here.\n        if (not qtutils.version_check('5.9') and\n                not error.ignore and\n                url.matches(self.url(requested=True), QUrl.RemoveScheme)):\n            self._show_error_page(url, str(error))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor* reverse_index_map_t;\n    const Tensor* grad_values_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"reverse_index_map\", &reverse_index_map_t));\n    OP_REQUIRES_OK(context, context->input(\"grad_values\", &grad_values_t));\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(reverse_index_map_t->shape()),\n        errors::InvalidArgument(\"reverse_index_map must be a vector, saw: \",\n                                reverse_index_map_t->shape().DebugString()));\n\n    const auto reverse_index_map = reverse_index_map_t->vec<int64>();\n    const auto grad_values = grad_values_t->vec<T>();\n\n    const int64 N = reverse_index_map_t->shape().dim_size(0);\n    const int64 N_full = grad_values_t->shape().dim_size(0);\n\n    Tensor* d_values_t;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                \"d_values\", TensorShape({N}), &d_values_t));\n    auto d_values = d_values_t->vec<T>();\n    Tensor* d_default_value_t;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"d_default_value\", TensorShape({}),\n                                            &d_default_value_t));\n    T& d_default_value = d_default_value_t->scalar<T>()();\n    d_default_value = T();\n\n    Tensor visited_t;\n    OP_REQUIRES_OK(context, context->allocate_temp(\n                                DT_BOOL, TensorShape({N_full}), &visited_t));\n    auto visited = visited_t.vec<bool>();\n    visited.device(d) = visited.constant(false);\n\n    for (int i = 0; i < N; ++i) {\n      // Locate the index of the output of the forward prop associated\n      // with this location in the input of the forward prop.  Copy\n      // the gradient into it.  Mark it as visited.\n      d_values(i) = grad_values(reverse_index_map(i));\n      visited(reverse_index_map(i)) = true;\n    }\n    for (int j = 0; j < N_full; ++j) {\n      // The default value gradient gets the accumulated remainder of\n      // the backprop values (since the default value was used to fill\n      // in these slots in the forward calculation).\n      if (!visited(j)) {\n        d_default_value += grad_values(j);\n      }\n    }\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def diff_objects(old_instance, new_instance, fields):\n    \"\"\"\n    Diff two objects by examining the given fields and\n    return a string.\n    \"\"\"\n    full_diff = []\n\n    for field in fields:\n        field_diff = []\n        old_value = getattr(old_instance, field.attname)\n        new_value = getattr(new_instance, field.attname)\n        for line in difflib.unified_diff(\n            str(old_value).split(\"\\n\"),\n            str(new_value).split(\"\\n\"),\n            fromfile=field.attname,\n            tofile=field.attname,\n            lineterm=\"\",\n        ):\n            field_diff.append(line)\n        full_diff.extend(field_diff)\n\n    return \"\\n\".join(full_diff)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    core::RefCountPtr<CollectiveGroupResource> resource;\n    OP_REQUIRES_OK_ASYNC(c, LookupResource(c, HandleFromInput(c, 1), &resource),\n                         done);\n\n    Tensor group_assignment = c->input(2);\n\n    OP_REQUIRES_OK_ASYNC(\n        c,\n        FillCollectiveParams(col_params, group_assignment, REDUCTION_COLLECTIVE,\n                             resource.get()),\n        done);\n    col_params->instance.shape = c->input(0).shape();\n    col_params->merge_op = merge_op_.get();\n    col_params->final_op = final_op_.get();\n    VLOG(1) << \"CollectiveReduceV3 group_size \" << col_params->group.group_size\n            << \" group_key \" << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    // Allocate the output tensor, trying to reuse the input.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(c,\n                         c->forward_input_or_allocate_output(\n                             {0}, 0, col_params->instance.shape, &output),\n                         done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static RList *create_cache_bins(RBinFile *bf, RDyldCache *cache) {\n\tRList *bins = r_list_newf ((RListFree)free_bin);\n\tif (!bins) {\n\t\treturn NULL;\n\t}\n\n\tchar *target_libs = NULL;\n\tRList *target_lib_names = NULL;\n\tint *deps = NULL;\n\ttarget_libs = r_sys_getenv (\"R_DYLDCACHE_FILTER\");\n\tif (target_libs) {\n\t\ttarget_lib_names = r_str_split_list (target_libs, \":\", 0);\n\t\tif (!target_lib_names) {\n\t\t\tr_list_free (bins);\n\t\t\treturn NULL;\n\t\t}\n\t\tdeps = R_NEWS0 (int, cache->hdr->imagesCount);\n\t\tif (!deps) {\n\t\t\tr_list_free (bins);\n\t\t\tr_list_free (target_lib_names);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tut32 i;\n\tfor (i = 0; i < cache->n_hdr; i++) {\n\t\tcache_hdr_t *hdr = &cache->hdr[i];\n\t\tut64 hdr_offset = cache->hdr_offset[i];\n\t\tut32 maps_index = cache->maps_index[i];\n\t\tcache_img_t *img = read_cache_images (cache->buf, hdr, hdr_offset);\n\t\tif (!img) {\n\t\t\tgoto next;\n\t\t}\n\n\t\tut32 j;\n\t\tut16 *depArray = NULL;\n\t\tcache_imgxtr_t *extras = NULL;\n\t\tif (target_libs) {\n\t\t\tHtPU *path_to_idx = NULL;\n\t\t\tif (cache->accel) {\n\t\t\t\tdepArray = R_NEWS0 (ut16, cache->accel->depListCount);\n\t\t\t\tif (!depArray) {\n\t\t\t\t\tgoto next;\n\t\t\t\t}\n\n\t\t\t\tif (r_buf_fread_at (cache->buf, cache->accel->depListOffset, (ut8*) depArray, \"s\", cache->accel->depListCount) != cache->accel->depListCount * 2) {\n\t\t\t\t\tgoto next;\n\t\t\t\t}\n\n\t\t\t\textras = read_cache_imgextra (cache->buf, hdr, cache->accel);\n\t\t\t\tif (!extras) {\n\t\t\t\t\tgoto next;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tpath_to_idx = create_path_to_index (cache->buf, img, hdr);\n\t\t\t}\n\n\t\t\tfor (j = 0; j < hdr->imagesCount; j++) {\n\t\t\t\tbool printing = !deps[j];\n\t\t\t\tchar *lib_name = get_lib_name (cache->buf, &img[j]);\n\t\t\t\tif (!lib_name) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (strstr (lib_name, \"libobjc.A.dylib\")) {\n\t\t\t\t\tdeps[j]++;\n\t\t\t\t}\n\t\t\t\tif (!r_list_find (target_lib_names, lib_name, string_contains)) {\n\t\t\t\t\tR_FREE (lib_name);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (printing) {\n\t\t\t\t\teprintf (\"FILTER: %s\\n\", lib_name);\n\t\t\t\t}\n\t\t\t\tR_FREE (lib_name);\n\t\t\t\tdeps[j]++;\n\n\t\t\t\tif (extras && depArray) {\n\t\t\t\t\tut32 k;\n\t\t\t\t\tfor (k = extras[j].dependentsStartArrayIndex; depArray[k] != 0xffff; k++) {\n\t\t\t\t\t\tut16 dep_index = depArray[k] & 0x7fff;\n\t\t\t\t\t\tdeps[dep_index]++;\n\n\t\t\t\t\t\tchar *dep_name = get_lib_name (cache->buf, &img[dep_index]);\n\t\t\t\t\t\tif (!dep_name) {\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (printing) {\n\t\t\t\t\t\t\teprintf (\"-> %s\\n\", dep_name);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfree (dep_name);\n\t\t\t\t\t}\n\t\t\t\t} else if (path_to_idx) {\n\t\t\t\t\tcarve_deps_at_address (cache, img, path_to_idx, img[j].address, deps, printing);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tht_pu_free (path_to_idx);\n\t\t\tR_FREE (depArray);\n\t\t\tR_FREE (extras);\n\t\t}\n\n\t\tfor (j = 0; j < hdr->imagesCount; j++) {\n\t\t\tif (deps && !deps[j]) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tut64 pa = va2pa (img[j].address, hdr->mappingCount, &cache->maps[maps_index], cache->buf, 0, NULL, NULL);\n\t\t\tif (pa == UT64_MAX) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tut8 magicbytes[4];\n\t\t\tr_buf_read_at (cache->buf, pa, magicbytes, 4);\n\t\t\tint magic = r_read_le32 (magicbytes);\n\t\t\tswitch (magic) {\n\t\t\tcase MH_MAGIC_64:\n\t\t\t{\n\t\t\t\tchar file[256];\n\t\t\t\tRDyldBinImage *bin = R_NEW0 (RDyldBinImage);\n\t\t\t\tif (!bin) {\n\t\t\t\t\tgoto next;\n\t\t\t\t}\n\t\t\t\tbin->header_at = pa;\n\t\t\t\tbin->hdr_offset = hdr_offset;\n\t\t\t\tbin->symbols_off = resolve_symbols_off (cache, pa);\n\t\t\t\tbin->va = img[j].address;\n\t\t\t\tif (r_buf_read_at (cache->buf, img[j].pathFileOffset, (ut8*) &file, sizeof (file)) == sizeof (file)) {\n\t\t\t\t\tfile[255] = 0;\n\t\t\t\t\tchar *last_slash = strrchr (file, '/');\n\t\t\t\t\tif (last_slash && *last_slash) {\n\t\t\t\t\t\tif (last_slash > file) {\n\t\t\t\t\t\t\tchar *scan = last_slash - 1;\n\t\t\t\t\t\t\twhile (scan > file && *scan != '/') {\n\t\t\t\t\t\t\t\tscan--;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif (*scan == '/') {\n\t\t\t\t\t\t\t\tbin->file = strdup (scan + 1);\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tbin->file = strdup (last_slash + 1);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tbin->file = strdup (last_slash + 1);\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tbin->file = strdup (file);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tr_list_append (bins, bin);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdefault:\n\t\t\t\teprintf (\"Unknown sub-bin\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\nnext:\n\t\tR_FREE (depArray);\n\t\tR_FREE (extras);\n\t\tR_FREE (img);\n\t}\n\tif (r_list_empty (bins)) {\n\t\tr_list_free (bins);\n\t\tbins = NULL;\n\t}\n\tR_FREE (deps);\n\tR_FREE (target_libs);\n\tr_list_free (target_lib_names);\n\treturn bins;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def sensitive_config_values(self) -> Set[tuple[str, str]]:  # noqa: UP006\n        if self.configuration_description is None:\n            return (\n                _get_empty_set_for_configuration()\n            )  # we can't use set() here because set is defined below # \u00af\\_(\u30c4)_/\u00af\n        flattened = {\n            (s, k): item\n            for s, s_c in self.configuration_description.items()\n            for k, item in s_c.get(\"options\").items()  # type: ignore[union-attr]\n        }\n        sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}\n        depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}\n        depr_section = {\n            (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections\n        }\n        sensitive.update(depr_section, depr_option)\n        return sensitive\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def sanitize_file_name(self, file_name):\n        file_name = html.unescape(file_name)\n        # Cleanup Windows-style path separators.\n        file_name = file_name[file_name.rfind('\\\\') + 1:].strip()\n        return os.path.basename(file_name)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def extract(self, to_path):\n        members = self._archive.getmembers()\n        leading = self.has_leading_dir(x.name for x in members)\n        for member in members:\n            name = member.name\n            if leading:\n                name = self.split_leading_dir(name)[1]\n            filename = os.path.join(to_path, name)\n            if member.isdir():\n                if filename:\n                    os.makedirs(filename, exist_ok=True)\n            else:\n                try:\n                    extracted = self._archive.extractfile(member)\n                except (KeyError, AttributeError) as exc:\n                    # Some corrupt tar files seem to produce this\n                    # (specifically bad symlinks)\n                    print(\"In the tar file %s the member %s is invalid: %s\" %\n                          (name, member.name, exc))\n                else:\n                    dirname = os.path.dirname(filename)\n                    if dirname:\n                        os.makedirs(dirname, exist_ok=True)\n                    with open(filename, 'wb') as outfile:\n                        shutil.copyfileobj(extracted, outfile)\n                        self._copy_permissions(member.mode, filename)\n                finally:\n                    if extracted:\n                        extracted.close()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor* input_indices;\n    const Tensor* input_values;\n    const Tensor* input_shape;\n    SparseTensorsMap* map;\n\n    OP_REQUIRES_OK(context, context->input(\"sparse_indices\", &input_indices));\n    OP_REQUIRES_OK(context, context->input(\"sparse_values\", &input_values));\n    OP_REQUIRES_OK(context, context->input(\"sparse_shape\", &input_shape));\n    OP_REQUIRES_OK(context, GetMap(context, true /* is_writing */, &map));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_indices->shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be a matrix but received shape \",\n                    input_indices->shape().DebugString()));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_values->shape()),\n                errors::InvalidArgument(\n                    \"Input values should be a vector but received shape \",\n                    input_values->shape().DebugString()));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shape should be a vector but received shape \",\n                    input_shape->shape().DebugString()));\n\n    int rank = input_shape->NumElements();\n\n    OP_REQUIRES(\n        context, rank > 1,\n        errors::InvalidArgument(\n            \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\n\n    auto input_shape_vec = input_shape->vec<int64_t>();\n    int new_num_elements = 1;\n    bool overflow_ocurred = false;\n    for (int i = 0; i < input_shape_vec.size(); i++) {\n      new_num_elements =\n          MultiplyWithoutOverflow(new_num_elements, input_shape_vec(i));\n      if (new_num_elements < 0) {\n        overflow_ocurred = true;\n        break;\n      }\n    }\n\n    OP_REQUIRES(\n        context, !overflow_ocurred,\n        errors::Internal(\"Encountered overflow from large input shape.\"));\n\n    TensorShape tensor_input_shape(input_shape_vec);\n    gtl::InlinedVector<int64_t, 8> std_order(rank);\n    std::iota(std_order.begin(), std_order.end(), 0);\n    SparseTensor input_st;\n    OP_REQUIRES_OK(context, SparseTensor::Create(*input_indices, *input_values,\n                                                 tensor_input_shape, std_order,\n                                                 &input_st));\n\n    const int64_t N = input_shape_vec(0);\n\n    Tensor sparse_handles(DT_INT64, TensorShape({N}));\n    auto sparse_handles_t = sparse_handles.vec<int64_t>();\n\n    OP_REQUIRES_OK(context, input_st.IndicesValid());\n\n    // We can generate the output shape proto string now, for all\n    // minibatch entries.\n    TensorShape output_shape;\n    OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\n                                input_shape_vec.data() + 1,\n                                input_shape->NumElements() - 1, &output_shape));\n\n    // Get groups by minibatch dimension\n    std::unordered_set<int64_t> visited;\n    sparse::GroupIterable minibatch = input_st.group({0});\n    for (const auto& subset : minibatch) {\n      const int64_t b = subset.group()[0];\n      visited.insert(b);\n      OP_REQUIRES(\n          context, b > -1 && b < N,\n          errors::InvalidArgument(\n              \"Received unexpected column 0 value in input SparseTensor: \", b,\n              \" < 0 or >= N (= \", N, \")\"));\n\n      const auto indices = subset.indices();\n      const auto values = subset.values<T>();\n      const int64_t num_entries = values.size();\n\n      Tensor output_indices = Tensor(DT_INT64, {num_entries, rank - 1});\n      Tensor output_values = Tensor(DataTypeToEnum<T>::value, {num_entries});\n\n      auto output_indices_t = output_indices.matrix<int64_t>();\n      auto output_values_t = output_values.vec<T>();\n\n      for (int i = 0; i < num_entries; ++i) {\n        for (int d = 1; d < rank; ++d) {\n          output_indices_t(i, d - 1) = indices(i, d);\n        }\n        output_values_t(i) = values(i);\n      }\n\n      SparseTensor st_i;\n      OP_REQUIRES_OK(context,\n                     SparseTensor::Create(output_indices, output_values,\n                                          output_shape, &st_i));\n      int64_t handle;\n      OP_REQUIRES_OK(context, map->AddSparseTensor(context, st_i, &handle));\n      sparse_handles_t(b) = handle;\n    }\n\n    // Fill in any gaps; we must provide an empty ST for batch entries\n    // the grouper didn't find.\n    if (visited.size() < N) {\n      Tensor empty_indices(DT_INT64, {0, rank - 1});\n      Tensor empty_values(DataTypeToEnum<T>::value, {0});\n      SparseTensor empty_st;\n      OP_REQUIRES_OK(context, SparseTensor::Create(empty_indices, empty_values,\n                                                   output_shape, &empty_st));\n\n      for (int64_t b = 0; b < N; ++b) {\n        // We skipped this batch entry.\n        if (visited.find(b) == visited.end()) {\n          int64_t handle;\n          OP_REQUIRES_OK(context,\n                         map->AddSparseTensor(context, empty_st, &handle));\n          sparse_handles_t(b) = handle;\n        }\n      }\n    }\n\n    context->set_output(0, sparse_handles);\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def is_select(self) -> bool:\n        # make sure we strip comments; prevents a bug with coments in the CTE\n        parsed = sqlparse.parse(self.strip_comments())\n        if parsed[0].get_type() == \"SELECT\":\n            return True\n\n        if parsed[0].get_type() != \"UNKNOWN\":\n            return False\n\n        # for `UNKNOWN`, check all DDL/DML explicitly: only `SELECT` DML is allowed,\n        # and no DDL is allowed\n        if any(token.ttype == DDL for token in parsed[0]) or any(\n            token.ttype == DML and token.value != \"SELECT\" for token in parsed[0]\n        ):\n            return False\n\n        # return false on `EXPLAIN`, `SET`, `SHOW`, etc.\n        if parsed[0][0].ttype == Keyword:\n            return False\n\n        return any(\n            token.ttype == DML and token.value == \"SELECT\" for token in parsed[0]\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& handle = ctx->input(0);\n    const string& name = handle.scalar<tstring>()();\n    Tensor val;\n    auto session_state = ctx->session_state();\n    OP_REQUIRES(ctx, session_state != nullptr,\n                errors::FailedPrecondition(\n                    \"GetSessionTensor called on null session state\"));\n    OP_REQUIRES_OK(ctx, session_state->GetTensor(name, &val));\n    ctx->set_output(0, val);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "async def _can_passwordless_sudo() -> bool:\n    process = await asyncio.create_subprocess_shell(\n        \"sudo -n true\",\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        shell=True,\n    )\n    _, _ = await process.communicate()\n    return process.returncode == 0\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def copy_with(self, **kwargs: typing.Any) -> \"URL\":\n        \"\"\"\n        Copy this URL, returning a new URL with some components altered.\n        Accepts the same set of parameters as the components that are made\n        available via properties on the `URL` class.\n\n        For example:\n\n        url = httpx.URL(\"https://www.example.com\").copy_with(username=\"jo@gmail.com\", password=\"a secret\")\n        assert url == \"https://jo%40email.com:a%20secret@www.example.com\"\n        \"\"\"\n        allowed = {\n            \"scheme\": str,\n            \"username\": str,\n            \"password\": str,\n            \"userinfo\": bytes,\n            \"host\": str,\n            \"port\": int,\n            \"netloc\": bytes,\n            \"path\": str,\n            \"query\": bytes,\n            \"raw_path\": bytes,\n            \"fragment\": str,\n            \"params\": object,\n        }\n\n        # Step 1\n        # ======\n        #\n        # Perform type checking for all supported keyword arguments.\n        for key, value in kwargs.items():\n            if key not in allowed:\n                message = f\"{key!r} is an invalid keyword argument for copy_with()\"\n                raise TypeError(message)\n            if value is not None and not isinstance(value, allowed[key]):\n                expected = allowed[key].__name__\n                seen = type(value).__name__\n                message = f\"Argument {key!r} must be {expected} but got {seen}\"\n                raise TypeError(message)\n\n        # Step 2\n        # ======\n        #\n        # Consolidate \"username\", \"password\", \"userinfo\", \"host\", \"port\" and \"netloc\"\n        # into a single \"authority\" keyword, for `rfc3986`.\n        if \"username\" in kwargs or \"password\" in kwargs:\n            # Consolidate \"username\" and \"password\" into \"userinfo\".\n            username = quote(kwargs.pop(\"username\", self.username) or \"\")\n            password = quote(kwargs.pop(\"password\", self.password) or \"\")\n            userinfo = f\"{username}:{password}\" if password else username\n            kwargs[\"userinfo\"] = userinfo.encode(\"ascii\")\n\n        if \"host\" in kwargs or \"port\" in kwargs:\n            # Consolidate \"host\" and \"port\" into \"netloc\".\n            host = kwargs.pop(\"host\", self.host) or \"\"\n            port = kwargs.pop(\"port\", self.port)\n\n            if host and \":\" in host and host[0] != \"[\":\n                # IPv6 addresses need to be escaped within square brackets.\n                host = f\"[{host}]\"\n\n            kwargs[\"netloc\"] = (\n                f\"{host}:{port}\".encode(\"ascii\")\n                if port is not None\n                else host.encode(\"ascii\")\n            )\n\n        if \"userinfo\" in kwargs or \"netloc\" in kwargs:\n            # Consolidate \"userinfo\" and \"netloc\" into authority.\n            userinfo = (kwargs.pop(\"userinfo\", self.userinfo) or b\"\").decode(\"ascii\")\n            netloc = (kwargs.pop(\"netloc\", self.netloc) or b\"\").decode(\"ascii\")\n            authority = f\"{userinfo}@{netloc}\" if userinfo else netloc\n            kwargs[\"authority\"] = authority\n\n        # Step 3\n        # ======\n        #\n        # Wrangle any \"path\", \"query\", \"raw_path\" and \"params\" keywords into\n        # \"query\" and \"path\" keywords for `rfc3986`.\n        if \"raw_path\" in kwargs:\n            # If \"raw_path\" is included, then split it into \"path\" and \"query\" components.\n            raw_path = kwargs.pop(\"raw_path\") or b\"\"\n            path, has_query, query = raw_path.decode(\"ascii\").partition(\"?\")\n            kwargs[\"path\"] = path\n            kwargs[\"query\"] = query if has_query else None\n\n        else:\n            if kwargs.get(\"path\") is not None:\n                # Ensure `kwargs[\"path\"] = <url quoted str>` for `rfc3986`.\n                kwargs[\"path\"] = quote(kwargs[\"path\"])\n\n            if kwargs.get(\"query\") is not None:\n                # Ensure `kwargs[\"query\"] = <str>` for `rfc3986`.\n                #\n                # Note that `.copy_with(query=None)` and `.copy_with(query=b\"\")`\n                # are subtly different. The `None` style will not include an empty\n                # trailing \"?\" character.\n                kwargs[\"query\"] = kwargs[\"query\"].decode(\"ascii\")\n\n            if \"params\" in kwargs:\n                # Replace any \"params\" keyword with the raw \"query\" instead.\n                #\n                # Ensure that empty params use `kwargs[\"query\"] = None` rather\n                # than `kwargs[\"query\"] = \"\"`, so that generated URLs do not\n                # include an empty trailing \"?\".\n                params = kwargs.pop(\"params\")\n                kwargs[\"query\"] = None if not params else str(QueryParams(params))\n\n        # Step 4\n        # ======\n        #\n        # Ensure any fragment component is quoted.\n        if kwargs.get(\"fragment\") is not None:\n            kwargs[\"fragment\"] = quote(kwargs[\"fragment\"])\n\n        # Step 5\n        # ======\n        #\n        # At this point kwargs may include keys for \"scheme\", \"authority\", \"path\",\n        # \"query\" and \"fragment\". Together these constitute the entire URL.\n        #\n        # See https://tools.ietf.org/html/rfc3986#section-3\n        #\n        #  foo://example.com:8042/over/there?name=ferret#nose\n        #  \\_/   \\______________/\\_________/ \\_________/ \\__/\n        #   |           |            |            |        |\n        # scheme     authority       path        query   fragment\n        return URL(self._uri_reference.copy_with(**kwargs).unsplit())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, hs, pusherdict):\n        self.hs = hs\n        self.store = self.hs.get_datastore()\n        self.storage = self.hs.get_storage()\n        self.clock = self.hs.get_clock()\n        self.state_handler = self.hs.get_state_handler()\n        self.user_id = pusherdict[\"user_name\"]\n        self.app_id = pusherdict[\"app_id\"]\n        self.app_display_name = pusherdict[\"app_display_name\"]\n        self.device_display_name = pusherdict[\"device_display_name\"]\n        self.pushkey = pusherdict[\"pushkey\"]\n        self.pushkey_ts = pusherdict[\"ts\"]\n        self.data = pusherdict[\"data\"]\n        self.last_stream_ordering = pusherdict[\"last_stream_ordering\"]\n        self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n        self.failing_since = pusherdict[\"failing_since\"]\n        self.timed_call = None\n        self._is_processing = False\n        self._group_unread_count_by_room = hs.config.push_group_unread_count_by_room\n\n        # This is the highest stream ordering we know it's safe to process.\n        # When new events arrive, we'll be given a window of new events: we\n        # should honour this rather than just looking for anything higher\n        # because of potential out-of-order event serialisation. This starts\n        # off as None though as we don't know any better.\n        self.max_stream_ordering = None\n\n        if \"data\" not in pusherdict:\n            raise PusherConfigException(\"No 'data' key for HTTP pusher\")\n        self.data = pusherdict[\"data\"]\n\n        self.name = \"%s/%s/%s\" % (\n            pusherdict[\"user_name\"],\n            pusherdict[\"app_id\"],\n            pusherdict[\"pushkey\"],\n        )\n\n        if self.data is None:\n            raise PusherConfigException(\"data can not be null for HTTP pusher\")\n\n        if \"url\" not in self.data:\n            raise PusherConfigException(\"'url' required in data for HTTP pusher\")\n        self.url = self.data[\"url\"]\n        self.http_client = hs.get_proxied_http_client()\n        self.data_minus_url = {}\n        self.data_minus_url.update(self.data)\n        del self.data_minus_url[\"url\"]\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def log_request(handler):\n    \"\"\"log a bit more information about each request than tornado's default\n\n    - move static file get success to debug-level (reduces noise)\n    - get proxied IP instead of proxy IP\n    - log referer for redirect and failed requests\n    - log user-agent for failed requests\n    \"\"\"\n    status = handler.get_status()\n    request = handler.request\n    try:\n        logger = handler.log\n    except AttributeError:\n        logger = access_log\n\n    if status < 300 or status == 304:\n        # Successes (or 304 FOUND) are debug-level\n        log_method = logger.debug\n    elif status < 400:\n        log_method = logger.info\n    elif status < 500:\n        log_method = logger.warning\n    else:\n        log_method = logger.error\n\n    request_time = 1000.0 * handler.request.request_time()\n    ns = dict(\n        status=status,\n        method=request.method,\n        ip=request.remote_ip,\n        uri=request.uri,\n        request_time=request_time,\n    )\n    msg = \"{status} {method} {uri} ({ip}) {request_time:.2f}ms\"\n    if status >= 400:\n        # log bad referers\n        ns[\"referer\"] = request.headers.get(\"Referer\", \"None\")\n        msg = msg + \" referer={referer}\"\n    if status >= 500 and status != 502:\n        # log all headers if it caused an error\n        log_method(json.dumps(dict(request.headers), indent=2))\n    log_method(msg.format(**ns))\n    prometheus_log_method(handler)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def redirect(redirect_path, redirect_params=None, **kw):\n    \"\"\"Redirect (via cherrypy.HTTPRedirect).\n\n    Raises the exception instead of returning it, this to allow\n    users to both call it as a function or to raise it as an exception.\n\n    \"\"\"\n    if not isinstance(redirect_path, basestring):\n        redirect_path = '/'.join(list(redirect_path))\n    if not redirect_path.startswith('/'):\n        path = request.path_info\n        check_app_root()\n        if path.startswith(request.app_root):\n            path = path[len(request.app_root):]\n        redirect_path = urlparse.urljoin(path, redirect_path)\n    raise cherrypy.HTTPRedirect(url(tgpath=redirect_path,\n        tgparams=redirect_params, **kw))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& indices = ctx->input(0);\n    const auto values = ctx->input(1).flat<Tidx>();\n    const Tensor& dense_shape = ctx->input(2);\n    const Tensor& size_t = ctx->input(3);\n    const auto weights = ctx->input(4).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    bool is_1d = dense_shape.NumElements() == 1;\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (is_1d) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(ctx,\n                       functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                           ctx, values, weights, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, values, weights, out, size));\n      }\n    } else {\n      const auto shape = dense_shape.flat<int64_t>();\n      const int64_t num_rows = shape(0);\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      const auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      const auto indices_mat = indices.matrix<int64_t>();\n      for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {\n        const int64_t batch = indices_mat(i, 0);\n        const Tidx bin = values(i);\n        if (bin < size) {\n          if (binary_output_) {\n            out(batch, bin) = T(1);\n          } else {\n            if (weights_size) {\n              out(batch, bin) += weights(i);\n            } else {\n              out(batch, bin) += T(1);\n            }\n          }\n        }\n      }\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _extract_tar_file(tar, filename, b_dest, b_temp_path, expected_hash=None):\n    n_filename = to_native(filename, errors='surrogate_or_strict')\n    try:\n        member = tar.getmember(n_filename)\n    except KeyError:\n        raise AnsibleError(\"Collection tar at '%s' does not contain the expected file '%s'.\" % (to_native(tar.name),\n                                                                                                n_filename))\n\n    with tempfile.NamedTemporaryFile(dir=b_temp_path, delete=False) as tmpfile_obj:\n        bufsize = 65536\n        sha256_digest = sha256()\n        with _tarfile_extract(tar, member) as tar_obj:\n            data = tar_obj.read(bufsize)\n            while data:\n                tmpfile_obj.write(data)\n                tmpfile_obj.flush()\n                sha256_digest.update(data)\n                data = tar_obj.read(bufsize)\n\n        actual_hash = sha256_digest.hexdigest()\n\n        if expected_hash and actual_hash != expected_hash:\n            raise AnsibleError(\"Checksum mismatch for '%s' inside collection at '%s'\"\n                               % (n_filename, to_native(tar.name)))\n\n        b_dest_filepath = os.path.join(b_dest, to_bytes(filename, errors='surrogate_or_strict'))\n        b_parent_dir = os.path.split(b_dest_filepath)[0]\n        if not os.path.exists(b_parent_dir):\n            # Seems like Galaxy does not validate if all file entries have a corresponding dir ftype entry. This check\n            # makes sure we create the parent directory even if it wasn't set in the metadata.\n            os.makedirs(b_parent_dir, mode=0o0755)\n\n        shutil.move(to_bytes(tmpfile_obj.name, errors='surrogate_or_strict'), b_dest_filepath)\n\n        # Default to rw-r--r-- and only add execute if the tar file has execute.\n        new_mode = 0o644\n        if stat.S_IMODE(member.mode) & stat.S_IXUSR:\n            new_mode |= 0o0111\n\n        os.chmod(b_dest_filepath, new_mode)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def logout(self, message, relaystate=None, samlresponse=None):\n        \"\"\"\n        Handle HTTP logout. The supported logout methods are stored\n        in each session. First all the SOAP sessions are logged out\n        then the HTTP Redirect method is used for any remaining\n        sessions.\n\n        The basic process is this:\n         1. A logout request is received. It is processed and the response\n            cached.\n         2. If any other SP's have also logged in as this user then the\n            first such session is popped off and a logout request is\n            generated and forwarded to the SP.\n         3. If a logout response is received then the user is marked\n            as logged out from that SP.\n         Repeat steps 2-3 until only the initial logout request is\n         left unhandled, at which time the pre-generated response is sent\n         back to the SP that originated the logout request.\n\n        The final logout response is always a redirect.\n        \"\"\"\n        logout = self.cfg.idp.get_logout_handler()\n\n        us = UserSession()\n\n        saml_sessions = self.cfg.idp.sessionfactory\n\n        logout_type = None\n        try:\n            if lasso.SAML2_FIELD_REQUEST in message:\n                logout_type = \"request\"\n                self._handle_logout_request(us, logout, saml_sessions,\n                                            message)\n            elif samlresponse:\n                logout_type = \"response\"\n                self._handle_logout_response(us, logout, saml_sessions,\n                                             message, samlresponse)\n            else:\n                raise cherrypy.HTTPError(400, 'Bad Request. Not a ' +\n                                         'logout request or response.')\n        except InvalidRequest as e:\n            raise cherrypy.HTTPError(400, 'Bad Request. %s' % e)\n        except UnknownProvider as e:\n            raise cherrypy.HTTPError(\n                400, 'Invalid logout %s: %s' % (logout_type, e)\n            )\n\n        # Fall through to handle any remaining sessions.\n\n        # Find the next SP to logout and send a LogoutRequest\n        logout_order = [\n            lasso.SAML2_METADATA_BINDING_SOAP,\n            lasso.SAML2_METADATA_BINDING_REDIRECT,\n        ]\n        (logout_mech, session) = saml_sessions.get_next_logout(\n            logout_mechs=logout_order)\n        while session:\n            self.debug('Going to log out %s' % session.provider_id)\n\n            try:\n                logout.setSessionFromDump(session.login_session)\n            except lasso.ProfileBadSessionDumpError as e:\n                self.error('Failed to load session: %s' % e)\n                raise cherrypy.HTTPRedirect(400, 'Failed to log out user: %s '\n                                            % e)\n            try:\n                if logout_mech == lasso.SAML2_METADATA_BINDING_REDIRECT:\n                    logout.initRequest(session.provider_id,\n                                       lasso.HTTP_METHOD_REDIRECT)\n                else:\n                    logout.initRequest(session.provider_id,\n                                       lasso.HTTP_METHOD_SOAP)\n            except lasso.ServerProviderNotFoundError:\n                self.error(\n                    'Service Provider %s not found. Trying next session' %\n                    session.provider_id\n                )\n                saml_sessions.remove_session(session)\n                (logout_mech, session) = saml_sessions.get_next_logout(\n                    logout_mechs=logout_order)\n                continue\n\n            try:\n                logout.buildRequestMsg()\n            except lasso.Error, e:\n                self.error('failure to build logout request msg: %s' % e)\n                raise cherrypy.HTTPRedirect(400, 'Failed to log out user: %s '\n                                            % e)\n\n            # Set the full list of session indexes for this provider to\n            # log out\n            self.debug('logging out provider id %s' % session.provider_id)\n            indexes = saml_sessions.get_session_id_by_provider_id(\n                session.provider_id\n            )\n            self.debug('Requesting logout for sessions %s' % (indexes,))\n            req = logout.get_request()\n            req.setSessionIndexes(indexes)\n\n            session.set_logoutstate(relaystate=logout.msgUrl,\n                                    request_id=logout.request.id)\n            saml_sessions.start_logout(session, initial=False)\n\n            self.debug('Request logout ID %s for session ID %s' %\n                       (logout.request.id, session.session_id))\n\n            if logout_mech == lasso.SAML2_METADATA_BINDING_REDIRECT:\n                self.debug('Redirecting to another SP to logout on %s at %s' %\n                           (logout.remoteProviderId, logout.msgUrl))\n                raise cherrypy.HTTPRedirect(logout.msgUrl)\n            else:\n                self.debug('SOAP request to another SP to logout on %s at %s' %\n                           (logout.remoteProviderId, logout.msgUrl))\n                if logout.msgBody:\n                    message = self._soap_logout(logout)\n                    try:\n                        self._handle_logout_response(us,\n                                                     logout,\n                                                     saml_sessions,\n                                                     message,\n                                                     samlresponse)\n                    except Exception as e:  # pylint: disable=broad-except\n                        self.error('SOAP SLO failed %s' % e)\n                else:\n                    self.error('Provider does not support SOAP')\n\n            (logout_mech, session) = saml_sessions.get_next_logout(\n                logout_mechs=logout_order)\n\n        # done while\n\n        # All sessions should be logged out now. Respond to the\n        # original request using the response we cached earlier.\n\n        try:\n            session = saml_sessions.get_initial_logout()\n        except ValueError:\n            self.debug('SLO get_last_session() unable to find last session')\n            raise cherrypy.HTTPError(400, 'Unable to determine logout state')\n\n        redirect = session.relaystate\n        if not redirect:\n            redirect = self.basepath\n\n        saml_sessions.remove_session(session)\n\n        # Log out of cherrypy session\n        user = us.get_user()\n        self._audit('Logged out user: %s [%s] from %s' %\n                    (user.name, user.fullname,\n                     session.provider_id))\n        us.logout(user)\n\n        self.debug('SLO redirect to %s' % redirect)\n\n        raise cherrypy.HTTPRedirect(redirect)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_header_lines(header):\n    \"\"\"\n    Splits the header into lines, putting multi-line headers together.\n    \"\"\"\n    r = []\n    lines = header.split(b\"\\n\")\n    for line in lines:\n        if line.startswith((b\" \", b\"\\t\")):\n            if not r:\n                # https://corte.si/posts/code/pathod/pythonservers/index.html\n                raise ParsingError('Malformed header line \"%s\"' % tostr(line))\n            r[-1] += line\n        else:\n            r.append(line)\n    return r\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status Examples::CreateSparseFeatureRepresentation(\n    const DeviceBase::CpuWorkerThreads& worker_threads, const int num_examples,\n    const int num_sparse_features, const ModelWeights& weights,\n    const OpInputList& sparse_example_indices_inputs,\n    const OpInputList& sparse_feature_indices_inputs,\n    const OpInputList& sparse_feature_values_inputs,\n    std::vector<Example>* const examples) {\n  mutex mu;\n  Status result;  // Guarded by mu\n  auto parse_partition = [&](const int64 begin, const int64 end) {\n    // The static_cast here is safe since begin and end can be at most\n    // num_examples which is an int.\n    for (int i = static_cast<int>(begin); i < end; ++i) {\n      auto example_indices =\n          sparse_example_indices_inputs[i].template flat<int64>();\n      auto feature_indices =\n          sparse_feature_indices_inputs[i].template flat<int64>();\n\n      // Parse features for each example. Features for a particular example\n      // are at the offsets (start_id, end_id]\n      int start_id = -1;\n      int end_id = 0;\n      for (int example_id = 0; example_id < num_examples; ++example_id) {\n        start_id = end_id;\n        while (end_id < example_indices.size() &&\n               example_indices(end_id) == example_id) {\n          ++end_id;\n        }\n        Example::SparseFeatures* const sparse_features =\n            &(*examples)[example_id].sparse_features_[i];\n        if (start_id < example_indices.size() &&\n            example_indices(start_id) == example_id) {\n          sparse_features->indices.reset(new UnalignedInt64Vector(\n              &(feature_indices(start_id)), end_id - start_id));\n          if (sparse_feature_values_inputs.size() > i) {\n            auto feature_weights =\n                sparse_feature_values_inputs[i].flat<float>();\n            sparse_features->values.reset(new UnalignedFloatVector(\n                &(feature_weights(start_id)), end_id - start_id));\n          }\n          // If features are non empty.\n          if (end_id - start_id > 0) {\n            // TODO(sibyl-Aix6ihai): Write this efficiently using vectorized\n            // operations from eigen.\n            for (int64 k = 0; k < sparse_features->indices->size(); ++k) {\n              const int64 feature_index = (*sparse_features->indices)(k);\n              if (!weights.SparseIndexValid(i, feature_index)) {\n                mutex_lock l(mu);\n                result = errors::InvalidArgument(\n                    \"Found sparse feature indices out of valid range: \",\n                    (*sparse_features->indices)(k));\n                return;\n              }\n            }\n          }\n        } else {\n          // Add a Tensor that has size 0.\n          sparse_features->indices.reset(\n              new UnalignedInt64Vector(&(feature_indices(0)), 0));\n          // If values exist for this feature group.\n          if (sparse_feature_values_inputs.size() > i) {\n            auto feature_weights =\n                sparse_feature_values_inputs[i].flat<float>();\n            sparse_features->values.reset(\n                new UnalignedFloatVector(&(feature_weights(0)), 0));\n          }\n        }\n      }\n    }\n  };\n  // For each column, the cost of parsing it is O(num_examples). We use\n  // num_examples here, as empirically Shard() creates the right amount of\n  // threads based on the problem size.\n  // TODO(sibyl-Aix6ihai): Tune this as a function of dataset size.\n  const int64 kCostPerUnit = num_examples;\n  Shard(worker_threads.num_threads, worker_threads.workers, num_sparse_features,\n        kCostPerUnit, parse_partition);\n  return result;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status ValidateInputsGenerateOutputs(\n      OpKernelContext* ctx, const Tensor** inputs, const Tensor** seq_len,\n      Tensor** log_prob, OpOutputList* decoded_indices,\n      OpOutputList* decoded_values, OpOutputList* decoded_shape) const {\n    Status status = ctx->input(\"inputs\", inputs);\n    if (!status.ok()) return status;\n    status = ctx->input(\"sequence_length\", seq_len);\n    if (!status.ok()) return status;\n\n    const TensorShape& inputs_shape = (*inputs)->shape();\n\n    if (inputs_shape.dims() != 3) {\n      return errors::InvalidArgument(\"inputs is not a 3-Tensor\");\n    }\n\n    const int64 max_time = inputs_shape.dim_size(0);\n    const int64 batch_size = inputs_shape.dim_size(1);\n\n    if (max_time == 0) {\n      return errors::InvalidArgument(\"max_time is 0\");\n    }\n    if (!TensorShapeUtils::IsVector((*seq_len)->shape())) {\n      return errors::InvalidArgument(\"sequence_length is not a vector\");\n    }\n\n    if (!(batch_size == (*seq_len)->dim_size(0))) {\n      return errors::FailedPrecondition(\n          \"len(sequence_length) != batch_size.  \",\n          \"len(sequence_length):  \", (*seq_len)->dim_size(0),\n          \" batch_size: \", batch_size);\n    }\n\n    auto seq_len_t = (*seq_len)->vec<int32>();\n\n    for (int b = 0; b < batch_size; ++b) {\n      if (!(seq_len_t(b) <= max_time)) {\n        return errors::FailedPrecondition(\"sequence_length(\", b,\n                                          \") <= \", max_time);\n      }\n    }\n\n    Status s = ctx->allocate_output(\n        \"log_probability\", TensorShape({batch_size, top_paths_}), log_prob);\n    if (!s.ok()) return s;\n\n    s = ctx->output_list(\"decoded_indices\", decoded_indices);\n    if (!s.ok()) return s;\n    s = ctx->output_list(\"decoded_values\", decoded_values);\n    if (!s.ok()) return s;\n    s = ctx->output_list(\"decoded_shape\", decoded_shape);\n    if (!s.ok()) return s;\n\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def send_commenting_notifications(self, changes):\n        \"\"\"\n        Sends notifications about any changes to comments to anyone who is subscribed.\n        \"\"\"\n        relevant_comment_ids = []\n        relevant_comment_ids.extend(comment.pk for comment in changes['resolved_comments'])\n        relevant_comment_ids.extend(comment.pk for comment, replies in changes['new_replies'])\n\n        # Skip if no changes were made\n        # Note: We don't email about edited comments so ignore those here\n        if (not changes['new_comments']\n                and not changes['deleted_comments']\n                and not changes['resolved_comments']\n                and not changes['new_replies']):\n            return\n\n        # Get global page comment subscribers\n        subscribers = PageSubscription.objects.filter(page=self.page, comment_notifications=True).select_related('user')\n        global_recipient_users = [subscriber.user for subscriber in subscribers if subscriber.user != self.request.user]\n\n        # Get subscribers to individual threads\n        replies = CommentReply.objects.filter(comment_id__in=relevant_comment_ids)\n        comments = Comment.objects.filter(id__in=relevant_comment_ids)\n        thread_users = get_user_model().objects.exclude(pk=self.request.user.pk).exclude(pk__in=subscribers.values_list('user_id', flat=True)).prefetch_related(\n            Prefetch('comment_replies', queryset=replies),\n            Prefetch(COMMENTS_RELATION_NAME, queryset=comments)\n        ).exclude(\n            Q(comment_replies__isnull=True) & Q(**{('%s__isnull' % COMMENTS_RELATION_NAME): True})\n        )\n\n        # Skip if no recipients\n        if not (global_recipient_users or thread_users):\n            return\n        thread_users = [(user, set(list(user.comment_replies.values_list('comment_id', flat=True)) + list(getattr(user, COMMENTS_RELATION_NAME).values_list('pk', flat=True)))) for user in thread_users]\n        mailed_users = set()\n\n        for current_user, current_threads in thread_users:\n            # We are trying to avoid calling send_notification for each user for performance reasons\n            # so group the users receiving the same thread notifications together here\n            if current_user in mailed_users:\n                continue\n            users = [current_user]\n            mailed_users.add(current_user)\n            for user, threads in thread_users:\n                if user not in mailed_users and threads == current_threads:\n                    users.append(user)\n                    mailed_users.add(user)\n            send_notification(users, 'updated_comments', {\n                'page': self.page,\n                'editor': self.request.user,\n                'new_comments': [comment for comment in changes['new_comments'] if comment.pk in threads],\n                'resolved_comments': [comment for comment in changes['resolved_comments'] if comment.pk in threads],\n                'deleted_comments': [],\n                'replied_comments': [\n                    {\n                        'comment': comment,\n                        'replies': replies,\n                    }\n                    for comment, replies in changes['new_replies']\n                    if comment.pk in threads\n                ]\n            })\n\n        return send_notification(global_recipient_users, 'updated_comments', {\n            'page': self.page,\n            'editor': self.request.user,\n            'new_comments': changes['new_comments'],\n            'resolved_comments': changes['resolved_comments'],\n            'deleted_comments': changes['deleted_comments'],\n            'replied_comments': [\n                {\n                    'comment': comment,\n                    'replies': replies,\n                }\n                for comment, replies in changes['new_replies']\n            ]\n        })\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool ParseAttrValue(StringPiece type, StringPiece text, AttrValue* out) {\n  // Parse type.\n  string field_name;\n  bool is_list = absl::ConsumePrefix(&type, \"list(\");\n  if (absl::ConsumePrefix(&type, \"string\")) {\n    field_name = \"s\";\n  } else if (absl::ConsumePrefix(&type, \"int\")) {\n    field_name = \"i\";\n  } else if (absl::ConsumePrefix(&type, \"float\")) {\n    field_name = \"f\";\n  } else if (absl::ConsumePrefix(&type, \"bool\")) {\n    field_name = \"b\";\n  } else if (absl::ConsumePrefix(&type, \"type\")) {\n    field_name = \"type\";\n  } else if (absl::ConsumePrefix(&type, \"shape\")) {\n    field_name = \"shape\";\n  } else if (absl::ConsumePrefix(&type, \"tensor\")) {\n    field_name = \"tensor\";\n  } else if (absl::ConsumePrefix(&type, \"func\")) {\n    field_name = \"func\";\n  } else if (absl::ConsumePrefix(&type, \"placeholder\")) {\n    field_name = \"placeholder\";\n  } else {\n    return false;\n  }\n  if (is_list && !absl::ConsumePrefix(&type, \")\")) {\n    return false;\n  }\n\n  // Construct a valid text proto message to parse.\n  string to_parse;\n  if (is_list) {\n    // TextFormat parser considers \"i: 7\" to be the same as \"i: [7]\",\n    // but we only want to allow list values with [].\n    StringPiece cleaned = text;\n    str_util::RemoveLeadingWhitespace(&cleaned);\n    str_util::RemoveTrailingWhitespace(&cleaned);\n    if (cleaned.size() < 2 || cleaned[0] != '[' ||\n        cleaned[cleaned.size() - 1] != ']') {\n      return false;\n    }\n    cleaned.remove_prefix(1);\n    str_util::RemoveLeadingWhitespace(&cleaned);\n    if (cleaned.size() == 1) {\n      // User wrote \"[]\", so return empty list without invoking the TextFormat\n      // parse which returns an error for \"i: []\".\n      out->Clear();\n      out->mutable_list();\n      return true;\n    }\n    to_parse = strings::StrCat(\"list { \", field_name, \": \", text, \" }\");\n  } else {\n    to_parse = strings::StrCat(field_name, \": \", text);\n  }\n\n  return ProtoParseFromString(to_parse, out);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, hs):\n        super().__init__(hs)\n\n        self.http_client = SimpleHttpClient(hs)\n        # We create a blacklisting instance of SimpleHttpClient for contacting identity\n        # servers specified by clients\n        self.blacklisting_http_client = SimpleHttpClient(\n            hs, ip_blacklist=hs.config.federation_ip_range_blacklist\n        )\n        self.federation_http_client = hs.get_http_client()\n        self.hs = hs\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _on_ssl_errors(self, error):\n        self._has_ssl_errors = True\n\n        url = error.url()\n        log.webview.debug(\"Certificate error: {}\".format(error))\n\n        if error.is_overridable():\n            error.ignore = shared.ignore_certificate_errors(\n                url, [error], abort_on=[self.abort_questions])\n        else:\n            log.webview.error(\"Non-overridable certificate error: \"\n                              \"{}\".format(error))\n\n        log.webview.debug(\"ignore {}, URL {}, requested {}\".format(\n            error.ignore, url, self.url(requested=True)))\n\n        # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-56207\n        show_cert_error = (\n            not qtutils.version_check('5.9') and\n            not error.ignore\n        )\n        # WORKAROUND for https://codereview.qt-project.org/c/qt/qtwebengine/+/270556\n        show_non_overr_cert_error = (\n            not error.is_overridable() and (\n                # Affected Qt versions:\n                # 5.13 before 5.13.2\n                # 5.12 before 5.12.6\n                # < 5.12\n                (qtutils.version_check('5.13') and\n                 not qtutils.version_check('5.13.2')) or\n                (qtutils.version_check('5.12') and\n                 not qtutils.version_check('5.12.6')) or\n                not qtutils.version_check('5.12')\n            )\n        )\n\n        # We can't really know when to show an error page, as the error might\n        # have happened when loading some resource.\n        # However, self.url() is not available yet and the requested URL\n        # might not match the URL we get from the error - so we just apply a\n        # heuristic here.\n        if ((show_cert_error or show_non_overr_cert_error) and\n                url.matches(self.data.last_navigation.url, QUrl.RemoveScheme)):\n            self._show_error_page(url, str(error))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "expandrow(UINT8 *dest, UINT8 *src, int n, int z, int xsize) {\n    UINT8 pixel, count;\n    int x = 0;\n\n    for (; n > 0; n--) {\n        pixel = *src++;\n        if (n == 1 && pixel != 0) {\n            return n;\n        }\n        count = pixel & RLE_MAX_RUN;\n        if (!count) {\n            return count;\n        }\n        if (x + count > xsize) {\n            return -1;\n        }\n        x += count;\n        if (pixel & RLE_COPY_FLAG) {\n            while (count--) {\n                *dest = *src++;\n                dest += z;\n            }\n\n        } else {\n            pixel = *src++;\n            while (count--) {\n                *dest = pixel;\n                dest += z;\n            }\n        }\n    }\n    return 0;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "ImagingFliDecode(Imaging im, ImagingCodecState state, UINT8* buf, Py_ssize_t bytes)\n{\n    UINT8* ptr;\n    int framesize;\n    int c, chunks, advance;\n    int l, lines;\n    int i, j, x = 0, y, ymax;\n\n    /* If not even the chunk size is present, we'd better leave */\n\n    if (bytes < 4)\n\treturn 0;\n\n    /* We don't decode anything unless we have a full chunk in the\n       input buffer (on the other hand, the Python part of the driver\n       makes sure this is always the case) */\n\n    ptr = buf;\n\n    framesize = I32(ptr);\n    if (framesize < I32(ptr))\n\treturn 0;\n\n    /* Make sure this is a frame chunk.  The Python driver takes\n       case of other chunk types. */\n\n    if (I16(ptr+4) != 0xF1FA) {\n\tstate->errcode = IMAGING_CODEC_UNKNOWN;\n\treturn -1;\n    }\n\n    chunks = I16(ptr+6);\n    ptr += 16;\n    bytes -= 16;\n\n    /* Process subchunks */\n    for (c = 0; c < chunks; c++) {\n\tUINT8* data;\n\tif (bytes < 10) {\n\t    state->errcode = IMAGING_CODEC_OVERRUN;\n\t    return -1;\n\t}\n\tdata = ptr + 6;\n\tswitch (I16(ptr+4)) {\n\tcase 4: case 11:\n\t    /* FLI COLOR chunk */\n\t    break; /* ignored; handled by Python code */\n\tcase 7:\n\t    /* FLI SS2 chunk (word delta) */\n\t    lines = I16(data); data += 2;\n\t    for (l = y = 0; l < lines && y < state->ysize; l++, y++) {\n\t\tUINT8* buf = (UINT8*) im->image[y];\n\t\tint p, packets;\n\t\tpackets = I16(data); data += 2;\n\t\twhile (packets & 0x8000) {\n\t\t    /* flag word */\n\t\t    if (packets & 0x4000) {\n\t\t\ty += 65536 - packets; /* skip lines */\n\t\t\tif (y >= state->ysize) {\n\t\t\t    state->errcode = IMAGING_CODEC_OVERRUN;\n\t\t\t    return -1;\n\t\t\t}\n\t\t\tbuf = (UINT8*) im->image[y];\n\t\t    } else {\n\t\t\t/* store last byte (used if line width is odd) */\n\t\t\tbuf[state->xsize-1] = (UINT8) packets;\n\t\t    }\n\t\t    packets = I16(data); data += 2;\n\t\t}\n\t\tfor (p = x = 0; p < packets; p++) {\n\t\t    x += data[0]; /* pixel skip */\n\t\t    if (data[1] >= 128) {\n\t\t\ti = 256-data[1]; /* run */\n\t\t\tif (x + i + i > state->xsize)\n\t\t\t    break;\n\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t    buf[x++] = data[2];\n\t\t\t    buf[x++] = data[3];\n\t\t\t}\n\t\t\tdata += 2 + 2;\n\t\t    } else {\n\t\t\ti = 2 * (int) data[1]; /* chunk */\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break;\n\t\t\tmemcpy(buf + x, data + 2, i);\n\t\t\tdata += 2 + i;\n\t\t\tx += i;\n\t\t    }\n\t\t}\n\t\tif (p < packets)\n\t\t    break; /* didn't process all packets */\n\t    }\n\t    if (l < lines) {\n\t\t/* didn't process all lines */\n\t\tstate->errcode = IMAGING_CODEC_OVERRUN;\n\t\treturn -1;\n\t    }\n\t    break;\n\tcase 12:\n\t    /* FLI LC chunk (byte delta) */\n\t    y = I16(data); ymax = y + I16(data+2); data += 4;\n\t    for (; y < ymax && y < state->ysize; y++) {\n\t\tUINT8* out = (UINT8*) im->image[y];\n\t\tint p, packets = *data++;\n\t\tfor (p = x = 0; p < packets; p++, x += i) {\n\t\t    x += data[0]; /* skip pixels */\n\t\t    if (data[1] & 0x80) {\n\t\t\ti = 256-data[1]; /* run */\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break;\n\t\t\tmemset(out + x, data[2], i);\n\t\t\tdata += 3;\n\t\t    } else {\n\t\t\ti = data[1]; /* chunk */\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break;\n\t\t\tmemcpy(out + x, data + 2, i);\n\t\t\tdata += i + 2;\n\t\t    }\n\t\t}\n\t\tif (p < packets)\n\t\t    break; /* didn't process all packets */\n\t    }\n\t    if (y < ymax) {\n\t\t/* didn't process all lines */\n\t\tstate->errcode = IMAGING_CODEC_OVERRUN;\n\t\treturn -1;\n\t    }\n\t    break;\n\tcase 13:\n\t    /* FLI BLACK chunk */\n\t    for (y = 0; y < state->ysize; y++)\n\t\tmemset(im->image[y], 0, state->xsize);\n\t    break;\n\tcase 15:\n\t    /* FLI BRUN chunk */\n\t    for (y = 0; y < state->ysize; y++) {\n\t\tUINT8* out = (UINT8*) im->image[y];\n\t\tdata += 1; /* ignore packetcount byte */\n\t\tfor (x = 0; x < state->xsize; x += i) {\n\t\t    if (data[0] & 0x80) {\n\t\t\ti = 256 - data[0];\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break; /* safety first */\n\t\t\tmemcpy(out + x, data + 1, i);\n\t\t\tdata += i + 1;\n\t\t    } else {\n\t\t\ti = data[0];\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break; /* safety first */\n\t\t\tmemset(out + x, data[1], i);\n\t\t\tdata += 2;\n\t\t    }\n\t\t}\n\t\tif (x != state->xsize) {\n\t\t    /* didn't unpack whole line */\n\t\t    state->errcode = IMAGING_CODEC_OVERRUN;\n\t\t    return -1;\n\t\t}\n\t    }\n\t    break;\n\tcase 16:\n\t    /* COPY chunk */\n\t    for (y = 0; y < state->ysize; y++) {\n\t\tUINT8* buf = (UINT8*) im->image[y];\n\t\tmemcpy(buf, data, state->xsize);\n\t\tdata += state->xsize;\n\t    }\n\t    break;\n\tcase 18:\n\t    /* PSTAMP chunk */\n\t    break; /* ignored */\n\tdefault:\n\t    /* unknown chunk */\n\t    /* printf(\"unknown FLI/FLC chunk: %d\\n\", I16(ptr+4)); */\n\t    state->errcode = IMAGING_CODEC_UNKNOWN;\n\t    return -1;\n\t}\n\tadvance = I32(ptr);\n\tptr += advance;\n\tbytes -= advance;\n    }\n\n    return -1; /* end of frame */\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    core::RefCountPtr<CollectiveGroupResource> resource;\n    OP_REQUIRES_OK_ASYNC(c, LookupResource(c, HandleFromInput(c, 1), &resource),\n                         done);\n\n    Tensor group_assignment = c->input(2);\n\n    OP_REQUIRES_OK_ASYNC(\n        c,\n        FillCollectiveParams(col_params, group_assignment,\n                             ALL_TO_ALL_COLLECTIVE, resource.get()),\n        done);\n    col_params->instance.shape = c->input(0).shape();\n    VLOG(1) << \"CollectiveAllToAll group_size \" << col_params->group.group_size\n            << \" group_key \" << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    // Allocate the output tensor, trying to reuse the input.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(c,\n                         c->forward_input_or_allocate_output(\n                             {0}, 0, col_params->instance.shape, &output),\n                         done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_request_handler(\n    dependant: Dependant,\n    body_field: Optional[ModelField] = None,\n    status_code: int = 200,\n    response_class: Union[Type[Response], DefaultPlaceholder] = Default(JSONResponse),\n    response_field: Optional[ModelField] = None,\n    response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n    response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n    response_model_by_alias: bool = True,\n    response_model_exclude_unset: bool = False,\n    response_model_exclude_defaults: bool = False,\n    response_model_exclude_none: bool = False,\n    dependency_overrides_provider: Optional[Any] = None,\n) -> Callable[[Request], Coroutine[Any, Any, Response]]:\n    assert dependant.call is not None, \"dependant.call must be a function\"\n    is_coroutine = asyncio.iscoroutinefunction(dependant.call)\n    is_body_form = body_field and isinstance(body_field.field_info, params.Form)\n    if isinstance(response_class, DefaultPlaceholder):\n        actual_response_class: Type[Response] = response_class.value\n    else:\n        actual_response_class = response_class\n\n    async def app(request: Request) -> Response:\n        try:\n            body = None\n            if body_field:\n                if is_body_form:\n                    body = await request.form()\n                else:\n                    body_bytes = await request.body()\n                    if body_bytes:\n                        body = await request.json()\n        except json.JSONDecodeError as e:\n            raise RequestValidationError([ErrorWrapper(e, (\"body\", e.pos))], body=e.doc)\n        except Exception as e:\n            raise HTTPException(\n                status_code=400, detail=\"There was an error parsing the body\"\n            ) from e\n        solved_result = await solve_dependencies(\n            request=request,\n            dependant=dependant,\n            body=body,\n            dependency_overrides_provider=dependency_overrides_provider,\n        )\n        values, errors, background_tasks, sub_response, _ = solved_result\n        if errors:\n            raise RequestValidationError(errors, body=body)\n        else:\n            raw_response = await run_endpoint_function(\n                dependant=dependant, values=values, is_coroutine=is_coroutine\n            )\n\n            if isinstance(raw_response, Response):\n                if raw_response.background is None:\n                    raw_response.background = background_tasks\n                return raw_response\n            response_data = await serialize_response(\n                field=response_field,\n                response_content=raw_response,\n                include=response_model_include,\n                exclude=response_model_exclude,\n                by_alias=response_model_by_alias,\n                exclude_unset=response_model_exclude_unset,\n                exclude_defaults=response_model_exclude_defaults,\n                exclude_none=response_model_exclude_none,\n                is_coroutine=is_coroutine,\n            )\n            response = actual_response_class(\n                content=response_data,\n                status_code=status_code,\n                background=background_tasks,  # type: ignore # in Starlette\n            )\n            response.headers.raw.extend(sub_response.headers.raw)\n            if sub_response.status_code:\n                response.status_code = sub_response.status_code\n            return response\n\n    return app\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "handle_keywordonly_args(struct compiling *c, const node *n, int start,\n                        asdl_seq *kwonlyargs, asdl_seq *kwdefaults)\n{\n    PyObject *argname;\n    node *ch;\n    expr_ty expression, annotation;\n    arg_ty arg = NULL;\n    int i = start;\n    int j = 0; /* index for kwdefaults and kwonlyargs */\n\n    if (kwonlyargs == NULL) {\n        ast_error(c, CHILD(n, start), \"named arguments must follow bare *\");\n        return -1;\n    }\n    assert(kwdefaults != NULL);\n    while (i < NCH(n)) {\n        ch = CHILD(n, i);\n        switch (TYPE(ch)) {\n            case vfpdef:\n            case tfpdef:\n                if (i + 1 < NCH(n) && TYPE(CHILD(n, i + 1)) == EQUAL) {\n                    expression = ast_for_expr(c, CHILD(n, i + 2));\n                    if (!expression)\n                        goto error;\n                    asdl_seq_SET(kwdefaults, j, expression);\n                    i += 2; /* '=' and test */\n                }\n                else { /* setting NULL if no default value exists */\n                    asdl_seq_SET(kwdefaults, j, NULL);\n                }\n                if (NCH(ch) == 3) {\n                    /* ch is NAME ':' test */\n                    annotation = ast_for_expr(c, CHILD(ch, 2));\n                    if (!annotation)\n                        goto error;\n                }\n                else {\n                    annotation = NULL;\n                }\n                ch = CHILD(ch, 0);\n                argname = NEW_IDENTIFIER(ch);\n                if (!argname)\n                    goto error;\n                if (forbidden_name(c, argname, ch, 0))\n                    goto error;\n                arg = arg(argname, annotation, NULL, LINENO(ch), ch->n_col_offset,\n                          c->c_arena);\n                if (!arg)\n                    goto error;\n                asdl_seq_SET(kwonlyargs, j++, arg);\n                i += 1; /* the name */\n                if (TYPE(CHILD(n, i)) == COMMA)\n                    i += 1; /* the comma, if present */\n                break;\n            case TYPE_COMMENT:\n                /* arg will be equal to the last argument processed */\n                arg->type_comment = NEW_TYPE_COMMENT(ch);\n                if (!arg->type_comment)\n                    goto error;\n                i += 1;\n                break;\n            case DOUBLESTAR:\n                return i;\n            default:\n                ast_error(c, ch, \"unexpected node\");\n                goto error;\n        }\n    }\n    return i;\n error:\n    return -1;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def index():\n    \"\"\" Index handler \"\"\"\n\n    send = request.vars.send\n    if DEMO_MODE:\n        session.authorized = True\n        session.last_time = t0\n    if not send:\n        send = URL('site')\n    if session.authorized:\n        redirect(send)\n    elif failed_login_count() >= allowed_number_of_attempts:\n        time.sleep(2 ** allowed_number_of_attempts)\n        raise HTTP(403)\n    elif request.vars.password:\n        if verify_password(request.vars.password[:1024]):\n            session.authorized = True\n            login_record(True)\n\n            if CHECK_VERSION:\n                session.check_version = True\n            else:\n                session.check_version = False\n\n            session.last_time = t0\n            if isinstance(send, list):  # ## why does this happen?\n                send = str(send[0])\n\n            redirect(send)\n        else:\n            times_denied = login_record(False)\n            if times_denied >= allowed_number_of_attempts:\n                response.flash = \\\n                    T('admin disabled because too many invalid login attempts')\n            elif times_denied == allowed_number_of_attempts - 1:\n                response.flash = \\\n                    T('You have one more login attempt before you are locked out')\n            else:\n                response.flash = T('invalid password.')\n    return dict(send=send)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def post(self):\n        \"\"\"This method handles the POST requests to add agents to the Cloud Verifier.\n\n        Currently, only agents resources are available for POSTing, i.e. /agents. All other POST uri's will return errors.\n        agents requests require a json block sent in the body\n        \"\"\"\n        session = get_session()\n        try:\n            rest_params = web_util.get_restful_params(self.request.uri)\n            if rest_params is None:\n                web_util.echo_json_response(\n                    self, 405, \"Not Implemented: Use /agents/ interface\")\n                return\n\n            if not web_util.validate_api_version(self, rest_params[\"api_version\"], logger):\n                return\n\n            if \"agents\" not in rest_params:\n                web_util.echo_json_response(self, 400, \"uri not supported\")\n                logger.warning('POST returning 400 response. uri not supported: %s', self.request.path)\n                return\n\n            agent_id = rest_params[\"agents\"]\n\n            if agent_id is not None:\n                # If the agent ID is not valid (wrong set of\n                # characters), just do nothing.\n                if not validators.valid_agent_id(agent_id):\n                    web_util.echo_json_response(self, 400, \"agent_id not not valid\")\n                    logger.error(\"POST received an invalid agent ID: %s\", agent_id)\n                    return\n\n                content_length = len(self.request.body)\n                if content_length == 0:\n                    web_util.echo_json_response(\n                        self, 400, \"Expected non zero content length\")\n                    logger.warning('POST returning 400 response. Expected non zero content length.')\n                else:\n                    json_body = json.loads(self.request.body)\n                    agent_data = {}\n                    agent_data['v'] = json_body['v']\n                    agent_data['ip'] = json_body['cloudagent_ip']\n                    agent_data['port'] = int(json_body['cloudagent_port'])\n                    agent_data['operational_state'] = states.START\n                    agent_data['public_key'] = \"\"\n                    agent_data['tpm_policy'] = json_body['tpm_policy']\n                    agent_data['meta_data'] = json_body['metadata']\n                    agent_data['allowlist'] = json_body['allowlist']\n                    agent_data['mb_refstate'] = json_body['mb_refstate']\n                    agent_data['ima_sign_verification_keys'] = json_body['ima_sign_verification_keys']\n                    agent_data['revocation_key'] = json_body['revocation_key']\n                    agent_data['accept_tpm_hash_algs'] = json_body['accept_tpm_hash_algs']\n                    agent_data['accept_tpm_encryption_algs'] = json_body['accept_tpm_encryption_algs']\n                    agent_data['accept_tpm_signing_algs'] = json_body['accept_tpm_signing_algs']\n                    agent_data['supported_version'] = json_body['supported_version']\n                    agent_data['hash_alg'] = \"\"\n                    agent_data['enc_alg'] = \"\"\n                    agent_data['sign_alg'] = \"\"\n                    agent_data['agent_id'] = agent_id\n                    agent_data['boottime'] = 0\n                    agent_data['ima_pcrs'] = []\n                    agent_data['pcr10'] = None\n                    agent_data['next_ima_ml_entry'] = 0\n                    agent_data['learned_ima_keyrings'] = {}\n                    agent_data['verifier_id'] = config.get('cloud_verifier', 'cloudverifier_id', fallback=cloud_verifier_common.DEFAULT_VERIFIER_ID)\n                    agent_data['verifier_ip'] = config.get('cloud_verifier', 'cloudverifier_ip')\n                    agent_data['verifier_port'] = config.get('cloud_verifier', 'cloudverifier_port')\n\n                    # We fetch the registrar data directly here because we require it for connecting to the agent\n                    # using mTLS\n                    registrar_client.init_client_tls('cloud_verifier')\n                    registrar_data = registrar_client.getData(config.get(\"cloud_verifier\", \"registrar_ip\"),\n                                                              config.get(\"cloud_verifier\", \"registrar_port\"), agent_id)\n                    if registrar_data is None:\n                        web_util.echo_json_response(self, 400,\n                                                    f\"Data for agent {agent_id} could not be found in registrar!\")\n                        logger.warning(\"Data for agent %s could not be found in registrar!\", agent_id)\n                        return\n\n                    agent_data['mtls_cert'] = registrar_data.get('mtls_cert', None)\n                    agent_data['ak_tpm'] = registrar_data['aik_tpm']\n\n                    # TODO: Always error for v1.0 version after initial upgrade\n                    if registrar_data.get('mtls_cert', None) is None and agent_data['supported_version'] != \"1.0\":\n                        web_util.echo_json_response(self, 400, \"mTLS certificate for agent is required!\")\n                        return\n\n                    is_valid, err_msg = cloud_verifier_common.validate_agent_data(agent_data)\n                    if not is_valid:\n                        web_util.echo_json_response(self, 400, err_msg)\n                        logger.warning(err_msg)\n                        return\n\n                    try:\n                        new_agent_count = session.query(\n                            VerfierMain).filter_by(agent_id=agent_id).count()\n                    except SQLAlchemyError as e:\n                        logger.error('SQLAlchemy Error: %s', e)\n                        raise e\n\n                    # don't allow overwriting\n\n                    if new_agent_count > 0:\n                        web_util.echo_json_response(\n                            self, 409, f\"Agent of uuid {agent_id} already exists\")\n                        logger.warning(\"Agent of uuid %s already exists\", agent_id)\n                    else:\n                        try:\n                            # Add the agent and data\n                            session.add(VerfierMain(**agent_data))\n                            session.commit()\n                        except SQLAlchemyError as e:\n                            logger.error('SQLAlchemy Error: %s', e)\n                            raise e\n\n                        # add default fields that are ephemeral\n                        for key,val in exclude_db.items():\n                            agent_data[key] = val\n\n                        # Prepare SSLContext for mTLS connections\n                        agent_mtls_cert_enabled = config.getboolean('cloud_verifier', 'agent_mtls_cert_enabled', fallback=False)\n                        mtls_cert = registrar_data.get('mtls_cert', None)\n                        agent_data['ssl_context'] = None\n                        if agent_mtls_cert_enabled and mtls_cert:\n                            agent_data['ssl_context'] = web_util.generate_agent_mtls_context(mtls_cert, self.mtls_options)\n\n                        if agent_data['ssl_context'] is None:\n                            logger.warning('Connecting to agent without mTLS: %s', agent_id)\n\n                        asyncio.ensure_future(\n                            process_agent(agent_data, states.GET_QUOTE))\n                        web_util.echo_json_response(self, 200, \"Success\")\n                        logger.info('POST returning 200 response for adding agent id: %s', agent_id)\n            else:\n                web_util.echo_json_response(self, 400, \"uri not supported\")\n                logger.warning(\"POST returning 400 response. uri not supported\")\n        except Exception as e:\n            web_util.echo_json_response(self, 400, f\"Exception error: {str(e)}\")\n            logger.warning(\"POST returning 400 response. Exception error: %s\", e)\n            logger.exception(e)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def __init__(self):\n    \"\"\"Initializes this class and attempts to create the graph.\n\n    This method attempts to create the graph through _construct_graph and\n    also creates all class variables that need to be populated by the override\n    function _construct_graph.\n    \"\"\"\n    self._graph = tf.Graph()\n    self._session = tf.compat.v1.Session(graph=self._graph)\n\n    # This lock is for  multi-threaded contexts where multiple threads\n    # share the same EvalSavedModel.\n    #\n    # Locking is required in the case where there are multiple threads using\n    # the same EvalMetricsGraph. Because the metrics variables are part of the\n    # session, and all threads share the same session, without a lock, the\n    # \"reset-update-get\" steps may not be atomic and there can be races.\n    #\n    # Having each thread have its own session would also work, but would\n    # require a bigger refactor.\n    # TODO(b/131727905): Investigate whether it's possible / better to have\n    # each thread have its own session.\n    self._lock = threading.Lock()\n\n    # Variables that need to be populated.\n\n    # The names of the metric.\n    self._metric_names = []\n\n    # Ops associated with reading and writing the metric variables.\n    self._metric_value_ops = []\n    self._metric_update_ops = []\n    self._metric_variable_assign_ops = []\n\n    # Nodes associated with the metric variables.\n    self._metric_variable_nodes = []\n\n    # Placeholders and feed input for the metric variables.\n    self._metric_variable_placeholders = []\n    self._perform_metrics_update_fn_feed_list = []\n    self._perform_metrics_update_fn_feed_list_keys = []\n\n    # Dict that maps Features Predictions Label keys to their tensors.\n    self._features_map = {}\n    self._predictions_map = {}\n    self._labels_map = {}\n\n    # Ops to set/update/reset all metric variables.\n    self._all_metric_variable_assign_ops = None\n    self._all_metric_update_ops = None\n    self._reset_variables_op = None\n\n    # Callable to perform metric update.\n    self._perform_metrics_update_fn = None\n\n    try:\n      self._construct_graph()\n    except (RuntimeError, TypeError, ValueError,\n            tf.errors.OpError) as exception:\n      general_util.reraise_augmented(exception, 'Failed to create graph.')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_helper(self, request: Request, member_id: int, token: str) -> ApiInviteHelper:\n        return ApiInviteHelper(request=request, member_id=member_id, instance=self, token=token)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def help_page(page_slug: str) -> str:\n    \"\"\"Fava's included documentation.\"\"\"\n    if page_slug not in HELP_PAGES:\n        abort(404)\n    html = markdown2.markdown_path(\n        (resource_path(\"help\") / (page_slug + \".md\")),\n        extras=[\"fenced-code-blocks\", \"tables\", \"header-ids\"],\n    )\n    return render_template(\n        \"_layout.html\",\n        active_page=\"help\",\n        page_slug=page_slug,\n        help_html=Markup(\n            render_template_string(\n                html,\n                beancount_version=beancount_version,\n                fava_version=fava_version,\n            )\n        ),\n        HELP_PAGES=HELP_PAGES,\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  static void launch(OpKernelContext* context, bool cudnn_use_autotune,\n                     const Tensor& input, const Tensor& filter,\n                     const std::array<int64, 3>& dilations,\n                     const std::array<int64, 3>& strides, const Padding padding,\n                     TensorFormat data_format, Tensor* output) {\n    OP_REQUIRES(context, data_format == FORMAT_NHWC,\n                errors::InvalidArgument(\"CPU implementation of Conv3D \"\n                                        \"currently only supports the NHWC \"\n                                        \"tensor format.\"));\n    OP_REQUIRES(context,\n                dilations[0] == 1 && dilations[1] == 1 && dilations[2] == 1,\n                errors::InvalidArgument(\"CPU implementation of Conv3D \"\n                                        \"currently only supports dilated rates \"\n                                        \"of 1.\"));\n    OP_REQUIRES(context, filter.dim_size(3) == input.dim_size(input.dims() - 1),\n                errors::InvalidArgument(\n                    \"Number of channels in filter (\", filter.dim_size(3),\n                    \") must match last dimension of input (\",\n                    input.dim_size(input.dims() - 1), \")\"));\n    functor::CuboidConvolution<CPUDevice, T>()(\n        context->eigen_device<CPUDevice>(), output->tensor<T, 5>(),\n        input.tensor<T, 5>(), filter.tensor<T, 5>(), strides[2], strides[1],\n        strides[0], BrainPadding2EigenPadding(padding));\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def datetime_cast_date_sql(self, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"DATE({sql})\", params\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def list_artifacts(self, path=None):\n        endpoint = \"/mlflow-artifacts/artifacts\"\n        url, tail = self.artifact_uri.split(endpoint, maxsplit=1)\n        root = tail.lstrip(\"/\")\n        params = {\"path\": posixpath.join(root, path) if path else root}\n        host_creds = _get_default_host_creds(url)\n        resp = http_request(host_creds, endpoint, \"GET\", params=params)\n        augmented_raise_for_status(resp)\n        file_infos = []\n        for f in resp.json().get(\"files\", []):\n            validate_path_is_safe(f[\"path\"])\n            file_info = FileInfo(\n                posixpath.join(path, f[\"path\"]) if path else f[\"path\"],\n                f[\"is_dir\"],\n                int(f[\"file_size\"]) if (\"file_size\" in f) else None,\n            )\n            file_infos.append(file_info)\n\n        return sorted(file_infos, key=lambda f: f.path)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def dump(self, user):\n        \"\"\"\n        Dump all sessions to debug log\n        \"\"\"\n        candidates = self._ss.get_user_sessions(user)\n\n        count = 0\n        for c in candidates:\n            key = c.keys()[0]\n            samlsession = self._data_to_samlsession(key, c[key])\n            self.debug('session %d: %s' % (count, samlsession.convert()))\n            count += 1\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def dump(self, user):\n        \"\"\"\n        Dump all sessions to debug log\n        \"\"\"\n        candidates = self._ss.get_user_sessions(user)\n\n        count = 0\n        for c in candidates:\n            key = c.keys()[0]\n            samlsession = self._data_to_samlsession(key, c[key])\n            self.debug('session %d: %s' % (count, samlsession.convert()))\n            count += 1\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def from_plist(self, content):\n        \"\"\"\n        Given some binary plist data, returns a Python dictionary of the decoded data.\n        \"\"\"\n        if biplist is None:\n            raise ImproperlyConfigured(\"Usage of the plist aspects requires biplist.\")\n\n        return biplist.readPlistFromString(content)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __format_attributes(elt_data):\n        \"\"\"\n            Private method which converts a single XML tag to a python dict.\n            It also checks that the elt_data given as argument is of type\n            xml.etree.ElementTree.Element\n\n            :param elt_data: XML Element to be parsed or string\n            to be converted to a XML Element\n\n            :return: Element\n        \"\"\"\n\n        rval = {}\n        if not et_iselement(elt_data):\n            raise NmapParserException(\n                \"Error while trying to parse supplied \"\n                \"data attributes: format is not XML or \"\n                \"XML tag is empty\"\n            )\n        try:\n            for dkey in elt_data.keys():\n                rval[dkey] = elt_data.get(dkey)\n                if rval[dkey] is None:\n                    raise NmapParserException(\n                        \"Error while trying to build-up \"\n                        \"element attributes: empty \"\n                        \"attribute {0}\".format(dkey)\n                    )\n        except Exception:\n            raise\n        return rval\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def delete(self, item):\n        \"\"\"\n        Deletes object\n        \"\"\"\n        raise NotImplementedError\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def main():\n    newrep = NmapParser.parse_fromfile(\n        \"libnmap/test/files/2_hosts_achange.xml\"\n    )\n    oldrep = NmapParser.parse_fromfile(\"libnmap/test/files/1_hosts.xml\")\n\n    print_diff(newrep, oldrep)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _setup(self):\n        cherrypy.request.hooks.attach('before_finalize', self._set_headers)\n        return super()._setup()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def install(self, path, b_temp_path):\n        if self.skip:\n            display.display(\"Skipping '%s' as it is already installed\" % to_text(self))\n            return\n\n        # Install if it is not\n        collection_path = os.path.join(path, self.namespace, self.name)\n        b_collection_path = to_bytes(collection_path, errors='surrogate_or_strict')\n        display.display(\"Installing '%s:%s' to '%s'\" % (to_text(self), self.latest_version, collection_path))\n\n        if self.b_path is None:\n            download_url = self._metadata.download_url\n            artifact_hash = self._metadata.artifact_sha256\n            headers = {}\n            self.api._add_auth_token(headers, download_url, required=False)\n\n            self.b_path = _download_file(download_url, b_temp_path, artifact_hash, self.api.validate_certs,\n                                         headers=headers)\n\n        if os.path.exists(b_collection_path):\n            shutil.rmtree(b_collection_path)\n        os.makedirs(b_collection_path)\n\n        try:\n            with tarfile.open(self.b_path, mode='r') as collection_tar:\n                files_member_obj = collection_tar.getmember('FILES.json')\n                with _tarfile_extract(collection_tar, files_member_obj) as files_obj:\n                    files = json.loads(to_text(files_obj.read(), errors='surrogate_or_strict'))\n\n                _extract_tar_file(collection_tar, 'MANIFEST.json', b_collection_path, b_temp_path)\n                _extract_tar_file(collection_tar, 'FILES.json', b_collection_path, b_temp_path)\n\n                for file_info in files['files']:\n                    file_name = file_info['name']\n                    if file_name == '.':\n                        continue\n\n                    if file_info['ftype'] == 'file':\n                        _extract_tar_file(collection_tar, file_name, b_collection_path, b_temp_path,\n                                          expected_hash=file_info['chksum_sha256'])\n                    else:\n                        os.makedirs(os.path.join(b_collection_path, to_bytes(file_name, errors='surrogate_or_strict')))\n        except Exception:\n            # Ensure we don't leave the dir behind in case of a failure.\n            shutil.rmtree(b_collection_path)\n\n            b_namespace_path = os.path.dirname(b_collection_path)\n            if not os.listdir(b_namespace_path):\n                os.rmdir(b_namespace_path)\n\n            raise\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "): { destroy: () => void; update: (t: () => TooltipContent) => void } {\n  let getter = text;"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def can_read_dag(dag_id: str) -> bool:\n    return get_airflow_app().appbuilder.sm.can_read_dag(dag_id, g.user)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def custom_login(self, request, extra_context=None):\n    next_url = (\n        request.GET.get(REDIRECT_FIELD_NAME)\n        or request.POST.get(REDIRECT_FIELD_NAME)\n        or reverse('admin:index')\n    )\n    return redirect_to_login(next_url)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def load(self, model, value):\n        \"\"\"\n        Converts unstructured data into structured data, recursively.\n        \"\"\"\n        try:\n            return self._cattrs_converter.structure(value, model)\n        except (ValueError, TypeError, ClassValidationError) as e:\n            raise SerializationException(str(e))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _download_file(  # noqa: WPS231, WPS213\n        self,\n        source_file: RemotePath,\n        local_file: LocalPath,\n        tmp_file: LocalPath | None,\n    ) -> tuple[FileDownloadStatus, RemotePath, LocalPath | None]:\n        if tmp_file:\n            log.info(\n                \"|%s| Downloading file '%s' to '%s' (via tmp '%s')\",\n                self.__class__.__name__,\n                source_file,\n                local_file,\n                tmp_file,\n            )\n        else:\n            log.info(\"|%s| Downloading file '%s' to '%s'\", self.__class__.__name__, source_file, local_file)\n\n        if not self.connection.path_exists(source_file):\n            log.warning(\"|%s| Missing file '%s', skipping\", self.__class__.__name__, source_file)\n            return FileDownloadStatus.MISSING, source_file, None\n\n        try:\n            remote_file = self.connection.resolve_file(source_file)\n\n            replace = False\n            if local_file.exists():\n                if self.options.if_exists == FileExistBehavior.ERROR:\n                    raise FileExistsError(f\"File {path_repr(local_file)} already exists\")\n\n                if self.options.if_exists == FileExistBehavior.IGNORE:\n                    log.warning(\"|Local FS| File %s already exists, skipping\", path_repr(local_file))\n                    return FileDownloadStatus.SKIPPED, remote_file, None\n\n                replace = True\n\n            if tmp_file:\n                # Files are loaded to temporary directory before moving them to target dir.\n                # This prevents operations with partly downloaded files\n\n                self.connection.download_file(remote_file, tmp_file, replace=replace)\n\n                # remove existing file only after new file is downloaded\n                # to avoid issues then there is no free space to download new file, but existing one is already gone\n                if replace and local_file.exists():\n                    log.warning(\"|Local FS| File %s already exists, overwriting\", path_repr(local_file))\n                    local_file.unlink()\n\n                local_file.parent.mkdir(parents=True, exist_ok=True)\n                shutil.move(tmp_file, local_file)\n            else:\n                # Direct download\n                self.connection.download_file(remote_file, local_file, replace=replace)\n\n            # Delete Remote\n            if self.options.delete_source:\n                self.connection.remove_file(remote_file)\n\n            return FileDownloadStatus.SUCCESSFUL, remote_file, local_file\n\n        except Exception as e:\n            if log.isEnabledFor(logging.DEBUG):\n                log.exception(\n                    \"|%s| Couldn't download file from source dir\",\n                    self.__class__.__name__,\n                    exc_info=e,\n                )\n            else:\n                log.exception(\n                    \"|%s| Couldn't download file from source dir: %s\",\n                    self.__class__.__name__,\n                    e,\n                    exc_info=False,\n                )\n            failed_file = FailedRemoteFile(\n                path=remote_file.path,\n                stats=remote_file.stats,\n                exception=e,\n            )\n            return FileDownloadStatus.FAILED, failed_file, None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status FillCollectiveParams(CollectiveParams* col_params,\n                              CollectiveType collective_type,\n                              const Tensor& group_size, const Tensor& group_key,\n                              const Tensor& instance_key) {\n    if (group_size.dims() > 0) {\n      return errors::InvalidArgument(\n          \"Unexpected dimensions on input group_size, got \",\n          group_size.shape().DebugString());\n    }\n    if (group_key.dims() > 0) {\n      return errors::InvalidArgument(\n          \"Unexpected dimensions on input group_key, got \",\n          group_key.shape().DebugString());\n    }\n    if (instance_key.dims() > 0) {\n      return errors::InvalidArgument(\n          \"Unexpected dimensions on input instance_key, got \",\n          instance_key.shape().DebugString());\n    }\n    col_params->name = name_;\n    col_params->group.device_type = device_type_;\n    col_params->group.group_size = group_size.unaligned_flat<int32>()(0);\n    if (col_params->group.group_size <= 0) {\n      return errors::InvalidArgument(\n          \"group_size must be positive integer but got \",\n          col_params->group.group_size);\n    }\n    col_params->group.group_key = group_key.unaligned_flat<int32>()(0);\n    col_params->instance.type = collective_type;\n    col_params->instance.instance_key = instance_key.unaligned_flat<int32>()(0);\n    col_params->instance.data_type = data_type_;\n    col_params->instance.impl_details.communication_hint = communication_hint_;\n    col_params->instance.impl_details.timeout_seconds = timeout_seconds_;\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "            active: user.hasOwnProperty(\"active\") ? !!user.active : true,\n            admin: user.hasOwnProperty(\"admin\") ? !!user.admin : false\n        };\n\n        return this.base.postJson(this.url(), data, opts);\n    };\n\n    OctoPrintAccessUsersClient.prototype.get = function (name, opts) {\n        if (!name) {\n            throw new OctoPrintClient.InvalidArgumentError(\"user name must be set\");\n        }\n\n        return this.base.get(this.url(name), opts);\n    };\n\n    OctoPrintAccessUsersClient.prototype.update = function (\n        name,\n        active,\n        admin,\n        permissions,\n        groups,\n        opts\n    ) {\n        if (!name) {\n            throw new OctoPrintClient.InvalidArgumentError(\"user name must be set\");\n        }\n\n        var data = {\n            active: !!active,\n            groups: groups,\n            permissions: permissions,\n            admin: !!admin\n        };\n        return this.base.putJson(this.url(name), data, opts);\n    };\n\n    OctoPrintAccessUsersClient.prototype.delete = function (name, opts) {\n        if (!name) {\n            throw new OctoPrintClient.InvalidArgumentError(\"user name must be set\");\n        }\n\n        return this.base.delete(this.url(name), opts);\n    };\n\n    OctoPrintAccessUsersClient.prototype.changePassword = function (\n        name,\n        password,\n        oldpw,\n        opts\n    ) {\n        if (_.isObject(oldpw)) {\n            opts = oldpw;\n            oldpw = undefined;\n        }\n\n        if (!name || !password) {\n            throw new OctoPrintClient.InvalidArgumentError(\n                \"user name and new password must be set\"\n            );\n        }\n\n        var data = {\n            password: password\n        };\n        if (oldpw) {\n            data[\"current\"] = oldpw;\n        }\n        return this.base.putJson(this.url(name, \"password\"), data, opts);\n    };\n\n    OctoPrintAccessUsersClient.prototype.generateApiKey = function (name, opts) {\n        if (!name) {\n            throw new OctoPrintClient.InvalidArgumentError(\"user name must be set\");\n        }\n\n        return this.base.postJson(this.url(name, \"apikey\"), opts);\n    };\n\n    OctoPrintAccessUsersClient.prototype.resetApiKey = function (name, opts) {\n        if (!name) {\n            throw new OctoPrintClient.InvalidArgumentError(\"user name must be set\");\n        }\n\n        return this.base.delete(this.url(name, \"apikey\"), opts);\n    };\n\n    OctoPrintAccessUsersClient.prototype.getSettings = function (name, opts) {\n        if (!name) {\n            throw new OctoPrintClient.InvalidArgumentError(\"user name must be set\");\n        }\n\n        return this.base.get(this.url(name, \"settings\"), opts);\n    };\n\n    OctoPrintAccessUsersClient.prototype.saveSettings = function (name, settings, opts) {\n        if (!name) {\n            throw new OctoPrintClient.InvalidArgumentError(\"user name must be set\");\n        }\n\n        settings = settings || {};\n        return this.base.patchJson(this.url(name, \"settings\"), settings, opts);\n    };\n\n    var OctoPrintAccessClient = function (base) {\n        this.base = base;\n\n        this.permissions = new OctoPrintAccessPermissionsClient(this);\n        this.groups = new OctoPrintAccessGroupsClient(this);\n        this.users = new OctoPrintAccessUsersClient(this);\n    };\n    OctoPrintClient.registerComponent(\"access\", OctoPrintAccessClient);\n    return OctoPrintAccessClient;\n});\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "            display: gettext(\"Site default\"),\n            english: undefined\n        };\n        self.locales = ko.observableArray(\n            [auto_locale].concat(\n                _.sortBy(_.values(AVAILABLE_LOCALES), function (n) {\n                    return n.display;\n                })\n            )\n        );\n        self.locale_languages = _.keys(AVAILABLE_LOCALES);\n\n        self.access_password = ko.observable(undefined);\n        self.access_repeatedPassword = ko.observable(undefined);\n        self.access_currentPassword = ko.observable(undefined);\n        self.access_currentPasswordMismatch = ko.observable(false);\n        self.access_apikey = ko.observable(undefined);\n        self.interface_language = ko.observable(undefined);\n\n        self.currentUser = ko.observable(undefined);\n        self.currentUser.subscribe(function (newUser) {\n            self.access_password(undefined);\n            self.access_repeatedPassword(undefined);\n            self.access_currentPassword(undefined);\n            self.access_currentPasswordMismatch(false);\n            self.access_apikey(undefined);\n            self.interface_language(\"_default\");\n\n            if (newUser !== undefined) {\n                self.access_apikey(newUser.apikey);\n                if (\n                    newUser.settings.hasOwnProperty(\"interface\") &&\n                    newUser.settings.interface.hasOwnProperty(\"language\")\n                ) {\n                    self.interface_language(newUser.settings.interface.language);\n                }\n            }\n        });\n        self.access_currentPassword.subscribe(function () {\n            self.access_currentPasswordMismatch(false);\n        });\n\n        self.passwordMismatch = ko.pureComputed(function () {\n            return self.access_password() !== self.access_repeatedPassword();\n        });\n\n        self.show = function (user) {\n            if (!CONFIG_ACCESS_CONTROL) return;\n\n            if (user === undefined) {\n                user = self.loginState.currentUser();\n            }\n\n            var process = function (user) {\n                self.currentUser(user);\n                self.userSettingsDialog.modal(\"show\");\n            };\n\n            // make sure we have the current user data, see #2534\n            OctoPrint.access.users\n                .get(user.name)\n                .done(function (data) {\n                    process(data);\n                })\n                .fail(function () {\n                    log.warn(\n                        \"Could not fetch current user data, proceeding with client side data copy\"\n                    );\n                    process(user);\n                });\n        };\n\n        self.save = function () {\n            if (!CONFIG_ACCESS_CONTROL) return;\n\n            self.userSettingsDialog.trigger(\"beforeSave\");\n\n            function process() {\n                var settings = {\n                    interface: {\n                        language: self.interface_language()\n                    }\n                };\n                self.updateSettings(self.currentUser().name, settings).done(function () {\n                    // close dialog\n                    self.currentUser(undefined);\n                    self.userSettingsDialog.modal(\"hide\");\n                    self.loginState.reloadUser();\n                });\n            }\n\n            if (self.access_password() && !self.passwordMismatch()) {\n                self.users\n                    .updatePassword(\n                        self.currentUser().name,\n                        self.access_password(),\n                        self.access_currentPassword()\n                    )\n                    .done(function () {\n                        process();\n                    })\n                    .fail(function (xhr) {\n                        if (xhr.status === 403) {\n                            self.access_currentPasswordMismatch(true);\n                        }\n                    });\n            } else {\n                process();\n            }\n        };\n\n        self.copyApikey = function () {\n            copyToClipboard(self.access_apikey());\n        };\n\n        self.generateApikey = function () {\n            if (!CONFIG_ACCESS_CONTROL) return;\n\n            var generate = function () {\n                self.users\n                    .generateApikey(self.currentUser().name)\n                    .done(function (response) {\n                        self.access_apikey(response.apikey);\n                    });\n            };\n\n            if (self.access_apikey()) {\n                showConfirmationDialog(\n                    gettext(\n                        \"This will generate a new API Key. The old API Key will cease to function immediately.\"\n                    ),\n                    generate\n                );\n            } else {\n                generate();\n            }\n        };\n\n        self.deleteApikey = function () {\n            if (!CONFIG_ACCESS_CONTROL) return;\n            if (!self.access_apikey()) return;\n\n            showConfirmationDialog(\n                gettext(\n                    \"This will delete the API Key. It will cease to to function immediately.\"\n                ),\n                function () {\n                    self.users.deleteApikey(self.currentUser().name).done(function () {\n                        self.access_apikey(undefined);\n                    });\n                }\n            );\n        };\n\n        self.updateSettings = function (username, settings) {\n            return OctoPrint.access.users.saveSettings(username, settings);\n        };\n\n        self.saveEnabled = function () {\n            return !self.passwordMismatch();\n        };\n\n        self.onStartup = function () {\n            self.userSettingsDialog = $(\"#usersettings_dialog\");\n        };\n\n        self.onAllBound = function (allViewModels) {\n            self.userSettingsDialog.on(\"show\", function () {\n                callViewModels(allViewModels, \"onUserSettingsShown\");\n            });\n            self.userSettingsDialog.on(\"hidden\", function () {\n                callViewModels(allViewModels, \"onUserSettingsHidden\");\n            });\n            self.userSettingsDialog.on(\"beforeSave\", function () {\n                callViewModels(allViewModels, \"onUserSettingsBeforeSave\");\n            });\n        };\n    }\n\n    OCTOPRINT_VIEWMODELS.push({\n        construct: UserSettingsViewModel,\n        dependencies: [\"loginStateViewModel\", \"accessViewModel\"],\n        elements: [\"#usersettings_dialog\"]\n    });\n});\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_integer(value):\n    return integer_validator(value)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_next_logout(self, peek=False,\n                        logout_mechs=None, user=None):\n        \"\"\""}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def project_version(request, client_id, project_name):\n    \"\"\"\n    get project deploy version\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :return: deploy version of project\n    \"\"\"\n    if request.method == 'GET':\n        # get client and project model\n        client = Client.objects.get(id=client_id)\n        project = Project.objects.get(name=project_name)\n        scrapyd = get_scrapyd(client)\n        # if deploy info exists in db, return it\n        if Deploy.objects.filter(client=client, project=project):\n            deploy = Deploy.objects.get(client=client, project=project)\n        # if deploy info does not exists in db, create deploy info\n        else:\n            try:\n                versions = scrapyd.list_versions(project_name)\n            except ConnectionError:\n                return JsonResponse({'message': 'Connect Error'}, status=500)\n            if len(versions) > 0:\n                version = versions[-1]\n                deployed_at = timezone.datetime.fromtimestamp(\n                    int(version), tz=pytz.timezone(TIME_ZONE))\n            else:\n                deployed_at = None\n            deploy, result = Deploy.objects.update_or_create(\n                client=client, project=project, deployed_at=deployed_at)\n        # return deploy json info\n        return JsonResponse(model_to_dict(deploy))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def error_page(request,message):\n   \"\"\"\n   This page is used to explain error messages to the user.\n   \"\"\"\n   if not test_user_authenticated(request): return login(request)\n   # FIXME: test and make sure we use this rather than throwing lots of tracebacks for\n   # field errors\n   t = get_template('error_page.tmpl')\n   message = message.replace(\"<Fault 1: \\\"<class 'cobbler.cexceptions.CX'>:'\",\"Remote exception: \")\n   message = message.replace(\"'\\\">\",\"\")\n   html = t.render(RequestContext(request,{\n       'version' : remote.version(request.session['token']),\n       'message' : message,\n       'username': username\n   }))\n   return HttpResponse(html)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def topic(self, topic):\n            \"\"\" Use mix of JSON and compact format to insert forged claims including long expiration \"\"\"\n            [header, payload, signature] = topic.split('.')\n            parsed_payload = loads(base64url_decode(payload))\n            parsed_payload['sub'] = 'bob'\n            parsed_payload['exp'] = 2000000000\n            fake_payload = base64url_encode((dumps(parsed_payload, separators=(',', ':'))))\n            return '{\"  ' + header + '.' + fake_payload + '.\":\"\",\"protected\":\"' + header + '\", \"payload\":\"' + payload + '\",\"signature\":\"' + signature + '\"}'\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def stop_servers(self):\n        global RESPONSE_QUEUE\n        RESPONSE_QUEUE.put(QUEUE_SENTINEL)\n        RESPONSE_QUEUE = queue.Queue()  # recreate so existing queue closes watchers but we don't have to wait here\n        while True:\n            try:\n                REQUEST_QUEUE.get(block=False)  # remove any pending requests (unlikely any exist, but safest)\n            except queue.Empty:\n                break\n        for proxy in self.proxies:\n            # noinspection PyBroadException\n            try:\n                proxy.stop()\n            except Exception:\n                pass\n        self.proxies = []\n        self.authorisation_requests = []  # these requests are no-longer valid\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def test_parse_recipe_url(self, url: ScrapeRecipeTest):\n        # Debugger should produce the same result as the scraper sees before cleaning\n        try:\n            if scraped_data := RecipeScraperPackage(url.url).scrape_url():\n                return scraped_data.schema.data\n        except ForceTimeoutException as e:\n            raise HTTPException(\n                status_code=408, detail=ErrorResponse.respond(message=\"Recipe Scraping Timed Out\")\n            ) from e\n\n        return \"recipe_scrapers was unable to scrape this URL\"\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def set_show_admin_dashboard_link(endpoint, values):\n    \"\"\"Sets the \"show_admin_dashboard_link\" variable application wide\n    in order to use it in the layout template.\n    \"\"\"\n\n    g.show_admin_dashboard_link = (\n        current_app.config[\"ACTIVATE_ADMIN_DASHBOARD\"]\n        and current_app.config[\"ADMIN_PASSWORD\"]\n    )\n    g.logout_form = LogoutForm()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def test_verify_zip_with_small_file_size_limit(self, zip_file):\n        \"\"\"We set the max file size to 1 byte, so the zip file should be rejected.\"\"\"\n        with pytest.raises(ValueError) as exc:\n            verify_zip(ZipFile(zip_file), 1)\n        assert \"File size exceeds maximum allowed size\" in str(exc.value)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse_meta(self, yaml_str):\n        \"\"\"Parse meta from yaml string, and validate yaml filed, return dict\"\"\"\n        try:\n            meta = yaml.load(yaml_str, Loader=yaml.FullLoader)\n        except yaml.YAMLError as e:\n            e.extra_msg = 'yaml format error'\n            raise\n\n        category, src_fname = self.get_category_and_file()\n        dst_fname = src_fname.replace(\n            \".{0}\".format(self.site_config['default_ext']), '.html')\n        meta.update({'category': category, 'filename': dst_fname})\n\n        if 'tag' in meta:\n            if isinstance(meta['tag'], basestring):\n                _tags = [t.strip() for t in meta['tag'].split(',')]\n                meta.update({'tag': _tags})\n\n        if \"title\" not in meta:\n            raise Exception(\"no 'title' in meta\")\n\n        return meta\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def check_alias(self, alias):\n        if FORBIDDEN_ALIAS_PATTERN.search(alias):\n            raise ValueError(\n                \"Column aliases cannot contain whitespace characters, quotation marks, \"\n                \"semicolons, or SQL comments.\"\n            )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def dump(data, stream=None, Dumper=Dumper, **kwds):\n    \"\"\"\n    Serialize a Python object into a YAML stream.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\nsafe_dump = dump"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def remote_static():\n    \"\"\"Stream *large* static files with special requirements.\"\"\"\n    file_path = request.args.get(\"file\") or \".\"\n\n    # Check that user is logged in or that file extension is valid\n    if controllers.check_session_tracks(file_path) is False:\n        return abort(403)\n\n    range_header = request.headers.get(\"Range\", None)\n    if not range_header and (file_path.endswith(\".bam\") or file_path.endswith(\".cram\")):\n        return abort(500)\n\n    new_resp = send_file_partial(file_path)\n    return new_resp\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def download_list(request):\n    downloads = Download.objects.all()\n    if request.user.is_authenticated:\n        downloads = downloads.filter(Q(is_public=True) | Q(users=request.user))\n    else:\n        downloads = downloads.filter(is_public=True)\n    return render(request, 'download/download_list.html',\n                  {'download_list': downloads})\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "            def close(self):\n                pass\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def model_from_config(config, custom_objects=None):\n  \"\"\"Instantiates a Keras model from its config.\n\n  Usage:\n  ```\n  # for a Functional API model\n  tf.keras.Model().from_config(model.get_config())\n\n  # for a Sequential model\n  tf.keras.Sequential().from_config(model.get_config())\n  ```\n\n  Args:\n      config: Configuration dictionary.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n\n  Returns:\n      A Keras model instance (uncompiled).\n\n  Raises:\n      TypeError: if `config` is not a dictionary.\n  \"\"\"\n  if isinstance(config, list):\n    raise TypeError('`model_from_config` expects a dictionary, not a list. '\n                    'Maybe you meant to use '\n                    '`Sequential.from_config(config)`?')\n  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\n  return deserialize(config, custom_objects=custom_objects)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self):\n        self.catalog_api = Manager()\n        self.identity_api = identity.Manager()\n        self.policy_api = policy.Manager()\n        self.token_api = token.Manager()\n        super(ServiceController, self).__init__()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "export async function requestTranslationsAPI<T>(\n  translationsUrl: string = '',\n  locale = '',\n  init: RequestInit = {},\n  serverSettings: ServerConnection.ISettings | undefined = undefined\n): Promise<T> {\n  // Make request to Jupyter API\n  const settings = serverSettings ?? ServerConnection.makeSettings();\n  translationsUrl =\n    translationsUrl || `${settings.appUrl}/${TRANSLATIONS_SETTINGS_URL}`;\n  const translationsBase = URLExt.join(settings.baseUrl, translationsUrl);\n  const requestUrl = URLExt.join(translationsBase, locale);\n  if (!requestUrl.startsWith(translationsBase)) {\n    throw new Error('Can only be used for translations requests');\n  }\n  let response: Response;\n  try {\n    response = await ServerConnection.makeRequest(requestUrl, init, settings);\n  } catch (error) {\n    throw new ServerConnection.NetworkError(error);\n  }\n\n  let data: any = await response.text();\n\n  if (data.length > 0) {\n    try {\n      data = JSON.parse(data);\n    } catch (error) {\n      console.error('Not a JSON response body.', response);\n    }\n  }\n\n  if (!response.ok) {\n    throw new ServerConnection.ResponseError(response, data.message || data);\n  }\n\n  return data;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def BounceMessageVerifier(*args, **kwargs):\n    warnings.warn(\n        \"utils.BounceMessageVerifier is deprecated. It is renamed to EventMessageVerifier.\",\n        RemovedInDjangoSES20Warning,\n    )\n\n    # parameter name is renamed from bounce_dict to notification.\n    if \"bounce_dict\" in kwargs:\n        kwargs[\"notification\"] = kwargs[\"bounce_dict\"]\n        del kwargs[\"bounce_dict\"]\n\n    return EventMessageVerifier(*args, **kwargs)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, socketio, mail, api, permissions, config):\n        super().__init__(socketio, mail, api, permissions, config)\n        self.r: RuleCollection = getattr(self.permissions, module_name)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "            def close(self):\n                pass\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def inner(*a, **k):\n            task = task_wrapper.s(*a, **k)\n            def enqueue_on_commit():\n                task_wrapper.huey.enqueue(task)\n            transaction.on_commit(enqueue_on_commit)\n            return HUEY._result_handle(task)\n        return inner"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _resolve_future(request_id, fut, result, log):\n    if fut is None:\n        log.warning('resolved unknown request: %r', request_id)\n        return\n    if fut.done():\n        if fut.cancelled():\n            log.debug('resolved cancelled request: %r', request_id)\n        elif fut.exception() is not None:\n            log.debug('resolved errored request: %r', request_id)\n        return\n    if isinstance(result, BaseException):\n        fut.set_exception(result)\n    else:\n        fut.set_result(result)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "inline int TensorProtoDataSize<qint32>(const TensorProto& t) {\n  return t.int_val_size();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def load(stream, Loader=Loader):\n    \"\"\"\n    Parse the first YAML document in a stream\n    and produce the corresponding Python object.\n\n    By default resolve only basic YAML tags, if an alternate Loader is\n    provided, may be dangerous.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        return loader.get_single_data()\n    finally:\n        loader.dispose()\nsafe_load = load"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def load_all(stream, Loader=Loader):\n    \"\"\"\n    Parse all YAML documents in a stream\n    and produce corresponding Python objects.\n\n    By default resolve only basic YAML tags, if an alternate Loader is\n    provided, may be dangerous.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        while loader.check_data():\n            yield loader.get_data()\n    finally:\n        loader.dispose()\nsafe_load_all = load_all"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def main():\n    \"\"\"Abilian Dev Tool command-line runner.\"\"\"\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  bool IsSupported(const NodeDef* node) const override {\n    if (!node || node->input_size() < 2) {\n      // Invalid node\n      return false;\n    }\n\n    return IsAnyMul(*node) && node->input(0) == node->input(1);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def post(self, request, class_path=None, pk=None):\n        job_model = self._get_job_model_or_404(class_path, pk)\n\n        job_form = job_model.job_class.as_form(request.POST, request.FILES) if job_model.job_class is not None else None\n        schedule_form = forms.JobScheduleForm(request.POST)\n        task_queue = request.POST.get(\"_task_queue\")\n\n        return_url = request.POST.get(\"_return_url\")\n        if return_url is not None and url_has_allowed_host_and_scheme(url=return_url, allowed_hosts=request.get_host()):\n            return_url = iri_to_uri(return_url)\n        else:\n            return_url = None\n\n        # Allow execution only if a worker process is running and the job is runnable.\n        if not get_worker_count(queue=task_queue):\n            messages.error(request, \"Unable to run or schedule job: Celery worker process not running.\")\n        elif not job_model.installed or job_model.job_class is None:\n            messages.error(request, \"Unable to run or schedule job: Job is not presently installed.\")\n        elif not job_model.enabled:\n            messages.error(request, \"Unable to run or schedule job: Job is not enabled to be run.\")\n        elif (\n            job_model.has_sensitive_variables\n            and request.POST.get(\"_schedule_type\") != JobExecutionType.TYPE_IMMEDIATELY\n        ):\n            messages.error(request, \"Unable to schedule job: Job may have sensitive input variables.\")\n        elif job_model.has_sensitive_variables and job_model.approval_required:\n            messages.error(\n                request,\n                \"Unable to run or schedule job: \"\n                \"This job is flagged as possibly having sensitive variables but is also flagged as requiring approval.\"\n                \"One of these two flags must be removed before this job can be scheduled or run.\",\n            )\n        elif job_form is not None and job_form.is_valid() and schedule_form.is_valid():\n            task_queue = job_form.cleaned_data.pop(\"_task_queue\", None)\n            dryrun = job_form.cleaned_data.get(\"dryrun\", False)\n            # Run the job. A new JobResult is created.\n            profile = job_form.cleaned_data.pop(\"_profile\")\n            schedule_type = schedule_form.cleaned_data[\"_schedule_type\"]\n\n            if (not dryrun and job_model.approval_required) or schedule_type in JobExecutionType.SCHEDULE_CHOICES:\n                crontab = \"\"\n\n                if schedule_type == JobExecutionType.TYPE_IMMEDIATELY:\n                    # The job must be approved.\n                    # If the schedule_type is immediate, we still create the task, but mark it for approval\n                    # as a once in the future task with the due date set to the current time. This means\n                    # when approval is granted, the task is immediately due for execution.\n                    schedule_type = JobExecutionType.TYPE_FUTURE\n                    schedule_datetime = timezone.now()\n                    schedule_name = f\"{job_model} - {schedule_datetime}\"\n\n                else:\n                    schedule_name = schedule_form.cleaned_data[\"_schedule_name\"]\n\n                    if schedule_type == JobExecutionType.TYPE_CUSTOM:\n                        crontab = schedule_form.cleaned_data[\"_recurrence_custom_time\"]\n                        # doing .get(\"key\", \"default\") returns None instead of \"default\" here for some reason\n                        schedule_datetime = schedule_form.cleaned_data.get(\"_schedule_start_time\")\n                        if schedule_datetime is None:\n                            # \"_schedule_start_time\" is checked against ScheduledJob.earliest_possible_time()\n                            # which returns timezone.now() + timedelta(seconds=15)\n                            schedule_datetime = timezone.now() + timedelta(seconds=20)\n                    else:\n                        schedule_datetime = schedule_form.cleaned_data[\"_schedule_start_time\"]\n\n                celery_kwargs = {\"nautobot_job_profile\": profile, \"queue\": task_queue}\n                scheduled_job = ScheduledJob(\n                    name=schedule_name,\n                    task=job_model.job_class.registered_name,\n                    job_model=job_model,\n                    start_time=schedule_datetime,\n                    description=f\"Nautobot job {schedule_name} scheduled by {request.user} for {schedule_datetime}\",\n                    kwargs=job_model.job_class.serialize_data(job_form.cleaned_data),\n                    celery_kwargs=celery_kwargs,\n                    interval=schedule_type,\n                    one_off=schedule_type == JobExecutionType.TYPE_FUTURE,\n                    queue=task_queue,\n                    user=request.user,\n                    approval_required=job_model.approval_required,\n                    crontab=crontab,\n                )\n                scheduled_job.validated_save()\n\n                if job_model.approval_required:\n                    messages.success(request, f\"Job {schedule_name} successfully submitted for approval\")\n                    return redirect(return_url if return_url else \"extras:scheduledjob_approval_queue_list\")\n                else:\n                    messages.success(request, f\"Job {schedule_name} successfully scheduled\")\n                    return redirect(return_url if return_url else \"extras:scheduledjob_list\")\n\n            else:\n                # Enqueue job for immediate execution\n                job_kwargs = job_model.job_class.prepare_job_kwargs(job_form.cleaned_data)\n                job_result = JobResult.enqueue_job(\n                    job_model,\n                    request.user,\n                    profile=profile,\n                    task_queue=task_queue,\n                    **job_model.job_class.serialize_data(job_kwargs),\n                )\n\n                if return_url:\n                    messages.info(\n                        request,\n                        format_html(\n                            'Job enqueued. <a href=\"{}\">Click here for the results.</a>',\n                            job_result.get_absolute_url(),\n                        ),\n                    )\n                    return redirect(return_url)\n\n                return redirect(\"extras:jobresult\", pk=job_result.pk)\n\n        if return_url:\n            return redirect(return_url)\n\n        template_name = \"extras/job.html\"\n        if job_model.job_class is not None and hasattr(job_model.job_class, \"template_name\"):\n            try:\n                get_template(job_model.job_class.template_name)\n                template_name = job_model.job_class.template_name\n            except TemplateDoesNotExist as err:\n                messages.error(request, f'Unable to render requested custom job template \"{template_name}\": {err}')\n\n        return render(\n            request,\n            template_name,\n            {\n                \"job_model\": job_model,\n                \"job_form\": job_form,\n                \"schedule_form\": schedule_form,\n            },\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def inner(fn):\n        @functools.wraps(fn)\n        def wrapper(self, password, *args, **kwargs):\n            if len(password) > max_length:\n                raise ValueError(\"Invalid password; Must be less than or equal\"\n                                 \" to %d bytes\" % max_length)\n            return fn(self, password, *args, **kwargs)\n        return wrapper\n    return inner"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def page_from_reference(href):\n    \"\"\"\n    Returns a tuple of the HTML page built and the new current word\n\n    :param href: The hypertext reference to be solved\n    :type href: str\n    :return: A tuple (page,word), where page is the new current HTML page\n             to be sent to the browser and\n             word is the new current word\n    :rtype: A tuple (str,str)\n    \"\"\"\n    word = href.word\n    pos_forms = defaultdict(list)\n    words = word.split(\",\")\n    words = [w for w in [w.strip().lower().replace(\" \", \"_\") for w in words] if w != \"\"]\n    if len(words) == 0:\n        # No words were found.\n        return \"\", \"Please specify a word to search for.\"\n\n    # This looks up multiple words at once.  This is probably not\n    # necessary and may lead to problems.\n    for w in words:\n        for pos in [wn.NOUN, wn.VERB, wn.ADJ, wn.ADV]:\n            form = wn.morphy(w, pos)\n            if form and form not in pos_forms[pos]:\n                pos_forms[pos].append(form)\n    body = \"\"\n    for pos, pos_str, name in _pos_tuples():\n        if pos in pos_forms:\n            body += _hlev(3, name) + \"\\n\"\n            for w in pos_forms[pos]:\n                # Not all words of exc files are in the database, skip\n                # to the next word if a KeyError is raised.\n                try:\n                    body += _collect_all_synsets(w, pos, href.synset_relations)\n                except KeyError:\n                    pass\n    if not body:\n        body = \"The word or words '%s' were not found in the dictionary.\" % word\n    return body, word\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void LerpKernel(const Context& ctx,\n                const DenseTensor& x,\n                const DenseTensor& y,\n                const DenseTensor& weight,\n                DenseTensor* out) {\n  PADDLE_ENFORCE_GT(\n      x.numel(),\n      0,\n      phi::errors::InvalidArgument(\"LerpKernel's input x must not empyt.\"));\n\n  PADDLE_ENFORCE_GT(\n      y.numel(),\n      0,\n      phi::errors::InvalidArgument(\"LerpKernel's input y must not empyt.\"));\n\n  int rank = out->dims().size();\n  PADDLE_ENFORCE_GE(\n      rank,\n      0,\n      phi::errors::InvalidArgument(\n          \"The number of dimensions for LerpOp must be \"\n          \"greater than or equal to 0, but the value received is %d.\",\n          rank));\n  PADDLE_ENFORCE_LE(\n      rank,\n      6,\n      phi::errors::InvalidArgument(\n          \"The number of dimensions for LerpOp must be \"\n          \"less than or equal to 6, but the value received is %d.\",\n          rank));\n  switch (rank) {\n    case 0:\n      LerpFunctionZero<Context, T>(ctx, x, y, weight, out);\n      break;\n    case 1:\n      LerpFunction<Context, T, 1>(ctx, x, y, weight, out);\n      break;\n    case 2:\n      LerpFunction<Context, T, 2>(ctx, x, y, weight, out);\n      break;\n    case 3:\n      LerpFunction<Context, T, 3>(ctx, x, y, weight, out);\n      break;\n    case 4:\n      LerpFunction<Context, T, 4>(ctx, x, y, weight, out);\n      break;\n    case 5:\n      LerpFunction<Context, T, 5>(ctx, x, y, weight, out);\n      break;\n    case 6:\n      LerpFunction<Context, T, 6>(ctx, x, y, weight, out);\n      break;\n  }\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def decryption_materials(self, encryption_context):\n        # type: (EncryptionContext) -> CryptographicMaterials\n        \"\"\"Return decryption materials.\n\n        :param EncryptionContext encryption_context: Encryption context for request\n        :raises AttributeError: if no decryption materials are available\n        \"\"\"\n        version = self._provider_store.version_from_material_description(encryption_context.material_description)\n        try:\n            _LOGGER.debug(\"Looking in cache for decryption materials provider version %d\", version)\n            _, provider = self._cache.get(version)\n        except KeyError:\n            _LOGGER.debug(\"Decryption materials provider not found in cache\")\n            try:\n                provider = self._provider_store.provider(self._material_name, version)\n                self._cache.put(version, (time.time(), provider))\n            except InvalidVersionError:\n                _LOGGER.exception(\"Unable to get decryption materials from provider store.\")\n                raise AttributeError(\"No decryption materials available\")\n\n        return provider.decryption_materials(encryption_context)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def decorator(fn):\n        task_wrapper = task(*args, **kwargs)(close_db(fn))\n\n        @wraps(fn)\n        def inner(*a, **k):\n            task = task_wrapper.s(*a, **k)\n            def enqueue_on_commit():\n                task_wrapper.huey.enqueue(task)\n            transaction.on_commit(enqueue_on_commit)\n            return HUEY._result_handle(task)\n        return inner\n    return decorator"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def version(self, extended=False):\n        \"\"\"\n        What version is cobbler?\n\n        If extended == False, returns a float for backwards compatibility\n         \n        If extended == True, returns a dict:\n\n            gitstamp      -- the last git commit hash\n            gitdate       -- the last git commit date on the builder machine\n            builddate     -- the time of the build\n            version       -- something like \"1.3.2\"\n            version_tuple -- something like [ 1, 3, 2 ]\n        \"\"\"\n        fd = open(\"/etc/cobbler/version\")\n        ydata = fd.read()\n        fd.close()\n        data = yaml.safe_load(ydata)\n        if not extended:\n            # for backwards compatibility and use with koan's comparisons\n            elems = data[\"version_tuple\"] \n            return int(elems[0]) + 0.1*int(elems[1]) + 0.001*int(elems[2])\n        else:\n            return data\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def test_app(environ, start_response):\n            \"\"\"A WSGI app that just reflects its HTTP environ.\"\"\"\n            start_response('200 OK', [])\n            http_environ_items = sorted(\n                '%s:%s' % (k, v) for k, v in environ.items()\n                if k.startswith('HTTP_')\n            )\n            yield (','.join(http_environ_items)).encode('utf-8')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const auto* params =\n      reinterpret_cast<const TfLiteGatherParams*>(node->builtin_data);\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* positions;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputPositions, &positions));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  if (positions->type == kTfLiteInt32) {\n    switch (input->type) {\n      case kTfLiteFloat32:\n        return Gather<float, int32_t>(context, *params, input, positions,\n                                      output);\n      case kTfLiteUInt8:\n        return Gather<uint8_t, int32_t>(context, *params, input, positions,\n                                        output);\n      case kTfLiteInt8:\n        return Gather<int8_t, int32_t>(context, *params, input, positions,\n                                       output);\n      case kTfLiteInt16:\n        return Gather<int16_t, int32_t>(context, *params, input, positions,\n                                        output);\n      case kTfLiteInt32:\n        return Gather<int32_t, int32_t>(context, *params, input, positions,\n                                        output);\n      case kTfLiteInt64:\n        return Gather<int64_t, int32_t>(context, *params, input, positions,\n                                        output);\n      case kTfLiteBool:\n        return Gather<bool, int32_t>(context, *params, input, positions,\n                                     output);\n      case kTfLiteString:\n        return GatherStrings<int32_t>(context, input, positions, output);\n      default:\n        context->ReportError(context, \"Type '%s' is not supported by gather.\",\n                             TfLiteTypeGetName(input->type));\n        return kTfLiteError;\n    }\n  }\n  if (positions->type == kTfLiteInt64) {\n    switch (input->type) {\n      case kTfLiteFloat32:\n        return Gather<float, int64_t>(context, *params, input, positions,\n                                      output);\n      case kTfLiteUInt8:\n        return Gather<uint8_t, int64_t>(context, *params, input, positions,\n                                        output);\n      case kTfLiteInt8:\n        return Gather<int8_t, int64_t>(context, *params, input, positions,\n                                       output);\n      case kTfLiteInt16:\n        return Gather<int16_t, int64_t>(context, *params, input, positions,\n                                        output);\n      case kTfLiteInt32:\n        return Gather<int32_t, int64_t>(context, *params, input, positions,\n                                        output);\n      case kTfLiteInt64:\n        return Gather<int64_t, int64_t>(context, *params, input, positions,\n                                        output);\n      case kTfLiteBool:\n        return Gather<bool, int64_t>(context, *params, input, positions,\n                                     output);\n      case kTfLiteString:\n        return GatherStrings<int64_t>(context, input, positions, output);\n      default:\n        context->ReportError(context, \"Type '%s' is not supported by gather.\",\n                             TfLiteTypeGetName(input->type));\n        return kTfLiteError;\n    }\n  }\n  context->ReportError(context,\n                       \"Positions of type '%s' are not supported by gather.\",\n                       TfLiteTypeGetName(positions->type));\n  return kTfLiteError;\n}\n}  // namespace gather"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def datetime_extract_sql(self, lookup_type, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return self.date_extract_sql(lookup_type, sql, params)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def escape(data):\n    \"\"\"\n    Escape user-provided input data for safe inclusion in HTML _and_ JS to prevent XSS.\n    \"\"\"\n    data = data.replace('&', '&amp;')\n    data = data.replace('>', '&gt;')\n    data = data.replace('<', '&lt;')\n    data = data.replace(\"'\", '')\n    data = data.replace('\"', '')\n    return data\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def sanitize_html(value):\n    \"\"\"Sanitizes HTML using the bleach library.\"\"\"\n    return bleach.clean(\n        value,\n        tags=current_app.config['COMMUNITIES_ALLOWED_TAGS'],\n        attributes=current_app.config['COMMUNITIES_ALLOWED_ATTRS'],\n        strip=True,\n    ).strip()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def contains_path_separator(path):\n    \"\"\"\n    Returns True if a path contains a path separator, False otherwise.\n    \"\"\"\n    return any((sep in path) for sep in (os.path.sep, os.path.altsep) if sep is not None)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, url):\n        self.url = url\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _process(tlist):\n        def get_next_comment():\n            # TODO(andi) Comment types should be unified, see related issue38\n            return tlist.token_next_by(i=sql.Comment, t=T.Comment)\n\n        def _get_insert_token(token):\n            \"\"\"Returns either a whitespace or the line breaks from token.\"\"\"\n            # See issue484 why line breaks should be preserved.\n            # Note: The actual value for a line break is replaced by \\n\n            # in SerializerUnicode which will be executed in the\n            # postprocessing state.\n            m = re.search(r'((\\r|\\n)+) *$', token.value)\n            if m is not None:\n                return sql.Token(T.Whitespace.Newline, m.groups()[0])\n            else:\n                return sql.Token(T.Whitespace, ' ')\n\n        tidx, token = get_next_comment()\n        while token:\n            pidx, prev_ = tlist.token_prev(tidx, skip_ws=False)\n            nidx, next_ = tlist.token_next(tidx, skip_ws=False)\n            # Replace by whitespace if prev and next exist and if they're not\n            # whitespaces. This doesn't apply if prev or next is a parenthesis.\n            if (prev_ is None or next_ is None\n                    or prev_.is_whitespace or prev_.match(T.Punctuation, '(')\n                    or next_.is_whitespace or next_.match(T.Punctuation, ')')):\n                # Insert a whitespace to ensure the following SQL produces\n                # a valid SQL (see #425).\n                if prev_ is not None and not prev_.match(T.Punctuation, '('):\n                    tlist.tokens.insert(tidx, _get_insert_token(token))\n                tlist.tokens.remove(token)\n            else:\n                tlist.tokens[tidx] = _get_insert_token(token)\n\n            tidx, token = get_next_comment()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_readable_dags() -> list[str]:\n    return get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def count(cls, **kwargs):\n        \"\"\"Return a count of server side resources given\n        filtering arguments in kwargs.\n        \"\"\"\n        url = recurly.base_uri() + cls.collection_path\n        if kwargs:\n            url = '%s?%s' % (url, urlencode(kwargs))\n        return Page.count_for_url(url)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def send(self, method, path, payload=None, headers=None):\n        \"\"\"Make a request to the API endpoint and return a JSON response.\n\n        This method will construct a URL using :param:`path` and make a request\n        to that endpoint using HTTP method set in :param:`method` with\n        necessary headers set. The JSON returned from the API server will\n        be serialized into :type:`dict`.\n\n        If :param:`payload` is given, that payload will be passed alongside\n        the request regardless of HTTP method used. Custom headers are\n        assignable via :param:`headers`, however please note that ``Accept``,\n        ``Content-Type`` and ``User-Agent`` are not overridable.\n\n        In case the response is an error, an appropriate exception from\n        :module:`omise.errors` will be raised. The application code is\n        responsible for handling these exceptions.\n\n        :param method: method for the request.\n        :param path: path components relative to API base to make the request.\n        :param payload: (optional) dict containing data to send to the server.\n        :param headers: (optional) dict containing custom headers.\n\n        :type method: str\n        :type path: str or list or tuple\n        :type payload: dict or None\n        :type headers: dict or None\n        :rtype: dict\n        \"\"\"\n        request_path = self._build_path(path)\n        request_payload = self._build_payload(payload)\n        request_headers = self._build_headers(headers)\n\n        logger.info('Sending HTTP request: %s %s', method.upper(), request_path)\n        logger.debug('Payload: %s', request_payload)\n        logger.debug('Headers: %s', request_headers)\n\n        response = getattr(requests, method)(\n            request_path,\n            data=request_payload,\n            headers=request_headers,\n            auth=(self.api_key, '')\n        ).json()\n\n        logger.info('Received HTTP response: %s', response)\n\n        if response.get('object') == 'error':\n            errors._raise_from_data(response)\n        return response\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def initialize(request, wordlist):\n    \"\"\"\n    handles parameter finding process for a single request object\n    returns 'skipped' (on error), list on success\n    \"\"\"\n    url = request['url']\n    if not url.startswith('http'):\n        print('%s %s is not a valid URL' % (bad, url))\n        return 'skipped'\n    print('%s Probing the target for stability' % run)\n    request['url'] = stable_request(url, request['headers'])\n    if not request['url']:\n        return 'skipped'\n    else:\n        fuzz = random_str(6)\n        response_1 = requester(request, {fuzz: fuzz[::-1]})\n        print('%s Analysing HTTP response for anomalies' % run)\n        fuzz = random_str(6)\n        response_2 = requester(request, {fuzz: fuzz[::-1]})\n        if type(response_1) == str or type(response_2) == str:\n            return 'skipped'\n        factors = define(response_1, response_2, fuzz, fuzz[::-1], wordlist)\n        print('%s Analysing HTTP response for potential parameter names' % run)\n        found = heuristic(response_1.text, wordlist)\n        if found:\n            num = len(found)\n            s = 's' if num > 1 else ''\n            print('%s Heuristic scanner found %i parameter%s: %s' % (good, num, s, ', '.join(found)))\n        print('%s Logicforcing the URL endpoint' % run)\n        populated = populate(wordlist)\n        param_groups = slicer(populated, int(len(wordlist)/mem.var['chunks']))\n        last_params = []\n        while True:\n            param_groups = narrower(request, factors, param_groups)\n            if mem.var['kill']:\n                return 'skipped'\n            param_groups = confirm(param_groups, last_params)\n            if not param_groups:\n                break\n        confirmed_params = []\n        for param in last_params:\n            reason = bruter(request, factors, param, mode='verify')\n            if reason:\n                name = list(param.keys())[0]\n                confirmed_params.append(name)\n                print('%s name: %s, factor: %s' % (res, name, reason))\n        return confirmed_params\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def safe_markup(raw_html: str) -> jinja2.Markup:\n    \"\"\"\n    Sanitise a raw HTML string to a set of allowed tags and attributes, and linkify any bare URLs.\n\n    Args\n        raw_html: Unsafe HTML.\n\n    Returns:\n        A Markup object ready to safely use in a Jinja template.\n    \"\"\"\n    return jinja2.Markup(\n        bleach.linkify(\n            bleach.clean(\n                raw_html,\n                tags=ALLOWED_TAGS,\n                attributes=ALLOWED_ATTRS,\n                # bleach master has this, but it isn't released yet\n                # protocols=ALLOWED_SCHEMES,\n                strip=True,\n            )\n        )\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def generate_key_if_missing(key):\n    value = env.str(key, \"\")\n\n    if value == \"\":\n        print(f'Warning: {key} not found in environment variables will generate a random key.')\n        value = get_random_secret_key()\n        try:\n            with open(env_file, 'a') as f:\n                f.write(f'\\n{key}={value}\\n')\n        except Exception as e:\n            print(f'Warning: failed to write {key} to .env file: {e}, new key will be regenerated on every server restart. If this key is used for signing, it will invalidate all existing sessions or tokens. Please set {key} in your environment variables to avoid this warning.')\n\n        os.environ[key] = value\n\n    return value\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def authenticate(self, request: Request):\n        auth = get_authorization_header(request)\n        if auth:\n            return None\n        return super().authenticate(request)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __del__(self):\r\n        \"\"\"Ensure all threads have been joined for cleanup.\"\"\"\r\n        try:\r\n            # it's possible the global state is not well defined, so just in\r\n            # case we'll wrap it all up in a try/except\r\n            self._terminated = True\r\n            if self._n_workers > 0:\r\n                LOGGER.debug(\"shutting down workers\")\r\n                self._worker_pool.close()\r\n                self._worker_pool.terminate()\r\n                # close down the log monitor thread\r\n                self._logging_queue.put(None)\r\n                timedout = not self._logging_monitor_thread.join(_MAX_TIMEOUT)\r\n                if timedout:\r\n                    LOGGER.debug(\r\n                        '_logging_monitor_thread %s timed out',\r\n                        self._logging_monitor_thread)\r\n\r\n            if self._logging_queue:\r\n                # Close down the logging monitor thread.\r\n                self._logging_queue.put(None)\r\n                self._logging_monitor_thread.join(_MAX_TIMEOUT)\r\n                # drain the queue if anything is left\r\n                while True:\r\n                    try:\r\n                        x = self._logging_queue.get_nowait()\r\n                        LOGGER.debug(\r\n                            \"the logging queue had this in it: %s\", x)\r\n                    except queue.Empty:\r\n                        break\r\n\r\n            self._taskgraph_started_event.set()\r\n            if self._n_workers >= 0:\r\n                self._executor_ready_event.set()\r\n                for executor_thread in self._task_executor_thread_list:\r\n                    try:\r\n                        executor_thread.join(_MAX_TIMEOUT)\r\n                        timedout = executor_thread.is_alive()\r\n                        if timedout:\r\n                            LOGGER.debug(\r\n                                'task executor thread timed out %s',\r\n                                executor_thread)\r\n                    except Exception:\r\n                        LOGGER.exception(\r\n                            \"Exception when joining %s\", executor_thread)\r\n                if self._reporting_interval is not None:\r\n                    LOGGER.debug(\"joining _monitor_thread.\")\r\n                    if self._logging_queue:\r\n                        self._logging_queue.put(None)\r\n                    self._execution_monitor_wait_event.set()\r\n                    self._execution_monitor_thread.join(_MAX_TIMEOUT)\r\n                    timedout = self._execution_monitor_thread.is_alive()\r\n                    if timedout:\r\n                        LOGGER.debug(\r\n                            '_monitor_thread %s timed out',\r\n                            self._execution_monitor_thread)\r\n                    for task in self._task_hash_map.values():\r\n                        # this is a shortcut to get the tasks to mark as joined\r\n                        task.task_done_executing_event.set()\r\n\r\n            # drain the task ready queue if there's anything left\r\n            while True:\r\n                try:\r\n                    x = self._task_ready_priority_queue.get_nowait()\r\n                    LOGGER.debug(\r\n                        \"task_ready_priority_queue not empty contains: %s\", x)\r\n                except queue.Empty:\r\n                    break\r\n            LOGGER.debug('taskgraph terminated')\r\n        except Exception:\r\n            LOGGER.exception('exception occurred during __del__')\r\n\r"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def connect(self, port=None):\n        ''' connect to the jail; nothing to do here '''\n\n        vvv(\"THIS IS A LOCAL JAIL DIR\", host=self.jail)\n\n        return self\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def to_plist(self, data, options=None):\n        \"\"\"\n        Given some Python data, produces binary plist output.\n        \"\"\"\n        options = options or {}\n\n        if biplist is None:\n            raise ImproperlyConfigured(\"Usage of the plist aspects requires biplist.\")\n\n        return biplist.writePlistToString(self.to_simple(data, options))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def auth_username_ci(self) -> str:\n        return self.appbuilder.get_app.config.get(\"AUTH_USERNAME_CI\", True)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _on_load_started(self) -> None:\n        self._progress = 0\n        self.data.viewing_source = False\n        self._set_load_status(usertypes.LoadStatus.loading)\n        self.load_started.emit()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def delete_trustee_and_trust(osc, context, cluster):\n    try:\n        kst = osc.keystone()\n\n        # The cluster which is upgraded from Liberty doesn't have trust_id\n        if cluster.trust_id:\n            kst.delete_trust(context, cluster)\n    except Exception:\n        # Exceptions are already logged by keystone().delete_trust\n        pass\n    try:\n        # The cluster which is upgraded from Liberty doesn't have\n        # trustee_user_id\n        if cluster.trustee_user_id:\n            osc.keystone().delete_trustee(cluster.trustee_user_id)\n    except Exception:\n        # Exceptions are already logged by keystone().delete_trustee\n        pass\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteEmbeddingLookupSparseParams*>(node->builtin_data);\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  const TfLiteTensor* ids;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &ids));\n  const TfLiteTensor* indices;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &indices));\n  const TfLiteTensor* dense_shape;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 2, &dense_shape));\n  const TfLiteTensor* weights;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 3, &weights));\n  const TfLiteTensor* value;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 4, &value));\n\n  const int lookup_rank = SizeOfDimension(indices, 1);\n  const int embedding_rank = NumDimensions(value);\n  const int num_lookups = SizeOfDimension(ids, 0);\n  const int num_rows = SizeOfDimension(value, 0);\n\n  // The last dimension gets replaced by the embedding.\n  const int output_rank = (lookup_rank - 1) + (embedding_rank - 1);\n\n  // Make sure that the actual dense shape of the sparse tensor represented by\n  // (loopkup, indices, dense_shape) is consistent.\n  TF_LITE_ENSURE_EQ(context, SizeOfDimension(dense_shape, 0), lookup_rank);\n\n  // Resize output tensor.\n  TfLiteIntArray* output_shape = TfLiteIntArrayCreate(output_rank);\n  TF_LITE_ENSURE(context, output_shape != nullptr);\n  int k = 0;\n  int embedding_size = 1;\n  int lookup_size = 1;\n  for (int i = 0; i < lookup_rank - 1; i++, k++) {\n    const int dim = dense_shape->data.i32[i];\n    lookup_size *= dim;\n    output_shape->data[k] = dim;\n  }\n  for (int i = 1; i < embedding_rank; i++, k++) {\n    const int dim = SizeOfDimension(value, i);\n    embedding_size *= dim;\n    output_shape->data[k] = dim;\n  }\n  TF_LITE_ENSURE_STATUS(context->ResizeTensor(context, output, output_shape));\n  const int output_size = lookup_size * embedding_size;\n  TfLiteTensorRealloc(output_size * sizeof(float), output);\n\n  float* output_ptr = GetTensorData<float>(output);\n  const float* weights_ptr = GetTensorData<float>(weights);\n  const float* value_ptr = GetTensorData<float>(value);\n\n  std::fill_n(output_ptr, output_size, 0.0f);\n\n  // Keep track of the current bucket for aggregation/combination.\n  int current_output_offset = 0;\n  float current_total_weight = 0.0;\n  float current_squares_weight = 0.0;\n  int num_elements = 0;\n\n  for (int i = 0; i < num_lookups; i++) {\n    int idx = ids->data.i32[i];\n    if (idx >= num_rows || idx < 0) {\n      context->ReportError(context,\n                           \"Embedding Lookup Sparse: index out of bounds. \"\n                           \"Got %d, and bounds are [0, %d]\",\n                           idx, num_rows - 1);\n      return kTfLiteError;\n    }\n\n    // Check where we need to aggregate.\n    const int example_indices_offset = i * lookup_rank;\n    int output_bucket = 0;\n    int stride = 1;\n    for (int k = (lookup_rank - 1) - 1; k >= 0; k--) {\n      output_bucket += indices->data.i32[example_indices_offset + k] * stride;\n      stride *= dense_shape->data.i32[k];\n    }\n    const int output_offset = output_bucket * embedding_size;\n\n    // If we are in a new aggregation bucket and the combiner is not the sum,\n    // go back and finalize the result of the previous bucket.\n    if (output_offset != current_output_offset) {\n      FinalizeAggregation(params->combiner, num_elements, current_total_weight,\n                          current_squares_weight, embedding_size,\n                          &output_ptr[current_output_offset]);\n\n      // Track next bucket.\n      num_elements = 0;\n      current_total_weight = 0.0;\n      current_squares_weight = 0.0;\n      current_output_offset = output_offset;\n    }\n\n    // Add element to aggregation.\n    ++num_elements;\n    const int example_embedding_offset = idx * embedding_size;\n    const float w = weights_ptr[i];\n    current_squares_weight += w * w;\n    current_total_weight += w;\n    for (int k = 0; k < embedding_size; k++) {\n      output_ptr[current_output_offset + k] +=\n          value_ptr[example_embedding_offset + k] * w;\n    }\n  }\n\n  // Finalize last bucket.\n  FinalizeAggregation(params->combiner, num_elements, current_total_weight,\n                      current_squares_weight, embedding_size,\n                      &GetTensorData<float>(output)[current_output_offset]);\n\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    auto value = ctx->input(0);\n    // Value should be at least rank 1. Also the 0th dimension should be\n    // at least loc_.\n    OP_REQUIRES(ctx, value.dims() >= 1,\n                errors::InvalidArgument(\"value should be at least rank 1.\"));\n    OP_REQUIRES(\n        ctx, value.dim_size(0) > loc_,\n        errors::InvalidArgument(\"0th dimension of value = \", value.dim_size(0),\n                                \" must be greater than loc_ = \", loc_));\n\n    auto update = ctx->input(1);\n\n    OP_REQUIRES(\n        ctx, value.dims() == update.dims(),\n        errors::InvalidArgument(\"value and update shape doesn't match: \",\n                                value.shape().DebugString(), \" vs. \",\n                                update.shape().DebugString()));\n    for (int i = 1; i < value.dims(); ++i) {\n      OP_REQUIRES(\n          ctx, value.dim_size(i) == update.dim_size(i),\n          errors::InvalidArgument(\"value and update shape doesn't match \",\n                                  value.shape().DebugString(), \" vs. \",\n                                  update.shape().DebugString()));\n    }\n    OP_REQUIRES(ctx, 1 == update.dim_size(0),\n                errors::InvalidArgument(\"update shape doesn't match: \",\n                                        update.shape().DebugString()));\n\n    Tensor output = value;  // This creates an alias intentionally.\n    const auto& d = ctx->eigen_device<Device>();\n    OP_REQUIRES_OK(\n        ctx, ::tensorflow::functor::DoParallelConcat(d, update, loc_, &output));\n    ctx->set_output(0, output);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def task_remove(request, task_id):\n    \"\"\"\n    remove task by task_id\n    :param request:\n    :return:\n    \"\"\"\n    if request.method == 'POST':\n        # delete job from DjangoJob\n        task = Task.objects.get(id=task_id)\n        clients = clients_of_task(task)\n        for client in clients:\n            job_id = get_job_id(client, task)\n            DjangoJob.objects.filter(id=job_id).delete()\n        # delete task\n        Task.objects.filter(id=task_id).delete()\n        return JsonResponse({'result': '1'})\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create_table(cls, client, table_name, read_units, write_units):\n        # type: (botocore.client.BaseClient, Text, int, int) -> None\n        \"\"\"Create the table for this MetaStore.\n\n        :param table: Pre-configured boto3 DynamoDB client object\n        :type table: boto3.resources.base.BaseClient\n        :param str table_name: Name of table to create\n        :param int read_units: Read capacity units to provision\n        :param int write_units: Write capacity units to provision\n        \"\"\"\n        _LOGGER.debug(\"Creating MetaStore table\")\n        try:\n            client.create_table(\n                TableName=table_name,\n                AttributeDefinitions=[\n                    {\"AttributeName\": MetaStoreAttributeNames.PARTITION.value, \"AttributeType\": \"S\"},\n                    {\"AttributeName\": MetaStoreAttributeNames.SORT.value, \"AttributeType\": \"N\"},\n                ],\n                KeySchema=[\n                    {\"AttributeName\": MetaStoreAttributeNames.PARTITION.value, \"KeyType\": \"HASH\"},\n                    {\"AttributeName\": MetaStoreAttributeNames.SORT.value, \"KeyType\": \"RANGE\"},\n                ],\n                ProvisionedThroughput={\"ReadCapacityUnits\": read_units, \"WriteCapacityUnits\": write_units},\n            )\n        except botocore.exceptions.ClientError as exc:\n            raise Exception(\"Could not create table\", exc)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_authorizedkey(self, key, comment=None):\n        \"\"\"\n        Add the given key to the user. Adding the key to his `authorized_keys`\n        file if it exists and adding it to database.\n        \"\"\"\n        # Parse and validate ssh key\n        assert key\n        key = authorizedkeys.check_publickey(key)\n\n        # Remove option & Remove comment for SQL storage\n        key = authorizedkeys.AuthorizedKey(\n            options=None, keytype=key.keytype, key=key.key, comment=comment or key.comment\n        )\n\n        # If a filename exists, use it by default.\n        filename = os.path.join(self.user_root, '.ssh', 'authorized_keys')\n        if os.path.isfile(filename):\n            with open(filename, mode=\"r+\", encoding='utf-8') as fh:\n                if authorizedkeys.exists(fh, key):\n                    raise DuplicateSSHKeyError(_(\"SSH key already exists\"))\n                logger.info(\"add key [%s] to [%s] authorized_keys\", key, self.username)\n                authorizedkeys.add(fh, key)\n        else:\n            # Also look in database.\n            logger.info(\"add key [%s] to [%s] database\", key, self.username)\n            try:\n                sshkey = SshKey(userid=self.userid, fingerprint=key.fingerprint, key=key.getvalue())\n                sshkey.add().flush()\n            except IntegrityError:\n                raise DuplicateSSHKeyError(\n                    _(\"Duplicate key. This key already exists or is associated to another user.\")\n                )\n        cherrypy.engine.publish('user_attr_changed', self, {'authorizedkeys': True})\n        cherrypy.engine.publish('authorizedkey_added', self, fingerprint=key.fingerprint, comment=comment)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _validate_model_name(name):\n    _original_validate_model_name(name)\n    if contains_path_separator(name):\n        raise MlflowException(\n            f\"Invalid name: '{name}'. Registered model name cannot contain path separator\",\n            INVALID_PARAMETER_VALUE,\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def info():\n    api = flask.current_app.config[\"PYLOAD_API\"]\n    conf = api.get_config_dict()\n    extra = os.uname() if hasattr(os, \"uname\") else tuple()\n\n    context = {\n        \"python\": sys.version,\n        \"os\": \" \".join((os.name, sys.platform) + extra),\n        \"version\": api.get_server_version(),\n        \"folder\": PKGDIR,\n        \"config_folder\": api.get_userdir(),\n        \"download\": conf[\"general\"][\"storage_folder\"][\"value\"],\n        \"freespace\": format.size(api.free_space()),\n        \"webif\": conf[\"webui\"][\"port\"][\"value\"],\n        \"language\": conf[\"general\"][\"language\"][\"value\"],\n    }\n    return render_template(\"info.html\", **context)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def inner(fn):\n        @functools.wraps(fn)\n        def wrapper(self, password, *args, **kwargs):\n            if len(password) > max_length:\n                raise ValueError(\"Invalid password; Must be less than or equal\"\n                                 \" to %d bytes\" % max_length)\n            return fn(self, password, *args, **kwargs)\n        return wrapper\n    return inner"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _deploy_image_loaded_to_local_registry(\n    build_ver: str,\n    docker_repository: str,\n    image_id: str,\n    auth_method: AuthorizationType,\n    vault_endpoint: str | None = None,\n    vault_endpoint_verify: str | bool | None = None,\n    vault_secret: str | None = None,\n) -> str:\n    docker_image = docker_repository + \":\" + build_ver\n    docker_image_latest = docker_repository + \":latest\"\n    tag_image(image_id, docker_repository, \"latest\")\n\n    logger.info(\"Deploying docker image tag=%s auth_method=%s\", docker_image, auth_method)\n    authenticate_to_registry(auth_method, vault_endpoint, vault_secret, vault_endpoint_verify)\n    bf_commons.run_process(['docker', 'push', docker_image])\n    bf_commons.run_process(['docker', 'push', docker_image_latest])\n\n    return docker_image\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_exception():\n    \"\"\"\n        Since the path matches the regular expression r'/api/*', this resource\n        automatically has CORS headers set.\n\n        Browsers will first make a preflight request to verify that the resource\n        allows cross-origin POSTs with a JSON Content-Type, which can be simulated\n        as:\n        $ curl --include -X OPTIONS http://127.0.0.1:5000/api/exception \\\n            --header Access-Control-Request-Method:POST \\\n            --header Access-Control-Request-Headers:Content-Type \\\n            --header Origin:www.examplesite.com\n        >> HTTP/1.0 200 OK\n        Content-Type: text/html; charset=utf-8\n        Allow: POST, OPTIONS\n        Access-Control-Allow-Origin: *\n        Access-Control-Allow-Headers: Content-Type\n        Access-Control-Allow-Methods: DELETE, GET, HEAD, OPTIONS, PATCH, POST, PUT\n        Content-Length: 0\n        Server: Werkzeug/0.9.6 Python/2.7.9\n        Date: Sat, 31 Jan 2015 22:25:22 GMT\n    \"\"\"\n    raise Exception(\"example\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def listify_lookup_plugin_terms(terms, basedir, inject):\n\n    if isinstance(terms, basestring):\n        # someone did:\n        #    with_items: alist\n        # OR\n        #    with_items: {{ alist }}\n\n        stripped = terms.strip()\n        if not (stripped.startswith('{') or stripped.startswith('[')) and not stripped.startswith(\"/\") and not stripped.startswith('set(['):\n            # if not already a list, get ready to evaluate with Jinja2\n            # not sure why the \"/\" is in above code :)\n            try:\n                new_terms = template.template(basedir, \"{{ %s }}\" % terms, inject)\n                if isinstance(new_terms, basestring) and new_terms.find(\"{{\") != -1:\n                    pass\n                else:\n                    terms = new_terms\n            except:\n                pass\n\n        if '{' in terms or '[' in terms:\n            # Jinja2 already evaluated a variable to a list.\n            # Jinja2-ified list needs to be converted back to a real type\n            # TODO: something a bit less heavy than eval\n            return safe_eval(terms)\n\n        if isinstance(terms, basestring):\n            terms = [ terms ]\n\n    return terms\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_columns_list(self):\n        \"\"\"\n        Returns a list of all the columns names\n        \"\"\"\n        return []\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def compare(response, factors, params):\n    \"\"\"\n    detects anomalies by comparing a HTTP response against a rule list\n    returns string, list (anomaly, list of parameters that caused it)\n    \"\"\"\n    these_headers = list(response.headers.keys())\n    these_headers.sort()\n    if factors['same_code'] and response.status_code != factors['same_code']:\n        return ('http code', params)\n    if factors['same_headers'] and these_headers != factors['same_headers']:\n        return ('http headers', params)\n    if factors['same_redirect'] and urlparse(response.headers.get('Location', '')).path != factors['same_redirect']:\n        return ('redirection', params)\n    if factors['same_body'] and response.text != factors['same_body']:\n        return ('body length', params)\n    if factors['lines_num'] and response.text.count('\\n') != factors['lines_num']:\n        return ('number of lines', params)\n    if factors['same_plaintext'] and remove_tags(response.text) != factors['same_plaintext']:\n        return ('text length', params)\n    if factors['lines_diff']:\n        for line in factors['lines_diff']:\n            if line not in response.text:\n                return ('lines', params)\n    if type(factors['param_missing']) == list:\n        for param in params.keys():\n            if len(param) < 5:\n                continue\n            if param not in factors['param_missing'] and re.search(r'[\\'\"\\s]%s[\\'\"\\s]' % param, response.text):\n                return ('param name reflection', params)\n    if factors['value_missing']:\n        for value in params.values():\n            if value in response.text and re.search(r'[\\'\"\\s]%s[\\'\"\\s]' % value, response.text):\n                return ('param value reflection', params)\n    return ('', [])\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "      TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n    if (tuple[index].has_value()) {\n      return errors::InvalidArgument(\n          \"The tensor for index '\", index, \"' for key '\", key.scalar<int64>()(),\n          \"' was already initialized '\", dtypes_.size(), \"'.\");\n    }\n\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _build_parser():\n    # type: () -> CommentIgnoringArgumentParser\n    \"\"\"Builds the argument parser.\n\n    :returns: Constructed argument parser\n    :rtype: argparse.ArgumentParser\n    \"\"\"\n    parser = CommentIgnoringArgumentParser(\n        description=\"Encrypt or decrypt data using the AWS Encryption SDK\",\n        epilog=\"For more usage instructions and examples, see: http://aws-encryption-sdk-cli.readthedocs.io/en/latest/\",\n        fromfile_prefix_chars=\"@\",\n    )\n\n    # For each argument added to this group, a dummy redirect argument must\n    # be added to the parent parser for each long form option string.\n    version_or_action = parser.add_mutually_exclusive_group(required=True)\n\n    version_or_action.add_argument(\"--version\", action=\"version\", version=_version_report())\n    parser.add_dummy_redirect_argument(\"--version\")\n\n    # For each argument added to this group, a dummy redirect argument must\n    # be added to the parent parser for each long form option string.\n    operating_action = version_or_action.add_mutually_exclusive_group()\n    operating_action.add_argument(\n        \"-e\", \"--encrypt\", dest=\"action\", action=\"store_const\", const=\"encrypt\", help=\"Encrypt data\"\n    )\n    parser.add_dummy_redirect_argument(\"--encrypt\")\n    operating_action.add_argument(\n        \"-d\", \"--decrypt\", dest=\"action\", action=\"store_const\", const=\"decrypt\", help=\"Decrypt data\"\n    )\n    parser.add_dummy_redirect_argument(\"--decrypt\")\n\n    # For each argument added to this group, a dummy redirect argument must\n    # be added to the parent parser for each long form option string.\n    metadata_group = parser.add_mutually_exclusive_group(required=True)\n\n    metadata_group.add_argument(\n        \"-S\",\n        \"--suppress-metadata\",\n        action=\"store_const\",\n        const=MetadataWriter(suppress_output=True)(),\n        dest=\"metadata_output\",\n        help=\"Suppress metadata output.\",\n    )\n    parser.add_dummy_redirect_argument(\"--suppress-metadata\")\n\n    metadata_group.add_argument(\n        \"--metadata-output\", type=MetadataWriter(), help=\"File to which to write metadata records\"\n    )\n    parser.add_dummy_redirect_argument(\"--metadata-output\")\n\n    parser.add_argument(\n        \"--overwrite-metadata\",\n        action=\"store_true\",\n        help=\"Force metadata output to overwrite contents of file rather than appending to file\",\n    )\n\n    parser.add_argument(\n        \"-w\",\n        \"--wrapping-keys\",\n        nargs=\"+\",\n        dest=\"wrapping_keys\",\n        action=\"append\",\n        required=True,\n        help=(\n            \"Identifying information for a wrapping key provider and wrapping keys. Each instance must include \"\n            \"a wrapping key provider identifier and identifiers for one or more wrapping key supplied by that \"\n            \" provider. ex: \"\n            \"--wrapping-keys provider=aws-kms key=$AWS_KMS_KEY_ARN\"\n        ),\n    )\n\n    parser.add_argument(\n        \"--commitment-policy\",\n        type=CommitmentPolicyArgs,\n        choices=list(CommitmentPolicyArgs),\n        default=CommitmentPolicyArgs.require_encrypt_require_decrypt,\n        help=(\n            \"Specifies the commitment policy for key commitment. \"\n            \"ex: \"\n            \"--commitment-policy=forbid-encrypt-allow-decrypt\"\n        ),\n    )\n\n    parser.add_argument(\n        \"--caching\",\n        nargs=\"+\",\n        required=False,\n        action=UniqueStoreAction,\n        help=(\n            \"Configuration options for a caching cryptographic materials manager and local cryptographic materials \"\n            'cache. Must consist of \"key=value\" pairs. If caching, at least \"capacity\" and \"max_age\" must be defined. '\n            \"ex: \"\n            \"--caching capacity=10 max_age=100.0\"\n        ),\n    )\n\n    parser.add_argument(\n        \"-i\",\n        \"--input\",\n        required=True,\n        action=UniqueStoreAction,\n        help='Input file or directory for encrypt/decrypt operation, or \"-\" for stdin.',\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output\",\n        required=True,\n        action=UniqueStoreAction,\n        help=\"Output file or directory for encrypt/decrypt operation, or - for stdout.\",\n    )\n\n    parser.add_argument(\"--encode\", action=\"store_true\", help=\"Base64-encode output after processing\")\n    parser.add_argument(\"--decode\", action=\"store_true\", help=\"Base64-decode input before processing\")\n\n    parser.add_argument(\n        \"-c\",\n        \"--encryption-context\",\n        nargs=\"+\",\n        action=UniqueStoreAction,\n        help=(\n            'key-value pair encryption context values (encryption only). Must a set of \"key=value\" pairs. '\n            \"ex: \"\n            \"-c key1=value1 key2=value2\"\n        ),\n    )\n\n    # Note: This is added as an argument for argparse API consistency, but it should not be used directly.\n    parser.add_argument(\n        \"--required-encryption-context-keys\", nargs=\"+\", action=UniqueStoreAction, help=argparse.SUPPRESS\n    )\n\n    parser.add_argument(\n        \"--algorithm\", action=UniqueStoreAction, help=\"Algorithm name (encryption only)\", choices=ALGORITHM_NAMES\n    )\n\n    parser.add_argument(\n        \"--frame-length\",\n        dest=\"frame_length\",\n        type=int,\n        action=UniqueStoreAction,\n        help=\"Frame length in bytes (encryption only)\",\n    )\n\n    parser.add_argument(\n        \"--max-length\",\n        type=int,\n        action=UniqueStoreAction,\n        help=(\n            \"Maximum frame length (for framed messages) or content length (for \"\n            \"non-framed messages) (decryption only)\"\n        ),\n    )\n\n    parser.add_argument(\n        \"--suffix\",\n        nargs=\"?\",\n        const=\"\",\n        action=UniqueStoreAction,\n        help=\"Custom suffix to use when target filename is not specified (empty if specified but no value provided)\",\n    )\n\n    parser.add_argument(\n        \"--interactive\",\n        action=\"store_true\",\n        help=\"Force aws-encryption-cli to prompt you for verification before overwriting existing files\",\n    )\n\n    parser.add_argument(\"--no-overwrite\", action=\"store_true\", help=\"Never overwrite existing files\")\n\n    parser.add_argument(\"-r\", \"-R\", \"--recursive\", action=\"store_true\", help=\"Allow operation on directories as input\")\n\n    parser.add_argument(\n        \"-v\",\n        dest=\"verbosity\",\n        action=\"count\",\n        help=\"Enables logging and sets detail level. Multiple -v options increases verbosity (max: 4).\",\n    )\n    parser.add_argument(\"-q\", \"--quiet\", action=\"store_true\", help=\"Suppresses most warning and diagnostic messages\")\n    return parser\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def delete(self, item: Model, raise_exception: bool = False) -> bool:\n        try:\n            self._delete_files(item)\n            self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), \"success\")\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), \"warning\")\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.database_error_message), \"danger\")\n            log.exception(\"Database error\")\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def handler500(request, template_name=\"dpaste/500.html\"):\n    context = {}\n    context.update(config.extra_template_context)\n    response = render(request, template_name, context, status=500)\n    add_never_cache_headers(response)\n    return response"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  explicit QuantizeAndDequantizeOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"signed_input\", &signed_input_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"num_bits\", &num_bits_));\n    OP_REQUIRES(ctx, num_bits_ > 0 && num_bits_ < (signed_input_ ? 62 : 63),\n                InvalidArgument(\"num_bits is out of range: \", num_bits_,\n                                \" with signed_input_ \", signed_input_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"range_given\", &range_given_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"input_min\", &input_min_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"input_max\", &input_max_));\n    if (range_given_) {\n      OP_REQUIRES(ctx, input_min_ <= input_max_,\n                  InvalidArgument(\"Invalid range: input_min \", input_min_,\n                                  \" > input_max \", input_max_));\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def main():\n    parser = argparse.ArgumentParser(\n        description=(\n            \"Upgrade ArcticDB storage. This script checks for and performs (with \"\n            \"`--run`) any data or config changes needed to support new versions of ArcticDB.\"\n        )\n    )\n    parser.add_argument(\n        \"--uri\",\n        required=True,\n        help=\"URI for Arctic instance to fix, see https://docs.arcticdb.io/api/arcticdb#arcticdb.Arctic.__init__\",\n    )\n    parser.add_argument(\n        \"--run\",\n        default=False,\n        action=\"store_true\",\n        help=\"Set to perform the storage update. This modifies the contents of your storage system.\",\n    )\n\n    args = parser.parse_args()\n\n    run(uri=args.uri, run=args.run)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, *expressions, ordering=(), **extra):\n        if not isinstance(ordering, (list, tuple)):\n            ordering = [ordering]\n        ordering = ordering or []\n        # Transform minus sign prefixed strings into an OrderBy() expression.\n        ordering = (\n            (OrderBy(F(o[1:]), descending=True) if isinstance(o, str) and o[0] == '-' else o)\n            for o in ordering\n        )\n        super().__init__(*expressions, **extra)\n        self.ordering = self._parse_expressions(*ordering)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def check_origin_api(self):\n        \"\"\"Check Origin for cross-site API requests.\n        \n        Copied from WebSocket with changes:\n        \n        - allow unspecified host/origin (e.g. scripts)\n        \"\"\"\n        if self.allow_origin == '*':\n            return True\n\n        host = self.request.headers.get(\"Host\")\n        origin = self.request.headers.get(\"Origin\")\n\n        # If no header is provided, assume it comes from a script/curl.\n        # We are only concerned with cross-site browser stuff here.\n        if origin is None or host is None:\n            return True\n        \n        origin = origin.lower()\n        origin_host = urlparse(origin).netloc\n        \n        # OK if origin matches host\n        if origin_host == host:\n            return True\n        \n        # Check CORS headers\n        if self.allow_origin:\n            allow = self.allow_origin == origin\n        elif self.allow_origin_pat:\n            allow = bool(self.allow_origin_pat.match(origin))\n        else:\n            # No CORS headers deny the request\n            allow = False\n        if not allow:\n            self.log.warn(\"Blocking Cross Origin API request.  Origin: %s, Host: %s\",\n                origin, host,\n            )\n        return allow\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  explicit UnravelIndexOp(OpKernelConstruction* ctx)\n      : OpKernel(ctx), dtidx_(DataTypeToEnum<Tidx>::v()) {}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def time_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        fields = {\n            \"hour\": \"%H:00:00\",\n            \"minute\": \"%H:%i:00\",\n            \"second\": \"%H:%i:%s\",\n        }\n        if lookup_type in fields:\n            format_str = fields[lookup_type]\n            return f\"CAST(DATE_FORMAT({sql}, %s) AS TIME)\", (*params, format_str)\n        else:\n            return f\"TIME({sql})\", params\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_savepoint_name(savepoint_name: str) -> None:\n    \"\"\"Validates a save point's name meets the required character set.\"\"\"\n    if not all(i in VALID_SAVEPOINT_CHARACTERS for i in savepoint_name):\n        raise ValueError(\n            \"Savepoint names can only contain the following characters:\"\n            f\" {VALID_SAVEPOINT_CHARACTERS}\"\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def password_max_length(max_length):\n    def inner(fn):\n        @functools.wraps(fn)\n        def wrapper(self, password, *args, **kwargs):\n            if len(password) > max_length:\n                raise ValueError(\"Invalid password; Must be less than or equal\"\n                                 \" to %d bytes\" % max_length)\n            return fn(self, password, *args, **kwargs)\n        return wrapper\n    return inner\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, hs: \"HomeServer\"):\n        self.notifier = hs.get_notifier()\n        self.server_name = hs.config.server.server_name\n        self.store = hs.get_datastores().main\n        self.event_auth_handler = hs.get_event_auth_handler()\n        self.event_handler = hs.get_event_handler()\n        self._storage_controllers = hs.get_storage_controllers()\n\n        self.hs = hs\n\n        # We only need to poke the federation sender explicitly if its on the\n        # same instance. Other federation sender instances will get notified by\n        # `synapse.app.generic_worker.FederationSenderHandler` when it sees it\n        # in the receipts stream.\n        self.federation_sender = None\n        if hs.should_send_federation():\n            self.federation_sender = hs.get_federation_sender()\n\n        # If we can handle the receipt EDUs we do so, otherwise we route them\n        # to the appropriate worker.\n        if hs.get_instance_name() in hs.config.worker.writers.receipts:\n            hs.get_federation_registry().register_edu_handler(\n                EduTypes.RECEIPT, self._received_remote_receipt\n            )\n        else:\n            hs.get_federation_registry().register_instances_for_edu(\n                EduTypes.RECEIPT,\n                hs.config.worker.writers.receipts,\n            )\n\n        self.clock = self.hs.get_clock()\n        self.state = hs.get_state_handler()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __reduce__(self):\n        # The traceback is not picklable so we need to remove it. Also, some custom exception\n        # values aren't picklable either. For user convenience, we try first to serialize it and\n        # we remove the value in case or error. As an optimization, we could have re-used the\n        # dumped value during unpickling, but this requires using \"pickle.loads()\" which is\n        # flagged as insecure by some security tools.\n        try:\n            pickle.dumps(self.value)\n        except pickle.PickleError:\n            return (RecordException, (self.type, None, None))\n        else:\n            return (RecordException, (self.type, self.value, None))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def copy_bytes(dst, src, length, length_bound):\n    annotation = f\"copy_bytes from {src} to {dst}\"\n\n    src = IRnode.from_list(src)\n    dst = IRnode.from_list(dst)\n    length = IRnode.from_list(length)\n\n    with src.cache_when_complex(\"src\") as (b1, src), length.cache_when_complex(\n        \"copy_bytes_count\"\n    ) as (b2, length), dst.cache_when_complex(\"dst\") as (b3, dst):\n\n        # fast code for common case where num bytes is small\n        # TODO expand this for more cases where num words is less than ~8\n        if length_bound <= 32:\n            copy_op = STORE(dst, LOAD(src))\n            ret = IRnode.from_list(copy_op, annotation=annotation)\n            return b1.resolve(b2.resolve(b3.resolve(ret)))\n\n        if dst.location == MEMORY and src.location in (MEMORY, CALLDATA, DATA):\n            # special cases: batch copy to memory\n            # TODO: iloadbytes\n            if src.location == MEMORY:\n                copy_op = [\"staticcall\", \"gas\", 4, src, length, dst, length]\n                gas_bound = _identity_gas_bound(length_bound)\n            elif src.location == CALLDATA:\n                copy_op = [\"calldatacopy\", dst, src, length]\n                gas_bound = _calldatacopy_gas_bound(length_bound)\n            elif src.location == DATA:\n                copy_op = [\"dloadbytes\", dst, src, length]\n                # note: dloadbytes compiles to CODECOPY\n                gas_bound = _codecopy_gas_bound(length_bound)\n\n            ret = IRnode.from_list(copy_op, annotation=annotation, add_gas_estimate=gas_bound)\n            return b1.resolve(b2.resolve(b3.resolve(ret)))\n\n        if dst.location == IMMUTABLES and src.location in (MEMORY, DATA):\n            # TODO istorebytes-from-mem, istorebytes-from-calldata(?)\n            # compile to identity, CODECOPY respectively.\n            pass\n\n        # general case, copy word-for-word\n        # pseudocode for our approach (memory-storage as example):\n        # for i in range(len, bound=MAX_LEN):\n        #   sstore(_dst + i, mload(src + i * 32))\n        i = IRnode.from_list(_freshname(\"copy_bytes_ix\"), typ=\"uint256\")\n\n        n = [\"div\", [\"ceil32\", length], 32]\n        n_bound = ceil32(length_bound) // 32\n\n        dst_i = add_ofst(dst, _mul(i, dst.location.word_scale))\n        src_i = add_ofst(src, _mul(i, src.location.word_scale))\n\n        copy_one_word = STORE(dst_i, LOAD(src_i))\n\n        main_loop = [\"repeat\", i, 0, n, n_bound, copy_one_word]\n\n        return b1.resolve(\n            b2.resolve(b3.resolve(IRnode.from_list(main_loop, annotation=annotation)))\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def auth_role_public(self) -> str:\n        return self.appbuilder.get_app.config[\"AUTH_ROLE_PUBLIC\"]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def name_is_coherent_with_trigger_type(cls, v, values, **kwargs):\n        if (\n            v\n            in [\n                ActionName.pull_application_list,\n                ActionName.pull_job_list,\n                ActionName.pull_profile_list,\n            ]\n            and values[\"trigger_type\"] != WorkflowType.pull\n        ):\n            raise ValueError(\n                \"`pull_application_list`, `pull_job_list` and `pull_profile_list`\"\n                \" are only available for\"\n                \" trigger_type={}\".format(WorkflowType.pull)\n            )\n        return v\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def analyse_text(text):\n        rv = 0.0\n        # dbt's ref function\n        if re.search(r'\\{\\{\\s*ref\\(.*\\)\\s*\\}\\}', text):\n            rv += 0.4\n        # dbt's source function\n        if re.search(r'\\{\\{\\s*source\\(.*\\)\\s*\\}\\}', text):\n            rv += 0.25\n        # Jinja macro\n        if re.search(r'\\{%-?\\s*macro \\w+\\(.*\\)\\s*-?%\\}', text):\n            rv += 0.15\n        return rv\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_keys(self, lst):\n        \"\"\"\n        return a list of pk values from object list\n        \"\"\"\n        pk_name = self.get_pk_name()\n        if self.is_pk_composite():\n            return [[getattr(item, pk) for pk in pk_name] for item in lst]\n        else:\n            return [getattr(item, pk_name) for item in lst]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def test_app(environ, start_response):\n            \"\"\"A WSGI app that just reflects its HTTP environ.\"\"\"\n            start_response('200 OK', [])\n            http_environ_items = sorted(\n                '%s:%s' % (k, v) for k, v in environ.items()\n                if k.startswith('HTTP_')\n            )\n            yield (','.join(http_environ_items)).encode('utf-8')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _process_received_message(self, msg: Union[ua.Message, ua.Acknowledge, ua.ErrorMessage]):\n        if msg is None:\n            pass\n        elif isinstance(msg, ua.Message):\n            self._call_callback(msg.request_id(), msg.body())\n        elif isinstance(msg, ua.Acknowledge):\n            self._call_callback(0, msg)\n        elif isinstance(msg, ua.ErrorMessage):\n            self.logger.fatal(\"Received an error: %r\", msg)\n            self.disconnect_socket()\n        else:\n            raise ua.UaError(f\"Unsupported message type: {msg}\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, socketio, mail, api, permissions, config):\n        super().__init__(socketio, mail, api, permissions, config)\n        self.r: RuleCollection = getattr(self.permissions, module_name)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def default(self, **kwargs):\n        # Process the parameters.\n        profile_form = UserProfileForm(obj=self.app.currentuser)\n        password_form = UserPasswordForm()\n        refresh_form = RefreshForm()\n        if profile_form.is_submitted():\n            if profile_form.validate():\n                profile_form.populate_obj(self.app.currentuser)\n                flash(_(\"Profile updated successfully.\"), level='success')\n                raise cherrypy.HTTPRedirect(\"\")\n            else:\n                flash(profile_form.error_message, level='error')\n        elif password_form.is_submitted():\n            if password_form.validate():\n                if password_form.populate_obj(self.app.currentuser):\n                    flash(_(\"Password updated successfully.\"), level='success')\n                    raise cherrypy.HTTPRedirect(\"\")\n            else:\n                flash(password_form.error_message, level='error')\n        elif refresh_form.is_submitted():\n            if refresh_form.validate():\n                refresh_form.populate_obj(self.app.currentuser)\n            else:\n                flash(refresh_form.error_message, level='error')\n        params = {\n            'profile_form': profile_form,\n            'password_form': password_form,\n            'refresh_form': refresh_form,\n        }\n        return self._compile_template(\"prefs_general.html\", **params)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_values_json(self, lst, list_columns):\n        \"\"\"\n        Converts list of objects from query to JSON\n        \"\"\"\n        result = []\n        for item in self.get_values(lst, list_columns):\n            for key, value in list(item.items()):\n                if isinstance(value, datetime.datetime) or isinstance(\n                    value, datetime.date\n                ):\n                    value = value.isoformat()\n                    item[key] = value\n                if isinstance(value, list):\n                    item[key] = [str(v) for v in value]\n            result.append(item)\n        return result\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& prefix = context->input(0);\n    const Tensor& tensor_names = context->input(1);\n    const Tensor& shape_and_slices = context->input(2);\n    OP_REQUIRES(context, tensor_names.NumElements() == dtypes_.size(),\n                errors::InvalidArgument(\"Got \", tensor_names.NumElements(),\n                                        \" tensor names, but \", dtypes_.size(),\n                                        \" expected dtypes.\"));\n    ValidateInputs(false /* not save op */, context, prefix, tensor_names,\n                   shape_and_slices);\n    if (!context->status().ok()) return;\n\n    const string& prefix_string = prefix.scalar<tstring>()();\n\n    // Intention: we plan to use the RestoreV2 op as a backward-compatible\n    // reader as we upgrade to the V2 format.  This allows transparent upgrade.\n    // We here attempt to read a V1 checkpoint, if \"prefix_string\" does not\n    // refer to a V2 checkpoint.\n    Env* env = Env::Default();\n    std::vector<string> paths;\n    if (!env->GetMatchingPaths(MetaFilename(prefix_string), &paths).ok() ||\n        paths.empty()) {\n      // Cannot find V2's metadata file, so \"prefix_string\" does not point to a\n      // V2 checkpoint.  Invokes the V1 read path instead.\n      for (size_t i = 0; i < tensor_names.NumElements(); ++i) {\n        RestoreTensor(context, &checkpoint::OpenTableTensorSliceReader,\n                      /* preferred_shard */ -1, /* restore_slice */ true,\n                      /* restore_index */ i);\n        if (!context->status().ok()) {\n          return;\n        }\n      }\n      return;\n    }\n    // If found, invokes the V2 reader.\n    OP_REQUIRES_OK(context, RestoreTensorsV2(context, prefix, tensor_names,\n                                             shape_and_slices, dtypes_));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def ready(self):\n        checks.register(check_deprecated_settings)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def organization_member(self) -> OrganizationMember:\n        return OrganizationMember.objects.select_related(\"organization\").get(pk=self.member_id)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create_table(\n        self,\n        name: str,\n        obj: ir.Table | pd.DataFrame | pa.Table | None = None,\n        *,\n        schema: sch.Schema | None = None,\n        database: str | None = None,\n        temp: bool | None = None,\n        overwrite: bool = False,\n        format: str = \"parquet\",\n    ) -> ir.Table:\n        \"\"\"Create a new table in Spark.\n\n        Parameters\n        ----------\n        name\n            Name of the new table.\n        obj\n            If passed, creates table from `SELECT` statement results\n        schema\n            Mutually exclusive with `obj`, creates an empty table with a schema\n        database\n            Database name\n        temp\n            Whether the new table is temporary\n        overwrite\n            If `True`, overwrite existing data\n        format\n            Format of the table on disk\n\n        Returns\n        -------\n        Table\n            The newly created table.\n\n        Examples\n        --------\n        >>> con.create_table(\n        ...     \"new_table_name\", table_expr\n        ... )  # quartodoc: +SKIP # doctest: +SKIP\n        \"\"\"\n        import pandas as pd\n        import pyarrow as pa\n        import pyarrow_hotfix  # noqa: F401\n\n        if obj is None and schema is None:\n            raise com.IbisError(\"The schema or obj parameter is required\")\n        if temp is True:\n            raise NotImplementedError(\n                \"PySpark backend does not yet support temporary tables\"\n            )\n        if obj is not None:\n            if isinstance(obj, pa.Table):\n                obj = obj.to_pandas()\n            if isinstance(obj, pd.DataFrame):\n                spark_df = self._session.createDataFrame(obj)\n                mode = \"overwrite\" if overwrite else \"error\"\n                spark_df.write.saveAsTable(name, format=format, mode=mode)\n                return None\n            else:\n                self._register_in_memory_tables(obj)\n\n            ast = self.compiler.to_ast(obj)\n            select = ast.queries[0]\n\n            statement = ddl.CTAS(\n                name,\n                select,\n                database=database,\n                can_exist=overwrite,\n                format=format,\n            )\n        else:\n            statement = ddl.CreateTableWithSchema(\n                name,\n                schema,\n                database=database,\n                format=format,\n                can_exist=overwrite,\n            )\n\n        self.raw_sql(statement.compile())\n        return self.table(name, database=database)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _get_bytes_to_sign(self):\n        \"\"\"\n        Creates the message used for signing SNS notifications.\n        This is used to verify the bounce message when it is received.\n        \"\"\"\n\n        # Depending on the message type the fields to add to the message\n        # differ so we handle that here.\n        msg_type = self._data.get(\"Type\")\n        if msg_type == \"Notification\":\n            fields_to_sign = [\n                \"Message\",\n                \"MessageId\",\n                \"Subject\",\n                \"Timestamp\",\n                \"TopicArn\",\n                \"Type\",\n            ]\n        elif (\n            msg_type == \"SubscriptionConfirmation\"\n            or msg_type == \"UnsubscribeConfirmation\"\n        ):\n            fields_to_sign = [\n                \"Message\",\n                \"MessageId\",\n                \"SubscribeURL\",\n                \"Timestamp\",\n                \"Token\",\n                \"TopicArn\",\n                \"Type\",\n            ]\n        else:\n            # Unrecognized type\n            logger.warning('Unrecognized SNS message Type: \"%s\"', msg_type)\n            return None\n\n        bytes_to_sign = []\n        for field in fields_to_sign:\n            field_value = self._data.get(field)\n            if not field_value:\n                continue\n\n            # Some notification types do not have all fields. Only add fields\n            # with values.\n            bytes_to_sign.append(f\"{field}\\n{field_value}\\n\")\n\n        return \"\".join(bytes_to_sign).encode()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _handle_control_frame(self, opcode, data):\n        if opcode == 8:  # connection close\n            self._remote_close_data = data\n            if not data:\n                status = 1000\n            elif len(data) > 1:\n                status = struct.unpack_from('!H', data)[0]\n                if not status or status not in VALID_CLOSE_STATUS:\n                    raise FailedConnectionError(\n                        1002,\n                        \"Unexpected close status code.\")\n                try:\n                    data = self.UTF8Decoder().decode(data[2:], True)\n                except (UnicodeDecodeError, ValueError):\n                    raise FailedConnectionError(\n                        1002,\n                        \"Close message data should be valid UTF-8.\")\n            else:\n                status = 1002\n            self.close(close_data=(status, ''))\n            raise ConnectionClosedError()\n        elif opcode == 9:  # ping\n            self.send(data, control_code=0xA)\n        elif opcode == 0xA:  # pong\n            pass\n        else:\n            raise FailedConnectionError(\n                1002, \"Unknown control frame received.\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def deploy_docker_image(\n    image_tar_path: str,\n    docker_repository: str,\n    auth_method: AuthorizationType = AuthorizationType.LOCAL_ACCOUNT,\n    vault_endpoint: T.Optional[str] = None,\n    vault_endpoint_verify: str | bool | None = None,\n    vault_secret: T.Optional[str] = None,\n) -> str:\n    if image_tar_path.endswith(\".toml\"):\n        deploy_fn = _deploy_docker_image_from_local_repo\n    else:\n        deploy_fn = _deploy_docker_image_from_fs\n    return deploy_fn(\n        image_tar_path,\n        docker_repository,\n        auth_method,\n        vault_endpoint,\n        vault_endpoint_verify,\n        vault_secret,\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def datetime_cast_time_sql(self, sql, params, tzname):\n        # Since `TimeField` values are stored as TIMESTAMP change to the\n        # default date and convert the field to the specified timezone.\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        convert_datetime_sql = (\n            f\"TO_TIMESTAMP(CONCAT('1900-01-01 ', TO_CHAR({sql}, 'HH24:MI:SS.FF')), \"\n            f\"'YYYY-MM-DD HH24:MI:SS.FF')\"\n        )\n        return (\n            f\"CASE WHEN {sql} IS NOT NULL THEN {convert_datetime_sql} ELSE NULL END\",\n            (*params, *params),\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _on_load_started(self) -> None:\n        self._progress = 0\n        self.data.viewing_source = False\n        self._set_load_status(usertypes.LoadStatus.loading)\n        self.load_started.emit()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __attrs_post_init__(self):\n        # type: () -> None\n        \"\"\"Initialize the cache.\"\"\"\n        self._version = None  # type: int # pylint: disable=attribute-defined-outside-init\n        self._last_updated = None  # type: float # pylint: disable=attribute-defined-outside-init\n        self._lock = Lock()  # pylint: disable=attribute-defined-outside-init\n        self._cache = BasicCache(self._cache_size)  # pylint: disable=attribute-defined-outside-init\n        self.refresh()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _dnsname_to_stdlib(name):\n    \"\"\"\n    Converts a dNSName SubjectAlternativeName field to the form used by the\n    standard library on the given Python version.\n\n    Cryptography produces a dNSName as a unicode string that was idna-decoded\n    from ASCII bytes. We need to idna-encode that string to get it back, and\n    then on Python 3 we also need to convert to unicode via UTF-8 (the stdlib\n    uses PyUnicode_FromStringAndSize on it, which decodes via UTF-8).\n\n    If the name cannot be idna-encoded then we return None signalling that\n    the name given should be skipped.\n    \"\"\"\n    def idna_encode(name):\n        \"\"\"\n        Borrowed wholesale from the Python Cryptography Project. It turns out\n        that we can't just safely call `idna.encode`: it can explode for\n        wildcard names. This avoids that problem.\n        \"\"\"\n        import idna\n\n        try:\n            for prefix in [u'*.', u'.']:\n                if name.startswith(prefix):\n                    name = name[len(prefix):]\n                    return prefix.encode('ascii') + idna.encode(name)\n            return idna.encode(name)\n        except idna.core.IDNAError:\n            return None\n\n    if ':' in name:\n        return name\n\n    name = idna_encode(name)\n    if name is None:\n        return None\n    elif sys.version_info >= (3, 0):\n        name = name.decode('utf-8')\n    return name\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def model_from_config(config, custom_objects=None):\n  \"\"\"Instantiates a Keras model from its config.\n\n  Usage:\n  ```\n  # for a Functional API model\n  tf.keras.Model().from_config(model.get_config())\n\n  # for a Sequential model\n  tf.keras.Sequential().from_config(model.get_config())\n  ```\n\n  Args:\n      config: Configuration dictionary.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n\n  Returns:\n      A Keras model instance (uncompiled).\n\n  Raises:\n      TypeError: if `config` is not a dictionary.\n  \"\"\"\n  if isinstance(config, list):\n    raise TypeError('`model_from_config` expects a dictionary, not a list. '\n                    'Maybe you meant to use '\n                    '`Sequential.from_config(config)`?')\n  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\n  return deserialize(config, custom_objects=custom_objects)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def process_response(self, request, response):\n        if settings.DEBUG:\n            response.headers[\"X-Frame-Options\"] = \"DENY\"\n            response.headers[\"X-Content-Type-Options\"] = \"nosniff\"\n            response.headers[\n                \"Content-Security-Policy\"\n            ] = \"script-src 'self' cdn.crowdin.com;\"\n\n        return response\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def clean_svg(self, field):\n        \"\"\"Sanitize SVG file before saving.\"\"\"\n        cleaned = sanitize_svg(field.file.read())\n        return BytesIO(bytes(cleaned, 'utf8'))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _check_args(self) -> None:\n        # Check tokens\n        tokens = self._tokens\n        if not (tokens and isinstance(tokens, list) and all(isinstance(token, str) for token in tokens) and (len(tokens) == len(set(tokens)))):\n            raise exc.ArgsError(\"Tokens must be a list of one or more unique strings.\")  # Tokens must not be logged.\n        log.debug(\"Number of unique tokens is %s.\", len(tokens))\n\n        # Check max cache size\n        max_cache_size = self._max_cache_size\n        max_cache_size = cast(Any, max_cache_size)\n        if (not isinstance(max_cache_size, int)) or (max_cache_size < 0):\n            raise exc.ArgsError(f\"Max cache size must be an integer \u22650, but it is {max_cache_size}.\")\n        log.debug(\"Max cache size is %s.\", max_cache_size)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _on_load_started(self) -> None:\n        self._progress = 0\n        self.data.viewing_source = False\n        self._set_load_status(usertypes.LoadStatus.loading)\n        self.load_started.emit()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "LoadCommand* Binary::add(const SegmentCommand& segment) {\n  /*\n   * To add a new segment in a Mach-O file, we need to:\n   *\n   * 1. Allocate space for a new Load command: LC_SEGMENT_64 / LC_SEGMENT\n   *    which must include the sections\n   * 2. Allocate space for the content of the provided segment\n   *\n   * For #1, the logic is to shift all the content after the end of the load command table.\n   * This modification is described in doc/sphinx/tutorials/11_macho_modification.rst.\n   *\n   * For #2, the easiest way is to place the content at the end of the Mach-O file and\n   * to make the LC_SEGMENT point to this area. It works as expected as long as\n   * the binary does not need to be signed.\n   *\n   * If the binary has to be signed, codesign and the underlying Apple libraries\n   * enforce that there is not data after the __LINKEDIT segment, otherwise we get\n   * this kind of error: \"main executable failed strict validation\".\n   * To comply with this check, we can shift the __LINKEDIT segment (c.f. ``shift_linkedit(...)``)\n   * such as the data of the new segment are located before __LINKEDIT.\n   * Nevertheless, we can't shift __LINKEDIT by an arbitrary value. For ARM and ARM64,\n   * ld/dyld enforces a segment alignment of \"4 * 4096\" as coded in ``Options::reconfigureDefaults``\n   * of ``ld64-609/src/ld/Option.cpp``:\n   *\n   * ```cpp\n   * ...\n   * <rdar://problem/13070042> Only third party apps should have 16KB page segments by default\n   * if (fEncryptable) {\n   *  if (fSegmentAlignment == 4096)\n   *    fSegmentAlignment = 4096*4;\n   * }\n   *\n   * // <rdar://problem/12258065> ARM64 needs 16KB page size for user land code\n   * // <rdar://problem/15974532> make armv7[s] use 16KB pages in user land code for iOS 8 or later\n   * if (fArchitecture == CPU_TYPE_ARM64 || (fArchitecture == CPU_TYPE_ARM) ) {\n   *   fSegmentAlignment = 4096*4;\n   * }\n   * ```\n   * Therefore, we must shift __LINKEDIT by at least 4 * 0x1000 for Mach-O files targeting ARM\n   */\n  LIEF_DEBUG(\"Adding the new segment '{}' ({} bytes)\", segment.name(), segment.content().size());\n  const uint32_t alignment = page_size();\n  const uint64_t new_fsize = align(segment.content().size(), alignment);\n  SegmentCommand new_segment = segment;\n\n  if (new_segment.file_size() == 0) {\n    new_segment.file_size(new_fsize);\n    new_segment.content_resize(new_fsize);\n  }\n\n  if (new_segment.virtual_size() == 0) {\n    const uint64_t new_size = align(new_segment.file_size(), alignment);\n    new_segment.virtual_size(new_size);\n  }\n\n  if (segment.sections().size() > 0) {\n    new_segment.nb_sections_ = segment.sections().size();\n  }\n\n  if (is64_) {\n    new_segment.command(LOAD_COMMAND_TYPES::LC_SEGMENT_64);\n    size_t needed_size = sizeof(details::segment_command_64);\n    needed_size += new_segment.numberof_sections() * sizeof(details::section_64);\n    new_segment.size(needed_size);\n  } else {\n    new_segment.command(LOAD_COMMAND_TYPES::LC_SEGMENT);\n    size_t needed_size = sizeof(details::segment_command_32);\n    needed_size += new_segment.numberof_sections() * sizeof(details::section_32);\n    new_segment.size(needed_size);\n  }\n\n  LIEF_DEBUG(\" -> sizeof(LC_SEGMENT): {}\", new_segment.size());\n\n  // Insert the segment before __LINKEDIT\n  const auto it_linkedit = std::find_if(std::begin(commands_), std::end(commands_),\n      [] (const std::unique_ptr<LoadCommand>& cmd) {\n        if (!SegmentCommand::classof(cmd.get())) {\n          return false;\n        }\n        return cmd->as<SegmentCommand>()->name() == \"__LINKEDIT\";\n      });\n\n  const bool has_linkedit = it_linkedit != std::end(commands_);\n\n  size_t pos = std::distance(std::begin(commands_), it_linkedit);\n\n  LIEF_DEBUG(\" -> index: {}\", pos);\n\n  auto* new_cmd = add(new_segment, pos);\n\n  if (new_cmd == nullptr) {\n    LIEF_WARN(\"Fail to insert new '{}' segment\", segment.name());\n    return nullptr;\n  }\n\n  auto* segment_added = new_cmd->as<SegmentCommand>();\n\n  if (!has_linkedit) {\n    /* If there are not __LINKEDIT segment we can point the Segment's content to the EOF\n     * NOTE(romain): I don't know if a binary without a __LINKEDIT segment exists\n     */\n    range_t new_va_ranges  = this->va_ranges();\n    range_t new_off_ranges = off_ranges();\n    if (segment.virtual_address() == 0 && segment_added->virtual_size() != 0) {\n      const uint64_t new_va = align(new_va_ranges.end, alignment);\n      segment_added->virtual_address(new_va);\n      size_t current_va = segment_added->virtual_address();\n      for (Section& section : segment_added->sections()) {\n        section.virtual_address(current_va);\n        current_va += section.size();\n      }\n    }\n\n    if (segment.file_offset() == 0 && segment_added->virtual_size() != 0) {\n      const uint64_t new_offset = align(new_off_ranges.end, alignment);\n      segment_added->file_offset(new_offset);\n      size_t current_offset = new_offset;\n      for (Section& section : segment_added->sections()) {\n        section.offset(current_offset);\n        current_offset += section.size();\n      }\n    }\n    refresh_seg_offset();\n    return segment_added;\n  }\n\n  uint64_t lnk_offset = 0;\n  uint64_t lnk_va     = 0;\n\n  if (const SegmentCommand* lnk = get_segment(\"__LINKEDIT\")) {\n    lnk_offset = lnk->file_offset();\n    lnk_va     = lnk->virtual_address();\n  }\n\n  // Make space for the content of the new segment\n  shift_linkedit(new_fsize);\n  LIEF_DEBUG(\" -> offset         : 0x{:06x}\", lnk_offset);\n  LIEF_DEBUG(\" -> virtual address: 0x{:06x}\", lnk_va);\n\n  segment_added->virtual_address(lnk_va);\n  segment_added->virtual_size(segment_added->virtual_size());\n  size_t current_va = segment_added->virtual_address();\n  for (Section& section : segment_added->sections()) {\n    section.virtual_address(current_va);\n    current_va += section.size();\n  }\n\n  segment_added->file_offset(lnk_offset);\n  size_t current_offset = lnk_offset;\n  for (Section& section : segment_added->sections()) {\n    section.offset(current_offset);\n    current_offset += section.size();\n  }\n\n  refresh_seg_offset();\n  return segment_added;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def encrypt(self, message):\n        \"\"\"Produce the PKCS#1 v1.5 encryption of a message.\n\n        This function is named ``RSAES-PKCS1-V1_5-ENCRYPT``, and it is specified in\n        `section 7.2.1 of RFC8017\n        <https://tools.ietf.org/html/rfc8017#page-28>`_.\n\n        :param message:\n            The message to encrypt, also known as plaintext. It can be of\n            variable length, but not longer than the RSA modulus (in bytes) minus 11.\n        :type message: bytes/bytearray/memoryview\n\n        :Returns: A byte string, the ciphertext in which the message is encrypted.\n            It is as long as the RSA modulus (in bytes).\n\n        :Raises ValueError:\n            If the RSA key length is not sufficiently long to deal with the given\n            message.\n        \"\"\"\n\n        # See 7.2.1 in RFC8017\n        k = self._key.size_in_bytes()\n        mLen = len(message)\n\n        # Step 1\n        if mLen > k - 11:\n            raise ValueError(\"Plaintext is too long.\")\n        # Step 2a\n        ps = []\n        while len(ps) != k - mLen - 3:\n            new_byte = self._randfunc(1)\n            if bord(new_byte[0]) == 0x00:\n                continue\n            ps.append(new_byte)\n        ps = b\"\".join(ps)\n        # Step 2b\n        em = b'\\x00\\x02' + ps + b'\\x00' + _copy_bytes(None, None, message)\n        # Step 3a (OS2IP)\n        em_int = bytes_to_long(em)\n        # Step 3b (RSAEP)\n        m_int = self._key._encrypt(em_int)\n        # Step 3c (I2OSP)\n        c = long_to_bytes(m_int, k)\n        return c\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "export const useJobDetail = () => {\n  const params = useParams() as { id: string };\n  const [msg, setMsg] = useState(\"Loading the job detail\");\n  const [refreshing, setRefresh] = useState(true);\n  const { data: job, isLoading } = useSWR(\n    [\"useJobDetail\", params.id],\n    async ([_, jobId]) => {\n      try {\n        const rsp = await getJobDetail(jobId);\n        return rsp.data;\n      } catch (e) {\n        setMsg(\"Job Query Error Please Check JobId\");\n        setRefresh(false);\n      }\n    },\n    { refreshInterval: refreshing ? API_REFRESH_INTERVAL_MS : 0 },\n  );\n\n  return {\n    job,\n    isLoading,\n    msg,\n    params,\n  };\n};\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _is_bhyve_hyper():\n    vmm_enabled = False\n    try:\n        stdout = subprocess.Popen(\n            [\"sysctl\", \"hw.vmm.create\"], stdout=subprocess.PIPE\n        ).communicate()[0]\n        vmm_enabled = len(salt.utils.stringutils.to_str(stdout).split('\"')[1]) != 0\n    except IndexError:\n        pass\n    return vmm_enabled\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    public List<String> getAllowedUrls() {\n        String allowedURL = prop.getProperty(TS_ALLOWED_URLS, DEFAULT_TS_ALLOWED_URLS);\n        return Arrays.asList(allowedURL.split(\",\"));\n    }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self,\n        url=None,\n        headers=dict(),\n        files=None,\n        method=None,\n        data=dict(),\n        params=dict(),\n        auth=None,\n        cookies=None,\n        timeout=None,\n        redirect=False,\n        allow_redirects=False,\n        proxies=None,\n        hooks=None,\n        config=None,\n        _poolmanager=None,\n        verify=None,\n        session=None,\n        cert=None):\n\n        #: Dictionary of configurations for this request.\n        self.config = dict(config or [])\n\n        #: Float describes the timeout of the request.\n        #  (Use socket.setdefaulttimeout() as fallback)\n        self.timeout = timeout\n\n        #: Request URL.\n        self.url = url\n\n        #: Dictionary of HTTP Headers to attach to the :class:`Request <Request>`.\n        self.headers = dict(headers or [])\n\n        #: Dictionary of files to multipart upload (``{filename: content}``).\n        self.files = None\n\n        #: HTTP Method to use.\n        self.method = method\n\n        #: Dictionary or byte of request body data to attach to the\n        #: :class:`Request <Request>`.\n        self.data = None\n\n        #: Dictionary or byte of querystring data to attach to the\n        #: :class:`Request <Request>`. The dictionary values can be lists for representing\n        #: multivalued query parameters.\n        self.params = None\n\n        #: True if :class:`Request <Request>` is part of a redirect chain (disables history\n        #: and HTTPError storage).\n        self.redirect = redirect\n\n        #: Set to True if full redirects are allowed (e.g. re-POST-ing of data at new ``Location``)\n        self.allow_redirects = allow_redirects\n\n        # Dictionary mapping protocol to the URL of the proxy (e.g. {'http': 'foo.bar:3128'})\n        self.proxies = dict(proxies or [])\n\n        # If no proxies are given, allow configuration by environment variables\n        # HTTP_PROXY and HTTPS_PROXY.\n        if not self.proxies and self.config.get('trust_env'):\n          if 'HTTP_PROXY' in os.environ:\n            self.proxies['http'] = os.environ['HTTP_PROXY']\n          if 'HTTPS_PROXY' in os.environ:\n            self.proxies['https'] = os.environ['HTTPS_PROXY']\n\n        self.data, self._enc_data = self._encode_params(data)\n        self.params, self._enc_params = self._encode_params(params)\n        self.files, self._enc_files = self._encode_files(files)\n\n        #: :class:`Response <Response>` instance, containing\n        #: content and metadata of HTTP Response, once :attr:`sent <send>`.\n        self.response = Response()\n\n        #: Authentication tuple or object to attach to :class:`Request <Request>`.\n        self.auth = auth\n\n        #: CookieJar to attach to :class:`Request <Request>`.\n        self.cookies = dict(cookies or [])\n\n        #: True if Request has been sent.\n        self.sent = False\n\n        #: Event-handling hooks.\n        self.hooks = {}\n\n        for event in HOOKS:\n            self.hooks[event] = []\n\n        hooks = hooks or {}\n\n        for (k, v) in list(hooks.items()):\n            self.register_hook(event=k, hook=v)\n\n        #: Session.\n        self.session = session\n\n        #: SSL Verification.\n        self.verify = verify\n\n        #: SSL Certificate\n        self.cert = cert\n\n        if headers:\n            headers = CaseInsensitiveDict(self.headers)\n        else:\n            headers = CaseInsensitiveDict()\n\n        # Add configured base headers.\n        for (k, v) in list(self.config.get('base_headers', {}).items()):\n            if k not in headers:\n                headers[k] = v\n\n        self.headers = headers\n        self._poolmanager = _poolmanager\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def format_date(self, data):\n        \"\"\"\n        A hook to control how dates are formatted.\n\n        Can be overridden at the ``Serializer`` level (``datetime_formatting``)\n        or globally (via ``settings.TASTYPIE_DATETIME_FORMATTING``).\n\n        Default is ``iso-8601``, which looks like \"2010-12-16\".\n        \"\"\"\n        if self.datetime_formatting == 'rfc-2822':\n            return format_date(data)\n\n        return data.isoformat()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "const mergeUrls = (base, input) => (\n  typeof input === 'string' ?\n  new URL (input, base).href :\n  base\n);\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def auth_ldap_server(self) -> str:\n        return self.appbuilder.get_app.config[\"AUTH_LDAP_SERVER\"]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def prepare(self):\n        if not self.check_origin():\n            raise web.HTTPError(404)\n        return super(APIHandler, self).prepare()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def login(\n        self, provider: Optional[str] = None, register: Optional[str] = None\n    ) -> Response:\n        log.debug(\"Provider: {0}\".format(provider))\n        if g.user is not None and g.user.is_authenticated:\n            log.debug(\"Already authenticated {0}\".format(g.user))\n            return redirect(self.appbuilder.get_url_for_index)\n\n        if provider is None:\n            if len(self.appbuilder.sm.oauth_providers) > 1:\n                return self.render_template(\n                    self.login_template,\n                    providers=self.appbuilder.sm.oauth_providers,\n                    title=self.title,\n                    appbuilder=self.appbuilder,\n                )\n            else:\n                provider = self.appbuilder.sm.oauth_providers[0][\"name\"]\n\n        log.debug(\"Going to call authorize for: {0}\".format(provider))\n        state = jwt.encode(\n            request.args.to_dict(flat=False),\n            self.appbuilder.app.config[\"SECRET_KEY\"],\n            algorithm=\"HS256\",\n        )\n        try:\n            if register:\n                log.debug(\"Login to Register\")\n                session[\"register\"] = True\n            if provider == \"twitter\":\n                return self.appbuilder.sm.oauth_remotes[provider].authorize_redirect(\n                    redirect_uri=url_for(\n                        \".oauth_authorized\",\n                        provider=provider,\n                        _external=True,\n                        state=state,\n                    )\n                )\n            else:\n                return self.appbuilder.sm.oauth_remotes[provider].authorize_redirect(\n                    redirect_uri=url_for(\n                        \".oauth_authorized\", provider=provider, _external=True\n                    ),\n                    state=state.decode(\"ascii\") if isinstance(state, bytes) else state,\n                )\n        except Exception as e:\n            log.error(\"Error on OAuth authorize: {0}\".format(e))\n            flash(as_unicode(self.invalid_login_message), \"warning\")\n            return redirect(self.appbuilder.get_url_for_index)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def post(self, request, class_path=None, slug=None):\n        job_model = self._get_job_model_or_404(class_path, slug)\n\n        job_form = (\n            job_model.job_class().as_form(request.POST, request.FILES) if job_model.job_class is not None else None\n        )\n        schedule_form = forms.JobScheduleForm(request.POST)\n        task_queue = request.POST.get(\"_task_queue\")\n\n        return_url = request.POST.get(\"_return_url\")\n        if return_url is not None and url_has_allowed_host_and_scheme(url=return_url, allowed_hosts=request.get_host()):\n            return_url = iri_to_uri(return_url)\n        else:\n            return_url = None\n\n        # Allow execution only if a worker process is running and the job is runnable.\n        if not get_worker_count(queue=task_queue):\n            messages.error(request, \"Unable to run or schedule job: Celery worker process not running.\")\n        elif not job_model.installed or job_model.job_class is None:\n            messages.error(request, \"Unable to run or schedule job: Job is not presently installed.\")\n        elif not job_model.enabled:\n            messages.error(request, \"Unable to run or schedule job: Job is not enabled to be run.\")\n        elif (\n            job_model.has_sensitive_variables\n            and request.POST.get(\"_schedule_type\") != JobExecutionType.TYPE_IMMEDIATELY\n        ):\n            messages.error(request, \"Unable to schedule job: Job may have sensitive input variables.\")\n        elif job_model.has_sensitive_variables and job_model.approval_required:\n            messages.error(\n                request,\n                \"Unable to run or schedule job: \"\n                \"This job is flagged as possibly having sensitive variables but is also flagged as requiring approval.\"\n                \"One of these two flags must be removed before this job can be scheduled or run.\",\n            )\n        elif job_form is not None and job_form.is_valid() and schedule_form.is_valid():\n            # Run the job. A new JobResult is created.\n            commit = job_form.cleaned_data.pop(\"_commit\")\n            profile = job_form.cleaned_data.pop(\"_profile\")\n            schedule_type = schedule_form.cleaned_data[\"_schedule_type\"]\n\n            if job_model.approval_required or schedule_type in JobExecutionType.SCHEDULE_CHOICES:\n                crontab = \"\"\n\n                if schedule_type == JobExecutionType.TYPE_IMMEDIATELY:\n                    # The job must be approved.\n                    # If the schedule_type is immediate, we still create the task, but mark it for approval\n                    # as a once in the future task with the due date set to the current time. This means\n                    # when approval is granted, the task is immediately due for execution.\n                    schedule_type = JobExecutionType.TYPE_FUTURE\n                    schedule_datetime = timezone.now()\n                    schedule_name = f\"{job_model} - {schedule_datetime}\"\n\n                else:\n                    schedule_name = schedule_form.cleaned_data[\"_schedule_name\"]\n\n                    if schedule_type == JobExecutionType.TYPE_CUSTOM:\n                        crontab = schedule_form.cleaned_data[\"_recurrence_custom_time\"]\n                        # doing .get(\"key\", \"default\") returns None instead of \"default\" here for some reason\n                        schedule_datetime = schedule_form.cleaned_data.get(\"_schedule_start_time\")\n                        if schedule_datetime is None:\n                            # \"_schedule_start_time\" is checked against ScheduledJob.earliest_possible_time()\n                            # which returns timezone.now() + timedelta(seconds=15)\n                            schedule_datetime = timezone.now() + timedelta(seconds=20)\n                    else:\n                        schedule_datetime = schedule_form.cleaned_data[\"_schedule_start_time\"]\n\n                job_kwargs = {\n                    \"data\": job_model.job_class.serialize_data(job_form.cleaned_data),\n                    \"request\": copy_safe_request(request),\n                    \"user\": request.user.pk,\n                    \"commit\": commit,\n                    \"name\": job_model.class_path,\n                    \"task_queue\": job_form.cleaned_data.get(\"_task_queue\", None),\n                }\n                if task_queue:\n                    job_kwargs[\"celery_kwargs\"] = {\"queue\": task_queue}\n\n                scheduled_job = ScheduledJob(\n                    name=schedule_name,\n                    task=\"nautobot.extras.jobs.scheduled_job_handler\",\n                    job_class=job_model.class_path,\n                    job_model=job_model,\n                    start_time=schedule_datetime,\n                    description=f\"Nautobot job {schedule_name} scheduled by {request.user} for {schedule_datetime}\",\n                    kwargs=job_kwargs,\n                    interval=schedule_type,\n                    one_off=schedule_type == JobExecutionType.TYPE_FUTURE,\n                    queue=task_queue,\n                    user=request.user,\n                    approval_required=job_model.approval_required,\n                    crontab=crontab,\n                )\n                scheduled_job.save()\n\n                if job_model.approval_required:\n                    messages.success(request, f\"Job {schedule_name} successfully submitted for approval\")\n                    return redirect(return_url if return_url else \"extras:scheduledjob_approval_queue_list\")\n                else:\n                    messages.success(request, f\"Job {schedule_name} successfully scheduled\")\n                    return redirect(return_url if return_url else \"extras:scheduledjob_list\")\n\n            else:\n                # Enqueue job for immediate execution\n                job_content_type = get_job_content_type()\n                job_result = JobResult.enqueue_job(\n                    run_job,\n                    job_model.class_path,\n                    job_content_type,\n                    request.user,\n                    celery_kwargs={\"queue\": task_queue},\n                    data=job_model.job_class.serialize_data(job_form.cleaned_data),\n                    request=copy_safe_request(request),\n                    commit=commit,\n                    profile=profile,\n                    task_queue=job_form.cleaned_data.get(\"_task_queue\", None),\n                )\n\n                if return_url:\n                    messages.info(\n                        request,\n                        format_html(\n                            'Job enqueued. <a href=\"{}\">Click here for the results.</a>',\n                            job_result.get_absolute_url(),\n                        ),\n                    )\n                    return redirect(return_url)\n\n                return redirect(\"extras:jobresult\", pk=job_result.pk)\n\n        if return_url:\n            return redirect(return_url)\n\n        template_name = \"extras/job.html\"\n        if job_model.job_class is not None and hasattr(job_model.job_class, \"template_name\"):\n            try:\n                get_template(job_model.job_class.template_name)\n                template_name = job_model.job_class.template_name\n            except TemplateDoesNotExist as err:\n                messages.error(request, f'Unable to render requested custom job template \"{template_name}\": {err}')\n\n        return render(\n            request,\n            template_name,\n            {\n                \"job_model\": job_model,\n                \"job_form\": job_form,\n                \"schedule_form\": schedule_form,\n            },\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def error_handler(response, factors):\n\t\"\"\"\n\tdecides what to do after performing a HTTP request\n\t\t'ok': continue normally\n\t\t'retry': retry this request\n\t\t'kill': stop processing this target\n\treturns str\n\t\"\"\"\n\tif type(response) != str and response.status_code in (400, 413, 418, 429, 503):\n\t\tif response.status_code == 503:\n\t\t\tmem.var['kill'] = True\n\t\t\tprint('%s Target is unable to process requests, try --stable switch' % bad)\n\t\t\treturn 'kill'\n\t\telif response.status_code in (429, 418):\n\t\t\tprint('%s Target has a rate limit in place, try --stable switch' % bad)\n\t\t\treturn 'kill'\n\t\telse:\n\t\t\tif factors['same_code'] != response.status_code:\n\t\t\t\tmem.var['kill'] = True\n\t\t\t\tprint('%s Server received a bad request. Try decreasing the chunk size with -c option' % bad)\n\t\t\t\treturn 'kill'\n\t\t\telse:\n\t\t\t\treturn 'ok'\n\telse:\n\t\tif 'Timeout' in response:\n\t\t\tif mem.var['timeout'] > 20:\n\t\t\t\tmem.var['kill'] = True\n\t\t\t\tprint('%s Connection timed out, unable to increase timeout further' % bad)\n\t\t\t\treturn 'kill'\n\t\t\telse:\n\t\t\t\tprint('%s Connection timed out, increased timeout by 5 seconds' % bad)\n\t\t\t\tmem.var['timeout'] += 5\n\t\t\t\treturn 'retry'\n\t\telif 'ConnectionRefused' in response:\n\t\t\treturn connection_refused()\n\t\telif type(response) == str:\n\t\t\tif '\\'' in response:\n\t\t\t\tprint('%s Encountered an error: %s' % (bad, response.split('\\'')[1]))\n\t\t\treturn 'kill'\n\treturn 'ok'\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def load(self, model, value):\n        \"\"\"\n        Converts unstructured data into structured data, recursively.\n        \"\"\"\n        try:\n            return self._cattrs_converter.structure(value, model)\n        except (ValueError, TypeError, ClassValidationError) as e:\n            raise SerializationException(str(e))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, *expressions, ordering=(), **extra):\n        if not isinstance(ordering, (list, tuple)):\n            ordering = [ordering]\n        ordering = ordering or []\n        # Transform minus sign prefixed strings into an OrderBy() expression.\n        ordering = (\n            (OrderBy(F(o[1:]), descending=True) if isinstance(o, str) and o[0] == '-' else o)\n            for o in ordering\n        )\n        super().__init__(*expressions, **extra)\n        self.ordering = self._parse_expressions(*ordering)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def process(value: Any, state: ParsingState) -> None:\n            \"\"\"Custom process method.\n\n            The function above us on the stack used 'opt' to\n            pick option from a dict, see if it is deprecated.\n            \"\"\"\n            # reach up the stack and get 'opt'\n            import inspect\n\n            frame = inspect.currentframe()\n            try:\n                opt = frame.f_back.f_locals.get(\"opt\")  # type: ignore\n            finally:\n                del frame\n\n            if opt in deprecated:\n                msg = (\n                    f\"DeprecationWarning: The option {opt!r} is deprecated, \"\n                    f\"use {preferred!r}.\"\n                )\n                echo(style(msg, fg=\"red\"), err=True)\n\n            return orig_process(value, state)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool Verify(const void* buf, size_t len, const OpResolver& resolver,\n            ErrorReporter* error_reporter) {\n  const Model* model = VerifyFlatbufferAndGetModel(buf, len);\n  if (model == nullptr) {\n    ReportError(error_reporter, \"Invalid flatbuffer format\");\n    return false;\n  }\n  if (model->version() != TFLITE_SCHEMA_VERSION) {\n    ReportError(error_reporter, \"Invalid model version %d\", model->version());\n    return false;\n  }\n  if (!VerifySubGraphs(*model, error_reporter)) {\n    return false;\n  }\n  if (!VerifyTensors(*model, error_reporter)) {\n    return false;\n  }\n  if (!VerifyOps(*model, resolver, error_reporter)) {\n    return false;\n  }\n  return true;\n}\n}  // namespace tflite"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get(self):\n        \"\"\"List Rules\n        ---\n        description: >-\n            List of all available rules at the server. The user must be\n            authenticated, but does not require any additional permissions to\n            view the rules.\\n\n\n            Accesible to users.\n\n        parameters:\n            - in: query\n              name: name\n              schema:\n                type: string\n              description: Filter by name of the rule\n            - in: query\n              name: operation\n              schema:\n                type: string\n              description: Get rules for a specific type of operation\n            - in: query\n              name: scope\n              schema:\n                type: string\n              description: Get rules for a specific scope\n            - in: query\n              name: role_id\n              schema:\n                type: integer\n              description: Get rules for a specific role\n            - in: query\n              name: user_id\n              schema:\n                type: integer\n              description: Get rules for a specific user. This includes the\n                rules that are part of the user's roles.\n            - in: query\n              name: page\n              schema:\n                type: integer\n              description: Page number for pagination (default=1)\n            - in: query\n              name: per_page\n              schema:\n                type: integer\n              description: Number of items per page (default=10)\n            - in: query\n              name: sort\n              schema:\n                type: string\n              description: >-\n                Sort by one or more fields, separated by a comma. Use a minus\n                sign (-) in front of the field to sort in descending order.\n            - in: query\n              name: no_pagination\n              schema:\n                type: int\n              description: >-\n                If set to 1, pagination is disabled and all items are returned.\n\n        responses:\n          200:\n            description: Ok\n          400:\n            description: Improper values for pagination or sorting parameters\n\n        security:\n            - bearerAuth: []\n\n        tags: [\"Rule\"]\n        \"\"\"\n        q = g.session.query(db.Rule)\n\n        args = request.args\n\n        # filter by any field of this endpoint\n        for param in ['name', 'operation', 'scope']:\n            if param in args:\n                q = q.filter(getattr(db.Rule, param) == args[param])\n\n        # find roles containing a specific rule\n        if 'role_id' in args:\n            q = q.join(db.role_rule_association).join(db.Role)\\\n                 .filter(db.Role.id == args['role_id'])\n\n        # find all rules of a specific user. This is done by first joining all\n        # tables to find all rules originating from a user's roles. Then, we\n        # do an outer join to find all rules that are directly assigned to the\n        # user.\n        if 'user_id' in args:\n            q = q.join(db.role_rule_association).join(db.Role)\\\n                 .join(db.Permission).join(db.User)\\\n                 .outerjoin(db.UserPermission,\n                            db.Rule.id == db.UserPermission.c.rule_id)\\\n                 .filter(or_(\n                    db.User.id == args['user_id'],\n                    db.UserPermission.c.user_id == args['user_id']\n                 ))\n\n        # paginate results\n        try:\n            page = Pagination.from_query(q, request)\n        except ValueError as e:\n            return {'msg': str(e)}, HTTPStatus.BAD_REQUEST\n\n        # model serialization\n        return self.response(page, rule_schema)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def del_version(request, client_id, project, version):\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        scrapyd = get_scrapyd(client)\n        result = scrapyd.delete_version(project=project, version=version)\n        return JsonResponse(result)\n"}, {"role": "assistant", "content": "False"}]}
