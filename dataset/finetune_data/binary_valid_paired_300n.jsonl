{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def read_config(self, config, **kwargs):\n        # FIXME: federation_domain_whitelist needs sytests\n        self.federation_domain_whitelist = None  # type: Optional[dict]\n        federation_domain_whitelist = config.get(\"federation_domain_whitelist\", None)\n\n        if federation_domain_whitelist is not None:\n            # turn the whitelist into a hash for speed of lookup\n            self.federation_domain_whitelist = {}\n\n            for domain in federation_domain_whitelist:\n                self.federation_domain_whitelist[domain] = True\n\n        self.federation_ip_range_blacklist = config.get(\n            \"federation_ip_range_blacklist\", []\n        )\n\n        # Attempt to create an IPSet from the given ranges\n        try:\n            self.federation_ip_range_blacklist = IPSet(\n                self.federation_ip_range_blacklist\n            )\n\n            # Always blacklist 0.0.0.0, ::\n            self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n        except Exception as e:\n            raise ConfigError(\n                \"Invalid range(s) provided in federation_ip_range_blacklist: %s\" % e\n            )\n\n        federation_metrics_domains = config.get(\"federation_metrics_domains\") or []\n        validate_config(\n            _METRICS_FOR_DOMAINS_SCHEMA,\n            federation_metrics_domains,\n            (\"federation_metrics_domains\",),\n        )\n        self.federation_metrics_domains = set(federation_metrics_domains)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status SparseTensorToCSRSparseMatrixCPUFunctor::operator()(\n    const int64 batch_size, const int num_rows,\n    TTypes<int64>::ConstMatrix indices, TTypes<int32>::Vec batch_ptr,\n    TTypes<int32>::Vec csr_row_ptr, TTypes<int32>::Vec csr_col_ind) {\n  // Validate inputs.\n  if (batch_ptr.size() != batch_size + 1) {\n    return errors::InvalidArgument(\n        \"Expected batch_ptr.size() == batch_size + 1. Got: \", batch_ptr.size(),\n        \" vs. \", batch_size + 1);\n  }\n  if (csr_row_ptr.size() != batch_size * (num_rows + 1)) {\n    return errors::InvalidArgument(\n        \"Expected csr_row_ptr.size() == batch_size * (num_rows + 1). Got: \",\n        csr_row_ptr.size(), \" vs. \", batch_size * (num_rows + 1));\n  }\n\n  const int64 total_nnz = indices.dimension(0);\n  const int rank = indices.dimension(1);\n  if (rank == 2 && batch_size != 1) {\n    return errors::InvalidArgument(\n        \"Expected batch_size == 1 when rank is 2. Got batch_size: \",\n        batch_size);\n  }\n  if (csr_col_ind.size() != total_nnz) {\n    return errors::InvalidArgument(\n        \"Expected csr_col_ind.size() == total_nnz. Got: \", csr_col_ind.size(),\n        \" vs. \", total_nnz);\n  }\n\n  int prev_batch = -1;\n  if (rank == 2) {\n    // For a single batch, the batch_ptrs are {0, total_nnz}.\n    batch_ptr(0) = 0;\n    ++prev_batch;\n\n    for (int64 i = 0; i < total_nnz; ++i) {\n      // For now, the rows pointers store the corresponding row counts.\n      csr_row_ptr(indices(i, 0) + 1) += 1;\n      csr_col_ind(i) = indices(i, 1);\n    }\n  } else {  // rank == 3\n    for (int64 i = 0; i < total_nnz; ++i) {\n      const int cur_batch = indices(i, 0);\n      // For now, the rows pointers store the corresponding row counts.\n      csr_row_ptr(cur_batch * (num_rows + 1) + indices(i, 1) + 1) += 1;\n      csr_col_ind(i) = indices(i, 2);\n\n      // We're at a new batch and might have skipped over empty batches.\n      while (prev_batch < cur_batch) {\n        // The previous batch ends at position i.\n        batch_ptr(prev_batch + 1) = i;\n        ++prev_batch;\n      }\n    }\n  }\n  // Set the last element of batch_ptr and account for trailing empty batches.\n  while (prev_batch < batch_size) {\n    batch_ptr(prev_batch + 1) = total_nnz;\n    ++prev_batch;\n  }\n\n  // Compute the cumulative row counts for each batch.\n  for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n    auto* row_ptr_batch = csr_row_ptr.data() + batch_idx * (num_rows + 1);\n    std::partial_sum(row_ptr_batch, row_ptr_batch + num_rows + 1,\n                     row_ptr_batch);\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _complete_multipart_upload_artifact(artifact_path):\n    \"\"\"\n    A request handler for `POST /mlflow-artifacts/mpu/complete` to complete a multipart upload\n    to `artifact_path` (a relative path from the root artifact directory).\n    \"\"\"\n    validate_path_is_safe(artifact_path)\n\n    request_message = _get_request_message(\n        CompleteMultipartUpload(),\n        schema={\n            \"path\": [_assert_required, _assert_string],\n            \"upload_id\": [_assert_string],\n            \"parts\": [_assert_required],\n        },\n    )\n    path = request_message.path\n    upload_id = request_message.upload_id\n    parts = [MultipartUploadPart.from_proto(part) for part in request_message.parts]\n\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    _validate_support_multipart_upload(artifact_repo)\n\n    artifact_repo.complete_multipart_upload(\n        path,\n        upload_id,\n        parts,\n        artifact_path,\n    )\n    return _wrap_response(CompleteMultipartUpload.Response())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def put_file(path):\n    try:\n        dest_path = sanitized_join(\n            path,\n            pathlib.Path(app.config[\"DATA_ROOT\"]),\n        )\n    except ValueError:\n        return flask.Response(\n            \"Not Found\",\n            404,\n            mimetype=\"text/plain\",\n        )\n\n    verification_key = flask.request.args.get(\"v\", \"\")\n    length = int(flask.request.headers.get(\"Content-Length\", 0))\n    hmac_input = \"{} {}\".format(path, length).encode(\"utf-8\")\n    key = app.config[\"SECRET_KEY\"]\n    mac = hmac.new(key, hmac_input, hashlib.sha256)\n    digest = mac.hexdigest()\n\n    if not hmac.compare_digest(digest, verification_key):\n        return flask.Response(\n            \"Invalid verification key\",\n            403,\n            mimetype=\"text/plain\",\n        )\n\n    content_type = flask.request.headers.get(\n        \"Content-Type\",\n        \"application/octet-stream\",\n    )\n\n    dest_path.parent.mkdir(parents=True, exist_ok=True, mode=0o770)\n    data_file, metadata_file = get_paths(dest_path)\n\n    try:\n        with write_file(data_file) as fout:\n            stream_file(flask.request.stream, fout, length)\n\n            with metadata_file.open(\"x\") as f:\n                json.dump(\n                    {\n                        \"headers\": {\"Content-Type\": content_type},\n                    },\n                    f,\n                )\n    except EOFError:\n        return flask.Response(\n            \"Bad Request\",\n            400,\n            mimetype=\"text/plain\",\n        )\n    except OSError as exc:\n        if exc.errno == errno.EEXIST:\n            return flask.Response(\n                \"Conflict\",\n                409,\n                mimetype=\"text/plain\",\n            )\n        raise\n\n    return flask.Response(\n        \"Created\",\n        201,\n        mimetype=\"text/plain\",\n    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def authenticate_by_pam(username, password):\n    @pam_conversation\n    def conv(\n        num_msg, msg, resp, appdata_ptr\n    ):  # pylint: disable=unused-argument\n        # it is: *resp = (pam_response *) calloc(num_msg, sizeof(pam_response))\n        resp[0] = cast(\n            calloc(num_msg, sizeof(pam_response)), POINTER(pam_response)\n        )\n        for i in range(num_msg):\n            if msg[i].contents.msg_style == PAM_PROMPT_ECHO_OFF:\n                resp.contents[i].resp = strdup(password.encode(\"utf8\"))\n                resp.contents[i].resp_retcode = 0\n        return 0\n\n    pamh = pam_handle()\n    conversation = pam_conv(conv)\n    returncode = pam_start(\n        PCSD_SERVICE.encode(\"utf8\"),\n        username.encode(\"utf8\"),\n        byref(conversation),\n        byref(pamh),\n    )\n    if returncode == PAM_SUCCESS:\n        returncode = pam_authenticate(pamh, 0)\n    pam_end(pamh, returncode)\n    return returncode == PAM_SUCCESS\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<T, 1>::ConstTensor& values,\n                        const typename TTypes<T, 1>::ConstTensor& value_range,\n                        int32_t nbins, typename TTypes<Tout, 1>::Tensor& out) {\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    Tensor index_to_bin_tensor;\n\n    TF_RETURN_IF_ERROR(context->forward_input_or_allocate_temp(\n        {0}, DataTypeToEnum<int32>::value, TensorShape({values.size()}),\n        &index_to_bin_tensor));\n    auto index_to_bin = index_to_bin_tensor.flat<int32>();\n\n    const double step = static_cast<double>(value_range(1) - value_range(0)) /\n                        static_cast<double>(nbins);\n    const double nbins_minus_1 = static_cast<double>(nbins - 1);\n\n    // The calculation is done by finding the slot of each value in `values`.\n    // With [a, b]:\n    //   step = (b - a) / nbins\n    //   (x - a) / step\n    // , then the entries are mapped to output.\n\n    // Bug fix: Switch the order of cwiseMin and int32-casting to avoid\n    // producing a negative index when casting an big int64 number to int32\n    index_to_bin.device(d) =\n        ((values.cwiseMax(value_range(0)) - values.constant(value_range(0)))\n             .template cast<double>() /\n         step)\n            .cwiseMin(nbins_minus_1)\n            .template cast<int32>();\n\n    out.setZero();\n    for (int32_t i = 0; i < index_to_bin.size(); i++) {\n      out(index_to_bin(i)) += Tout(1);\n    }\n    return Status::OK();\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(\n        self,\n        targets=\"127.0.0.1\",\n        options=\"-sT\",\n        event_callback=None,\n        safe_mode=True,\n        fqp=None,\n    ):\n        \"\"\"\n        Constructor of NmapProcess class.\n\n        :param targets: hosts to be scanned. Could be a string of hosts \\\n        separated with a coma or a python list of hosts/ip.\n        :type targets: string or list\n\n        :param options: list of nmap options to be applied to scan. \\\n        These options are all documented in nmap's man pages.\n\n        :param event_callback: callable function which will be ran \\\n        each time nmap process outputs data. This function will receive \\\n        two parameters:\n\n            1. the nmap process object\n            2. the data produced by nmap process. See readme for examples.\n\n        :param safe_mode: parameter to protect unsafe options like -oN, -oG, \\\n        -iL, -oA,...\n\n        :param fqp: full qualified path, if None, nmap will be searched \\\n        in the PATH\n\n        :return: NmapProcess object\n\n        \"\"\"\n        Thread.__init__(self)\n        unsafe_opts = set(\n            [\n                \"-oG\",\n                \"-oN\",\n                \"-iL\",\n                \"-oA\",\n                \"-oS\",\n                \"-oX\",\n                \"--iflist\",\n                \"--resume\",\n                \"--stylesheet\",\n                \"--datadir\",\n            ]\n        )\n        # more reliable than just using os.name() (cygwin)\n        self.__is_windows = platform.system() == \"Windows\"\n        if fqp:\n            if os.path.isfile(fqp) and os.access(fqp, os.X_OK):\n                self.__nmap_binary = fqp\n            else:\n                raise EnvironmentError(1, \"wrong path or not executable\", fqp)\n        else:\n            nmap_binary_name = \"nmap\"\n            self.__nmap_binary = self._whereis(nmap_binary_name)\n        self.__nmap_fixed_options = \"-oX - -vvv --stats-every 1s\"\n\n        if self.__nmap_binary is None:\n            em = \"nmap is not installed or could not be found in system path\"\n            raise EnvironmentError(1, em)\n\n        if isinstance(targets, str):\n            self.__nmap_targets = targets.replace(\" \", \"\").split(\",\")\n        elif isinstance(targets, list):\n            self.__nmap_targets = targets\n        else:\n            raise Exception(\n                \"Supplied target list should be either a string or a list\"\n            )\n\n        self._nmap_options = set(options.split())\n        if safe_mode and not self._nmap_options.isdisjoint(unsafe_opts):\n            raise Exception(\n                \"unsafe options activated while safe_mode is set True\"\n            )\n        self.__nmap_dynamic_options = options\n        self.__sudo_run = \"\"\n        self.__nmap_command_line = self.get_command_line()\n\n        if event_callback and callable(event_callback):\n            self.__nmap_event_callback = event_callback\n        else:\n            self.__nmap_event_callback = None\n        (\n            self.DONE,\n            self.READY,\n            self.RUNNING,\n            self.CANCELLED,\n            self.FAILED,\n        ) = range(5)\n        self._run_init()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& true_classes = context->input(0);\n    OP_REQUIRES(context, true_classes.dims() == 2,\n                errors::InvalidArgument(\"true_classes must be a matrix\"));\n    const int32_t batch_size = true_classes.dim_size(0);\n    OP_REQUIRES(\n        context, true_classes.dim_size(1) == num_true_,\n        errors::InvalidArgument(\"true_classes must have \"\n                                \"num_true columns, expected: \",\n                                true_classes.dim_size(1), \" was: \", num_true_));\n    CHECK(sampler_) << \"CandidateSamplerOp did not set sampler_\";\n\n    if (unique_) {\n      OP_REQUIRES(context, num_sampled_ <= sampler_->range(),\n                  errors::InvalidArgument(\"Sampler's range is too small.\"));\n    }\n\n    // Output candidates and expected_count.\n    Tensor* out_sampled_candidates = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({num_sampled_}),\n                                            &out_sampled_candidates));\n\n    Tensor* out_true_expected_count = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                1, TensorShape({batch_size, num_true_}),\n                                &out_true_expected_count));\n    Tensor* out_sampled_expected_count = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(2, TensorShape({num_sampled_}),\n                                            &out_sampled_expected_count));\n\n    gtl::ArraySlice<int64_t> true_candidate(\n        true_classes.matrix<int64_t>().data(), batch_size * num_true_);\n    gtl::MutableArraySlice<int64_t> sampled_candidate(\n        out_sampled_candidates->vec<int64_t>().data(), num_sampled_);\n    gtl::MutableArraySlice<float> true_expected_count(\n        out_true_expected_count->matrix<float>().data(),\n        batch_size * num_true_);\n    gtl::MutableArraySlice<float> sampled_expected_count(\n        out_sampled_expected_count->vec<float>().data(), num_sampled_);\n\n    // Approximately conservatively estimate the number of samples required.\n    // In cases where rejection sampling is used we may occasionally use more\n    // samples than expected, which will result in reused random bits.\n    const int64_t samples32 = 2048 * num_sampled_;\n\n    // Pick sampled candidates.\n    auto local_gen = generator_.ReserveSamples32(samples32);\n    random::SimplePhilox random(&local_gen);\n    sampler_->SampleBatchGetExpectedCount(&random, unique_, sampled_candidate,\n                                          sampled_expected_count,\n                                          true_candidate, true_expected_count);\n\n    if (sampler_->NeedsUpdates()) {\n      sampler_->Update(true_candidate);\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, instance, filename):\n        \"\"\"\n        Apply (if callable) or prepend (if a string) upload_to to the filename,\n        then delegate further processing of the name to the storage backend.\n        Until the storage layer, all file paths are expected to be Unix style\n        (with forward slashes).\n        \"\"\"\n        if callable(self.upload_to):\n            filename = self.upload_to(instance, filename)\n        else:\n            dirname = datetime.datetime.now().strftime(self.upload_to)\n            filename = posixpath.join(dirname, filename)\n        return self.storage.generate_filename(filename)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _rpsl_db_query_to_graphql_out(query: RPSLDatabaseQuery, info: GraphQLResolveInfo):\n    \"\"\"\n    Given an RPSL database query, execute it and clean up the output\n    to be suitable to return to GraphQL.\n\n    Main changes are:\n    - Enum handling\n    - Adding the asn and prefix fields if applicable\n    - Ensuring the right fields are returned as a list of strings or a string\n    \"\"\"\n    database_handler = info.context['request'].app.state.database_handler\n    if info.context.get('sql_trace'):\n        if 'sql_queries' not in info.context:\n            info.context['sql_queries'] = [repr(query)]\n        else:\n            info.context['sql_queries'].append(repr(query))\n\n    for row in database_handler.execute_query(query, refresh_on_error=True):\n        graphql_result = {snake_to_camel_case(k): v for k, v in row.items() if k != 'parsed_data'}\n        if 'object_text' in row:\n            graphql_result['objectText'] = remove_auth_hashes(row['object_text'])\n        if 'rpki_status' in row:\n            graphql_result['rpkiStatus'] = row['rpki_status']\n        if row.get('ip_first') is not None and row.get('prefix_length'):\n            graphql_result['prefix'] = row['ip_first'] + '/' + str(row['prefix_length'])\n        if row.get('asn_first') is not None and row.get('asn_first') == row.get('asn_last'):\n            graphql_result['asn'] = row['asn_first']\n\n        object_type = resolve_rpsl_object_type(row)\n        for key, value in row.get('parsed_data', dict()).items():\n            if key == 'auth':\n                value = remove_auth_hashes(value)\n            graphql_type = schema.graphql_types[object_type][key]\n            if graphql_type == 'String' and isinstance(value, list):\n                value = '\\n'.join(value)\n            graphql_result[snake_to_camel_case(key)] = value\n        yield graphql_result\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_annotation(self, annotation, alias, is_summary=False, select=True):\n        \"\"\"Add a single annotation expression to the Query.\"\"\"\n        annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,\n                                                   summarize=is_summary)\n        if select:\n            self.append_annotation_mask([alias])\n        else:\n            self.set_annotation_mask(set(self.annotation_select).difference({alias}))\n        self.annotations[alias] = annotation\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor x = context->input(0);\n    const Tensor& scale = context->input(1);\n    const Tensor& offset = context->input(2);\n    const Tensor& estimated_mean = context->input(3);\n    const Tensor& estimated_variance = context->input(4);\n    const Tensor* side_input = has_side_input_ ? &context->input(5) : nullptr;\n\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(context, offset.dims() == 1,\n                errors::InvalidArgument(\"offset must be 1-dimensional\",\n                                        offset.shape().DebugString()));\n    OP_REQUIRES(context, estimated_mean.dims() == 1,\n                errors::InvalidArgument(\"estimated_mean must be 1-dimensional\",\n                                        estimated_mean.shape().DebugString()));\n    OP_REQUIRES(\n        context, estimated_variance.dims() == 1,\n        errors::InvalidArgument(\"estimated_variance must be 1-dimensional\",\n                                estimated_variance.shape().DebugString()));\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64 in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64 in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64 in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64 in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64 in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    if (has_side_input_) {\n      OP_REQUIRES(context, side_input->shape() == x.shape(),\n                  errors::InvalidArgument(\n                      \"side_input shape must be equal to input shape: \",\n                      side_input->shape().DebugString(),\n                      \" != \", x.shape().DebugString()));\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity) {\n      // NOTE(ezhulenev): This requirement is coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx.\n      OP_REQUIRES(\n          context, !is_training_ || x.dim_size(3) % 4 == 0,\n          errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                  \"channel dimension to be a multiple of 4.\"));\n    }\n\n    Tensor* y = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, alloc_shape, &y));\n\n    Tensor* batch_mean = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {3}, 1, scale.shape(), &batch_mean));\n    Tensor* batch_var = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {4}, 2, scale.shape(), &batch_var));\n    Tensor* saved_mean = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(3, scale.shape(), &saved_mean));\n    Tensor* saved_maybe_inv_var = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(4, scale.shape(),\n                                                     &saved_maybe_inv_var));\n\n    if (is_training_) {\n      functor::FusedBatchNorm<Device, T, U, true>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    } else {\n      functor::FusedBatchNorm<Device, T, U, false>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, y->CopyFrom(*y, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def complete_reg(request):\n    \"\"\"Completes the registeration, called by API\"\"\"\n    try:\n        data = cbor.decode(request.body)\n\n        client_data = CollectedClientData(data['clientDataJSON'])\n        att_obj = AttestationObject((data['attestationObject']))\n        server = getServer()\n        auth_data = server.register_complete(\n            request.session['fido_state'],\n            client_data,\n            att_obj\n        )\n        encoded = websafe_encode(auth_data.credential_data)\n        uk = User_Keys()\n        uk.username = request.user.username\n        uk.properties = {\"device\": encoded, \"type\": att_obj.fmt, }\n        uk.owned_by_enterprise = getattr(settings, \"MFA_OWNED_BY_ENTERPRISE\", False)\n        uk.key_type = \"FIDO2\"\n        uk.save()\n        return HttpResponse(simplejson.dumps({'status': 'OK'}))\n    except Exception as exp:\n        import traceback\n        print(traceback.format_exc())\n        try:\n            from raven.contrib.django.raven_compat.models import client\n            client.captureException()\n        except:\n            pass\n        return HttpResponse(simplejson.dumps({'status': 'ERR', \"message\": \"Error on server, please try again later\"}))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def lookup_path(\n        self, path: str\n    ) -> typing.Tuple[str, typing.Optional[os.stat_result]]:\n        for directory in self.all_directories:\n            joined_path = os.path.join(directory, path)\n            if self.follow_symlink:\n                full_path = os.path.abspath(joined_path)\n            else:\n                full_path = os.path.realpath(joined_path)\n            directory = os.path.realpath(directory)\n            if os.path.commonprefix([full_path, directory]) != directory:\n                # Don't allow misbehaving clients to break out of the static files\n                # directory.\n                continue\n            try:\n                return full_path, os.stat(full_path)\n            except (FileNotFoundError, NotADirectoryError):\n                continue\n        return \"\", None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def element():\n    element_url = src_url = request.args.get('url')\n    if element_url.startswith('gAAAAA'):\n        try:\n            cipher_suite = Fernet(g.session_key)\n            src_url = cipher_suite.decrypt(element_url.encode()).decode()\n        except (InvalidSignature, InvalidToken) as e:\n            return render_template(\n                'error.html',\n                error_message=str(e)), 401\n\n    src_type = request.args.get('type')\n\n    try:\n        file_data = g.user_request.send(base_url=src_url).content\n        tmp_mem = io.BytesIO()\n        tmp_mem.write(file_data)\n        tmp_mem.seek(0)\n\n        return send_file(tmp_mem, mimetype=src_type)\n    except exceptions.RequestException:\n        pass\n\n    empty_gif = base64.b64decode(\n        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n    return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, expression, delimiter, **extra):\n        super().__init__(expression, delimiter=delimiter, **extra)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def encode(self, types: Iterable[TypeStr], args: Iterable[Any]) -> bytes:\n        \"\"\"\n        Encodes the python values in ``args`` as a sequence of binary values of\n        the ABI types in ``types`` via the head-tail mechanism.\n\n        :param types: A list or tuple of string representations of the ABI types\n            that will be used for encoding e.g.  ``('uint256', 'bytes[]',\n            '(int,int)')``\n        :param args: A list or tuple of python values to be encoded.\n\n        :returns: The head-tail encoded binary representation of the python\n            values in ``args`` as values of the ABI types in ``types``.\n        \"\"\"\n        validate_list_like_param(types, \"types\")\n        validate_list_like_param(args, \"args\")\n\n        encoders = [self._registry.get_encoder(type_str) for type_str in types]\n\n        encoder = TupleEncoder(encoders=encoders)\n\n        return encoder(args)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status KernelAndDeviceOp::Run(\n    ScopedStepContainer* step_container, const EagerKernelArgs& inputs,\n    std::vector<EagerKernelRet>* outputs,\n    CancellationManager* cancellation_manager,\n    const absl::optional<EagerRemoteFunctionParams>& remote_func_params) {\n  OpKernelContext::Params params;\n  params.device = device_;\n  params.frame_iter = FrameAndIter(0, 0);\n  params.inputs = inputs.GetTensorValues();\n  params.op_kernel = kernel_.get();\n  params.resource_manager = device_->resource_manager();\n  params.input_alloc_attrs = &input_alloc_attrs_;\n  params.output_attr_array = output_alloc_attrs_.data();\n  params.function_library = flr_;\n  params.slice_reader_cache = &slice_reader_cache_;\n  params.rendezvous = rendezvous_;\n  OpExecutionState* op_execution_state = nullptr;\n\n  CancellationManager default_cancellation_manager;\n  if (cancellation_manager) {\n    params.cancellation_manager = cancellation_manager;\n  } else if (kernel_->is_deferred()) {\n    op_execution_state = new OpExecutionState;\n    params.cancellation_manager = &op_execution_state->cancellation_manager;\n    params.inc_num_deferred_ops_function = [op_execution_state]() {\n      op_execution_state->Ref();\n    };\n    params.dec_num_deferred_ops_function = [op_execution_state]() {\n      op_execution_state->Unref();\n    };\n  } else {\n    params.cancellation_manager = &default_cancellation_manager;\n  }\n\n  params.log_memory = log_memory_;\n\n  params.runner = get_runner();\n\n  params.step_container =\n      step_container == nullptr ? &step_container_ : step_container;\n  auto step_container_cleanup = gtl::MakeCleanup([step_container, this] {\n    if (step_container == nullptr) {\n      this->step_container_.CleanUp();\n    }\n  });\n\n  params.collective_executor =\n      collective_executor_ ? collective_executor_->get() : nullptr;\n\n  OpKernelContext context(&params);\n\n  {\n    port::ScopedFlushDenormal flush;\n    port::ScopedSetRound round(FE_TONEAREST);\n    // 'AnnotatedTraceMe' will trace both scheduling time on host and execution\n    // time on device of the OpKernel.\n    profiler::AnnotatedTraceMe activity(\n        [&] { return kernel_->TraceString(context, /*verbose=*/false); },\n        profiler::TraceMeLevel::kInfo);\n    device_->Compute(kernel_.get(), &context);\n  }\n\n  // Clean up execution op_execution_state if deferred ops aren't running.\n  if (op_execution_state != nullptr) {\n    op_execution_state->Unref();\n  }\n\n  if (!context.status().ok()) return context.status();\n\n  if (outputs != nullptr) {\n    outputs->clear();\n    for (int i = 0; i < context.num_outputs(); ++i) {\n      outputs->push_back(Tensor(*context.mutable_output(i)));\n    }\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get(self, path: str) -> None:\n        parts = path.split(\"/\")\n        component_name = parts[0]\n        component_root = self._registry.get_component_path(component_name)\n        if component_root is None:\n            self.write(\"not found\")\n            self.set_status(404)\n            return\n\n        filename = \"/\".join(parts[1:])\n        abspath = os.path.join(component_root, filename)\n\n        LOGGER.debug(\"ComponentRequestHandler: GET: %s -> %s\", path, abspath)\n\n        try:\n            with open(abspath, \"rb\") as file:\n                contents = file.read()\n        except (OSError) as e:\n            LOGGER.error(f\"ComponentRequestHandler: GET {path} read error\", exc_info=e)\n            self.write(\"read error\")\n            self.set_status(404)\n            return\n\n        self.write(contents)\n        self.set_header(\"Content-Type\", self.get_content_type(abspath))\n\n        self.set_extra_headers(path)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def decrypt_data(token, secret, data):\n    \"\"\" Decrypt the data with the given secret key. \"\"\"\n    if data.startswith(ENCRYPT_MARKER):\n        try:\n            # encrypted data\n            encoded_data = data[len(ENCRYPT_MARKER):]\n            aes_key = generate_aes_key(token, secret)\n            decoded_data = base64.b64decode(encoded_data)\n            iv = decoded_data[:16]\n            encrypted_data = decoded_data[16:]\n            cipher = AES.new(aes_key, AES.MODE_CFB, iv)\n            decrypted_data = cipher.decrypt(encrypted_data)\n            return json.loads(decrypted_data)\n        except:\n            raise DecryptError('data appeared to be corrupted')\n    else:\n        # doesn't appear to be encrypted data\n        return data\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_extra(self, select, select_params, where, params, tables, order_by):\n        \"\"\"\n        Add data to the various extra_* attributes for user-created additions\n        to the query.\n        \"\"\"\n        if select:\n            # We need to pair any placeholder markers in the 'select'\n            # dictionary with their parameters in 'select_params' so that\n            # subsequent updates to the select dictionary also adjust the\n            # parameters appropriately.\n            select_pairs = {}\n            if select_params:\n                param_iter = iter(select_params)\n            else:\n                param_iter = iter([])\n            for name, entry in select.items():\n                entry = str(entry)\n                entry_params = []\n                pos = entry.find(\"%s\")\n                while pos != -1:\n                    if pos == 0 or entry[pos - 1] != \"%\":\n                        entry_params.append(next(param_iter))\n                    pos = entry.find(\"%s\", pos + 2)\n                select_pairs[name] = (entry, entry_params)\n            self.extra.update(select_pairs)\n        if where or params:\n            self.where.add(ExtraWhere(where, params), AND)\n        if tables:\n            self.extra_tables += tuple(tables)\n        if order_by:\n            self.extra_order_by = order_by\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):\n        if not isinstance(arcname, str):\n            raise ValueError(\"Unsupported arcname\")\n        if isinstance(data, str):\n            self.writef(io.BytesIO(data.encode(\"UTF-8\")), arcname)\n        elif isinstance(data, bytes) or isinstance(data, bytearray) or isinstance(data, memoryview):\n            self.writef(io.BytesIO(data), arcname)\n        else:\n            raise ValueError(\"Unsupported data type.\")\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self):\n        dag_id = request.form.get('dag_id')\n        task_id = request.form.get('task_id')\n        origin = request.form.get('origin')\n        dag = current_app.dag_bag.get_dag(dag_id)\n        task = dag.get_task(task_id)\n\n        execution_date = request.form.get('execution_date')\n        execution_date = timezone.parse(execution_date)\n        ignore_all_deps = request.form.get('ignore_all_deps') == \"true\"\n        ignore_task_deps = request.form.get('ignore_task_deps') == \"true\"\n        ignore_ti_state = request.form.get('ignore_ti_state') == \"true\"\n\n        executor = ExecutorLoader.get_default_executor()\n        valid_celery_config = False\n        valid_kubernetes_config = False\n\n        try:\n            from airflow.executors.celery_executor import CeleryExecutor\n            valid_celery_config = isinstance(executor, CeleryExecutor)\n        except ImportError:\n            pass\n\n        try:\n            from airflow.executors.kubernetes_executor import KubernetesExecutor\n            valid_kubernetes_config = isinstance(executor, KubernetesExecutor)\n        except ImportError:\n            pass\n\n        if not valid_celery_config and not valid_kubernetes_config:\n            flash(\"Only works with the Celery or Kubernetes executors, sorry\", \"error\")\n            return redirect(origin)\n\n        ti = models.TaskInstance(task=task, execution_date=execution_date)\n        ti.refresh_from_db()\n\n        # Make sure the task instance can be run\n        dep_context = DepContext(\n            deps=RUNNING_DEPS,\n            ignore_all_deps=ignore_all_deps,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state)\n        failed_deps = list(ti.get_failed_dep_statuses(dep_context=dep_context))\n        if failed_deps:\n            failed_deps_str = \", \".join(\n                [\"{}: {}\".format(dep.dep_name, dep.reason) for dep in failed_deps])\n            flash(\"Could not queue task instance for execution, dependencies not met: \"\n                  \"{}\".format(failed_deps_str),\n                  \"error\")\n            return redirect(origin)\n\n        executor.start()\n        executor.queue_task_instance(\n            ti,\n            ignore_all_deps=ignore_all_deps,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state)\n        executor.heartbeat()\n        flash(\n            \"Sent {} to the message queue, \"\n            \"it should start any moment now.\".format(ti))\n        return redirect(origin)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "m3ApiRawFunction(m3_wasi_generic_fd_read)\n{\n    m3ApiReturnType  (uint32_t)\n    m3ApiGetArg      (__wasi_fd_t          , fd)\n    m3ApiGetArgMem   (wasi_iovec_t *       , wasi_iovs)\n    m3ApiGetArg      (__wasi_size_t        , iovs_len)\n    m3ApiGetArgMem   (__wasi_size_t *      , nread)\n\n    m3ApiCheckMem(wasi_iovs,    iovs_len * sizeof(wasi_iovec_t));\n    m3ApiCheckMem(nread,        sizeof(__wasi_size_t));\n\n#if defined(HAS_IOVEC)\n    struct iovec iovs[iovs_len];\n    copy_iov_to_host(_mem, iovs, wasi_iovs, iovs_len);\n\n    ssize_t ret = readv(fd, iovs, iovs_len);\n    if (ret < 0) { m3ApiReturn(errno_to_wasi(errno)); }\n    m3ApiWriteMem32(nread, ret);\n    m3ApiReturn(__WASI_ERRNO_SUCCESS);\n#else\n    ssize_t res = 0;\n    for (__wasi_size_t i = 0; i < iovs_len; i++) {\n        void* addr = m3ApiOffsetToPtr(m3ApiReadMem32(&wasi_iovs[i].buf));\n        size_t len = m3ApiReadMem32(&wasi_iovs[i].buf_len);\n        if (len == 0) continue;\n\n        int ret = read (fd, addr, len);\n        if (ret < 0) m3ApiReturn(errno_to_wasi(errno));\n        res += ret;\n        if ((size_t)ret < len) break;\n    }\n    m3ApiWriteMem32(nread, res);\n    m3ApiReturn(__WASI_ERRNO_SUCCESS);\n#endif\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_users(self, email):\n        \"\"\"Given an email, return matching user(s) who should receive a reset.\n\n        This allows subclasses to more easily customize the default policies\n        that prevent inactive users and users with unusable passwords from\n        resetting their password.\n        \"\"\"\n        active_users = UserModel._default_manager.filter(**{\n            '%s__iexact' % UserModel.get_email_field_name(): email,\n            'is_active': True,\n        })\n        return (u for u in active_users if u.has_usable_password())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool IsHostMemoryArg(const EagerOperation& op, const NodeDef* node_def,\n                     const Device* op_device, const KernelDef* kernel_def,\n                     const int port_id) {\n  if (op.is_function()) return false;\n  if (node_def == nullptr) return false;\n  if (kernel_def == nullptr || op_device == nullptr) return false;\n  const auto& host_memory_args = kernel_def->host_memory_arg();\n  const OpDef& op_def = OpRegistry::Global()->LookUp(op.Name())->op_def;\n  const int arg_id = OpPortIdToArgId(*node_def, op_def.input_arg(), port_id);\n  return std::find(host_memory_args.begin(), host_memory_args.end(),\n                   op_def.input_arg(arg_id).name()) != host_memory_args.end();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def authenticate(self, context, auth=None):\n        \"\"\"Authenticate credentials and return a token.\n\n        Accept auth as a dict that looks like::\n\n            {\n                \"auth\":{\n                    \"passwordCredentials\":{\n                        \"username\":\"test_user\",\n                        \"password\":\"mypass\"\n                    },\n                    \"tenantName\":\"customer-x\"\n                }\n            }\n\n        In this case, tenant is optional, if not provided the token will be\n        considered \"unscoped\" and can later be used to get a scoped token.\n\n        Alternatively, this call accepts auth with only a token and tenant\n        that will return a token that is scoped to that tenant.\n        \"\"\"\n\n        token_id = uuid.uuid4().hex\n        if 'passwordCredentials' in auth:\n            username = auth['passwordCredentials'].get('username', '')\n            password = auth['passwordCredentials'].get('password', '')\n            tenant_name = auth.get('tenantName', None)\n\n            user_id = auth['passwordCredentials'].get('userId', None)\n            if username:\n                user_ref = self.identity_api.get_user_by_name(\n                        context=context, user_name=username)\n                if user_ref:\n                    user_id = user_ref['id']\n\n            # more compat\n            tenant_id = auth.get('tenantId', None)\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                if tenant_ref:\n                    tenant_id = tenant_ref['id']\n\n            try:\n                auth_info = self.identity_api.authenticate(context=context,\n                                                           user_id=user_id,\n                                                           password=password,\n                                                           tenant_id=tenant_id)\n                (user_ref, tenant_ref, metadata_ref) = auth_info\n\n                # If the user is disabled don't allow them to authenticate\n                if not user_ref.get('enabled', True):\n                    LOG.warning('User %s is disabled' % user_id)\n                    raise exception.Unauthorized()\n            except AssertionError as e:\n                raise exception.Unauthorized(e.message)\n\n            token_ref = self.token_api.create_token(\n                    context, token_id, dict(id=token_id,\n                                            user=user_ref,\n                                            tenant=tenant_ref,\n                                            metadata=metadata_ref))\n            if tenant_ref:\n                catalog_ref = self.catalog_api.get_catalog(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'],\n                        metadata=metadata_ref)\n            else:\n                catalog_ref = {}\n\n        elif 'token' in auth:\n            token = auth['token'].get('id', None)\n\n            tenant_name = auth.get('tenantName')\n\n            # more compat\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                tenant_id = tenant_ref['id']\n            else:\n                tenant_id = auth.get('tenantId', None)\n\n            try:\n                old_token_ref = self.token_api.get_token(context=context,\n                                                         token_id=token)\n            except exception.NotFound:\n                raise exception.Unauthorized()\n\n            user_ref = old_token_ref['user']\n\n            # If the user is disabled don't allow them to authenticate\n            current_user_ref = self.identity_api.get_user(\n                                                    context=context,\n                                                    user_id=user_ref['id'])\n            if not current_user_ref.get('enabled', True):\n                LOG.warning('User %s is disabled' % user_ref['id'])\n                raise exception.Unauthorized()\n\n            tenants = self.identity_api.get_tenants_for_user(context,\n                                                             user_ref['id'])\n            if tenant_id:\n                assert tenant_id in tenants\n\n            tenant_ref = self.identity_api.get_tenant(context=context,\n                                                      tenant_id=tenant_id)\n            if tenant_ref:\n                metadata_ref = self.identity_api.get_metadata(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'])\n                catalog_ref = self.catalog_api.get_catalog(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'],\n                        metadata=metadata_ref)\n            else:\n                metadata_ref = {}\n                catalog_ref = {}\n\n            token_ref = self.token_api.create_token(\n                    context, token_id, dict(id=token_id,\n                                            user=user_ref,\n                                            tenant=tenant_ref,\n                                            metadata=metadata_ref,\n                                            expires=old_token_ref['expires']))\n\n        # TODO(termie): optimize this call at some point and put it into the\n        #               the return for metadata\n        # fill out the roles in the metadata\n        roles_ref = []\n        for role_id in metadata_ref.get('roles', []):\n            roles_ref.append(self.identity_api.get_role(context, role_id))\n        logging.debug('TOKEN_REF %s', token_ref)\n        return self._format_authenticate(token_ref, roles_ref, catalog_ref)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_url(url):\n    if os.path.exists(url):\n        with open(url, 'r') as opener:\n            content = opener.read()\n    else:\n        content = url\n\n    urls = []\n    for _url in content.splitlines():\n        if not _url:\n            continue\n\n        urls.append((_validate(_url), _url))\n    \n    return urls\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, sydent):\n        self.sydent = sydent\n\n        root = Resource()\n        matrix = Resource()\n        identity = Resource()\n        api = Resource()\n        v1 = self.sydent.servlets.v1\n        v2 = self.sydent.servlets.v2\n\n        validate = Resource()\n        validate_v2 = Resource()\n        email = Resource()\n        email_v2 = Resource()\n        msisdn = Resource()\n        msisdn_v2 = Resource()\n\n        threepid_v1 = Resource()\n        threepid_v2 = Resource()\n        unbind = self.sydent.servlets.threepidUnbind\n\n        pubkey = Resource()\n        ephemeralPubkey = Resource()\n\n        root.putChild(b'_matrix', matrix)\n        matrix.putChild(b'identity', identity)\n        identity.putChild(b'api', api)\n        identity.putChild(b'v2', v2)\n        api.putChild(b'v1', v1)\n\n        validate.putChild(b'email', email)\n        validate.putChild(b'msisdn', msisdn)\n\n        validate_v2.putChild(b'email', email_v2)\n        validate_v2.putChild(b'msisdn', msisdn_v2)\n\n        v1.putChild(b'validate', validate)\n\n        v1.putChild(b'lookup', self.sydent.servlets.lookup)\n        v1.putChild(b'bulk_lookup', self.sydent.servlets.bulk_lookup)\n\n        v1.putChild(b'pubkey', pubkey)\n        pubkey.putChild(b'isvalid', self.sydent.servlets.pubkeyIsValid)\n        pubkey.putChild(b'ed25519:0', self.sydent.servlets.pubkey_ed25519)\n        pubkey.putChild(b'ephemeral', ephemeralPubkey)\n        ephemeralPubkey.putChild(b'isvalid', self.sydent.servlets.ephemeralPubkeyIsValid)\n\n        threepid_v2.putChild(b'getValidated3pid', self.sydent.servlets.getValidated3pidV2)\n        threepid_v2.putChild(b'bind', self.sydent.servlets.threepidBindV2)\n        threepid_v2.putChild(b'unbind', unbind)\n\n        threepid_v1.putChild(b'getValidated3pid', self.sydent.servlets.getValidated3pid)\n        threepid_v1.putChild(b'unbind', unbind)\n        if self.sydent.enable_v1_associations:\n            threepid_v1.putChild(b'bind', self.sydent.servlets.threepidBind)\n\n        v1.putChild(b'3pid', threepid_v1)\n\n        email.putChild(b'requestToken', self.sydent.servlets.emailRequestCode)\n        email.putChild(b'submitToken', self.sydent.servlets.emailValidate)\n\n        email_v2.putChild(b'requestToken', self.sydent.servlets.emailRequestCodeV2)\n        email_v2.putChild(b'submitToken', self.sydent.servlets.emailValidateV2)\n\n        msisdn.putChild(b'requestToken', self.sydent.servlets.msisdnRequestCode)\n        msisdn.putChild(b'submitToken', self.sydent.servlets.msisdnValidate)\n\n        msisdn_v2.putChild(b'requestToken', self.sydent.servlets.msisdnRequestCodeV2)\n        msisdn_v2.putChild(b'submitToken', self.sydent.servlets.msisdnValidateV2)\n\n        v1.putChild(b'store-invite', self.sydent.servlets.storeInviteServlet)\n\n        v1.putChild(b'sign-ed25519', self.sydent.servlets.blindlySignStuffServlet)\n\n        # v2\n        # note v2 loses the /api so goes on 'identity' not 'api'\n        identity.putChild(b'v2', v2)\n\n        # v2 exclusive APIs\n        v2.putChild(b'terms', self.sydent.servlets.termsServlet)\n        account = self.sydent.servlets.accountServlet\n        v2.putChild(b'account', account)\n        account.putChild(b'register', self.sydent.servlets.registerServlet)\n        account.putChild(b'logout', self.sydent.servlets.logoutServlet)\n\n        # v2 versions of existing APIs\n        v2.putChild(b'validate', validate_v2)\n        v2.putChild(b'pubkey', pubkey)\n        v2.putChild(b'3pid', threepid_v2)\n        v2.putChild(b'store-invite', self.sydent.servlets.storeInviteServletV2)\n        v2.putChild(b'sign-ed25519', self.sydent.servlets.blindlySignStuffServletV2)\n        v2.putChild(b'lookup', self.sydent.servlets.lookup_v2)\n        v2.putChild(b'hash_details', self.sydent.servlets.hash_details)\n\n        self.factory = Site(root)\n        self.factory.displayTracebacks = False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ValidatePaddingValues(absl::Span<const int64_t> input_dimensions,\n                             absl::Span<const int64_t> window_dimensions,\n                             absl::Span<const int64_t> window_strides) {\n  bool ok = input_dimensions.size() == window_dimensions.size() &&\n            input_dimensions.size() == window_strides.size();\n  if (!ok) {\n    return InvalidArgument(\n        \"Want input dimensions size %u = window dimensions size %u = window \"\n        \"strides size %u\",\n        input_dimensions.size(), window_dimensions.size(),\n        window_strides.size());\n  }\n  return OkStatus();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def read_config(self, config, **kwargs):\n        # FIXME: federation_domain_whitelist needs sytests\n        self.federation_domain_whitelist = None  # type: Optional[dict]\n        federation_domain_whitelist = config.get(\"federation_domain_whitelist\", None)\n\n        if federation_domain_whitelist is not None:\n            # turn the whitelist into a hash for speed of lookup\n            self.federation_domain_whitelist = {}\n\n            for domain in federation_domain_whitelist:\n                self.federation_domain_whitelist[domain] = True\n\n        ip_range_blacklist = config.get(\"ip_range_blacklist\", [])\n\n        # Attempt to create an IPSet from the given ranges\n        try:\n            self.ip_range_blacklist = IPSet(ip_range_blacklist)\n        except Exception as e:\n            raise ConfigError(\"Invalid range(s) provided in ip_range_blacklist: %s\" % e)\n        # Always blacklist 0.0.0.0, ::\n        self.ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n\n        # The federation_ip_range_blacklist is used for backwards-compatibility\n        # and only applies to federation and identity servers. If it is not given,\n        # default to ip_range_blacklist.\n        federation_ip_range_blacklist = config.get(\n            \"federation_ip_range_blacklist\", ip_range_blacklist\n        )\n        try:\n            self.federation_ip_range_blacklist = IPSet(federation_ip_range_blacklist)\n        except Exception as e:\n            raise ConfigError(\n                \"Invalid range(s) provided in federation_ip_range_blacklist: %s\" % e\n            )\n        # Always blacklist 0.0.0.0, ::\n        self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n\n        federation_metrics_domains = config.get(\"federation_metrics_domains\") or []\n        validate_config(\n            _METRICS_FOR_DOMAINS_SCHEMA,\n            federation_metrics_domains,\n            (\"federation_metrics_domains\",),\n        )\n        self.federation_metrics_domains = set(federation_metrics_domains)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status SparseTensorToCSRSparseMatrixCPUFunctor::operator()(\n    const int64 batch_size, const int num_rows,\n    TTypes<int64>::ConstMatrix indices, TTypes<int32>::Vec batch_ptr,\n    TTypes<int32>::Vec csr_row_ptr, TTypes<int32>::Vec csr_col_ind) {\n  // Validate inputs.\n  if (batch_ptr.size() != batch_size + 1) {\n    return errors::InvalidArgument(\n        \"Expected batch_ptr.size() == batch_size + 1. Got: \", batch_ptr.size(),\n        \" vs. \", batch_size + 1);\n  }\n  if (csr_row_ptr.size() != batch_size * (num_rows + 1)) {\n    return errors::InvalidArgument(\n        \"Expected csr_row_ptr.size() == batch_size * (num_rows + 1). Got: \",\n        csr_row_ptr.size(), \" vs. \", batch_size * (num_rows + 1));\n  }\n\n  const int64 total_nnz = indices.dimension(0);\n  const int rank = indices.dimension(1);\n  if (rank == 2 && batch_size != 1) {\n    return errors::InvalidArgument(\n        \"Expected batch_size == 1 when rank is 2. Got batch_size: \",\n        batch_size);\n  }\n  if (csr_col_ind.size() != total_nnz) {\n    return errors::InvalidArgument(\n        \"Expected csr_col_ind.size() == total_nnz. Got: \", csr_col_ind.size(),\n        \" vs. \", total_nnz);\n  }\n\n  int prev_batch = -1;\n  if (rank == 2) {\n    // For a single batch, the batch_ptrs are {0, total_nnz}.\n    batch_ptr(0) = 0;\n    ++prev_batch;\n\n    for (int64 i = 0; i < total_nnz; ++i) {\n      // For now, the rows pointers store the corresponding row counts.\n      int64 ix = indices(i, 0) + 1;\n      if (ix >= csr_row_ptr.size()) {\n        return errors::InvalidArgument(\"Got an index \", ix,\n                                       \" that is outside of csr_row_ptr\");\n      }\n      csr_row_ptr(indices(i, 0) + 1) += 1;\n      csr_col_ind(i) = indices(i, 1);\n    }\n  } else {  // rank == 3\n    for (int64 i = 0; i < total_nnz; ++i) {\n      const int cur_batch = indices(i, 0);\n      // For now, the rows pointers store the corresponding row counts.\n      csr_row_ptr(cur_batch * (num_rows + 1) + indices(i, 1) + 1) += 1;\n      csr_col_ind(i) = indices(i, 2);\n\n      // We're at a new batch and might have skipped over empty batches.\n      while (prev_batch < cur_batch) {\n        // The previous batch ends at position i.\n        batch_ptr(prev_batch + 1) = i;\n        ++prev_batch;\n      }\n    }\n  }\n  // Set the last element of batch_ptr and account for trailing empty batches.\n  while (prev_batch < batch_size) {\n    batch_ptr(prev_batch + 1) = total_nnz;\n    ++prev_batch;\n  }\n\n  // Compute the cumulative row counts for each batch.\n  for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n    auto* row_ptr_batch = csr_row_ptr.data() + batch_idx * (num_rows + 1);\n    std::partial_sum(row_ptr_batch, row_ptr_batch + num_rows + 1,\n                     row_ptr_batch);\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _complete_multipart_upload_artifact(artifact_path):\n    \"\"\"\n    A request handler for `POST /mlflow-artifacts/mpu/complete` to complete a multipart upload\n    to `artifact_path` (a relative path from the root artifact directory).\n    \"\"\"\n    artifact_path = validate_path_is_safe(artifact_path)\n\n    request_message = _get_request_message(\n        CompleteMultipartUpload(),\n        schema={\n            \"path\": [_assert_required, _assert_string],\n            \"upload_id\": [_assert_string],\n            \"parts\": [_assert_required],\n        },\n    )\n    path = request_message.path\n    upload_id = request_message.upload_id\n    parts = [MultipartUploadPart.from_proto(part) for part in request_message.parts]\n\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    _validate_support_multipart_upload(artifact_repo)\n\n    artifact_repo.complete_multipart_upload(\n        path,\n        upload_id,\n        parts,\n        artifact_path,\n    )\n    return _wrap_response(CompleteMultipartUpload.Response())\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def put_file(path):\n    try:\n        data_file, metadata_file = get_paths(app.config[\"DATA_ROOT\"], path)\n    except werkzeug.exceptions.NotFound:\n        return flask.Response(\n            \"Not Found\",\n            404,\n            mimetype=\"text/plain\",\n        )\n\n    verification_key = flask.request.args.get(\"v\", \"\")\n    length = int(flask.request.headers.get(\"Content-Length\", 0))\n    hmac_input = \"{} {}\".format(path, length).encode(\"utf-8\")\n    key = app.config[\"SECRET_KEY\"]\n    mac = hmac.new(key, hmac_input, hashlib.sha256)\n    digest = mac.hexdigest()\n\n    if not hmac.compare_digest(digest, verification_key):\n        return flask.Response(\n            \"Invalid verification key\",\n            403,\n            mimetype=\"text/plain\",\n        )\n\n    content_type = flask.request.headers.get(\n        \"Content-Type\",\n        \"application/octet-stream\",\n    )\n\n    data_file.parent.mkdir(parents=True, exist_ok=True, mode=0o770)\n\n    try:\n        with write_file(data_file) as fout:\n            stream_file(flask.request.stream, fout, length)\n\n            with metadata_file.open(\"x\") as f:\n                json.dump(\n                    {\n                        \"headers\": {\"Content-Type\": content_type},\n                    },\n                    f,\n                )\n    except EOFError:\n        return flask.Response(\n            \"Bad Request\",\n            400,\n            mimetype=\"text/plain\",\n        )\n    except OSError as exc:\n        if exc.errno == errno.EEXIST:\n            return flask.Response(\n                \"Conflict\",\n                409,\n                mimetype=\"text/plain\",\n            )\n        raise\n\n    return flask.Response(\n        \"Created\",\n        201,\n        mimetype=\"text/plain\",\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def authenticate_by_pam(username, password):\n    @pam_conversation\n    def conv(\n        num_msg, msg, resp, appdata_ptr\n    ):  # pylint: disable=unused-argument\n        # it is: *resp = (pam_response *) calloc(num_msg, sizeof(pam_response))\n        resp[0] = cast(\n            calloc(num_msg, sizeof(pam_response)), POINTER(pam_response)\n        )\n        for i in range(num_msg):\n            if msg[i].contents.msg_style == PAM_PROMPT_ECHO_OFF:\n                resp.contents[i].resp = strdup(password.encode(\"utf8\"))\n                resp.contents[i].resp_retcode = 0\n        return 0\n\n    pamh = pam_handle()\n    conversation = pam_conv(conv)\n    returncode = pam_start(\n        PCSD_SERVICE.encode(\"utf8\"),\n        username.encode(\"utf8\"),\n        byref(conversation),\n        byref(pamh),\n    )\n    if returncode == PAM_SUCCESS:\n        returncode = pam_authenticate(pamh, 0)\n    if returncode == PAM_SUCCESS:\n        returncode = pam_acct_mgmt(pamh, 0)\n    pam_end(pamh, returncode)\n    return returncode == PAM_SUCCESS\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<T, 1>::ConstTensor& values,\n                        const typename TTypes<T, 1>::ConstTensor& value_range,\n                        int32_t nbins, typename TTypes<Tout, 1>::Tensor& out) {\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    Tensor index_to_bin_tensor;\n\n    TF_RETURN_IF_ERROR(context->forward_input_or_allocate_temp(\n        {0}, DataTypeToEnum<int32>::value, TensorShape({values.size()}),\n        &index_to_bin_tensor));\n    auto index_to_bin = index_to_bin_tensor.flat<int32>();\n\n    const double step = static_cast<double>(value_range(1) - value_range(0)) /\n                        static_cast<double>(nbins);\n    const double nbins_minus_1 = static_cast<double>(nbins - 1);\n\n    // We cannot handle NANs in the algorithm below (due to the case to int32)\n    const Eigen::Tensor<int32, 1, 1> nans_tensor =\n        values.isnan().template cast<int32>();\n    const Eigen::Tensor<int32, 0, 1> reduced_tensor = nans_tensor.sum();\n    const int num_nans = reduced_tensor(0);\n    if (num_nans > 0) {\n      return errors::InvalidArgument(\"Histogram values must not contain NaN\");\n    }\n\n    // The calculation is done by finding the slot of each value in `values`.\n    // With [a, b]:\n    //   step = (b - a) / nbins\n    //   (x - a) / step\n    // , then the entries are mapped to output.\n\n    // Bug fix: Switch the order of cwiseMin and int32-casting to avoid\n    // producing a negative index when casting an big int64 number to int32\n    index_to_bin.device(d) =\n        ((values.cwiseMax(value_range(0)) - values.constant(value_range(0)))\n             .template cast<double>() /\n         step)\n            .cwiseMin(nbins_minus_1)\n            .template cast<int32>();\n\n    out.setZero();\n    for (int32_t i = 0; i < index_to_bin.size(); i++) {\n      out(index_to_bin(i)) += Tout(1);\n    }\n    return Status::OK();\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(\n        self,\n        targets=\"127.0.0.1\",\n        options=\"-sT\",\n        event_callback=None,\n        safe_mode=True,\n        fqp=None,\n    ):\n        \"\"\"\n        Constructor of NmapProcess class.\n\n        :param targets: hosts to be scanned. Could be a string of hosts \\\n        separated with a coma or a python list of hosts/ip.\n        :type targets: string or list\n\n        :param options: list of nmap options to be applied to scan. \\\n        These options are all documented in nmap's man pages.\n\n        :param event_callback: callable function which will be ran \\\n        each time nmap process outputs data. This function will receive \\\n        two parameters:\n\n            1. the nmap process object\n            2. the data produced by nmap process. See readme for examples.\n\n        :param safe_mode: parameter to protect unsafe options like -oN, -oG, \\\n        -iL, -oA,...\n\n        :param fqp: full qualified path, if None, nmap will be searched \\\n        in the PATH\n\n        :return: NmapProcess object\n\n        \"\"\"\n        Thread.__init__(self)\n        unsafe_opts = set(\n            [\n                \"-oG\",\n                \"-oN\",\n                \"-iL\",\n                \"-oA\",\n                \"-oS\",\n                \"-oX\",\n                \"--iflist\",\n                \"--resume\",\n                \"--stylesheet\",\n                \"--datadir\",\n            ]\n        )\n        # more reliable than just using os.name() (cygwin)\n        self.__is_windows = platform.system() == \"Windows\"\n        if fqp:\n            if os.path.isfile(fqp) and os.access(fqp, os.X_OK):\n                self.__nmap_binary = fqp\n            else:\n                raise EnvironmentError(1, \"wrong path or not executable\", fqp)\n        else:\n            nmap_binary_name = \"nmap\"\n            self.__nmap_binary = self._whereis(nmap_binary_name)\n        self.__nmap_fixed_options = \"-oX - -vvv --stats-every 1s\"\n\n        if self.__nmap_binary is None:\n            em = \"nmap is not installed or could not be found in system path\"\n            raise EnvironmentError(1, em)\n\n        if isinstance(targets, str):\n            self.__nmap_targets = targets.replace(\" \", \"\").split(\",\")\n        elif isinstance(targets, list):\n            self.__nmap_targets = targets\n        else:\n            raise Exception(\n                \"Supplied target list should be either a string or a list\"\n            )\n\n        for target in self.__nmap_targets:\n            self.__validate_target(target)\n\n        self._nmap_options = set(options.split())\n        if safe_mode and not self._nmap_options.isdisjoint(unsafe_opts):\n            raise Exception(\n                \"unsafe options activated while safe_mode is set True\"\n            )\n        self.__nmap_dynamic_options = options\n        self.__sudo_run = \"\"\n        self.__nmap_command_line = self.get_command_line()\n\n        if event_callback and callable(event_callback):\n            self.__nmap_event_callback = event_callback\n        else:\n            self.__nmap_event_callback = None\n        (\n            self.DONE,\n            self.READY,\n            self.RUNNING,\n            self.CANCELLED,\n            self.FAILED,\n        ) = range(5)\n        self._run_init()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& true_classes = context->input(0);\n    OP_REQUIRES(context, true_classes.dims() == 2,\n                errors::InvalidArgument(\"true_classes must be a matrix\"));\n    const int32_t batch_size = true_classes.dim_size(0);\n    OP_REQUIRES(\n        context, true_classes.dim_size(1) == num_true_,\n        errors::InvalidArgument(\"true_classes must have \"\n                                \"num_true columns, expected: \",\n                                true_classes.dim_size(1), \" was: \", num_true_));\n    CHECK(sampler_) << \"CandidateSamplerOp did not set sampler_\";\n\n    if (unique_) {\n      OP_REQUIRES(context, num_sampled_ <= sampler_->range(),\n                  errors::InvalidArgument(\"Sampler's range is too small.\"));\n    }\n\n    // Output candidates and expected_count.\n    Tensor* out_sampled_candidates = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({num_sampled_}),\n                                            &out_sampled_candidates));\n\n    Tensor* out_true_expected_count = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                1, TensorShape({batch_size, num_true_}),\n                                &out_true_expected_count));\n    Tensor* out_sampled_expected_count = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(2, TensorShape({num_sampled_}),\n                                            &out_sampled_expected_count));\n\n    gtl::ArraySlice<int64_t> true_candidate(\n        true_classes.matrix<int64_t>().data(), batch_size * num_true_);\n\n    for (const auto& candidate : true_candidate) {\n      OP_REQUIRES(context, candidate >= 0 && candidate < sampler_->range(),\n                  errors::InvalidArgument(\"`true_candidate` out of range [\", 0,\n                                          \", \", sampler_->range(),\n                                          \"), received \", candidate));\n    }\n\n    gtl::MutableArraySlice<int64_t> sampled_candidate(\n        out_sampled_candidates->vec<int64_t>().data(), num_sampled_);\n    gtl::MutableArraySlice<float> true_expected_count(\n        out_true_expected_count->matrix<float>().data(),\n        batch_size * num_true_);\n    gtl::MutableArraySlice<float> sampled_expected_count(\n        out_sampled_expected_count->vec<float>().data(), num_sampled_);\n\n    // Approximately conservatively estimate the number of samples required.\n    // In cases where rejection sampling is used we may occasionally use more\n    // samples than expected, which will result in reused random bits.\n    const int64_t samples32 = 2048 * num_sampled_;\n\n    // Pick sampled candidates.\n    auto local_gen = generator_.ReserveSamples32(samples32);\n    random::SimplePhilox random(&local_gen);\n    sampler_->SampleBatchGetExpectedCount(&random, unique_, sampled_candidate,\n                                          sampled_expected_count,\n                                          true_candidate, true_expected_count);\n\n    if (sampler_->NeedsUpdates()) {\n      sampler_->Update(true_candidate);\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, instance, filename):\n        \"\"\"\n        Apply (if callable) or prepend (if a string) upload_to to the filename,\n        then delegate further processing of the name to the storage backend.\n        Until the storage layer, all file paths are expected to be Unix style\n        (with forward slashes).\n        \"\"\"\n        filename = validate_file_name(filename)\n        if callable(self.upload_to):\n            filename = self.upload_to(instance, filename)\n        else:\n            dirname = datetime.datetime.now().strftime(self.upload_to)\n            filename = posixpath.join(dirname, filename)\n        return self.storage.generate_filename(filename)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _rpsl_db_query_to_graphql_out(query: RPSLDatabaseQuery, info: GraphQLResolveInfo):\n    \"\"\"\n    Given an RPSL database query, execute it and clean up the output\n    to be suitable to return to GraphQL.\n\n    Main changes are:\n    - Enum handling\n    - Adding the asn and prefix fields if applicable\n    - Ensuring the right fields are returned as a list of strings or a string\n    \"\"\"\n    database_handler = info.context['request'].app.state.database_handler\n    if info.context.get('sql_trace'):\n        if 'sql_queries' not in info.context:\n            info.context['sql_queries'] = [repr(query)]\n        else:\n            info.context['sql_queries'].append(repr(query))\n\n    for row in database_handler.execute_query(query, refresh_on_error=True):\n        graphql_result = {snake_to_camel_case(k): v for k, v in row.items() if k != 'parsed_data'}\n        if 'object_text' in row:\n            graphql_result['objectText'] = remove_auth_hashes(row['object_text'])\n        if 'rpki_status' in row:\n            graphql_result['rpkiStatus'] = row['rpki_status']\n        if row.get('ip_first') is not None and row.get('prefix_length'):\n            graphql_result['prefix'] = row['ip_first'] + '/' + str(row['prefix_length'])\n        if row.get('asn_first') is not None and row.get('asn_first') == row.get('asn_last'):\n            graphql_result['asn'] = row['asn_first']\n\n        object_type = resolve_rpsl_object_type(row)\n        for key, value in row.get('parsed_data', dict()).items():\n            if key == 'auth':\n                value = [remove_auth_hashes(v) for v in value]\n            graphql_type = schema.graphql_types[object_type][key]\n            if graphql_type == 'String' and isinstance(value, list):\n                value = '\\n'.join(value)\n            graphql_result[snake_to_camel_case(key)] = value\n        yield graphql_result\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_annotation(self, annotation, alias, is_summary=False, select=True):\n        \"\"\"Add a single annotation expression to the Query.\"\"\"\n        self.check_alias(alias)\n        annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,\n                                                   summarize=is_summary)\n        if select:\n            self.append_annotation_mask([alias])\n        else:\n            self.set_annotation_mask(set(self.annotation_select).difference({alias}))\n        self.annotations[alias] = annotation\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor x = context->input(0);\n    const Tensor& scale = context->input(1);\n    const Tensor& offset = context->input(2);\n    const Tensor& estimated_mean = context->input(3);\n    const Tensor& estimated_variance = context->input(4);\n    const Tensor* side_input = has_side_input_ ? &context->input(5) : nullptr;\n\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(context, offset.dims() == 1,\n                errors::InvalidArgument(\"offset must be 1-dimensional\",\n                                        offset.shape().DebugString()));\n    OP_REQUIRES(context, estimated_mean.dims() == 1,\n                errors::InvalidArgument(\"estimated_mean must be 1-dimensional\",\n                                        estimated_mean.shape().DebugString()));\n    OP_REQUIRES(\n        context, estimated_variance.dims() == 1,\n        errors::InvalidArgument(\"estimated_variance must be 1-dimensional\",\n                                estimated_variance.shape().DebugString()));\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64 in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64 in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64 in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64 in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64 in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    const auto num_channels = GetTensorDim(x, tensor_format_, 'C');\n    OP_REQUIRES(\n        context, scale.NumElements() == num_channels,\n        errors::InvalidArgument(\"scale must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                scale.NumElements(), \" and \", num_channels));\n    OP_REQUIRES(\n        context, offset.NumElements() == num_channels,\n        errors::InvalidArgument(\"offset must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                offset.NumElements(), \" and \", num_channels));\n    if (estimated_mean.NumElements() != 0) {\n      OP_REQUIRES(context, estimated_mean.NumElements() == num_channels,\n                  errors::InvalidArgument(\n                      \"mean must be empty or have the same number of \"\n                      \"elements as the channels of x, got \",\n                      estimated_mean.NumElements(), \" and \", num_channels));\n    }\n    if (estimated_variance.NumElements() != 0) {\n      OP_REQUIRES(context, estimated_variance.NumElements() == num_channels,\n                  errors::InvalidArgument(\n                      \"variance must be empty or have the same number of \"\n                      \"elements as the channels of x, got \",\n                      estimated_variance.NumElements(), \" and \", num_channels));\n    }\n\n    if (has_side_input_) {\n      OP_REQUIRES(context, side_input->shape() == x.shape(),\n                  errors::InvalidArgument(\n                      \"side_input shape must be equal to input shape: \",\n                      side_input->shape().DebugString(),\n                      \" != \", x.shape().DebugString()));\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity) {\n      // NOTE(ezhulenev): This requirement is coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx.\n      OP_REQUIRES(\n          context, !is_training_ || num_channels % 4 == 0,\n          errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                  \"channel dimension to be a multiple of 4.\"));\n    }\n\n    Tensor* y = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, alloc_shape, &y));\n\n    Tensor* batch_mean = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {3}, 1, scale.shape(), &batch_mean));\n    Tensor* batch_var = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {4}, 2, scale.shape(), &batch_var));\n    Tensor* saved_mean = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(3, scale.shape(), &saved_mean));\n    Tensor* saved_maybe_inv_var = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(4, scale.shape(),\n                                                     &saved_maybe_inv_var));\n\n    if (is_training_) {\n      functor::FusedBatchNorm<Device, T, U, true>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    } else {\n      functor::FusedBatchNorm<Device, T, U, false>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, y->CopyFrom(*y, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def complete_reg(request):\n    \"\"\"Completes the registeration, called by API\"\"\"\n    try:\n        if not \"fido_state\" in request.session:\n            return JsonResponse({'status': 'ERR', \"message\": \"FIDO Status can't be found, please try again\"})\n        data = cbor.decode(request.body)\n\n        client_data = CollectedClientData(data['clientDataJSON'])\n        att_obj = AttestationObject((data['attestationObject']))\n        server = getServer()\n        auth_data = server.register_complete(\n            request.session.pop['fido_state'],\n            client_data,\n            att_obj\n        )\n        encoded = websafe_encode(auth_data.credential_data)\n        uk = User_Keys()\n        uk.username = request.user.username\n        uk.properties = {\"device\": encoded, \"type\": att_obj.fmt, }\n        uk.owned_by_enterprise = getattr(settings, \"MFA_OWNED_BY_ENTERPRISE\", False)\n        uk.key_type = \"FIDO2\"\n        uk.save()\n        return HttpResponse(simplejson.dumps({'status': 'OK'}))\n    except Exception as exp:\n        import traceback\n        print(traceback.format_exc())\n        try:\n            from raven.contrib.django.raven_compat.models import client\n            client.captureException()\n        except:\n            pass\n        return JsonResponse({'status': 'ERR', \"message\": \"Error on server, please try again later\"})\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def lookup_path(\n        self, path: str\n    ) -> typing.Tuple[str, typing.Optional[os.stat_result]]:\n        for directory in self.all_directories:\n            joined_path = os.path.join(directory, path)\n            if self.follow_symlink:\n                full_path = os.path.abspath(joined_path)\n            else:\n                full_path = os.path.realpath(joined_path)\n            directory = os.path.realpath(directory)\n            if os.path.commonpath([full_path, directory]) != directory:\n                # Don't allow misbehaving clients to break out of the static files\n                # directory.\n                continue\n            try:\n                return full_path, os.stat(full_path)\n            except (FileNotFoundError, NotADirectoryError):\n                continue\n        return \"\", None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def element():\n    empty_gif = base64.b64decode(\n        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n    element_url = src_url = request.args.get('url')\n    if element_url.startswith('gAAAAA'):\n        try:\n            cipher_suite = Fernet(g.session_key)\n            src_url = cipher_suite.decrypt(element_url.encode()).decode()\n        except (InvalidSignature, InvalidToken) as e:\n            return render_template(\n                'error.html',\n                error_message=str(e)), 401\n\n    src_type = request.args.get('type')\n\n    # Ensure requested element is from a valid domain\n    domain = urlparse.urlparse(src_url).netloc\n    if not validators.domain(domain):\n        return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n\n    try:\n        file_data = g.user_request.send(base_url=src_url).content\n        tmp_mem = io.BytesIO()\n        tmp_mem.write(file_data)\n        tmp_mem.seek(0)\n\n        return send_file(tmp_mem, mimetype=src_type)\n    except exceptions.RequestException:\n        pass\n\n    return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, expression, delimiter, **extra):\n        delimiter_expr = Value(str(delimiter))\n        super().__init__(expression, delimiter_expr, **extra)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def encode(self, types: Iterable[TypeStr], args: Iterable[Any]) -> bytes:\n        \"\"\"\n        Encodes the python values in ``args`` as a sequence of binary values of\n        the ABI types in ``types`` via the head-tail mechanism.\n\n        :param types: A list or tuple of string representations of the ABI types\n            that will be used for encoding e.g.  ``('uint256', 'bytes[]',\n            '(int,int)')``\n        :param args: A list or tuple of python values to be encoded.\n\n        :returns: The head-tail encoded binary representation of the python\n            values in ``args`` as values of the ABI types in ``types``.\n        \"\"\"\n        # validate encode types\n        validate_codec_types(types)\n\n        # validate encode args\n        validate_list_like_param(args, \"args\")\n\n        encoders = [self._registry.get_encoder(type_str) for type_str in types]\n\n        encoder = TupleEncoder(encoders=encoders)\n\n        return encoder(args)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status KernelAndDeviceOp::Run(\n    ScopedStepContainer* step_container, const EagerKernelArgs& inputs,\n    std::vector<EagerKernelRet>* outputs,\n    CancellationManager* cancellation_manager,\n    const absl::optional<EagerRemoteFunctionParams>& remote_func_params) {\n  OpKernelContext::Params params;\n  params.device = device_;\n  params.frame_iter = FrameAndIter(0, 0);\n  params.inputs = inputs.GetTensorValues();\n  params.op_kernel = kernel_.get();\n  params.resource_manager = device_->resource_manager();\n  params.input_alloc_attrs = &input_alloc_attrs_;\n  params.output_attr_array = output_alloc_attrs_.data();\n  params.function_library = flr_;\n  params.slice_reader_cache = &slice_reader_cache_;\n  params.rendezvous = rendezvous_;\n  OpExecutionState* op_execution_state = nullptr;\n\n  CancellationManager default_cancellation_manager;\n  if (cancellation_manager) {\n    params.cancellation_manager = cancellation_manager;\n  } else if (kernel_->is_deferred()) {\n    op_execution_state = new OpExecutionState;\n    params.cancellation_manager = &op_execution_state->cancellation_manager;\n    params.inc_num_deferred_ops_function = [op_execution_state]() {\n      op_execution_state->Ref();\n    };\n    params.dec_num_deferred_ops_function = [op_execution_state]() {\n      op_execution_state->Unref();\n    };\n  } else {\n    params.cancellation_manager = &default_cancellation_manager;\n  }\n\n  params.log_memory = log_memory_;\n\n  params.runner = get_runner();\n\n  params.step_container =\n      step_container == nullptr ? &step_container_ : step_container;\n  auto step_container_cleanup = gtl::MakeCleanup([step_container, this] {\n    if (step_container == nullptr) {\n      this->step_container_.CleanUp();\n    }\n  });\n\n  params.collective_executor =\n      collective_executor_ ? collective_executor_->get() : nullptr;\n\n  OpKernelContext context(&params);\n\n  {\n    port::ScopedFlushDenormal flush;\n    port::ScopedSetRound round(FE_TONEAREST);\n    // 'AnnotatedTraceMe' will trace both scheduling time on host and execution\n    // time on device of the OpKernel.\n    profiler::AnnotatedTraceMe activity(\n        [&] { return kernel_->TraceString(context, /*verbose=*/false); },\n        profiler::TraceMeLevel::kInfo);\n    device_->Compute(kernel_.get(), &context);\n  }\n\n  // Clean up execution op_execution_state if deferred ops aren't running.\n  if (op_execution_state != nullptr) {\n    op_execution_state->Unref();\n  }\n\n  if (!context.status().ok()) return context.status();\n\n  if (outputs != nullptr) {\n    outputs->clear();\n    for (int i = 0; i < context.num_outputs(); ++i) {\n      const auto* output_tensor = context.mutable_output(i);\n      if (output_tensor != nullptr) {\n        outputs->push_back(Tensor(*output_tensor));\n      } else {\n        outputs->push_back(Tensor());\n      }\n    }\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get(self, path: str) -> None:\n        parts = path.split(\"/\")\n        component_name = parts[0]\n        component_root = self._registry.get_component_path(component_name)\n        if component_root is None:\n            self.write(\"not found\")\n            self.set_status(404)\n            return\n\n        # follow symlinks to get an accurate normalized path\n        component_root = os.path.realpath(component_root)\n        filename = \"/\".join(parts[1:])\n        abspath = os.path.realpath(os.path.join(component_root, filename))\n\n        # Do NOT expose anything outside of the component root.\n        if os.path.commonprefix([component_root, abspath]) != component_root:\n            self.write(\"forbidden\")\n            self.set_status(403)\n            return\n\n        LOGGER.debug(\"ComponentRequestHandler: GET: %s -> %s\", path, abspath)\n\n        try:\n            with open(abspath, \"rb\") as file:\n                contents = file.read()\n        except (OSError) as e:\n            LOGGER.error(f\"ComponentRequestHandler: GET {path} read error\", exc_info=e)\n            self.write(\"read error\")\n            self.set_status(404)\n            return\n\n        self.write(contents)\n        self.set_header(\"Content-Type\", self.get_content_type(abspath))\n\n        self.set_extra_headers(path)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def decrypt_data(key, data):\n    \"\"\" Decrypt the data with the given secret key. \"\"\"\n    iv = data[:16]\n    cipher = AES.new(key, AES.MODE_CBC, iv)\n    try:\n        result = cipher.decrypt(data[16:])\n    except Exception:\n        raise DecryptError('Encrypted data appears to be corrupted.')\n\n    # Strip the last n padding bytes where n is the last value in\n    # the plaintext\n    padding = ord(result[-1])\n    return result[:-1 * padding]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_extra(self, select, select_params, where, params, tables, order_by):\n        \"\"\"\n        Add data to the various extra_* attributes for user-created additions\n        to the query.\n        \"\"\"\n        if select:\n            # We need to pair any placeholder markers in the 'select'\n            # dictionary with their parameters in 'select_params' so that\n            # subsequent updates to the select dictionary also adjust the\n            # parameters appropriately.\n            select_pairs = {}\n            if select_params:\n                param_iter = iter(select_params)\n            else:\n                param_iter = iter([])\n            for name, entry in select.items():\n                self.check_alias(name)\n                entry = str(entry)\n                entry_params = []\n                pos = entry.find(\"%s\")\n                while pos != -1:\n                    if pos == 0 or entry[pos - 1] != \"%\":\n                        entry_params.append(next(param_iter))\n                    pos = entry.find(\"%s\", pos + 2)\n                select_pairs[name] = (entry, entry_params)\n            self.extra.update(select_pairs)\n        if where or params:\n            self.where.add(ExtraWhere(where, params), AND)\n        if tables:\n            self.extra_tables += tuple(tables)\n        if order_by:\n            self.extra_order_by = order_by\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):\n        if not check_archive_path(arcname):\n            raise ValueError(f\"Specified path is bad: {arcname}\")\n        return self._writestr(data, arcname)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self):\n        dag_id = request.form.get('dag_id')\n        task_id = request.form.get('task_id')\n        origin = get_safe_url(request.form.get('origin'))\n        dag = current_app.dag_bag.get_dag(dag_id)\n        task = dag.get_task(task_id)\n\n        execution_date = request.form.get('execution_date')\n        execution_date = timezone.parse(execution_date)\n        ignore_all_deps = request.form.get('ignore_all_deps') == \"true\"\n        ignore_task_deps = request.form.get('ignore_task_deps') == \"true\"\n        ignore_ti_state = request.form.get('ignore_ti_state') == \"true\"\n\n        executor = ExecutorLoader.get_default_executor()\n        valid_celery_config = False\n        valid_kubernetes_config = False\n\n        try:\n            from airflow.executors.celery_executor import CeleryExecutor\n            valid_celery_config = isinstance(executor, CeleryExecutor)\n        except ImportError:\n            pass\n\n        try:\n            from airflow.executors.kubernetes_executor import KubernetesExecutor\n            valid_kubernetes_config = isinstance(executor, KubernetesExecutor)\n        except ImportError:\n            pass\n\n        if not valid_celery_config and not valid_kubernetes_config:\n            flash(\"Only works with the Celery or Kubernetes executors, sorry\", \"error\")\n            return redirect(origin)\n\n        ti = models.TaskInstance(task=task, execution_date=execution_date)\n        ti.refresh_from_db()\n\n        # Make sure the task instance can be run\n        dep_context = DepContext(\n            deps=RUNNING_DEPS,\n            ignore_all_deps=ignore_all_deps,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state)\n        failed_deps = list(ti.get_failed_dep_statuses(dep_context=dep_context))\n        if failed_deps:\n            failed_deps_str = \", \".join(\n                [\"{}: {}\".format(dep.dep_name, dep.reason) for dep in failed_deps])\n            flash(\"Could not queue task instance for execution, dependencies not met: \"\n                  \"{}\".format(failed_deps_str),\n                  \"error\")\n            return redirect(origin)\n\n        executor.start()\n        executor.queue_task_instance(\n            ti,\n            ignore_all_deps=ignore_all_deps,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state)\n        executor.heartbeat()\n        flash(\n            \"Sent {} to the message queue, \"\n            \"it should start any moment now.\".format(ti))\n        return redirect(origin)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "m3ApiRawFunction(m3_wasi_generic_fd_read)\n{\n    m3ApiReturnType  (uint32_t)\n    m3ApiGetArg      (__wasi_fd_t          , fd)\n    m3ApiGetArgMem   (wasi_iovec_t *       , wasi_iovs)\n    m3ApiGetArg      (__wasi_size_t        , iovs_len)\n    m3ApiGetArgMem   (__wasi_size_t *      , nread)\n\n    m3ApiCheckMem(wasi_iovs,    iovs_len * sizeof(wasi_iovec_t));\n    m3ApiCheckMem(nread,        sizeof(__wasi_size_t));\n\n#if defined(HAS_IOVEC)\n    struct iovec iovs[iovs_len];\n    const void* mem_check = copy_iov_to_host(runtime, _mem, iovs, wasi_iovs, iovs_len);\n    if (mem_check != m3Err_none) {\n        return mem_check;\n    }\n\n    ssize_t ret = readv(fd, iovs, iovs_len);\n    if (ret < 0) { m3ApiReturn(errno_to_wasi(errno)); }\n    m3ApiWriteMem32(nread, ret);\n    m3ApiReturn(__WASI_ERRNO_SUCCESS);\n#else\n    ssize_t res = 0;\n    for (__wasi_size_t i = 0; i < iovs_len; i++) {\n        void* addr = m3ApiOffsetToPtr(m3ApiReadMem32(&wasi_iovs[i].buf));\n        size_t len = m3ApiReadMem32(&wasi_iovs[i].buf_len);\n        if (len == 0) continue;\n        m3ApiCheckMem(addr,     len);\n        int ret = read (fd, addr, len);\n        if (ret < 0) m3ApiReturn(errno_to_wasi(errno));\n        res += ret;\n        if ((size_t)ret < len) break;\n    }\n    m3ApiWriteMem32(nread, res);\n    m3ApiReturn(__WASI_ERRNO_SUCCESS);\n#endif\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_users(self, email):\n        \"\"\"Given an email, return matching user(s) who should receive a reset.\n\n        This allows subclasses to more easily customize the default policies\n        that prevent inactive users and users with unusable passwords from\n        resetting their password.\n        \"\"\"\n        email_field_name = UserModel.get_email_field_name()\n        active_users = UserModel._default_manager.filter(**{\n            '%s__iexact' % email_field_name: email,\n            'is_active': True,\n        })\n        return (\n            u for u in active_users\n            if u.has_usable_password() and\n            _unicode_ci_compare(email, getattr(u, email_field_name))\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool IsHostMemoryArg(const EagerOperation& op, const NodeDef* node_def,\n                     const Device* op_device, const KernelDef* kernel_def,\n                     const int port_id) {\n  if (op.is_function()) return false;\n  if (node_def == nullptr) return false;\n  if (kernel_def == nullptr || op_device == nullptr) return false;\n  const auto& host_memory_args = kernel_def->host_memory_arg();\n  const OpDef& op_def = OpRegistry::Global()->LookUp(op.Name())->op_def;\n  const int arg_id = OpPortIdToArgId(*node_def, op_def.input_arg(), port_id);\n  // Fail if argument ID not found.\n  if (arg_id < 0) {\n    return false;\n  }\n  return std::find(host_memory_args.begin(), host_memory_args.end(),\n                   op_def.input_arg(arg_id).name()) != host_memory_args.end();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def authenticate(self, context, auth=None):\n        \"\"\"Authenticate credentials and return a token.\n\n        Accept auth as a dict that looks like::\n\n            {\n                \"auth\":{\n                    \"passwordCredentials\":{\n                        \"username\":\"test_user\",\n                        \"password\":\"mypass\"\n                    },\n                    \"tenantName\":\"customer-x\"\n                }\n            }\n\n        In this case, tenant is optional, if not provided the token will be\n        considered \"unscoped\" and can later be used to get a scoped token.\n\n        Alternatively, this call accepts auth with only a token and tenant\n        that will return a token that is scoped to that tenant.\n        \"\"\"\n\n        token_id = uuid.uuid4().hex\n        if 'passwordCredentials' in auth:\n            username = auth['passwordCredentials'].get('username', '')\n            password = auth['passwordCredentials'].get('password', '')\n            tenant_name = auth.get('tenantName', None)\n\n            user_id = auth['passwordCredentials'].get('userId', None)\n            if username:\n                user_ref = self.identity_api.get_user_by_name(\n                        context=context, user_name=username)\n                if user_ref:\n                    user_id = user_ref['id']\n\n            # more compat\n            tenant_id = auth.get('tenantId', None)\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                if tenant_ref:\n                    tenant_id = tenant_ref['id']\n\n            try:\n                auth_info = self.identity_api.authenticate(context=context,\n                                                           user_id=user_id,\n                                                           password=password,\n                                                           tenant_id=tenant_id)\n                (user_ref, tenant_ref, metadata_ref) = auth_info\n\n                # If the user is disabled don't allow them to authenticate\n                if not user_ref.get('enabled', True):\n                    LOG.warning('User %s is disabled' % user_id)\n                    raise exception.Unauthorized()\n\n                # If the tenant is disabled don't allow them to authenticate\n                if tenant_ref and not tenant_ref.get('enabled', True):\n                    LOG.warning('Tenant %s is disabled' % tenant_id)\n                    raise exception.Unauthorized()\n            except AssertionError as e:\n                raise exception.Unauthorized(e.message)\n\n            token_ref = self.token_api.create_token(\n                    context, token_id, dict(id=token_id,\n                                            user=user_ref,\n                                            tenant=tenant_ref,\n                                            metadata=metadata_ref))\n            if tenant_ref:\n                catalog_ref = self.catalog_api.get_catalog(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'],\n                        metadata=metadata_ref)\n            else:\n                catalog_ref = {}\n\n        elif 'token' in auth:\n            token = auth['token'].get('id', None)\n\n            tenant_name = auth.get('tenantName')\n\n            # more compat\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                tenant_id = tenant_ref['id']\n            else:\n                tenant_id = auth.get('tenantId', None)\n\n            try:\n                old_token_ref = self.token_api.get_token(context=context,\n                                                         token_id=token)\n            except exception.NotFound:\n                raise exception.Unauthorized()\n\n            user_ref = old_token_ref['user']\n\n            # If the user is disabled don't allow them to authenticate\n            current_user_ref = self.identity_api.get_user(\n                                                    context=context,\n                                                    user_id=user_ref['id'])\n            if not current_user_ref.get('enabled', True):\n                LOG.warning('User %s is disabled' % user_ref['id'])\n                raise exception.Unauthorized()\n\n            tenants = self.identity_api.get_tenants_for_user(context,\n                                                             user_ref['id'])\n            if tenant_id:\n                assert tenant_id in tenants\n\n            tenant_ref = self.identity_api.get_tenant(context=context,\n                                                      tenant_id=tenant_id)\n\n            # If the tenant is disabled don't allow them to authenticate\n            if tenant_ref and not tenant_ref.get('enabled', True):\n                LOG.warning('Tenant %s is disabled' % tenant_id)\n                raise exception.Unauthorized()\n\n            if tenant_ref:\n                metadata_ref = self.identity_api.get_metadata(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'])\n                catalog_ref = self.catalog_api.get_catalog(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'],\n                        metadata=metadata_ref)\n            else:\n                metadata_ref = {}\n                catalog_ref = {}\n\n            token_ref = self.token_api.create_token(\n                    context, token_id, dict(id=token_id,\n                                            user=user_ref,\n                                            tenant=tenant_ref,\n                                            metadata=metadata_ref,\n                                            expires=old_token_ref['expires']))\n\n        # TODO(termie): optimize this call at some point and put it into the\n        #               the return for metadata\n        # fill out the roles in the metadata\n        roles_ref = []\n        for role_id in metadata_ref.get('roles', []):\n            roles_ref.append(self.identity_api.get_role(context, role_id))\n        logging.debug('TOKEN_REF %s', token_ref)\n        return self._format_authenticate(token_ref, roles_ref, catalog_ref)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_url(url):\n    urls = []\n    for _url in url.splitlines():\n        if not _url:\n            continue\n\n        urls.append((_validate(_url), _url))\n    \n    return urls\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, sydent):\n        self.sydent = sydent\n\n        root = Resource()\n        matrix = Resource()\n        identity = Resource()\n        api = Resource()\n        v1 = self.sydent.servlets.v1\n        v2 = self.sydent.servlets.v2\n\n        validate = Resource()\n        validate_v2 = Resource()\n        email = Resource()\n        email_v2 = Resource()\n        msisdn = Resource()\n        msisdn_v2 = Resource()\n\n        threepid_v1 = Resource()\n        threepid_v2 = Resource()\n        unbind = self.sydent.servlets.threepidUnbind\n\n        pubkey = Resource()\n        ephemeralPubkey = Resource()\n\n        root.putChild(b'_matrix', matrix)\n        matrix.putChild(b'identity', identity)\n        identity.putChild(b'api', api)\n        identity.putChild(b'v2', v2)\n        api.putChild(b'v1', v1)\n\n        validate.putChild(b'email', email)\n        validate.putChild(b'msisdn', msisdn)\n\n        validate_v2.putChild(b'email', email_v2)\n        validate_v2.putChild(b'msisdn', msisdn_v2)\n\n        v1.putChild(b'validate', validate)\n\n        v1.putChild(b'lookup', self.sydent.servlets.lookup)\n        v1.putChild(b'bulk_lookup', self.sydent.servlets.bulk_lookup)\n\n        v1.putChild(b'pubkey', pubkey)\n        pubkey.putChild(b'isvalid', self.sydent.servlets.pubkeyIsValid)\n        pubkey.putChild(b'ed25519:0', self.sydent.servlets.pubkey_ed25519)\n        pubkey.putChild(b'ephemeral', ephemeralPubkey)\n        ephemeralPubkey.putChild(b'isvalid', self.sydent.servlets.ephemeralPubkeyIsValid)\n\n        threepid_v2.putChild(b'getValidated3pid', self.sydent.servlets.getValidated3pidV2)\n        threepid_v2.putChild(b'bind', self.sydent.servlets.threepidBindV2)\n        threepid_v2.putChild(b'unbind', unbind)\n\n        threepid_v1.putChild(b'getValidated3pid', self.sydent.servlets.getValidated3pid)\n        threepid_v1.putChild(b'unbind', unbind)\n        if self.sydent.enable_v1_associations:\n            threepid_v1.putChild(b'bind', self.sydent.servlets.threepidBind)\n\n        v1.putChild(b'3pid', threepid_v1)\n\n        email.putChild(b'requestToken', self.sydent.servlets.emailRequestCode)\n        email.putChild(b'submitToken', self.sydent.servlets.emailValidate)\n\n        email_v2.putChild(b'requestToken', self.sydent.servlets.emailRequestCodeV2)\n        email_v2.putChild(b'submitToken', self.sydent.servlets.emailValidateV2)\n\n        msisdn.putChild(b'requestToken', self.sydent.servlets.msisdnRequestCode)\n        msisdn.putChild(b'submitToken', self.sydent.servlets.msisdnValidate)\n\n        msisdn_v2.putChild(b'requestToken', self.sydent.servlets.msisdnRequestCodeV2)\n        msisdn_v2.putChild(b'submitToken', self.sydent.servlets.msisdnValidateV2)\n\n        v1.putChild(b'store-invite', self.sydent.servlets.storeInviteServlet)\n\n        v1.putChild(b'sign-ed25519', self.sydent.servlets.blindlySignStuffServlet)\n\n        # v2\n        # note v2 loses the /api so goes on 'identity' not 'api'\n        identity.putChild(b'v2', v2)\n\n        # v2 exclusive APIs\n        v2.putChild(b'terms', self.sydent.servlets.termsServlet)\n        account = self.sydent.servlets.accountServlet\n        v2.putChild(b'account', account)\n        account.putChild(b'register', self.sydent.servlets.registerServlet)\n        account.putChild(b'logout', self.sydent.servlets.logoutServlet)\n\n        # v2 versions of existing APIs\n        v2.putChild(b'validate', validate_v2)\n        v2.putChild(b'pubkey', pubkey)\n        v2.putChild(b'3pid', threepid_v2)\n        v2.putChild(b'store-invite', self.sydent.servlets.storeInviteServletV2)\n        v2.putChild(b'sign-ed25519', self.sydent.servlets.blindlySignStuffServletV2)\n        v2.putChild(b'lookup', self.sydent.servlets.lookup_v2)\n        v2.putChild(b'hash_details', self.sydent.servlets.hash_details)\n\n        self.factory = Site(root)\n        self.factory.requestFactory = SizeLimitingRequest\n        self.factory.displayTracebacks = False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ValidatePaddingValues(absl::Span<const int64_t> input_dimensions,\n                             absl::Span<const int64_t> window_dimensions,\n                             absl::Span<const int64_t> window_strides) {\n  bool ok = input_dimensions.size() == window_dimensions.size() &&\n            input_dimensions.size() == window_strides.size();\n  if (!ok) {\n    return InvalidArgument(\n        \"Want input dimensions size %u = window dimensions size %u = window \"\n        \"strides size %u\",\n        input_dimensions.size(), window_dimensions.size(),\n        window_strides.size());\n  }\n  for (size_t i = 0; i < input_dimensions.size(); ++i) {\n    if (window_dimensions[i] <= 0) {\n      return InvalidArgument(\"Window dimension %u has non-positive size %d\", i,\n                             window_dimensions[i]);\n    }\n    if (window_strides[i] <= 0) {\n      return InvalidArgument(\"Window dimension %u has non-positive stride %d\",\n                             i, window_strides[i]);\n    }\n  }\n  return OkStatus();\n}\n"}, {"role": "assistant", "content": "False"}]}
