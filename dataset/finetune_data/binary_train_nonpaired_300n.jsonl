{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def failure(cls, request):\n        '''\n        Creates an RPCMessage instance containing exception information,\n        when the exception is from user-defined handlers or upper adaptation layers.\n\n        It must be called in an exception handler context, where ``sys.exc_info()``\n        returns a non-null tuple.\n        '''\n        exc_info = sys.exc_info()\n        return cls(\n            request.peer_id,\n            RPCMessageTypes.FAILURE,\n            request.method, request.order_key, request.seq_id,\n            ErrorMetadata(exc_info[0].__name__, traceback.format_exc()),\n            None,\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_send_leave_request(self, origin, content, room_id):\n        logger.debug(\"on_send_leave_request: content: %s\", content)\n\n        room_version = yield self.store.get_room_version(room_id)\n        format_ver = room_version_to_event_format(room_version)\n        pdu = event_from_pdu_json(content, format_ver)\n\n        origin_host, _ = parse_server_name(origin)\n        yield self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_leave_request: pdu sigs: %s\", pdu.signatures)\n        yield self.handler.on_send_leave_request(origin, pdu)\n        return 200, {}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& indices_tensor = ctx->input(0);\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(indices_tensor.shape()) ||\n                    TensorShapeUtils::IsScalar(indices_tensor.shape()),\n                errors::InvalidArgument(\n                    \"The indices can only be scalar or vector, got \\\"\",\n                    indices_tensor.shape().DebugString(), \"\\\"\"));\n\n    const Tensor& dims_tensor = ctx->input(1);\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(dims_tensor.shape()),\n        errors::InvalidArgument(\"The indices can only be 1-D, got \\\"\",\n                                dims_tensor.shape().DebugString(), \"\\\"\"));\n\n    auto dims = dims_tensor.vec<Tidx>();\n\n    // Chek to make sure indices is not out of boundary\n    Eigen::Tensor<Tidx, 0, Eigen::RowMajor> dims_prod_eigen = dims.prod();\n    Tidx dims_prod = dims_prod_eigen();\n    const Tidx* indices = indices_tensor.flat<Tidx>().data();\n    int64 size = indices_tensor.NumElements();\n    bool check = std::all_of(indices, indices + size,\n                             [&](Tidx index) { return index < dims_prod; });\n    OP_REQUIRES(ctx, check,\n                errors::InvalidArgument(\"index is out of bound as with dims\"));\n\n    Eigen::array<bool, 1> reverse({true});\n\n    Tensor strides_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<Tidx>::value,\n                                      TensorShape({dims_tensor.NumElements()}),\n                                      &strides_tensor));\n\n    auto strides = strides_tensor.vec<Tidx>();\n    strides = dims.reverse(reverse)\n                  .scan(0, Eigen::internal::ProdReducer<Tidx>(), false)\n                  .reverse(reverse);\n\n    Tensor strides_shifted_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<Tidx>::value,\n                                      TensorShape({dims_tensor.NumElements()}),\n                                      &strides_shifted_tensor));\n\n    auto strides_shifted = strides_shifted_tensor.vec<Tidx>();\n    strides_shifted = dims.reverse(reverse)\n                          .scan(0, Eigen::internal::ProdReducer<Tidx>(), true)\n                          .reverse(reverse);\n\n    Tensor* output_tensor = nullptr;\n    if (TensorShapeUtils::IsScalar(indices_tensor.shape())) {\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({dims_tensor.NumElements()}),\n                                    &output_tensor));\n\n      auto output = output_tensor->vec<Tidx>();\n\n      output = output.constant(indices_tensor.scalar<Tidx>()());\n      output = output.binaryExpr(strides, mod_op<Tidx>()) / strides_shifted;\n    } else {\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0,\n                                    TensorShape({dims_tensor.NumElements(),\n                                                 indices_tensor.NumElements()}),\n                                    &output_tensor));\n\n      auto output = output_tensor->matrix<Tidx>();\n\n      Eigen::array<Eigen::Index, 2> reshape{\n          {static_cast<Eigen::Index>(dims_tensor.NumElements()), 1}};\n      Eigen::array<Eigen::Index, 2> bcast(\n          {1, static_cast<Eigen::Index>(indices_tensor.NumElements())});\n      Eigen::array<Eigen::Index, 2> indices_reshape{\n          {1, static_cast<Eigen::Index>(indices_tensor.NumElements())}};\n      Eigen::array<Eigen::Index, 2> indices_bcast(\n          {static_cast<Eigen::Index>(dims_tensor.NumElements()), 1});\n\n      output = indices_tensor.vec<Tidx>()\n                   .reshape(indices_reshape)\n                   .broadcast(indices_bcast);\n      output = output.binaryExpr(strides.reshape(reshape).broadcast(bcast),\n                                 mod_op<Tidx>()) /\n               strides_shifted.reshape(reshape).broadcast(bcast);\n    }\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def callback(_, pdu, redacted):\n            with logcontext.PreserveLoggingContext(ctx):\n                if not check_event_content_hash(pdu):\n                    logger.warn(\n                        \"Event content has been tampered, redacting %s: %s\",\n                        pdu.event_id, pdu.get_pdu_json()\n                    )\n                    return redacted\n\n                if self.spam_checker.check_event_for_spam(pdu):\n                    logger.warn(\n                        \"Event contains spam, redacting %s: %s\",\n                        pdu.event_id, pdu.get_pdu_json()\n                    )\n                    return redacted\n\n                return pdu\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse(self):\n        \"\"\"\n        Parse the POST data and break it into a FILES MultiValueDict and a POST\n        MultiValueDict.\n\n        Return a tuple containing the POST and FILES dictionary, respectively.\n        \"\"\"\n        from django.http import QueryDict\n\n        encoding = self._encoding\n        handlers = self._upload_handlers\n\n        # HTTP spec says that Content-Length >= 0 is valid\n        # handling content-length == 0 before continuing\n        if self._content_length == 0:\n            return QueryDict(encoding=self._encoding), MultiValueDict()\n\n        # See if any of the handlers take care of the parsing.\n        # This allows overriding everything if need be.\n        for handler in handlers:\n            result = handler.handle_raw_input(\n                self._input_data,\n                self._meta,\n                self._content_length,\n                self._boundary,\n                encoding,\n            )\n            # Check to see if it was handled\n            if result is not None:\n                return result[0], result[1]\n\n        # Create the data structures to be used later.\n        self._post = QueryDict(mutable=True)\n        self._files = MultiValueDict()\n\n        # Instantiate the parser and stream:\n        stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n        # Whether or not to signal a file-completion at the beginning of the loop.\n        old_field_name = None\n        counters = [0] * len(handlers)\n\n        # Number of bytes that have been read.\n        num_bytes_read = 0\n        # To count the number of keys in the request.\n        num_post_keys = 0\n        # To limit the amount of data read from the request.\n        read_size = None\n\n        try:\n            for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                if old_field_name:\n                    # We run this at the beginning of the next loop\n                    # since we cannot be sure a file is complete until\n                    # we hit the next boundary/part of the multipart content.\n                    self.handle_file_complete(old_field_name, counters)\n                    old_field_name = None\n\n                try:\n                    disposition = meta_data['content-disposition'][1]\n                    field_name = disposition['name'].strip()\n                except (KeyError, IndexError, AttributeError):\n                    continue\n\n                transfer_encoding = meta_data.get('content-transfer-encoding')\n                if transfer_encoding is not None:\n                    transfer_encoding = transfer_encoding[0].strip()\n                field_name = force_text(field_name, encoding, errors='replace')\n\n                if item_type == FIELD:\n                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                    num_post_keys += 1\n                    if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and\n                            settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):\n                        raise TooManyFieldsSent(\n                            'The number of GET/POST parameters exceeded '\n                            'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'\n                        )\n\n                    # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                    if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                        read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n\n                    # This is a post field, we can just set it in the post\n                    if transfer_encoding == 'base64':\n                        raw_data = field_stream.read(size=read_size)\n                        num_bytes_read += len(raw_data)\n                        try:\n                            data = base64.b64decode(raw_data)\n                        except binascii.Error:\n                            data = raw_data\n                    else:\n                        data = field_stream.read(size=read_size)\n                        num_bytes_read += len(data)\n\n                    # Add two here to make the check consistent with the\n                    # x-www-form-urlencoded check that includes '&='.\n                    num_bytes_read += len(field_name) + 2\n                    if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and\n                            num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):\n                        raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')\n\n                    self._post.appendlist(field_name, force_text(data, encoding, errors='replace'))\n                elif item_type == FILE:\n                    # This is a file, use the handler...\n                    file_name = disposition.get('filename')\n                    if file_name:\n                        file_name = force_text(file_name, encoding, errors='replace')\n                        file_name = self.sanitize_file_name(file_name)\n                    if not file_name:\n                        continue\n\n                    content_type, content_type_extra = meta_data.get('content-type', ('', {}))\n                    content_type = content_type.strip()\n                    charset = content_type_extra.get('charset')\n\n                    try:\n                        content_length = int(meta_data.get('content-length')[0])\n                    except (IndexError, TypeError, ValueError):\n                        content_length = None\n\n                    counters = [0] * len(handlers)\n                    try:\n                        for handler in handlers:\n                            try:\n                                handler.new_file(\n                                    field_name, file_name, content_type,\n                                    content_length, charset, content_type_extra,\n                                )\n                            except StopFutureHandlers:\n                                break\n\n                        for chunk in field_stream:\n                            if transfer_encoding == 'base64':\n                                # We only special-case base64 transfer encoding\n                                # We should always decode base64 chunks by multiple of 4,\n                                # ignoring whitespace.\n\n                                stripped_chunk = b\"\".join(chunk.split())\n\n                                remaining = len(stripped_chunk) % 4\n                                while remaining != 0:\n                                    over_chunk = field_stream.read(4 - remaining)\n                                    stripped_chunk += b\"\".join(over_chunk.split())\n                                    remaining = len(stripped_chunk) % 4\n\n                                try:\n                                    chunk = base64.b64decode(stripped_chunk)\n                                except Exception as exc:\n                                    # Since this is only a chunk, any error is an unfixable error.\n                                    raise MultiPartParserError(\"Could not decode base64 data.\") from exc\n\n                            for i, handler in enumerate(handlers):\n                                chunk_length = len(chunk)\n                                chunk = handler.receive_data_chunk(chunk, counters[i])\n                                counters[i] += chunk_length\n                                if chunk is None:\n                                    # Don't continue if the chunk received by\n                                    # the handler is None.\n                                    break\n\n                    except SkipFile:\n                        self._close_files()\n                        # Just use up the rest of this file...\n                        exhaust(field_stream)\n                    else:\n                        # Handle file upload completions on next iteration.\n                        old_field_name = field_name\n                else:\n                    # If this is neither a FIELD or a FILE, just exhaust the stream.\n                    exhaust(stream)\n        except StopUpload as e:\n            self._close_files()\n            if not e.connection_reset:\n                exhaust(self._input_data)\n        else:\n            # Make sure that the request data is all fed\n            exhaust(self._input_data)\n\n        # Signal that the upload has completed.\n        # any() shortcircuits if a handler's upload_complete() returns a value.\n        any(handler.upload_complete() for handler in handlers)\n        self._post._mutable = False\n        return self._post, self._files\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def get_file(\n        self,\n        url: str,\n        output_stream: BinaryIO,\n        max_size: Optional[int] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:\n        \"\"\"GETs a file from a given URL\n        Args:\n            url: The URL to GET\n            output_stream: File to write the response body to.\n            headers: A map from header name to a list of values for that header\n        Returns:\n            A tuple of the file length, dict of the response\n            headers, absolute URI of the response and HTTP response code.\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            SynapseError: if the response is not a 2xx, the remote file is too large, or\n               another exception happens during the download.\n        \"\"\"\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", url, headers=Headers(actual_headers))\n\n        resp_headers = dict(response.headers.getAllRawHeaders())\n\n        if (\n            b\"Content-Length\" in resp_headers\n            and max_size\n            and int(resp_headers[b\"Content-Length\"][0]) > max_size\n        ):\n            logger.warning(\"Requested URL is too large > %r bytes\" % (max_size,))\n            raise SynapseError(\n                502,\n                \"Requested file is too large > %r bytes\" % (max_size,),\n                Codes.TOO_LARGE,\n            )\n\n        if response.code > 299:\n            logger.warning(\"Got %d when downloading %s\" % (response.code, url))\n            raise SynapseError(502, \"Got error %d\" % (response.code,), Codes.UNKNOWN)\n\n        # TODO: if our Content-Type is HTML or something, just read the first\n        # N bytes into RAM rather than saving it all to disk only to read it\n        # straight back in again\n\n        try:\n            length = await make_deferred_yieldable(\n                readBodyToFile(response, output_stream, max_size)\n            )\n        except SynapseError:\n            # This can happen e.g. because the body is too large.\n            raise\n        except Exception as e:\n            raise SynapseError(502, (\"Failed to download remote body: %s\" % e)) from e\n\n        return (\n            length,\n            resp_headers,\n            response.request.absoluteURI.decode(\"ascii\"),\n            response.code,\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_safe_url(url, host=None):\n    \"\"\"\n    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to\n    a different host and uses a safe scheme).\n\n    Always returns ``False`` on an empty url.\n    \"\"\"\n    if url is not None:\n        url = url.strip()\n    if not url:\n        return False\n    # Chrome treats \\ completely as /\n    url = url.replace('\\\\', '/')\n    # Chrome considers any URL with more than two slashes to be absolute, but\n    # urlparse is not so flexible. Treat any url with three slashes as unsafe.\n    if url.startswith('///'):\n        return False\n    url_info = urlparse(url)\n    # Forbid URLs like http:///example.com - with a scheme, but without a hostname.\n    # In that URL, example.com is not the hostname but, a path component. However,\n    # Chrome will still consider example.com to be the hostname, so we must not\n    # allow this syntax.\n    if not url_info.netloc and url_info.scheme:\n        return False\n    # Forbid URLs that start with control characters. Some browsers (like\n    # Chrome) ignore quite a few control characters at the start of a\n    # URL and might consider the URL as scheme relative.\n    if unicodedata.category(url[0])[0] == 'C':\n        return False\n    return ((not url_info.netloc or url_info.netloc == host) and\n            (not url_info.scheme or url_info.scheme in ['http', 'https']))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n        key = self.make_key(key, version=version)\n        return self._cache.add(key, value, self.get_backend_timeout(timeout))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input_sizes = context->input(0);\n    const Tensor& filter = context->input(1);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(input_sizes.shape()),\n        errors::InvalidArgument(\n            \"Conv2DBackpropInput: input_sizes input must be 1-dim, not \",\n            input_sizes.dims()));\n    TensorShape input_shape;\n    const int32* in_sizes_data = input_sizes.template flat<int32>().data();\n\n    for (int i = 0; i < input_sizes.NumElements(); ++i) {\n      OP_REQUIRES(context, in_sizes_data[i] >= 0,\n                  errors::InvalidArgument(\"Dimension \", i,\n                                          \" of input_sizes must be >= 0\"));\n      input_shape.AddDim(in_sizes_data[i]);\n    }\n    const TensorShape& filter_shape = filter.shape();\n    EXTRACT_AND_VERIFY_DIMENSIONS(\"DepthwiseConv2DBackpropInput\");\n\n    Tensor* in_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, input_shape, &in_backprop));\n\n    // If there is nothing to compute, return.\n    if (input_shape.num_elements() == 0) {\n      return;\n    }\n\n    // If in_depth==1, this operation is just a standard convolution.\n    // Depthwise convolution is a special case of cuDNN's grouped convolution.\n    bool use_cudnn = std::is_same<Device, GPUDevice>::value &&\n                     (in_depth == 1 ||\n                      (use_cudnn_grouped_conv_ &&\n                       IsCudnnSupportedFilterSize(/*filter_rows=*/filter_rows,\n                                                  /*filter_cols=*/filter_cols,\n                                                  /*in_depth=*/in_depth,\n                                                  /*out_depth=*/out_depth)));\n\n    VLOG(2) << \"DepthwiseConv2dNativeBackpropInput: \"\n            << \" Input: [\" << batch << \", \" << input_rows << \", \" << input_cols\n            << \", \" << in_depth << \"]; Filter: [\" << filter_rows << \", \"\n            << filter_cols << \", \" << in_depth << \", \" << depth_multiplier\n            << \"]; Output: [\" << batch << \", \" << out_rows << \", \" << out_cols\n            << \", \" << out_depth << \"], stride = \" << stride_\n            << \", pad_rows = \" << pad_top << \", pad_cols = \" << pad_left\n            << \", Use cuDNN: \" << use_cudnn;\n\n    if (use_cudnn) {\n      // Reshape from TF depthwise filter to cuDNN grouped convolution filter:\n      //\n      //                  | TensorFlow       | cuDNN\n      // --------------------------------------------------------------------\n      // filter_out_depth | depth_multiplier | depth_multiplier * group_count\n      // filter_in_depth  | in_depth         | in_depth / group_count\n      //\n      // For depthwise convolution, we have group_count == in_depth.\n      int32_t filter_in_depth = 1;\n      TensorShape shape =\n          TensorShape{filter_rows, filter_cols, filter_in_depth, out_depth};\n      Tensor reshaped_filter(/*type=*/dtype_);\n      OP_REQUIRES(\n          context, reshaped_filter.CopyFrom(filter, shape),\n          errors::Internal(\n              \"Failed to reshape filter tensor for grouped convolution.\"));\n      // TODO(yangzihao): Send in arbitrary dilation rates after the dilated\n      // conv is supported.\n      launcher_(context, /*use_cudnn=*/true, cudnn_use_autotune_, out_backprop,\n                reshaped_filter, /*row_dilation=*/1, /*col_dilation=*/1,\n                stride_, stride_, padding_, explicit_paddings_, in_backprop,\n                data_format_);\n      return;\n    }\n\n    auto out_backprop_ptr = out_backprop.template flat<T>().data();\n    auto filter_ptr = filter.template flat<T>().data();\n    auto in_backprop_ptr = in_backprop->template flat<T>().data();\n    LaunchDepthwiseConvBackpropInputOp<Device, T>()(\n        context, args, out_backprop_ptr, filter_ptr, in_backprop_ptr,\n        data_format_);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def createRequest(self, op, req, outgoing_data):\n        \"\"\"Return a new QNetworkReply object.\n\n        Args:\n             op: Operation op\n             req: const QNetworkRequest & req\n             outgoing_data: QIODevice * outgoingData\n\n        Return:\n            A QNetworkReply.\n        \"\"\"\n        proxy_factory = objreg.get('proxy-factory', None)\n        if proxy_factory is not None:\n            proxy_error = proxy_factory.get_error()\n            if proxy_error is not None:\n                return networkreply.ErrorNetworkReply(\n                    req, proxy_error, QNetworkReply.UnknownProxyError,\n                    self)\n\n        scheme = req.url().scheme()\n        if scheme in self._scheme_handlers:\n            result = self._scheme_handlers[scheme](req)\n            if result is not None:\n                result.setParent(self)\n                return result\n\n        for header, value in shared.custom_headers(url=req.url()):\n            req.setRawHeader(header, value)\n\n        host_blocker = objreg.get('host-blocker')\n        if host_blocker.is_blocked(req.url()):\n            log.webview.info(\"Request to {} blocked by host blocker.\".format(\n                req.url().host()))\n            return networkreply.ErrorNetworkReply(\n                req, HOSTBLOCK_ERROR_STRING, QNetworkReply.ContentAccessDenied,\n                self)\n\n        # There are some scenarios where we can't figure out current_url:\n        # - There's a generic NetworkManager, e.g. for downloads\n        # - The download was in a tab which is now closed.\n        current_url = QUrl()\n\n        if self._tab_id is not None:\n            assert self._win_id is not None\n            try:\n                tab = objreg.get('tab', scope='tab', window=self._win_id,\n                                 tab=self._tab_id)\n                current_url = tab.url()\n            except (KeyError, RuntimeError):\n                # https://github.com/qutebrowser/qutebrowser/issues/889\n                # Catching RuntimeError because we could be in the middle of\n                # the webpage shutdown here.\n                current_url = QUrl()\n\n        if 'log-requests' in self._args.debug_flags:\n            operation = debug.qenum_key(QNetworkAccessManager, op)\n            operation = operation.replace('Operation', '').upper()\n            log.webview.debug(\"{} {}, first-party {}\".format(\n                operation,\n                req.url().toDisplayString(),\n                current_url.toDisplayString()))\n\n        self.set_referer(req, current_url)\n        return super().createRequest(op, req, outgoing_data)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_file_hash(self):\n        if self.file_hash == \"\":\n            with self.open_file() as f:\n                self._set_file_hash(f.read())\n\n            self.save(update_fields=[\"file_hash\"])\n\n        return self.file_hash\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void TensorSliceReader::LoadShard(int shard) const {\n  CHECK_LT(shard, sss_.size());\n  if (sss_[shard] || !status_.ok()) {\n    return;  // Already loaded, or invalid.\n  }\n  string value;\n  SavedTensorSlices sts;\n  const string fname = fnames_[shard];\n  VLOG(1) << \"Reading meta data from file \" << fname << \"...\";\n  Table* table;\n  Status s = open_function_(fname, &table);\n  if (!s.ok()) {\n    status_ = errors::DataLoss(\"Unable to open table file \", fname, \": \",\n                               s.ToString());\n    return;\n  }\n  sss_[shard].reset(table);\n  if (!(table->Get(kSavedTensorSlicesKey, &value) &&\n        ParseProtoUnlimited(&sts, value))) {\n    status_ = errors::Internal(\n        \"Failed to find the saved tensor slices at the beginning of the \"\n        \"checkpoint file: \",\n        fname);\n    return;\n  }\n  status_ = CheckVersions(sts.meta().versions(), TF_CHECKPOINT_VERSION,\n                          TF_CHECKPOINT_VERSION_MIN_PRODUCER, \"Checkpoint\",\n                          \"checkpoint\");\n  if (!status_.ok()) return;\n  for (const SavedSliceMeta& ssm : sts.meta().tensor()) {\n    TensorShape ssm_shape(ssm.shape());\n    for (const TensorSliceProto& tsp : ssm.slice()) {\n      TensorSlice ss_slice(tsp);\n      status_ = RegisterTensorSlice(ssm.name(), ssm_shape, ssm.type(), fname,\n                                    ss_slice, &tensors_);\n      if (!status_.ok()) return;\n    }\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus ResizeOutputTensors(TfLiteContext* context, TfLiteNode* node,\n                                 const TfLiteTensor* input,\n                                 const TfLiteTensor* size_splits,\n                                 const TfLiteTensor* axis) {\n  int axis_value = GetTensorData<int>(axis)[0];\n  if (axis_value < 0) {\n    axis_value += NumDimensions(input);\n  }\n\n  std::vector<int64_t> size_splits_vector;\n  if (size_splits->type == kTfLiteInt32) {\n    GetSizeSplitsVector<int32_t>(size_splits, &size_splits_vector);\n  } else if (size_splits->type == kTfLiteInt64) {\n    GetSizeSplitsVector<int64_t>(size_splits, &size_splits_vector);\n  } else {\n    context->ReportError(context, \"size_splits only support type int32|int64.\");\n    return kTfLiteError;\n  }\n\n  int minus_one_index = -1;\n  int64_t size_splits_sum = 0;\n\n  for (int i = 0; i < size_splits_vector.size(); ++i) {\n    if (size_splits_vector.at(i) == -1) {\n      if (minus_one_index == -1) {\n        minus_one_index = i;\n      } else {\n        context->ReportError(context,\n                             \"The size_splits contains more than one -1.\");\n      }\n    } else {\n      size_splits_sum += size_splits_vector.at(i);\n    }\n  }\n\n  const int input_size = SizeOfDimension(input, axis_value);\n\n  if (minus_one_index != -1) {\n    if (size_splits_sum > input_size) {\n      context->ReportError(\n          context,\n          \"The sum of size_splits must be less than the dimension of value.\");\n    } else {\n      size_splits_vector[minus_one_index] = input_size - size_splits_sum;\n    }\n  } else if (size_splits_sum != input_size) {\n    context->ReportError(\n        context,\n        \"The size_splits must sum to the dimension of value along axis.\");\n  }\n\n  for (int i = 0; i < NumOutputs(node); ++i) {\n    TfLiteIntArray* output_dims = TfLiteIntArrayCopy(input->dims);\n    output_dims->data[axis_value] = size_splits_vector.at(i);\n    TfLiteTensor* output;\n    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, i, &output));\n    TF_LITE_ENSURE_STATUS(context->ResizeTensor(context, output, output_dims));\n  }\n\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def create_trustee_and_trust(osc, cluster):\n    try:\n        password = utils.generate_password(length=18)\n        trustee = osc.keystone().create_trustee(\n            cluster.uuid,\n            password,\n        )\n        cluster.trustee_username = trustee.name\n        cluster.trustee_user_id = trustee.id\n        cluster.trustee_password = password\n        trust = osc.keystone().create_trust(trustee.id)\n        cluster.trust_id = trust.id\n    except Exception:\n        LOG.exception(\n            _LE('Failed to create trustee and trust for Cluster: %s'),\n            cluster.uuid)\n        raise exception.TrusteeOrTrustToClusterFailed(\n            cluster_uuid=cluster.uuid)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void SparseFillEmptyRowsOpImpl(OpKernelContext* context,\n                               AsyncOpKernel::DoneCallback done = nullptr) {\n  // Note that setting this empty lambda as the default parameter value directly\n  // can cause strange compiler/linker errors, so we do it like this instead.\n  if (!done) {\n    done = [] {};\n  }\n\n  const int kIndicesInput = 0;\n  const int kValuesInput = 1;\n  const int kDenseShapeInput = 2;\n  const int kDefaultValueInput = 3;\n\n  const Tensor& indices_t = context->input(kIndicesInput);\n  const Tensor& values_t = context->input(kValuesInput);\n  const Tensor& dense_shape_t = context->input(kDenseShapeInput);\n  const Tensor& default_value_t = context->input(kDefaultValueInput);\n\n  OP_REQUIRES_ASYNC(\n      context, TensorShapeUtils::IsVector(dense_shape_t.shape()),\n      errors::InvalidArgument(\"dense_shape must be a vector, saw: \",\n                              dense_shape_t.shape().DebugString()),\n      done);\n  OP_REQUIRES_ASYNC(context, TensorShapeUtils::IsMatrix(indices_t.shape()),\n                    errors::InvalidArgument(\"indices must be a matrix, saw: \",\n                                            indices_t.shape().DebugString()),\n                    done);\n  OP_REQUIRES_ASYNC(context, TensorShapeUtils::IsVector(values_t.shape()),\n                    errors::InvalidArgument(\"values must be a vector, saw: \",\n                                            values_t.shape().DebugString()),\n                    done);\n  OP_REQUIRES_ASYNC(\n      context, TensorShapeUtils::IsScalar(default_value_t.shape()),\n      errors::InvalidArgument(\"default_value must be a scalar, saw: \",\n                              default_value_t.shape().DebugString()),\n      done);\n  // TODO(ebrevdo): add shape checks between values, indices,\n  // Also add check that dense rank > 0.\n  OP_REQUIRES_ASYNC(context, dense_shape_t.NumElements() != 0,\n                    errors::InvalidArgument(\"Dense shape cannot be empty.\"),\n                    done);\n\n  using FunctorType = functor::SparseFillEmptyRows<Device, T, Tindex>;\n  OP_REQUIRES_OK_ASYNC(context,\n                       FunctorType()(context, default_value_t, indices_t,\n                                     values_t, dense_shape_t, done),\n                       done);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def clone(\n        self,\n        pdf_dest: Any,\n        force_duplicate: bool = False,\n        ignore_fields: Optional[Sequence[Union[str, int]]] = (),\n    ) -> \"ContentStream\":\n        \"\"\"\n        Clone object into pdf_dest.\n\n        Args:\n            pdf_dest:\n            force_duplicate:\n            ignore_fields:\n\n        Returns:\n            The cloned ContentStream\n        \"\"\"\n        try:\n            if self.indirect_reference.pdf == pdf_dest and not force_duplicate:  # type: ignore\n                return self\n        except Exception:\n            pass\n\n        d__ = cast(\n            \"ContentStream\",\n            self._reference_clone(\n                self.__class__(None, None), pdf_dest, force_duplicate\n            ),\n        )\n        if ignore_fields is None:\n            ignore_fields = []\n        d__._clone(self, pdf_dest, force_duplicate, ignore_fields)\n        return d__\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse(self):\n        \"\"\"\n        Parse the POST data and break it into a FILES MultiValueDict and a POST\n        MultiValueDict.\n\n        Return a tuple containing the POST and FILES dictionary, respectively.\n        \"\"\"\n        from django.http import QueryDict\n\n        encoding = self._encoding\n        handlers = self._upload_handlers\n\n        # HTTP spec says that Content-Length >= 0 is valid\n        # handling content-length == 0 before continuing\n        if self._content_length == 0:\n            return QueryDict(encoding=self._encoding), MultiValueDict()\n\n        # See if any of the handlers take care of the parsing.\n        # This allows overriding everything if need be.\n        for handler in handlers:\n            result = handler.handle_raw_input(\n                self._input_data,\n                self._meta,\n                self._content_length,\n                self._boundary,\n                encoding,\n            )\n            # Check to see if it was handled\n            if result is not None:\n                return result[0], result[1]\n\n        # Create the data structures to be used later.\n        self._post = QueryDict(mutable=True)\n        self._files = MultiValueDict()\n\n        # Instantiate the parser and stream:\n        stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n        # Whether or not to signal a file-completion at the beginning of the loop.\n        old_field_name = None\n        counters = [0] * len(handlers)\n\n        # Number of bytes that have been read.\n        num_bytes_read = 0\n        # To count the number of keys in the request.\n        num_post_keys = 0\n        # To limit the amount of data read from the request.\n        read_size = None\n        # Whether a file upload is finished.\n        uploaded_file = True\n\n        try:\n            for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                if old_field_name:\n                    # We run this at the beginning of the next loop\n                    # since we cannot be sure a file is complete until\n                    # we hit the next boundary/part of the multipart content.\n                    self.handle_file_complete(old_field_name, counters)\n                    old_field_name = None\n                    uploaded_file = True\n\n                try:\n                    disposition = meta_data['content-disposition'][1]\n                    field_name = disposition['name'].strip()\n                except (KeyError, IndexError, AttributeError):\n                    continue\n\n                transfer_encoding = meta_data.get('content-transfer-encoding')\n                if transfer_encoding is not None:\n                    transfer_encoding = transfer_encoding[0].strip()\n                field_name = force_str(field_name, encoding, errors='replace')\n\n                if item_type == FIELD:\n                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                    num_post_keys += 1\n                    if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and\n                            settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):\n                        raise TooManyFieldsSent(\n                            'The number of GET/POST parameters exceeded '\n                            'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'\n                        )\n\n                    # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                    if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                        read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n\n                    # This is a post field, we can just set it in the post\n                    if transfer_encoding == 'base64':\n                        raw_data = field_stream.read(size=read_size)\n                        num_bytes_read += len(raw_data)\n                        try:\n                            data = base64.b64decode(raw_data)\n                        except binascii.Error:\n                            data = raw_data\n                    else:\n                        data = field_stream.read(size=read_size)\n                        num_bytes_read += len(data)\n\n                    # Add two here to make the check consistent with the\n                    # x-www-form-urlencoded check that includes '&='.\n                    num_bytes_read += len(field_name) + 2\n                    if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and\n                            num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):\n                        raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')\n\n                    self._post.appendlist(field_name, force_str(data, encoding, errors='replace'))\n                elif item_type == FILE:\n                    # This is a file, use the handler...\n                    file_name = disposition.get('filename')\n                    if file_name:\n                        file_name = force_str(file_name, encoding, errors='replace')\n                        file_name = self.sanitize_file_name(file_name)\n                    if not file_name:\n                        continue\n\n                    content_type, content_type_extra = meta_data.get('content-type', ('', {}))\n                    content_type = content_type.strip()\n                    charset = content_type_extra.get('charset')\n\n                    try:\n                        content_length = int(meta_data.get('content-length')[0])\n                    except (IndexError, TypeError, ValueError):\n                        content_length = None\n\n                    counters = [0] * len(handlers)\n                    uploaded_file = False\n                    try:\n                        for handler in handlers:\n                            try:\n                                handler.new_file(\n                                    field_name, file_name, content_type,\n                                    content_length, charset, content_type_extra,\n                                )\n                            except StopFutureHandlers:\n                                break\n\n                        for chunk in field_stream:\n                            if transfer_encoding == 'base64':\n                                # We only special-case base64 transfer encoding\n                                # We should always decode base64 chunks by multiple of 4,\n                                # ignoring whitespace.\n\n                                stripped_chunk = b\"\".join(chunk.split())\n\n                                remaining = len(stripped_chunk) % 4\n                                while remaining != 0:\n                                    over_chunk = field_stream.read(4 - remaining)\n                                    stripped_chunk += b\"\".join(over_chunk.split())\n                                    remaining = len(stripped_chunk) % 4\n\n                                try:\n                                    chunk = base64.b64decode(stripped_chunk)\n                                except Exception as exc:\n                                    # Since this is only a chunk, any error is an unfixable error.\n                                    raise MultiPartParserError(\"Could not decode base64 data.\") from exc\n\n                            for i, handler in enumerate(handlers):\n                                chunk_length = len(chunk)\n                                chunk = handler.receive_data_chunk(chunk, counters[i])\n                                counters[i] += chunk_length\n                                if chunk is None:\n                                    # Don't continue if the chunk received by\n                                    # the handler is None.\n                                    break\n\n                    except SkipFile:\n                        self._close_files()\n                        # Just use up the rest of this file...\n                        exhaust(field_stream)\n                    else:\n                        # Handle file upload completions on next iteration.\n                        old_field_name = field_name\n                else:\n                    # If this is neither a FIELD or a FILE, just exhaust the stream.\n                    exhaust(stream)\n        except StopUpload as e:\n            self._close_files()\n            if not e.connection_reset:\n                exhaust(self._input_data)\n        else:\n            if not uploaded_file:\n                for handler in handlers:\n                    handler.upload_interrupted()\n            # Make sure that the request data is all fed\n            exhaust(self._input_data)\n\n        # Signal that the upload has completed.\n        # any() shortcircuits if a handler's upload_complete() returns a value.\n        any(handler.upload_complete() for handler in handlers)\n        self._post._mutable = False\n        return self._post, self._files\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_paths(base_path: pathlib.Path):\n    data_file = pathlib.Path(str(base_path) + \".data\")\n    metadata_file = pathlib.Path(str(base_path) + \".meta\")\n\n    return data_file, metadata_file\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  QInt32() {}\n  QInt32(const int8_t v) : value(v) {}"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    const findHeadingElement = (cell: Cell): void => {\n      model.getCellHeadings(cell).forEach(async heading => {\n        const elementId = await getIdForHeading(heading, this.parser!);\n\n        const selector = elementId\n          ? `h${heading.level}[id=\"${elementId}\"]`\n          : `h${heading.level}`;\n\n        if (heading.outputIndex !== undefined) {\n          // Code cell\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              (heading.cellRef as CodeCell).outputArea.widgets[\n                heading.outputIndex\n              ].node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        } else {\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              heading.cellRef.node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        }\n      });\n    };\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def gather_tree(ids, parents):\n    r\"\"\"\n    To be used after beam search. After beam search, we get selected ids at\n    each time step and the corresponding parents in the search tree. Both ids\n    and parents have the layout :attr:`[max_time, batch_size, beam_size]`. Then\n    :attr:`gather_tree` is used to backtrace from the last time step and\n    generate the full sequences by collecting selected ids.\n\n    Here is an example:\n\n    .. code-block:: text\n\n            Given:\n                ids = [[[2 2]\n                        [6 1]]\n                       [[3 9]\n                        [6 1]]\n                       [[0 1]\n                        [9 0]]]\n                parents = [[[0 0]\n                            [1 1]]\n                           [[1 0]\n                            [1 0]]\n                           [[0 0]\n                            [0 1]]]\n\n            Then:\n                gather_tree(ids, parents)\n                         = [[[2 2]\n                             [1 6]]\n                            [[3 3]\n                             [6 1]]\n                            [[0 1]\n                             [9 0]]]\n\n    Args:\n        ids(Tensor): A Tensor with shape :attr:`[length, batch_size, beam_size]`\n            and data type :attr:`int32` or :attr:`int64`. It contains the selected\n            ids of all time steps.\n        parents(Tensor): A Tensor with the same shape and data type as :attr:`ids`,\n            It contains the parents corresponding to selected ids when searching\n            among beams.\n\n    Returns:\n            A Tensor with the same shape and data type as :attr:`ids`. \\\n            It contains the full sequences. The sequences are collected from \\\n            :attr:`ids` by backtracing according to :attr:`parents`.\n\n    Examples:\n        .. code-block:: python\n\n            import paddle\n\n            ids = paddle.to_tensor([[[2, 2], [6, 1]], [[3, 9], [6, 1]], [[0, 1], [9, 0]]])\n\n            parents = paddle.to_tensor([[[0, 0], [1, 1]], [[1, 0], [1, 0]], [[0, 0], [0, 1]]])\n\n            final_sequences = paddle.nn.functional.gather_tree(ids, parents)\n            # [[[2, 2], [1, 6]], [[3, 3], [6, 1]], [[0, 1], [9, 0]]]\n\n    \"\"\"\n    if in_dygraph_mode():\n        return _C_ops.gather_tree(ids, parents)\n    else:\n        if _in_legacy_dygraph():\n            return _legacy_C_ops.gather_tree(ids, parents)\n        else:\n            helper = LayerHelper('gather_tree', **locals())\n            check_variable_and_dtype(ids, 'ids', ['int32', 'int64'],\n                                     'gather_tree')\n            check_variable_and_dtype(parents, 'parents', ['int32', 'int64'],\n                                     'gather_tree')\n            out = helper.create_variable_for_type_inference(dtype=ids.dtype)\n\n            helper.append_op(type=\"gather_tree\",\n                             inputs={\n                                 \"Ids\": ids,\n                                 \"Parents\": parents\n                             },\n                             outputs={\"Out\": out})\n\n            return out\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, *, openapi: GeneratorData) -> None:\n        self.openapi: GeneratorData = openapi\n        self.env: Environment = Environment(loader=PackageLoader(__package__), trim_blocks=True, lstrip_blocks=True)\n\n        self.project_name: str = self.project_name_override or f\"{utils.kebab_case(openapi.title).lower()}-client\"\n        self.project_dir: Path = Path.cwd() / self.project_name\n\n        self.package_name: str = self.package_name_override or self.project_name.replace(\"-\", \"_\")\n        self.package_dir: Path = self.project_dir / self.package_name\n        self.package_description: str = f\"A client library for accessing {self.openapi.title}\"\n        self.version: str = openapi.version\n\n        self.env.filters.update(self.TEMPLATE_FILTERS)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, instance, filename):\n        \"\"\"\n        Apply (if callable) or prepend (if a string) upload_to to the filename,\n        then delegate further processing of the name to the storage backend.\n        Until the storage layer, all file paths are expected to be Unix style\n        (with forward slashes).\n        \"\"\"\n        if callable(self.upload_to):\n            filename = self.upload_to(instance, filename)\n        else:\n            dirname = datetime.datetime.now().strftime(str(self.upload_to))\n            filename = posixpath.join(dirname, filename)\n        return self.storage.generate_filename(filename)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs(\n    OpKernelContext* context, TensorInputs* inputs,\n    TensorShapes* input_matrix_shapes, TensorShape* batch_shape) {\n  int input_rank = -1;\n  for (int i = 0; i < NumMatrixInputs(context); ++i) {\n    const Tensor& in = context->input(i);\n    if (i == 0) {\n      input_rank = in.dims();\n      OP_REQUIRES(\n          context, input_rank >= 2,\n          errors::InvalidArgument(\"Input tensor \", i,\n                                  \" must have rank >= 2, got \", input_rank));\n      // If the tensor rank is greater than 2, we consider the inner-most\n      // dimensions as matrices, and loop over all the other outer (\"batch\")\n      // dimensions to compute the results.\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        batch_shape->AddDim(in.dim_size(dim));\n      }\n    } else {\n      // Make sure that all inputs have the same rank and outer dimensions.\n      OP_REQUIRES(context, input_rank == in.dims(),\n                  errors::InvalidArgument(\n                      \"All input tensors must have the same rank.\"));\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        OP_REQUIRES(\n            context, in.dim_size(dim) == batch_shape->dim_size(dim),\n            errors::InvalidArgument(\n                \"All input tensors must have the same outer dimensions.\"));\n      }\n    }\n\n    const int row_dimension = input_rank - 2;\n    const int col_dimension = input_rank - 1;\n    const int64_t num_rows = in.dim_size(row_dimension);\n    const int64_t num_cols = in.dim_size(col_dimension);\n    input_matrix_shapes->emplace_back(\n        std::initializer_list<int64_t>({num_rows, num_cols}));\n    inputs->emplace_back(&in);\n  }\n  // Have the derived class validate that the inputs are as expected.\n  ValidateInputMatrixShapes(context, *input_matrix_shapes);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def include_is_allowed(filepath):\n    for root in settings.ALLOWED_INCLUDE_ROOTS:\n        if filepath.startswith(root):\n            return True\n    return False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def get_metric_variables(self) -> List[Any]:\n    \"\"\"Returns a list containing the metric variable values.\"\"\"\n    result = self._session.run(fetches=self._metric_variable_nodes)\n    return result\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def respond_error(self, context, exception):\n        context.respond_server_error()\n        stack = traceback.format_exc()\n        return \"\"\"\n        <html>\n            <body>\n\n                <style>\n                    body {\n                        font-family: sans-serif;\n                        color: #888;\n                        text-align: center;\n                    }\n\n                    body pre {\n                        width: 600px;\n                        text-align: left;\n                        margin: auto;\n                        font-family: monospace;\n                    }\n                </style>\n\n                <img src=\"/ajenti:static/main/error.jpeg\" />\n                <br/>\n                <p>\n                    Server error\n                </p>\n                <pre>\n%s\n                </pre>\n            </body>\n        </html>\n        \"\"\" % stack\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def resolve_rpsl_object_journal(rpsl_object, info: GraphQLResolveInfo):\n    \"\"\"\n    Resolve a journal subquery on an RPSL object.\n    \"\"\"\n    database_handler = info.context['request'].app.state.database_handler\n    access_list = f\"sources.{rpsl_object['source']}.nrtm_access_list\"\n    if not is_client_permitted(info.context['request'].client.host, access_list):\n        raise GraphQLError(f\"Access to journal denied for source {rpsl_object['source']}\")\n\n    query = RPSLDatabaseJournalQuery()\n    query.sources([rpsl_object['source']]).rpsl_pk(rpsl_object['rpslPk'])\n    for row in database_handler.execute_query(query, refresh_on_error=True):\n        response = {snake_to_camel_case(k): v for k, v in row.items()}\n        response['operation'] = response['operation'].name\n        if response['origin']:\n            response['origin'] = response['origin'].name\n        yield response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def reset_metric_variables(self) -> None:\n    \"\"\"Reset metric variable values to their initial values.\"\"\"\n    self._session.run(self._reset_variables_op)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& in0 = context->input(0);\n    const Tensor& in1 = context->input(1);\n    const int dims = in0.dims();\n    constexpr int kMinDims = 0;\n    constexpr int kMaxDims = 5;\n    OP_REQUIRES(context, kMinDims <= dims && dims <= kMaxDims,\n                errors::Unimplemented(\"inputs rank not in [\", kMinDims, \",\",\n                                      kMaxDims, \"]: \", dims));\n    OP_REQUIRES(\n        context,\n        TensorShapeUtils::IsMatrix(in1.shape()) && in1.dim_size(1) == 2,\n        errors::InvalidArgument(\"paddings must be a matrix with 2 columns: \",\n                                in1.shape().DebugString()));\n    OP_REQUIRES(\n        context, dims == in1.dim_size(0),\n        errors::InvalidArgument(\n            \"The first dimension of paddings must be the rank of inputs\",\n            in1.shape().DebugString(), \" \", in0.shape().DebugString()));\n\n    // Compute the shape of the output tensor, and allocate it.\n    TensorShape output_shape;\n    typename TTypes<Tpaddings>::ConstMatrix paddings = in1.matrix<Tpaddings>();\n    for (int d = 0; d < dims; ++d) {\n      const Tpaddings before = paddings(d, 0);  // Pad before existing elements.\n      const Tpaddings after = paddings(d, 1);   // Pad after existing elements.\n      OP_REQUIRES(context, before >= 0 && after >= 0,\n                  errors::InvalidArgument(\n                      \"Paddings must be non-negative: \", before, \", \", after));\n\n      const int64_t out_size = in0.dim_size(d) - (before + after);\n      if (offset_ == 0) {  // SYMMETRIC mode.\n        OP_REQUIRES(context, before <= out_size && after <= out_size,\n                    errors::InvalidArgument(\"paddings must be no greater \"\n                                            \"than the output dimension size: \",\n                                            before, \", \", after,\n                                            \" greater than \", out_size));\n      } else if (offset_ == 1) {  // REFLECT mode.\n        OP_REQUIRES(context, before < out_size && after < out_size,\n                    errors::InvalidArgument(\"paddings must be less than\"\n                                            \" the output dimension size: \",\n                                            before, \", \", after,\n                                            \" not less than \", out_size));\n      }\n      output_shape.AddDim(out_size);\n    }\n\n    if (output_shape == in0.shape()) {\n      context->set_output(0, in0);\n      return;\n    }\n\n    Tensor scratch;\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<T>::value,\n                                                   in0.shape(), &scratch));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &output));\n\n#define MIRROR_PAD_GRAD_CASE(k)                                           \\\n  case k: {                                                               \\\n    functor::MirrorPadGrad<Device, T, Tpaddings, k>()(                    \\\n        context->eigen_device<Device>(), To32Bit(output->tensor<T, k>()), \\\n        To32Bit(in0.tensor<T, k>()), paddings, offset_,                   \\\n        To32Bit(scratch.tensor<T, k>()));                                 \\\n    break;                                                                \\\n  }\n\n    // Invoke the dims-specific implementation.\n    switch (dims) {\n      MIRROR_PAD_GRAD_CASE(1);\n      MIRROR_PAD_GRAD_CASE(2);\n      MIRROR_PAD_GRAD_CASE(3);\n      MIRROR_PAD_GRAD_CASE(4);\n      MIRROR_PAD_GRAD_CASE(5);\n      default:\n        OP_REQUIRES(context, false,\n                    errors::InvalidArgument(\"Unsupported rank: \",\n                                            in0.shape().DebugString()));\n    }\n#undef MIRROR_PAD_GRAD_CASE\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render(self):\n        if len(self.params) == 1:\n            # Retrieve the user portrait\n            user = decleanId(self.params[0])\n            portrait = self.portal_membership.getPersonalPortrait(user)\n        elif len(self.params) == 0:\n            current_user_id = self.portal_membership.getAuthenticatedMember().getId()\n            portrait = self.portal_membership.getPersonalPortrait(current_user_id)\n        else:\n            raise Exception(\n                \"Must supply exactly zero (own portrait) or one parameter \" \"(user id)\"\n            )\n        # User uploaded portraits have a meta_type of \"Image\"\n        if not portrait or isDefaultPortrait(portrait):\n            self.request.response.setStatus(404)\n            return None\n\n        self.request.response.setStatus(200)\n        self.request.response.setHeader(\"Content-Type\", portrait.content_type)\n\n        return stream_data(portrait)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  explicit FractionalMaxPoolOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pooling_ratio\", &pooling_ratio_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"pseudo_random\", &pseudo_random_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n\n    OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                errors::InvalidArgument(\"pooling_ratio field must \"\n                                        \"specify 4 dimensions\"));\n\n    OP_REQUIRES(\n        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n        errors::Unimplemented(\"Fractional max pooling is not yet \"\n                              \"supported on the batch nor channel dimension.\"));\n\n    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed2\", &seed2_));\n    if (deterministic_) {\n      // If both seeds are not set when deterministic_ is true, force set seeds.\n      if ((seed_ == 0) && (seed2_ == 0)) {\n        seed_ = random::New64();\n        seed2_ = random::New64();\n      }\n    } else {\n      OP_REQUIRES(\n          context, (seed_ == 0) && (seed2_ == 0),\n          errors::InvalidArgument(\n              \"Both seed and seed2 should be 0 if deterministic is false.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ConstantFolding::EvaluateOneFoldable(const NodeDef& node,\n                                            std::vector<NodeDef>* outputs,\n                                            bool* result_too_large) {\n  TensorVector inputs;\n  TensorVector output_tensors;\n  auto inputs_cleanup = gtl::MakeCleanup([&inputs, &output_tensors] {\n    for (const auto& input : inputs) {\n      delete input.tensor;\n    }\n    for (const auto& output : output_tensors) {\n      if (output.tensor) {\n        delete output.tensor;\n      }\n    }\n  });\n\n  size_t total_inputs_size = 0;\n  for (const auto& input : node.input()) {\n    const TensorId input_tensor = ParseTensorName(input);\n    if (input_tensor.index() < 0) {\n      // Control dependency\n      break;\n    }\n    const NodeDef* input_node = node_map_->GetNode(input);\n    if (!IsReallyConstant(*input_node)) {\n      return Status(error::INVALID_ARGUMENT,\n                    strings::StrCat(\"Can't fold \", node.name(), \", its \", input,\n                                    \" isn't constant\"));\n    }\n    TF_RETURN_IF_ERROR(CheckAttrExists(*input_node, \"value\"));\n    const TensorProto& raw_val = input_node->attr().at(\"value\").tensor();\n    if (raw_val.dtype() == DT_INVALID) {\n      return Status(\n          error::INVALID_ARGUMENT,\n          strings::StrCat(\"A tensor in the input node, with TensorId of \",\n                          input_tensor.ToString(),\n                          \" has a dtype of DT_INVALID.\"));\n    }\n    Tensor* value = new Tensor(raw_val.dtype(), raw_val.tensor_shape());\n    if (!value->FromProto(raw_val)) {\n      delete (value);\n      return errors::InvalidArgument(\"Unable to make Tensor from proto for \",\n                                     node.name(), \" with shape \",\n                                     raw_val.tensor_shape().DebugString());\n    }\n    inputs.emplace_back(value);\n    total_inputs_size += value->TotalBytes();\n  }\n\n  TF_RETURN_IF_ERROR(EvaluateNode(node, inputs, &output_tensors));\n  if (output_tensors.empty()) {\n    return Status(error::INVALID_ARGUMENT, \"Expected at least one output.\");\n  }\n\n  outputs->resize(output_tensors.size());\n  for (size_t i = 0; i < output_tensors.size(); i++) {\n    string node_name = OptimizedNodeName(node, \"-folded\");\n    if (output_tensors.size() > 1) {\n      node_name = strings::StrCat(node_name, \"-\", i);\n    }\n    if (output_tensors[i].tensor) {\n      Status s = CreateNodeDef(node_name, output_tensors[i], &outputs->at(i),\n                               total_inputs_size);\n      if (!s.ok()) {\n        *result_too_large = true;\n        return s;\n      }\n    } else {\n      // Create an empty NodeDef to identify dead outputs (e.g. the output of a\n      // switch that's not selected by the switch predicate).\n      outputs->at(i) = NodeDef();\n    }\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def local_get_cobbler_xmlrpc_url():\n    # Load xmlrpc port\n    try:\n        fh = open(\"/etc/cobbler/settings\")\n        data = yaml.load(fh.read())\n        fh.close()\n    except:\n       traceback.print_exc()\n       raise CX(\"/etc/cobbler/settings is not a valid YAML file\")\n    return \"http://%s:%s\" % (\"127.0.0.1\",data.get(\"xmlrpc_port\",\"25151\"))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def to_yaml(self, **kwargs):\n    \"\"\"Returns a yaml string containing the network configuration.\n\n    To load a network from a yaml save file, use\n    `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n\n    `custom_objects` should be a dictionary mapping\n    the names of custom losses / layers / etc to the corresponding\n    functions / classes.\n\n    Args:\n        **kwargs: Additional keyword arguments\n            to be passed to `yaml.dump()`.\n\n    Returns:\n        A YAML string.\n\n    Raises:\n        ImportError: if yaml module is not found.\n    \"\"\"\n    if yaml is None:\n      raise ImportError(\n          'Requires yaml module installed (`pip install pyyaml`).')\n    return yaml.dump(self._updated_config(), **kwargs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_conn(self) -> Connection:\n        \"\"\"Establish a connection to Drillbit.\"\"\"\n        conn_md = self.get_connection(getattr(self, self.conn_name_attr))\n        creds = f\"{conn_md.login}:{conn_md.password}@\" if conn_md.login else \"\"\n        if \"/\" in conn_md.host or \"&\" in conn_md.host:\n            raise ValueError(\"Drill host should not contain '/&' characters\")\n        engine = create_engine(\n            f'{conn_md.extra_dejson.get(\"dialect_driver\", \"drill+sadrill\")}://{creds}'\n            f\"{conn_md.host}:{conn_md.port}/\"\n            f'{conn_md.extra_dejson.get(\"storage_plugin\", \"dfs\")}'\n        )\n\n        self.log.info(\n            \"Connected to the Drillbit at %s:%s as user %s\", conn_md.host, conn_md.port, conn_md.login\n        )\n        return engine.raw_connection()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _dump(self, file=None, format=None):\n        import tempfile\n        if not file:\n            file = tempfile.mktemp()\n        self.load()\n        if not format or format == \"PPM\":\n            self.im.save_ppm(file)\n        else:\n            file = file + \".\" + format\n            self.save(file, format)\n        return file\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  static port::StatusOr<CudnnRnnSequenceTensorDescriptor> Create(\n      GpuExecutor* parent, int max_seq_length, int batch_size, int data_size,\n      cudnnDataType_t data_type) {\n    CHECK_GT(max_seq_length, 0);\n    int dims[] = {batch_size, data_size, 1};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    TensorDescriptor tensor_desc = CreateTensorDescriptor();\n    RETURN_IF_CUDNN_ERROR(cudnnSetTensorNdDescriptor(\n        /*tensorDesc=*/tensor_desc.get(), /*dataType=*/data_type,\n        /*nbDims=*/sizeof(dims) / sizeof(dims[0]), /*dimA=*/dims,\n        /*strideA=*/strides));\n    return CudnnRnnSequenceTensorDescriptor(parent, max_seq_length, batch_size,\n                                            data_size, data_type,\n                                            nullptr,\n                                            std::move(tensor_desc));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, handler):\n        self.handler = handler\n        self.protocol_version = None\n        self.support_legacy_versions = True\n        self.supported_protocols = []\n        self.origin_checker = None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "load_header (XwdLoader *loader) // gconstpointer in, gsize in_max_len, XwdHeader *header_out)\n{\n    XwdHeader *h = &loader->header;\n    XwdHeader in;\n    const guint32 *p = (const guint32 *) &in;\n\n    if (!file_mapping_taste (loader->mapping, &in, 0, sizeof (in)))\n        return FALSE;\n\n    h->header_size = g_ntohl (*(p++));\n    h->file_version = g_ntohl (*(p++));\n    h->pixmap_format = g_ntohl (*(p++));\n    h->pixmap_depth = g_ntohl (*(p++));\n    h->pixmap_width = g_ntohl (*(p++));\n    h->pixmap_height = g_ntohl (*(p++));\n    h->x_offset = g_ntohl (*(p++));\n    h->byte_order = g_ntohl (*(p++));\n    h->bitmap_unit = g_ntohl (*(p++));\n    h->bitmap_bit_order = g_ntohl (*(p++));\n    h->bitmap_pad = g_ntohl (*(p++));\n    h->bits_per_pixel = g_ntohl (*(p++));\n    h->bytes_per_line = g_ntohl (*(p++));\n    h->visual_class = g_ntohl (*(p++));\n    h->red_mask = g_ntohl (*(p++));\n    h->green_mask = g_ntohl (*(p++));\n    h->blue_mask = g_ntohl (*(p++));\n    h->bits_per_rgb = g_ntohl (*(p++));\n    h->color_map_entries = g_ntohl (*(p++));\n    h->n_colors = g_ntohl (*(p++));\n    h->window_width = g_ntohl (*(p++));\n    h->window_height = g_ntohl (*(p++));\n    h->window_x = g_ntohl (*(p++));\n    h->window_y = g_ntohl (*(p++));\n    h->window_border_width = g_ntohl (*(p++));\n\n    /* Only support the most common/useful subset of XWD files out there;\n     * namely, that corresponding to screen dumps from modern X.Org servers. */\n\n    ASSERT_HEADER (h->header_size >= sizeof (XwdHeader));\n    ASSERT_HEADER (h->file_version == 7);\n    ASSERT_HEADER (h->pixmap_depth == 24);\n\n    /* Xvfb sets bits_per_rgb to 8, but 'convert' uses 24 for the same image data. One\n     * of them is likely misunderstanding. Let's be lenient and accept either. */\n    ASSERT_HEADER (h->bits_per_rgb == 8 || h->bits_per_rgb == 24);\n\n    ASSERT_HEADER (h->bytes_per_line >= h->pixmap_width * (h->bits_per_pixel / 8));\n    ASSERT_HEADER (compute_pixel_type (loader) < CHAFA_PIXEL_MAX);\n\n    loader->file_data = file_mapping_get_data (loader->mapping, &loader->file_data_len);\n    if (!loader->file_data)\n        return FALSE;\n\n    ASSERT_HEADER (loader->file_data_len >= h->header_size\n                   + h->n_colors * sizeof (XwdColor)\n                   + h->pixmap_height * h->bytes_per_line);\n\n    loader->image_data = (const guint8 *) loader->file_data\n        + h->header_size + h->n_colors * sizeof (XwdColor);\n\n    return TRUE;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _read_lockfile(self, lockfile: Mapping[str, Any]) -> None:\n        root = self.environment.project.root\n        with cd(root):\n            for package in lockfile.get(\"package\", []):\n                version = package.get(\"version\")\n                if version:\n                    package[\"version\"] = f\"=={version}\"\n                package_name = package.pop(\"name\")\n                req_dict = {\n                    k: v for k, v in package.items() if k not in (\"dependencies\", \"requires_python\", \"summary\", \"files\")\n                }\n                req = Requirement.from_req_dict(package_name, req_dict)\n                if req.is_file_or_url and req.path and not req.url:  # type: ignore[attr-defined]\n                    req.url = path_to_url(posixpath.join(root, req.path))  # type: ignore[attr-defined]\n                can = make_candidate(req, name=package_name, version=version)\n                can.hashes = package.get(\"files\", [])\n                can_id = self._identify_candidate(can)\n                self.packages[can_id] = can\n                candidate_info: CandidateInfo = (\n                    package.get(\"dependencies\", []),\n                    package.get(\"requires_python\", \"\"),\n                    package.get(\"summary\", \"\"),\n                )\n                self.candidate_info[can_id] = candidate_info\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void ImmutableConstantOp::Compute(OpKernelContext* ctx) {\n  std::unique_ptr<MemmappedTensorAllocator> allocator(\n      new MemmappedTensorAllocator());\n\n  OP_REQUIRES_OK(ctx,\n                 allocator->InitializeFromRegion(region_name_, ctx->env()));\n  ctx->set_output(0, Tensor(allocator.get(), dtype_, shape_));\n  OP_REQUIRES_OK(ctx, allocator->allocation_status());\n  // Allocator is owned by the tensor from this point.\n  allocator.release()->set_delete_on_deallocate();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    constexpr int tensor_in_and_out_dims = 4;\n\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n      OP_REQUIRES(\n          context, pooling_ratio_[i] <= input_size[i],\n          errors::InvalidArgument(\n              \"Pooling ratio cannot be bigger than input tensor dim size.\"));\n    }\n    // Output size.\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n\n    // Generate pooling sequence.\n    std::vector<int64_t> row_cum_seq;\n    std::vector<int64_t> col_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    row_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                          &generator, pseudo_random_);\n    col_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                          &generator, pseudo_random_);\n\n    // Prepare output.\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_row_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\n                     1, TensorShape({static_cast<int64_t>(row_cum_seq.size())}),\n                     &output_row_seq_tensor));\n    Tensor* output_col_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\n                     2, TensorShape({static_cast<int64_t>(col_cum_seq.size())}),\n                     &output_col_seq_tensor));\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n    // out_count corresponds to number of elements in each pooling cell.\n    Eigen::Matrix<T, Eigen::Dynamic, 1> out_count(out_mat.cols());\n\n    // Initializes the output tensor and out_count with 0.\n    out_mat.setZero();\n    out_count.setZero();\n\n    auto output_row_seq_flat = output_row_seq_tensor->flat<int64_t>();\n    auto output_col_seq_flat = output_col_seq_tensor->flat<int64_t>();\n\n    // Set output tensors.\n    for (int i = 0; i < row_cum_seq.size(); ++i) {\n      output_row_seq_flat(i) = row_cum_seq[i];\n    }\n\n    for (int i = 0; i < col_cum_seq.size(); ++i) {\n      output_col_seq_flat(i) = col_cum_seq[i];\n    }\n\n    // For both input and output,\n    // 0: batch\n    // 1: row / row\n    // 2: col / col\n    // 3: depth / channel\n    const int64_t row_max = input_size[1] - 1;\n    const int64_t col_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // row sequence.\n      for (int64_t hs = 0; hs < row_cum_seq.size() - 1; ++hs) {\n        // row start and end.\n        const int64_t row_start = row_cum_seq[hs];\n        int64_t row_end =\n            overlapping_ ? row_cum_seq[hs + 1] : row_cum_seq[hs + 1] - 1;\n        row_end = std::min(row_end, row_max);\n\n        // col sequence.\n        for (int64_t ws = 0; ws < col_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // col start and end.\n          const int64_t col_start = col_cum_seq[ws];\n          int64_t col_end =\n              overlapping_ ? col_cum_seq[ws + 1] : col_cum_seq[ws + 1] - 1;\n          col_end = std::min(col_end, col_max);\n          for (int64_t h = row_start; h <= row_end; ++h) {\n            for (int64_t w = col_start; w <= col_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) += in_mat.col(in_offset);\n              out_count(out_offset)++;\n            }\n          }\n        }\n      }\n    }\n    DCHECK_GT(out_count.minCoeff(), 0);\n    out_mat.array().rowwise() /= out_count.transpose().array();\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def check_secret_key(self) -> None:\n        if self.config[\"SECRET_KEY\"] == CHANGE_ME_SECRET_KEY:\n            top_banner = 80 * \"-\" + \"\\n\" + 36 * \" \" + \"WARNING\\n\" + 80 * \"-\"\n            bottom_banner = 80 * \"-\" + \"\\n\" + 80 * \"-\"\n            logger.warning(top_banner)\n            logger.warning(\n                \"A Default SECRET_KEY was detected, please use superset_config.py \"\n                \"to override it.\\n\"\n                \"Use a strong complex alphanumeric string and use a tool to help\"\n                \" you generate \\n\"\n                \"a sufficiently random sequence, ex: openssl rand -base64 42\"\n            )\n            logger.warning(bottom_banner)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def cookies(self):\n        \"\"\"The session cookies.\"\"\"\n        return self._cookies\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_snippet_path(self, path):\r\n        \"\"\"Get snippet path.\"\"\"\r\n\r\n        snippet = None\r\n        for base in self.base_path:\r\n            if os.path.exists(base):\r\n                if os.path.isdir(base):\r\n                    filename = os.path.join(base, path)\r\n                    if os.path.exists(filename):\r\n                        snippet = filename\r\n                        break\r\n                else:\r\n                    basename = os.path.basename(base)\r\n                    dirname = os.path.dirname(base)\r\n                    if basename.lower() == path.lower():\r\n                        filename = os.path.join(dirname, path)\r\n                        if os.path.exists(filename):\r\n                            snippet = filename\r\n                            break\r\n        return snippet\r\n\r"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_package(self, name, links, dest=Destination.QUEUE):\n        \"\"\"\n        Adds a package, with links to desired destination.\n\n        :param name: name of the new package\n        :param links: list of urls\n        :param dest: `Destination`\n        :return: package id of the new package\n        \"\"\"\n        if self.pyload.config.get(\"general\", \"folder_per_package\"):\n            folder = name\n        else:\n            folder = \"\"\n\n        folder = (\n            folder.replace(\"http://\", \"\")\n            .replace(\"https://\", \"\")\n            .replace(\":\", \"\")\n            .replace(\"/\", \"_\")\n            .replace(\"\\\\\", \"_\")\n        )\n\n        package_id = self.pyload.files.add_package(name, folder, Destination(dest))\n\n        self.pyload.files.add_links(links, package_id)\n\n        self.pyload.log.info(\n            self._(\"Added package {name} containing {count:d} links\").format(\n                name=name, count=len(links)\n            )\n        )\n\n        self.pyload.files.save()\n\n        return package_id\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _keyify(key):\n    return _key_pattern.sub(' ', key.lower())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& a = context->input(0);\n    const Tensor& b = context->input(1);\n    const float min_a = context->input(2).flat<float>()(0);\n    const float max_a = context->input(3).flat<float>()(0);\n    const float min_b = context->input(4).flat<float>()(0);\n    const float max_b = context->input(5).flat<float>()(0);\n\n    // Make sure that we have valid quantization ranges for the input buffers.\n    // If the difference between the min and max is negative or zero, it makes\n    // it hard to do meaningful intermediate operations on the values.\n    OP_REQUIRES(context, (max_a > min_a),\n                errors::InvalidArgument(\"max_a must be larger than min_a.\"));\n    OP_REQUIRES(context, (max_b > min_b),\n                errors::InvalidArgument(\"max_b must be larger than min_b.\"));\n    const int32_t offset_a = FloatToQuantizedUnclamped<T1>(0.0f, min_a, max_a);\n    const int32_t offset_b = FloatToQuantizedUnclamped<T2>(0.0f, min_b, max_b);\n    const int32_t offset_c = 0;\n    const int32_t mult_c = 1;\n    const int32_t shift_c = 0;\n\n    // Check that the dimensions of the two matrices are valid.\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(a.shape()),\n                errors::InvalidArgument(\"In[0] is not a matrix\"));\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(b.shape()),\n                errors::InvalidArgument(\"In[1] is not a matrix\"));\n    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n    dim_pair[0].first = transpose_a_ ? 0 : 1;\n    dim_pair[0].second = transpose_b_ ? 1 : 0;\n\n    OP_REQUIRES(context,\n                a.dim_size(dim_pair[0].first) == b.dim_size(dim_pair[0].second),\n                errors::InvalidArgument(\"Matrix size-incompatible: In[0]: \",\n                                        a.shape().DebugString(),\n                                        \", In[1]: \", b.shape().DebugString()));\n\n    OP_REQUIRES(context, ((shift_c >= 0) && (shift_c <= 31)),\n                errors::InvalidArgument(\"shift_c must be between 0 and 31, \"\n                                        \"inclusive.\"));\n\n    int a_dim_remaining = 1 - dim_pair[0].first;\n    int b_dim_remaining = 1 - dim_pair[0].second;\n    TensorShape out_shape(\n        {a.dim_size(a_dim_remaining), b.dim_size(b_dim_remaining)});\n    Tensor* c = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &c));\n    CHECK(c);\n\n    const T1* a_data = a.flat<T1>().data();\n    const T2* b_data = b.flat<T2>().data();\n    Toutput* c_data = c->flat<Toutput>().data();\n\n    const bool transpose_c = false;\n    const size_t m = a.dim_size(a_dim_remaining);\n    const size_t n = b.dim_size(b_dim_remaining);\n    const size_t k = a.dim_size(dim_pair[0].first);\n    const size_t lda = a.dim_size(1);\n    const size_t ldb = b.dim_size(1);\n    const size_t ldc = n;\n\n    if (meta::IsSupportedAndEnabled() && std::is_same<T1, quint8>() &&\n        std::is_same<T2, quint8>() && std::is_same<Toutput, qint32>() &&\n        (offset_c == 0) && (mult_c == 1) && (shift_c == 0) &&\n        (transpose_c == false) && (k <= 2048)) {\n      // Gemmlowp/meta code path works on 32 & 64 bit Arm with NEON Simd and\n      // allows optimized quantized 8bit to 32bit gemm.\n      meta::QuantizedGemm(context, transpose_a_, transpose_b_, a_data, b_data,\n                          c_data, m, n, k, -offset_a, -offset_b, lda, ldb, ldc);\n    } else if (std::is_same<T1, quint8>() && std::is_same<T2, quint8>() &&\n               std::is_same<Toutput, qint32>() && (offset_c == 0) &&\n               (mult_c == 1) && (shift_c == 0) && (transpose_c == false)) {\n      // The gemmlowp optimized library only works for a particular set of data\n      // types, so check if we meet those requirements and fall back to a slower\n      // reference implementation if not.\n      if (transpose_a_) {\n        if (transpose_b_) {\n          GemmlowpMultiply<true, true, false>(context, a_data, b_data, c_data,\n                                              m, n, k, offset_a, offset_b, lda,\n                                              ldb, ldc);\n        } else {\n          GemmlowpMultiply<true, false, false>(context, a_data, b_data, c_data,\n                                               m, n, k, offset_a, offset_b, lda,\n                                               ldb, ldc);\n        }\n      } else {\n        if (transpose_b_) {\n          GemmlowpMultiply<false, true, false>(context, a_data, b_data, c_data,\n                                               m, n, k, offset_a, offset_b, lda,\n                                               ldb, ldc);\n        } else {\n          GemmlowpMultiply<false, false, false>(context, a_data, b_data, c_data,\n                                                m, n, k, offset_a, offset_b,\n                                                lda, ldb, ldc);\n        }\n      }\n    } else {\n      ReferenceGemm<T1, T2, Toutput>(\n          transpose_a_, transpose_b_, transpose_c, m, n, k, a_data, offset_a,\n          lda, b_data, offset_b, ldb, c_data, shift_c, offset_c, mult_c, ldc);\n    }\n\n    float min_c_value;\n    float max_c_value;\n    QuantizationRangeForMultiplication<T1, T2, Toutput>(\n        min_a, max_a, min_b, max_b, &min_c_value, &max_c_value);\n    Tensor* c_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &c_min));\n    c_min->flat<float>()(0) = min_c_value;\n\n    Tensor* c_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &c_max));\n    c_max->flat<float>()(0) = max_c_value;\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def bad_url(prefix):\n    p = request.fullpath\n    if p.endswith(\"/\"):\n        p = p[:-1]\n    p = p.rsplit('/', 1)[0]\n    p += \"/simple/%s/\" % prefix\n\n    return redirect(p)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def authenticate(template, title):\n    \"\"\"\n    Authentication management method.\n    :param template: the template name that should be rendered\n    :returns: rendered final page or a page with error message\n    \"\"\"\n    global AUTH_ISSUERS, SAML_SUPPORT, AUTH_TYPE, COOKIES\n    valid_token_dict = validate_webui_token()\n    if not valid_token_dict:\n        COOKIES.append({'key': 'rucio-requested-path', 'value': request.environ.get('REQUEST_URI')})\n    else:\n        return access_granted(valid_token_dict, template, title)\n\n    # login without any known server config\n    if not AUTH_TYPE:\n        return add_cookies(make_response(render_template(\"select_login_method.html\", oidc_issuers=AUTH_ISSUERS, saml_support=SAML_SUPPORT)))\n    # for AUTH_TYPE predefined by the server continue\n    else:\n        if AUTH_TYPE == 'userpass':\n            return redirect('/login', code=303)\n        elif AUTH_TYPE == 'x509':\n            return x509token_auth(None)\n        elif AUTH_TYPE == 'x509_userpass':\n            if request.environ.get('SSL_CLIENT_VERIFY') == 'SUCCESS':\n                return x509token_auth(None)\n            return render_template(\"no_certificate.html\")\n        elif AUTH_TYPE == 'oidc':\n            return oidc_auth(None, AUTH_ISSUER_WEBUI)\n        elif AUTH_TYPE == 'saml':\n            return saml_auth(\"GET\")\n        return render_template('problem.html', msg='Invalid auth type')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_send_leave_request(self, origin, pdu):\n        \"\"\" We have received a leave event for a room. Fully process it.\"\"\"\n        event = pdu\n\n        logger.debug(\n            \"on_send_leave_request: Got event: %s, signatures: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        event.internal_metadata.outlier = False\n\n        context = yield self._handle_new_event(origin, event)\n\n        event_allowed = yield self.third_party_event_rules.check_event_allowed(\n            event, context\n        )\n        if not event_allowed:\n            logger.info(\"Sending of leave %s forbidden by third-party rules\", event)\n            raise SynapseError(\n                403, \"This event is not allowed in this context\", Codes.FORBIDDEN\n            )\n\n        logger.debug(\n            \"on_send_leave_request: After _handle_new_event: %s, sigs: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        return None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, redirect_to):\n        super(HttpResponseRedirect, self).__init__()\n        self['Location'] = iri_to_uri(redirect_to)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    const findHeadingElement = (cell: Cell): void => {\n      model.getCellHeadings(cell).forEach(async heading => {\n        const elementId = await getIdForHeading(heading, this.parser!);\n\n        const selector = elementId\n          ? `h${heading.level}[id=\"${elementId}\"]`\n          : `h${heading.level}`;\n\n        if (heading.outputIndex !== undefined) {\n          // Code cell\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              (heading.cellRef as CodeCell).outputArea.widgets[\n                heading.outputIndex\n              ].node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        } else {\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              heading.cellRef.node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        }\n      });\n    };\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def event_from_pdu_json(pdu_json, outlier=False):\n    \"\"\"Construct a FrozenEvent from an event json received over federation\n\n    Args:\n        pdu_json (object): pdu as received over federation\n        outlier (bool): True to mark this event as an outlier\n\n    Returns:\n        FrozenEvent\n\n    Raises:\n        SynapseError: if the pdu is missing required fields\n    \"\"\"\n    # we could probably enforce a bunch of other fields here (room_id, sender,\n    # origin, etc etc)\n    assert_params_in_request(pdu_json, ('event_id', 'type'))\n    event = FrozenEvent(\n        pdu_json\n    )\n\n    event.internal_metadata.outlier = outlier\n\n    return event\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    const Tensor& tag = c->input(0);\n    const Tensor& tensor = c->input(1);\n    OP_REQUIRES(c, TensorShapeUtils::IsScalar(tag.shape()),\n                errors::InvalidArgument(\"Tag must be a scalar\"));\n    OP_REQUIRES(c, tensor.dims() >= 2 && tensor.dims() <= 3,\n                errors::InvalidArgument(\"Tensor must be 3-D or 2-D, got: \",\n                                        tensor.shape().DebugString()));\n    const string& base_tag = tag.scalar<tstring>()();\n\n    float sample_rate = sample_rate_attr_;\n    if (!has_sample_rate_attr_) {\n      const Tensor& sample_rate_tensor = c->input(2);\n      sample_rate = sample_rate_tensor.scalar<float>()();\n    }\n    OP_REQUIRES(c, sample_rate > 0.0f,\n                errors::InvalidArgument(\"sample_rate must be > 0\"));\n\n    const int batch_size = tensor.dim_size(0);\n    const int64_t length_frames = tensor.dim_size(1);\n    const int64_t num_channels =\n        tensor.dims() == 2 ? 1 : tensor.dim_size(tensor.dims() - 1);\n\n    Summary s;\n    const int N = std::min<int>(max_outputs_, batch_size);\n    for (int i = 0; i < N; ++i) {\n      Summary::Value* v = s.add_value();\n      if (max_outputs_ > 1) {\n        v->set_tag(strings::StrCat(base_tag, \"/audio/\", i));\n      } else {\n        v->set_tag(strings::StrCat(base_tag, \"/audio\"));\n      }\n\n      Summary::Audio* sa = v->mutable_audio();\n      sa->set_sample_rate(sample_rate);\n      sa->set_num_channels(num_channels);\n      sa->set_length_frames(length_frames);\n      sa->set_content_type(\"audio/wav\");\n\n      auto values =\n          tensor.shaped<float, 3>({batch_size, length_frames, num_channels});\n      const float* data =\n          tensor.NumElements() == 0 ? nullptr : &values(i, 0, 0);\n\n      size_t sample_rate_truncated = lrintf(sample_rate);\n      if (sample_rate_truncated == 0) {\n        sample_rate_truncated = 1;\n      }\n      OP_REQUIRES_OK(c, wav::EncodeAudioAsS16LEWav(\n                            data, sample_rate_truncated, num_channels,\n                            length_frames, sa->mutable_encoded_audio_string()));\n    }\n\n    Tensor* summary_tensor = nullptr;\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape({}), &summary_tensor));\n    CHECK(SerializeToTString(s, &summary_tensor->scalar<tstring>()()));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_header_end(self) -> None:\n        message = (MultiPartMessage.HEADER_END, b\"\")\n        self.messages.append(message)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create_code_for_user(cls, user, next=None):\n        if not user.is_active:\n            return None\n\n        code = cls.generate_code()\n        login_code = LoginCode(user=user, code=code)\n        if next is not None:\n            login_code.next = next\n        login_code.save()\n        return login_code\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    PartialTensorShape element_shape;\n    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(0), &element_shape));\n    int32 num_elements = c->input(1).scalar<int32>()();\n    TensorList output;\n    output.element_shape = element_shape;\n    output.element_dtype = element_dtype_;\n    output.tensors().resize(num_elements, Tensor(DT_INVALID));\n    Tensor* result;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape{}, &result, attr));\n    result->scalar<Variant>()() = std::move(output);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n    OP_REQUIRES_OK(\n        ctx, functor::UpperBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n\n    PoolParameters params{context,\n                          ksize_,\n                          stride_,\n                          padding_,\n                          /*explicit_paddings=*/{},\n                          FORMAT_NHWC,\n                          tensor_in.shape()};\n    if (!context->status().ok()) {\n      return;\n    }\n\n    TensorShape out_shape({params.tensor_in_batch, params.out_height,\n                           params.out_width, params.depth});\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n    Tensor* argmax = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, out_shape, &argmax));\n\n    LaunchMaxPoolingWithArgmax<Device, T, Targmax>::launch(\n        context, params, tensor_in, output, argmax, propagate_nans_,\n        include_batch_in_index_);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _wget_download(url, fullname):\n    # using wget to download url\n    tmp_fullname = fullname + \"_tmp\"\n    # \u2013user-agent\n    command = f'wget -O {tmp_fullname} -t {DOWNLOAD_RETRY_LIMIT} {url}'\n    subprc = subprocess.Popen(\n        command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n    )\n    _ = subprc.communicate()\n\n    if subprc.returncode != 0:\n        raise RuntimeError(\n            f'{command} failed. Please make sure `wget` is installed or {url} exists'\n        )\n\n    shutil.move(tmp_fullname, fullname)\n\n    return fullname\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def delete_shelf(shelf_id):\n    cur_shelf = ub.session.query(ub.Shelf).filter(ub.Shelf.id == shelf_id).first()\n    try:\n        delete_shelf_helper(cur_shelf)\n    except InvalidRequestError:\n        ub.session.rollback()\n        log.error(\"Settings DB is not Writeable\")\n        flash(_(\"Settings DB is not Writeable\"), category=\"error\")\n    return redirect(url_for('web.index'))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status TrySimplify(NodeDef* consumer, string* simplified_node_name) override {\n    NodeDef* producer;\n    TF_RETURN_IF_ERROR(GetInputNode(consumer->input(0), &producer));\n    const bool producer_is_cast = IsCastLike(*producer);\n    const bool can_optimize =\n        !IsCheckNumerics(*producer) &&\n        ((producer_is_cast && IsValuePreserving(*consumer)) ||\n         (IsValuePreserving(*producer) && IsCastLike(*consumer)));\n    if (!can_optimize || IsControlFlow(*producer) ||\n        IsInPreserveSet(*producer) ||\n        producer->device() != consumer->device()) {\n      return Status::OK();\n    }\n\n    const NodeDef* cast_like_node = producer_is_cast ? producer : consumer;\n    const OpDef* cast_like_op_def = nullptr;\n    TF_RETURN_IF_ERROR(OpRegistry::Global()->LookUpOpDef(cast_like_node->op(),\n                                                         &cast_like_op_def));\n    DataType cast_src_type;\n    TF_RETURN_IF_ERROR(InputTypeForNode(*cast_like_node, *cast_like_op_def, 0,\n                                        &cast_src_type));\n    DataType cast_dst_type;\n    TF_RETURN_IF_ERROR(OutputTypeForNode(*cast_like_node, *cast_like_op_def, 0,\n                                         &cast_dst_type));\n    if (!IsFixedSizeType(cast_src_type) || !IsFixedSizeType(cast_dst_type)) {\n      return Status::OK();\n    } else if (producer_is_cast &&\n               DataTypeSize(cast_dst_type) <= DataTypeSize(cast_src_type)) {\n      return Status::OK();\n    } else if (!producer_is_cast &&\n               DataTypeSize(cast_dst_type) >= DataTypeSize(cast_src_type)) {\n      return Status::OK();\n    }\n\n    // Check that nodes were not already optimized.\n    const string optimized_producer_name = OptimizedNodeName(\n        ParseNodeScopeAndName(producer->name()), DataTypeString(cast_dst_type));\n    const string optimized_consumer_name = OptimizedNodeName(\n        ParseNodeScopeAndName(consumer->name()), DataTypeString(cast_src_type));\n    const bool is_already_optimized =\n        ctx().node_map->NodeExists(optimized_consumer_name) ||\n        ctx().node_map->NodeExists(optimized_producer_name);\n    if (is_already_optimized) {\n      return Status::OK();\n    }\n\n    // Add copies of consumer and producer in reverse order.\n    NodeDef* input;\n    TF_RETURN_IF_ERROR(GetInputNode(producer->input(0), &input));\n    // Create new producer node.\n    NodeDef* new_producer = AddCopyNode(optimized_consumer_name, consumer);\n    new_producer->set_input(0, producer->input(0));\n    ctx().node_map->AddOutput(input->name(), new_producer->name());\n\n    // Create new consumer node.\n    NodeDef* new_consumer = AddCopyNode(optimized_producer_name, producer);\n    new_consumer->set_input(0, new_producer->name());\n\n    NodeDef* new_value_preserving =\n        producer_is_cast ? new_producer : new_consumer;\n    const DataType new_input_type =\n        producer_is_cast ? cast_src_type : cast_dst_type;\n    // Update the input type of the value-preserving node. The input and\n    // output types of the cast-like nodes remain the same.\n    TF_RETURN_IF_ERROR(SetInputType(new_input_type, new_value_preserving));\n    // Make sure there is a kernel registered for the value preserving op\n    // with the new input type.\n    TF_RETURN_IF_ERROR(IsKernelRegisteredForNode(*new_value_preserving));\n    ctx().node_map->AddOutput(new_producer->name(), new_consumer->name());\n\n    AddToOptimizationQueue(new_producer);\n    *simplified_node_name = new_consumer->name();\n\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def wrapper(self, *args, **kwargs):\n        try:\n            result = method(self, *args, **kwargs)\n        except web.HTTPError as e:\n            status = e.status_code\n            message = e.log_message\n            self.log.warn(message)\n            self.set_status(e.status_code)\n            self.finish(json.dumps(dict(message=message)))\n        except Exception:\n            self.log.error(\"Unhandled error in API request\", exc_info=True)\n            status = 500\n            message = \"Unknown server error\"\n            t, value, tb = sys.exc_info()\n            self.set_status(status)\n            tb_text = ''.join(traceback.format_exception(t, value, tb))\n            reply = dict(message=message, traceback=tb_text)\n            self.finish(json.dumps(reply))\n        else:\n            return result\n    return wrapper"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _init_file(self):\n        \"\"\"Init the password file, set the password for it.\n        \"\"\"\n\n        password = None\n        while 1:\n            if not password:\n                password = self._getpass(\"Please set a password for your new keyring\")\n                password2 = self._getpass('Password (again): ')\n                if password != password2:\n                    sys.stderr.write(\"Error: Your passwords didn't match\\n\")\n                    password = None\n                    continue\n            if '' == password.strip():\n                # forbid the blank password\n                sys.stderr.write(\"Error: blank passwords aren't allowed.\\n\")\n                password = None\n                continue\n            if len(password) > _BLOCK_SIZE:\n                # block size of AES is less than 32\n                sys.stderr.write(\"Error: password can't be longer than 32.\\n\")\n                password = None\n                continue\n            break\n\n        # hash the password\n        import crypt\n        self.crypted_password = crypt.crypt(password, password)\n\n        # write down the initialization\n        config = ConfigParser.RawConfigParser()\n        config.add_section(_KEYRING_SETTING)\n        config.set(_KEYRING_SETTING, _CRYPTED_PASSWORD, self.crypted_password)\n\n        config_file = open(self.file_path,'w')\n        config.write(config_file)\n\n        if config_file:\n            config_file.close()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    auto x = ctx->input(0);\n    auto i = ctx->input(1);\n    auto v = ctx->input(2);\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(i.shape()),\n                errors::InvalidArgument(\"i must be a vector. \",\n                                        i.shape().DebugString()));\n    OP_REQUIRES(ctx, x.dims() == v.dims(),\n                errors::InvalidArgument(\n                    \"x and v shape doesn't match (ranks differ): \",\n                    x.shape().DebugString(), \" vs. \", v.shape().DebugString()));\n    for (int i = 1; i < x.dims(); ++i) {\n      OP_REQUIRES(\n          ctx, x.dim_size(i) == v.dim_size(i),\n          errors::InvalidArgument(\"x and v shape doesn't match at index \", i,\n                                  \" : \", x.shape().DebugString(), \" vs. \",\n                                  v.shape().DebugString()));\n    }\n    OP_REQUIRES(ctx, i.dim_size(0) == v.dim_size(0),\n                errors::InvalidArgument(\n                    \"i and x shape doesn't match at index 0: \",\n                    i.shape().DebugString(), \" vs. \", v.shape().DebugString()));\n\n    Tensor y = x;  // This creates an alias intentionally.\n    // Skip processing if tensors are empty.\n    if (x.NumElements() > 0 || v.NumElements() > 0) {\n      OP_REQUIRES_OK(ctx, DoCompute(ctx, i, v, &y));\n    }\n    ctx->set_output(0, y);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def guard(dest, ev_args):\n    next(dest)\n    depth = 1\n    while True:\n        ev = yield\n        if ev[0] == \"start\":\n            depth += 1\n        elif ev[0] == \"end\":\n            depth -= 1\n        try:\n            dest.send(ev)\n        except StopIteration as exc:\n            return exc.value\n        except Exception as exc:\n            error = exc\n            break\n    while depth > 0:\n        ev_type, *_ = yield\n        if ev_type == \"end\":\n            depth -= 1\n    raise error\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_session_id_by_provider_id(self, provider_id):\n        \"\"\"\n        Return a tuple of logged-in session IDs by provider_id\n        \"\"\"\n        candidates = self._ss.get_user_sessions(self.user)\n\n        session_ids = []\n        for c in candidates:\n            key = c.keys()[0]\n            if c[key].get('provider_id') == provider_id:\n                samlsession = self._data_to_samlsession(key, c[key])\n                session_ids.append(samlsession.session_id.encode('utf-8'))\n\n        return tuple(session_ids)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def real_download(self, filename, info_dict):\n        self.report_destination(filename)\n        tmpfilename = self.temp_name(filename)\n\n        try:\n            started = time.time()\n            retval = self._call_downloader(tmpfilename, info_dict)\n        except KeyboardInterrupt:\n            if not info_dict.get('is_live'):\n                raise\n            # Live stream downloading cancellation should be considered as\n            # correct and expected termination thus all postprocessing\n            # should take place\n            retval = 0\n            self.to_screen('[%s] Interrupted by user' % self.get_basename())\n\n        if retval == 0:\n            status = {\n                'filename': filename,\n                'status': 'finished',\n                'elapsed': time.time() - started,\n            }\n            if filename != '-':\n                fsize = os.path.getsize(encodeFilename(tmpfilename))\n                self.try_rename(tmpfilename, filename)\n                status.update({\n                    'downloaded_bytes': fsize,\n                    'total_bytes': fsize,\n                })\n            self._hook_progress(status, info_dict)\n            return True\n        else:\n            self.to_stderr('\\n')\n            self.report_error('%s exited with code %d' % (\n                self.get_basename(), retval))\n            return False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) final {\n    const Tensor& superdiag = context->input(0);\n    const Tensor& maindiag = context->input(1);\n    const Tensor& subdiag = context->input(2);\n    const Tensor& rhs = context->input(3);\n\n    const int ndims = rhs.dims();\n    int64 batch_size = 1;\n    for (int i = 0; i < ndims - 2; i++) {\n      batch_size *= rhs.dim_size(i);\n    }\n    const int m = rhs.dim_size(ndims - 2);\n    const int n = rhs.dim_size(ndims - 1);\n\n    // Allocate output.\n    Tensor* output;\n    OP_REQUIRES_OK(context, context->allocate_output(0, rhs.shape(), &output));\n\n    const Eigen::GpuDevice& device = context->eigen_device<Eigen::GpuDevice>();\n    GpuLaunchConfig cfg = GetGpuLaunchConfig(1, device);\n    TF_CHECK_OK(GpuLaunchKernel(\n        TridiagonalMatMulKernel<Scalar>, cfg.block_count, cfg.thread_per_block,\n        0, device.stream(), batch_size, m, n, superdiag.flat<Scalar>().data(),\n        maindiag.flat<Scalar>().data(), subdiag.flat<Scalar>().data(),\n        rhs.flat<Scalar>().data(), output->flat<Scalar>().data()));\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def upload_template(filename, destination, context=None, use_jinja=False,\n    template_dir=None, use_sudo=False, backup=True, mirror_local_mode=False,\n    mode=None):\n    \"\"\"\n    Render and upload a template text file to a remote host.\n\n    ``filename`` should be the path to a text file, which may contain `Python\n    string interpolation formatting\n    <http://docs.python.org/release/2.5.4/lib/typesseq-strings.html>`_ and will\n    be rendered with the given context dictionary ``context`` (if given.)\n\n    Alternately, if ``use_jinja`` is set to True and you have the Jinja2\n    templating library available, Jinja will be used to render the template\n    instead. Templates will be loaded from the invoking user's current working\n    directory by default, or from ``template_dir`` if given.\n\n    The resulting rendered file will be uploaded to the remote file path\n    ``destination`` (which should include the desired remote filename.) If the\n    destination file already exists, it will be renamed with a ``.bak``\n    extension unless ``backup=False`` is specified.\n\n    By default, the file will be copied to ``destination`` as the logged-in\n    user; specify ``use_sudo=True`` to use `sudo` instead.\n\n    In some use cases, it is desirable to force a newly uploaded file to match\n    the mode of its local counterpart (such as when uploading executable\n    scripts). To do this, specify ``mirror_local_mode=True``.\n\n    Alternately, you may use the ``mode`` kwarg to specify an exact mode, in\n    the same vein as ``os.chmod`` or the Unix ``chmod`` command.\n    \"\"\"\n    basename = os.path.basename(filename)\n\n    # This temporary file should not be automatically deleted on close, as we\n    # need it there to upload it (Windows locks the file for reading while\n    # open).\n    tempdir = tempfile.mkdtemp()\n    tempfile_name = os.path.join(tempdir, basename)\n    output = open(tempfile_name, \"w+b\")\n    # Init\n    text = None\n    if use_jinja:\n        try:\n            from jinja2 import Environment, FileSystemLoader\n            jenv = Environment(loader=FileSystemLoader(template_dir or '.'))\n            text = jenv.get_template(filename).render(**context or {})\n        except ImportError, e:\n            abort(\"tried to use Jinja2 but was unable to import: %s\" % e)\n    else:\n        with open(filename) as inputfile:\n            text = inputfile.read()\n        if context:\n            text = text % context\n    output.write(text)\n    output.close()\n\n    # Back up any original file \n    func = use_sudo and sudo or run\n    # Back up any original file (need to do figure out ultimate destination)\n    if backup:\n        to_backup = destination\n        with settings(hide('everything'), warn_only=True):\n            # Is destination a directory?\n            if func('test -f %s' % to_backup).failed:\n                # If so, tack on the filename to get \"real\" destination\n                to_backup = destination + '/' + basename\n        if exists(to_backup):\n            func(\"cp %s %s.bak\" % (to_backup, to_backup))\n\n    # Upload the file.\n    put(\n        local_path=tempfile_name,\n        remote_path=destination,\n        use_sudo=use_sudo,\n        mirror_local_mode=mirror_local_mode,\n        mode=mode\n    )\n    output.close()\n    os.remove(tempfile_name)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Subgraph::Invoke() {\n  if (!consistent_) {\n    ReportError(\"Invoke called on model that is not consistent.\");\n    return kTfLiteError;\n  }\n\n  TfLiteStatus status = kTfLiteOk;\n  if (state_ == kStateUninvokable) {\n    ReportError(\"Invoke called on model that is not ready.\");\n    return kTfLiteError;\n  } else if (memory_planner_ && !memory_planner_->HasNonPersistentMemory()) {\n    ReportError(\"Non-persistent memory is not available.\");\n    return kTfLiteError;\n  }\n  TFLITE_SCOPED_TAGGED_DEFAULT_PROFILE(profiler_.get(), \"Invoke\");\n\n  // Invocations are always done in node order.\n  // Note that calling Invoke repeatedly will cause the original memory plan to\n  // be reused, unless either ResizeInputTensor() or AllocateTensors() has been\n  // called.\n  for (int execution_plan_index = 0;\n       execution_plan_index < execution_plan_.size(); execution_plan_index++) {\n    if (execution_plan_index == next_execution_plan_index_to_prepare_) {\n      TF_LITE_ENSURE_STATUS(PrepareOpsAndTensors());\n      TF_LITE_ENSURE(&context_, next_execution_plan_index_to_prepare_ >=\n                                    execution_plan_index);\n    }\n    int node_index = execution_plan_[execution_plan_index];\n    TfLiteNode& node = nodes_and_registration_[node_index].first;\n    const TfLiteRegistration& registration =\n        nodes_and_registration_[node_index].second;\n\n    const char* op_name = nullptr;\n    if (profiler_) op_name = GetTFLiteOpName(registration);\n    TFLITE_SCOPED_TAGGED_OPERATOR_PROFILE(profiler_.get(), op_name, node_index);\n\n    for (int i = 0; i < node.inputs->size; ++i) {\n      int tensor_index = node.inputs->data[i];\n      if (tensor_index == kTfLiteOptionalTensor) {\n        continue;\n      }\n      TfLiteTensor* tensor = &tensors_[tensor_index];\n      if (tensor->delegate && tensor->delegate != node.delegate &&\n          tensor->data_is_stale) {\n        TF_LITE_ENSURE_STATUS(EnsureTensorDataIsReadable(tensor_index));\n      }\n      if (tensor->data.raw == nullptr && tensor->bytes > 0) {\n        if (registration.builtin_code == kTfLiteBuiltinReshape && i == 1) {\n          // In general, having a tensor here with no buffer will be an error.\n          // However, for the reshape operator, the second input tensor is only\n          // used for the shape, not for the data. Thus, null buffer is ok.\n          continue;\n        } else {\n          // In all other cases, we need to return an error as otherwise we will\n          // trigger a null pointer dereference (likely).\n          ReportError(\"Input tensor %d lacks data\", tensor_index);\n          return kTfLiteError;\n        }\n      }\n    }\n\n    if (check_cancelled_func_ != nullptr &&\n        check_cancelled_func_(cancellation_data_)) {\n      ReportError(\"Client requested cancel during Invoke()\");\n      return kTfLiteError;\n    }\n\n    EnsureTensorsVectorCapacity();\n    tensor_resized_since_op_invoke_ = false;\n    if (OpInvoke(registration, &node) != kTfLiteOk) {\n      return ReportOpError(&context_, node, registration, node_index,\n                           \"failed to invoke\");\n    }\n\n    // Force execution prep for downstream ops if the latest op triggered the\n    // resize of a dynamic tensor.\n    if (tensor_resized_since_op_invoke_ &&\n        HasDynamicTensor(context_, node.outputs)) {\n      next_execution_plan_index_to_prepare_ = execution_plan_index + 1;\n\n      // This happens when an intermediate dynamic tensor is resized.\n      // We don't have to prepare all the ops, but we need to recompute\n      // the allocation plan.\n      if (next_execution_plan_index_to_plan_allocation_ >\n          next_execution_plan_index_to_prepare_) {\n        next_execution_plan_index_to_plan_allocation_ =\n            next_execution_plan_index_to_prepare_;\n        if (memory_planner_) {\n          TF_LITE_ENSURE_STATUS(memory_planner_->ResetAllocationsAfter(\n              next_execution_plan_index_to_plan_allocation_ - 1));\n        }\n      }\n    }\n  }\n\n  return status;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def do_template(self, data, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, disable_lookups=False):\n        if self.jinja2_native and not isinstance(data, string_types):\n            return data\n\n        # For preserving the number of input newlines in the output (used\n        # later in this method)\n        data_newlines = _count_newlines_from_end(data)\n\n        if fail_on_undefined is None:\n            fail_on_undefined = self._fail_on_undefined_errors\n\n        try:\n            # allows template header overrides to change jinja2 options.\n            if overrides is None:\n                myenv = self.environment\n            else:\n                myenv = self.environment.overlay(overrides)\n\n            # Get jinja env overrides from template\n            if hasattr(data, 'startswith') and data.startswith(JINJA2_OVERRIDE):\n                eol = data.find('\\n')\n                line = data[len(JINJA2_OVERRIDE):eol]\n                data = data[eol + 1:]\n                for pair in line.split(','):\n                    (key, val) = pair.split(':')\n                    key = key.strip()\n                    setattr(myenv, key, ast.literal_eval(val.strip()))\n\n            if escape_backslashes:\n                # Allow users to specify backslashes in playbooks as \"\\\\\" instead of as \"\\\\\\\\\".\n                data = _escape_backslashes(data, myenv)\n\n            try:\n                t = myenv.from_string(data)\n            except TemplateSyntaxError as e:\n                raise AnsibleError(\"template error while templating string: %s. String: %s\" % (to_native(e), to_native(data)))\n            except Exception as e:\n                if 'recursion' in to_native(e):\n                    raise AnsibleError(\"recursive loop detected in template string: %s\" % to_native(data))\n                else:\n                    return data\n\n            if disable_lookups:\n                t.globals['query'] = t.globals['q'] = t.globals['lookup'] = self._fail_lookup\n\n            jvars = AnsibleJ2Vars(self, t.globals)\n\n            self.cur_context = new_context = t.new_context(jvars, shared=True)\n            rf = t.root_render_func(new_context)\n\n            try:\n                if self.jinja2_native:\n                    res = ansible_native_concat(rf)\n                else:\n                    res = j2_concat(rf)\n                if getattr(new_context, 'unsafe', False):\n                    res = wrap_var(res)\n            except TypeError as te:\n                if 'AnsibleUndefined' in to_native(te):\n                    errmsg = \"Unable to look up a name or access an attribute in template string (%s).\\n\" % to_native(data)\n                    errmsg += \"Make sure your variable name does not contain invalid characters like '-': %s\" % to_native(te)\n                    raise AnsibleUndefinedVariable(errmsg)\n                else:\n                    display.debug(\"failing because of a type error, template data is: %s\" % to_text(data))\n                    raise AnsibleError(\"Unexpected templating type error occurred on (%s): %s\" % (to_native(data), to_native(te)))\n\n            if self.jinja2_native and not isinstance(res, string_types):\n                return res\n\n            if preserve_trailing_newlines:\n                # The low level calls above do not preserve the newline\n                # characters at the end of the input data, so we use the\n                # calculate the difference in newlines and append them\n                # to the resulting output for parity\n                #\n                # jinja2 added a keep_trailing_newline option in 2.7 when\n                # creating an Environment.  That would let us make this code\n                # better (remove a single newline if\n                # preserve_trailing_newlines is False).  Once we can depend on\n                # that version being present, modify our code to set that when\n                # initializing self.environment and remove a single trailing\n                # newline here if preserve_newlines is False.\n                res_newlines = _count_newlines_from_end(res)\n                if data_newlines > res_newlines:\n                    res += self.environment.newline_sequence * (data_newlines - res_newlines)\n            return res\n        except (UndefinedError, AnsibleUndefinedVariable) as e:\n            if fail_on_undefined:\n                raise AnsibleUndefinedVariable(e)\n            else:\n                display.debug(\"Ignoring undefined failure: %s\" % to_text(e))\n                return data\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_user_roles(self, context, user_id, tenant_id=None):\n        \"\"\"Get the roles for a user and tenant pair.\n\n        Since we're trying to ignore the idea of user-only roles we're\n        not implementing them in hopes that the idea will die off.\n\n        \"\"\"\n        if tenant_id is None:\n            raise exception.NotImplemented(message='User roles not supported: '\n                                                   'tenant ID required')\n\n        user = self.identity_api.get_user(context, user_id)\n        if user is None:\n            raise exception.UserNotFound(user_id=user_id)\n        tenant = self.identity_api.get_tenant(context, tenant_id)\n        if tenant is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        roles = self.identity_api.get_roles_for_user_and_tenant(\n                context, user_id, tenant_id)\n        return {'roles': [self.identity_api.get_role(context, x)\n                          for x in roles]}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_valid_filename(s):\n    \"\"\"\n    Return the given string converted to a string that can be used for a clean\n    filename. Remove leading and trailing spaces; convert other spaces to\n    underscores; and remove anything that is not an alphanumeric, dash,\n    underscore, or dot.\n    >>> get_valid_filename(\"john's portrait in 2004.jpg\")\n    'johns_portrait_in_2004.jpg'\n    \"\"\"\n    s = str(s).strip().replace(' ', '_')\n    return re.sub(r'(?u)[^-\\w.]', '', s)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def decorated(*args, **kwargs):\n            __tracebackhide__ = True  # Hide from pytest traceback.\n\n            appbuilder = current_app.appbuilder\n\n            dag_id = (\n                request.args.get(\"dag_id\")\n                or request.form.get(\"dag_id\")\n                or (request.is_json and request.json.get(\"dag_id\"))\n                or None\n            )\n            if appbuilder.sm.check_authorization(permissions, dag_id):\n                return func(*args, **kwargs)\n            elif not g.user.is_anonymous and not g.user.perms:\n                return (\n                    render_template(\n                        \"airflow/no_roles_permissions.html\",\n                        hostname=get_hostname()\n                        if conf.getboolean(\"webserver\", \"EXPOSE_HOSTNAME\")\n                        else \"redact\",\n                        logout_url=appbuilder.get_url_for_logout,\n                    ),\n                    403,\n                )\n            else:\n                access_denied = \"Access is Denied\"\n                flash(access_denied, \"danger\")\n            return redirect(\n                url_for(\n                    appbuilder.sm.auth_view.__class__.__name__ + \".login\",\n                    next=request.url,\n                )\n            )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def generate_nonce(length=8):\n    \"\"\"Generate pseudorandom number.\"\"\"\n    return ''.join([str(random.randint(0, 9)) for i in range(length)])\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "ImagingSgiRleDecode(Imaging im, ImagingCodecState state, UINT8 *buf, Py_ssize_t bytes) {\n    UINT8 *ptr;\n    SGISTATE *c;\n    int err = 0;\n    int status;\n\n    /* size check */\n    if (im->xsize > INT_MAX / im->bands || im->ysize > INT_MAX / im->bands) {\n        state->errcode = IMAGING_CODEC_MEMORY;\n        return -1;\n    }\n\n    /* Get all data from File descriptor */\n    c = (SGISTATE *)state->context;\n    _imaging_seek_pyFd(state->fd, 0L, SEEK_END);\n    c->bufsize = _imaging_tell_pyFd(state->fd);\n    c->bufsize -= SGI_HEADER_SIZE;\n\n    c->tablen = im->bands * im->ysize;\n    /* below, we populate the starttab and lentab into the bufsize,\n       each with 4 bytes per element of tablen\n       Check here before we allocate any memory\n    */\n    if (c->bufsize < 8 * c->tablen) {\n        state->errcode = IMAGING_CODEC_OVERRUN;\n        return -1;\n    }\n\n    ptr = malloc(sizeof(UINT8) * c->bufsize);\n    if (!ptr) {\n        state->errcode = IMAGING_CODEC_MEMORY;\n        return -1;\n    }\n    _imaging_seek_pyFd(state->fd, SGI_HEADER_SIZE, SEEK_SET);\n    _imaging_read_pyFd(state->fd, (char *)ptr, c->bufsize);\n\n    /* decoder initialization */\n    state->count = 0;\n    state->y = 0;\n    if (state->ystep < 0) {\n        state->y = im->ysize - 1;\n    } else {\n        state->ystep = 1;\n    }\n\n    /* Allocate memory for RLE tables and rows */\n    free(state->buffer);\n    state->buffer = NULL;\n    /* malloc overflow check above */\n    state->buffer = calloc(im->xsize * im->bands, sizeof(UINT8) * 2);\n    c->starttab = calloc(c->tablen, sizeof(UINT32));\n    c->lengthtab = calloc(c->tablen, sizeof(UINT32));\n    if (!state->buffer || !c->starttab || !c->lengthtab) {\n        err = IMAGING_CODEC_MEMORY;\n        goto sgi_finish_decode;\n    }\n    /* populate offsets table */\n    for (c->tabindex = 0, c->bufindex = 0; c->tabindex < c->tablen;\n         c->tabindex++, c->bufindex += 4) {\n        read4B(&c->starttab[c->tabindex], &ptr[c->bufindex]);\n    }\n    /* populate lengths table */\n    for (c->tabindex = 0, c->bufindex = c->tablen * sizeof(UINT32);\n         c->tabindex < c->tablen;\n         c->tabindex++, c->bufindex += 4) {\n        read4B(&c->lengthtab[c->tabindex], &ptr[c->bufindex]);\n    }\n\n    state->count += c->tablen * sizeof(UINT32) * 2;\n\n    /* read compressed rows */\n    for (c->rowno = 0; c->rowno < im->ysize; c->rowno++, state->y += state->ystep) {\n        for (c->channo = 0; c->channo < im->bands; c->channo++) {\n            c->rleoffset = c->starttab[c->rowno + c->channo * im->ysize];\n            c->rlelength = c->lengthtab[c->rowno + c->channo * im->ysize];\n            c->rleoffset -= SGI_HEADER_SIZE;\n\n            if (c->rleoffset + c->rlelength > c->bufsize) {\n                state->errcode = IMAGING_CODEC_OVERRUN;\n                goto sgi_finish_decode;\n            }\n\n            /* row decompression */\n            if (c->bpc == 1) {\n                status = expandrow(\n                    &state->buffer[c->channo],\n                    &ptr[c->rleoffset],\n                    c->rlelength,\n                    im->bands,\n                    im->xsize);\n            } else {\n                status = expandrow2(\n                    &state->buffer[c->channo * 2],\n                    &ptr[c->rleoffset],\n                    c->rlelength,\n                    im->bands,\n                    im->xsize);\n            }\n            if (status == -1) {\n                state->errcode = IMAGING_CODEC_OVERRUN;\n                goto sgi_finish_decode;\n            } else if (status == 1) {\n                goto sgi_finish_decode;\n            }\n\n            state->count += c->rlelength;\n        }\n\n        /* store decompressed data in image */\n        state->shuffle((UINT8 *)im->image[state->y], state->buffer, im->xsize);\n    }\n\n    c->bufsize++;\n\nsgi_finish_decode:;\n\n    free(c->starttab);\n    free(c->lengthtab);\n    free(ptr);\n    if (err != 0) {\n        state->errcode = err;\n        return -1;\n    }\n    return state->count - c->bufsize;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_initial_logout(self):\n        \"\"\"\n        Get the initial logout request.\n\n        Return None if no sessions in INIT_LOGOUT state.\n        \"\"\"\n        candidates = self._ss.get_user_sessions(self.user)\n\n        # FIXME: what does it mean if there are multiple in init? We\n        #        just return the first one for now. How do we know\n        #        it's the \"right\" one if multiple logouts are started\n        #        at the same time from different SPs?\n        for c in candidates:\n            key = c.keys()[0]\n            if int(c[key].get('logoutstate', 0)) == INIT_LOGOUT:\n                samlsession = self._data_to_samlsession(key, c[key])\n                return samlsession\n        return None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_image_dimensions(file_or_path, close=False):\n    \"\"\"\n    Returns the (width, height) of an image, given an open file or a path.  Set\n    'close' to True to close the file at the end if it is initially in an open\n    state.\n    \"\"\"\n    # Try to import PIL in either of the two ways it can end up installed.\n    try:\n        from PIL import ImageFile as PILImageFile\n    except ImportError:\n        import ImageFile as PILImageFile\n\n    p = PILImageFile.Parser()\n    if hasattr(file_or_path, 'read'):\n        file = file_or_path\n        file_pos = file.tell()\n        file.seek(0)\n    else:\n        file = open(file_or_path, 'rb')\n        close = True\n    try:\n        while 1:\n            data = file.read(1024)\n            if not data:\n                break\n            p.feed(data)\n            if p.image:\n                return p.image.size\n        return None\n    finally:\n        if close:\n            file.close()\n        else:\n            file.seek(file_pos)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "                repeatedPassword: ko.observable(undefined),\n                passwordMismatch: ko.pureComputed(function () {\n                    return self.editor.password() !== self.editor.repeatedPassword();\n                }),\n                apikey: ko.observable(undefined),\n                active: ko.observable(undefined),\n                permissionSelectable: function (permission) {\n                    return true;\n                },\n                permissionSelected: function (permission) {\n                    var index = self.editor.permissions().indexOf(permission);\n                    return index >= 0;\n                },\n                togglePermission: function (permission) {\n                    var permissions = self.editor.permissions();\n                    var index = permissions.indexOf(permission);\n                    if (index < 0) {\n                        permissions.push(permission);\n                    } else {\n                        permissions.splice(index, 1);\n                    }\n                    self.editor.permissions(permissions);\n                },\n                groupSelected: function (group) {\n                    var index = self.editor.groups().indexOf(group);\n                    return index >= 0;\n                },\n                toggleGroup: function (group) {\n                    var groups = self.editor.groups();\n                    var index = groups.indexOf(group);\n                    if (index < 0) {\n                        groups.push(group);\n                    } else {\n                        groups.splice(index, 1);\n                    }\n                    self.editor.groups(groups);\n                },\n                joinedGroupPermissions: function (group) {\n                    return access.permissionList(group);\n                },\n                header: ko.observable(undefined),\n                new: ko.observable(true),\n                confirm: undefined,\n                valid: ko.pureComputed(function () {\n                    return (\n                        self.editor.name() &&\n                        self.editor.name().trim() &&\n                        (!self.editor.new() ||\n                            (self.editor.password() &&\n                                self.editor.password().trim() &&\n                                !self.editor.passwordMismatch()))\n                    );\n                }),\n                dangerRestricted: function () {\n                    return false;\n                },\n                dangerRestrictedText: gettext(\n                    \"This user may not have dangerous permissions.\"\n                )\n            };\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def cli(configfile, observed_since, setup_only=False, troubleshoot=False):\n    '''\n    Tenable.io -> Jira Cloud Transformer & Ingester\n    '''\n    # Load the config, but ensure that any additional fields are additive to the\n    # basic field set.\n    config_from_file = yaml.load(configfile, Loader=yaml.Loader)\n    fields = config_from_file.pop('custom_fields', list())\n    config = dict_merge(base_config(), config_from_file)\n    config['fields'] = config['fields'] + fields\n\n    if config['tenable'].get('tio_transform_tags'):\n        attr_cache = config['tenable'].get('tio_asset_attr_cache', list())\n        tag_attrs = config['tenable'].get('tio_transform_tags', list())\n        config['tenable']['tio_asset_attr_cache'] = attr_cache + tag_attrs\n\n\n    # Get the logging definition and define any defaults as need be.\n    log = config.get('log', {})\n    log_lvls = {'debug': 10, 'info': 20, 'warn': 30, 'error': 40}\n    log['level'] = log_lvls[log.get('level', 'warn')]\n    log['format'] = log.get('format',\n        '%(asctime)-15s %(name)s %(levelname)s %(message)s')\n\n    # Configure the root logging facility\n    if troubleshoot:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=log['format'],\n            filename='tenable_debug.log'\n        )\n    else:\n        logging.basicConfig(**log)\n\n    # Output some basic information detailing the config file used and the\n    # python version & system arch.\n    logging.info('Tenable2JiraCloud Version {}'.format(__version__))\n    logging.info('Using configuration file {}'.format(configfile.name))\n    uname = platform.uname()\n    logging.info('Running on Python {} {}/{}'.format(\n        '.'.join([str(i) for i in sys.version_info][0:3]),\n        uname[0], uname[-2]))\n\n    # instantiate the Jira object\n    jira = Jira(\n        'https://{}/rest/api/3'.format(config['jira']['address']),\n        config['jira']['api_username'],\n        config['jira']['api_token']\n    )\n\n    # Initiate the Tenable.io API model, the Ingester model, and start the\n    # ingestion and data transformation.\n    if config['tenable'].get('platform') == 'tenable.io':\n        if not observed_since:\n            # if no since field is supplied, then look in the config file to see\n            # if an age was applied, if not, then use the default of 30 days.\n            observed_since = arrow.now()\\\n                .shift(days=-config['tenable'].get('tio_age', 30))\\\n                .floor('day').timestamp()\n\n        source = TenableIO(\n            access_key=config['tenable'].get('access_key'),\n            secret_key=config['tenable'].get('secret_key'),\n            vendor='Tenable',\n            product='JiraCloud',\n            build=__version__\n        )\n        if int(source.session.details().get('permissions')) < 64:\n            logging.error('API Keys tie to non-admin user.')\n    elif config['tenable'].get('platform') == 'tenable.sc':\n        source = TenableSC(\n            config['tenable'].get('address'),\n            port=int(config['tenable'].get('port', 443)),\n            username=config['tenable'].get('username'),\n            password=config['tenable'].get('password'),\n            access_key=config['tenable'].get('access_key'),\n            secret_key=config['tenable'].get('secret_key'),\n            vendor='Tenable',\n            product='JiraCloud',\n            build=__version__\n        )\n    else:\n        logging.error('No valid Tenable platform configuration defined.')\n        exit(1)\n    ingest = Tio2Jira(source, jira, config)\n\n    if troubleshoot:\n        # if the troubleshooting flag is set, then we will be collecting some\n        # basic information and outputting it to the screen in a format that\n        # Github issues would expect to format it all pretty.  This should help\n        # reduce the amount of time that is spent with back-and-forth debugging.\n        try:\n            ingest.ingest(int(observed_since))\n        except:\n            logging.exception('Caught the following Exception')\n\n        # Some basic redaction of sensitive data, such as API Keys, Usernames,\n        # Passwords, and hostnames.\n        addr = config_from_file['jira']['address']\n        sc_addr = 'NOTHING_TO_SEE_HERE_AT_ALL'\n        config_from_file['jira']['address'] = '<REDACTED>'\n        config_from_file['jira']['api_token'] = '<REDACTED>'\n        config_from_file['jira']['api_username'] = '<REDACTED>'\n        config_from_file['project']['leadAccountId'] = '<REDACTED>'\n        if config_from_file['tenable'].get('address'):\n            sc_addr = config_from_file['tenable']['address']\n            config_from_file['tenable']['address'] = '<REDACTED>'\n        if config_from_file['tenable'].get('access_key'):\n            config_from_file['tenable']['access_key'] = '<REDACTED>'\n        if config_from_file['tenable'].get('secret_key'):\n            config_from_file['tenable']['secret_key'] = '<REDACTED>'\n        if config_from_file['tenable'].get('username'):\n            config_from_file['tenable']['username'] = '<REDACTED>'\n        if config_from_file['tenable'].get('password'):\n            config_from_file['tenable']['password'] = '<REDACTED>'\n\n        output = troubleshooting.format(\n            configfile=yaml.dump(config_from_file, default_flow_style=False),\n            logging=open('tenable_debug.log').read() \\\n                .replace(addr, '<JIRA_CLOUD_HOST>') \\\n                .replace(sc_addr, '<TENABLE_SC_HOST>'),\n            issuetypes='\\n'.join(\n                [\n                    '{id}: {name}'.format(**a)\n                    for a in jira.issue_types.list()\n                    if a.get('name').lower() in ['task', 'subtask', 'sub-task']\n                ]\n            )\n        )\n        print(output)\n        print('\\n'.join([\n            '/-------------------------------NOTICE-----------------------------------\\\\',\n            '| The output above is helpful for us to troubleshoot exactly what is     |',\n            '| happening within the code and offer a diagnosis for how to correct.    |',\n            '| Please note that while some basic redaction has already been performed |',\n            '| that we ask you to review the information you\\'re about to send and     |',\n            '| ensure that nothing deemed sensitive is transmitted.                   |',\n            '| ---------------------------------------------------------------------- |',\n            '| -- Copy of output saved to \"issue_debug.md\"                            |',\n            '\\\\------------------------------------------------------------------------/'\n        ]))\n        with open('issue_debug.md', 'w') as reportfile:\n            print(output, file=reportfile)\n        os.remove('tenable_debug.log')\n    elif not setup_only:\n        ingest.ingest(observed_since)\n\n        # If we are expected to continually re-run the transformer, then we will\n        # need to track the passage of time and run every X hours, where X is\n        # defined by the user in the configuration.\n        if config.get('service', {}).get('interval', 0) > 0:\n            sleeper = int(config['service']['interval']) * 3600\n            while True:\n                last_run = int(time.time())\n                logging.info(\n                    'Sleeping for {}h'.format(sleeper/3600))\n                time.sleep(sleeper)\n                logging.info(\n                    'Initiating ingest with observed_since={}'.format(last_run))\n                ingest.ingest(last_run)\n    elif setup_only:\n        # In setup-only mode, the ingest will not run, and instead a config file\n        # will be generated that will have all of the JIRA identifiers baked in\n        # and will also inform the integration to ignore the screen builder.\n        # When using this config, if there are any changes to the code, then\n        # this config will need to be re-generated.\n        config['screen']['no_create'] = True\n        logging.info('Set to setup-only.  Will not run ingest.')\n        logging.info('The following is the updated config file from the setup.')\n        with open('generated_config.yaml', 'w') as outfile:\n            outfile.write(yaml.dump(config, Dumper=yaml.Dumper))\n        logging.info('Generated \"generated_config.yaml\" config file.')\n        logging.info('This config file should be updated for every new version of this integration.')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def unarchive(request, bookmark_id: int):\n    bookmark = Bookmark.objects.get(pk=bookmark_id)\n    unarchive_bookmark(bookmark)\n    return_url = request.GET.get('return_url')\n    return_url = return_url if return_url else reverse('bookmarks:archived')\n    return HttpResponseRedirect(return_url)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _get_most_recent_version(self, allow_local):\n        # type: (bool) -> CryptographicMaterialsProvider\n        \"\"\"Get the most recent version of the provider.\n\n        If allowing local and we cannot obtain the lock, just return the most recent local\n        version. Otherwise, wait for the lock and ask the provider store for the most recent\n        version of the provider.\n\n        :param bool allow_local: Should we allow returning the local version if we cannot obtain the lock?\n        :returns: version and corresponding cryptographic materials provider\n        :rtype: CryptographicMaterialsProvider\n        \"\"\"\n        acquired = self._lock.acquire(not allow_local)\n\n        if not acquired:\n            # We failed to acquire the lock.\n            # If blocking, we will never reach this point.\n            # If not blocking, we want whatever the latest local version is.\n            _LOGGER.debug(\"Failed to acquire lock. Returning the last cached version.\")\n            version = self._version\n            return self._cache.get(version)\n\n        try:\n            max_version = self._get_max_version()\n            try:\n                provider = self._cache.get(max_version)\n            except KeyError:\n                provider = self._get_provider(max_version)\n            received_version = self._provider_store.version_from_material_description(\n                provider._material_description  # pylint: disable=protected-access\n            )\n\n            _LOGGER.debug(\"Caching materials provider version %d\", received_version)\n            self._version = received_version  # pylint: disable=attribute-defined-outside-init\n            self._last_updated = time.time()  # pylint: disable=attribute-defined-outside-init\n            self._cache.put(received_version, provider)\n        finally:\n            self._lock.release()\n\n        _LOGGER.debug(\"New latest version is %d\", self._version)\n\n        return provider\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static int ReadHuffmanCodeLengths(\n    VP8LDecoder* const dec, const int* const code_length_code_lengths,\n    int num_symbols, int* const code_lengths) {\n  int ok = 0;\n  VP8LBitReader* const br = &dec->br_;\n  int symbol;\n  int max_symbol;\n  int prev_code_len = DEFAULT_CODE_LENGTH;\n  HuffmanCode table[1 << LENGTHS_TABLE_BITS];\n\n  if (!VP8LBuildHuffmanTable(table, LENGTHS_TABLE_BITS,\n                             code_length_code_lengths,\n                             NUM_CODE_LENGTH_CODES)) {\n    goto End;\n  }\n\n  if (VP8LReadBits(br, 1)) {    // use length\n    const int length_nbits = 2 + 2 * VP8LReadBits(br, 3);\n    max_symbol = 2 + VP8LReadBits(br, length_nbits);\n    if (max_symbol > num_symbols) {\n      goto End;\n    }\n  } else {\n    max_symbol = num_symbols;\n  }\n\n  symbol = 0;\n  while (symbol < num_symbols) {\n    const HuffmanCode* p;\n    int code_len;\n    if (max_symbol-- == 0) break;\n    VP8LFillBitWindow(br);\n    p = &table[VP8LPrefetchBits(br) & LENGTHS_TABLE_MASK];\n    VP8LSetBitPos(br, br->bit_pos_ + p->bits);\n    code_len = p->value;\n    if (code_len < kCodeLengthLiterals) {\n      code_lengths[symbol++] = code_len;\n      if (code_len != 0) prev_code_len = code_len;\n    } else {\n      const int use_prev = (code_len == kCodeLengthRepeatCode);\n      const int slot = code_len - kCodeLengthLiterals;\n      const int extra_bits = kCodeLengthExtraBits[slot];\n      const int repeat_offset = kCodeLengthRepeatOffsets[slot];\n      int repeat = VP8LReadBits(br, extra_bits) + repeat_offset;\n      if (symbol + repeat > num_symbols) {\n        goto End;\n      } else {\n        const int length = use_prev ? prev_code_len : 0;\n        while (repeat-- > 0) code_lengths[symbol++] = length;\n      }\n    }\n  }\n  ok = 1;\n\n End:\n  if (!ok) return VP8LSetError(dec, VP8_STATUS_BITSTREAM_ERROR);\n  return ok;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_headers_finished(self) -> None:\n        message = (MultiPartMessage.HEADERS_FINISHED, b\"\")\n        self.messages.append(message)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "inline void Conv3DTranspose(\n    const Conv3DTransposeParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& bias_shape,\n    const float* bias_data, const RuntimeShape& output_shape,\n    float* output_data) {\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int stride_depth = params.stride_depth;\n  const int pad_width = params.padding_values.width;\n  const int pad_height = params.padding_values.height;\n  const int pad_depth = params.padding_values.depth;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_num_channels = MatchingDim(input_shape, 4, filter_shape, 4);\n  const int output_num_channels = output_shape.Dims(4);\n  const int input_depth = input_shape.Dims(1);\n  const int input_height = input_shape.Dims(2);\n  const int input_width = input_shape.Dims(3);\n  const int filter_depth = filter_shape.Dims(0);\n  const int filter_height = filter_shape.Dims(1);\n  const int filter_width = filter_shape.Dims(2);\n  const int output_depth = output_shape.Dims(1);\n  const int output_height = output_shape.Dims(2);\n  const int output_width = output_shape.Dims(3);\n  if (bias_data) {\n    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_num_channels);\n  }\n\n  // Initializes the output array to zero.\n  const int num_elements = output_shape.FlatSize();\n  for (int i = 0; i < num_elements; i++) {\n    output_data[i] = 0.0f;\n  }\n\n  // Loop through input elements one at a time.\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int in_d = 0; in_d < input_depth; ++in_d) {\n      for (int in_y = 0; in_y < input_height; ++in_y) {\n        for (int in_x = 0; in_x < input_width; ++in_x) {\n          for (int in_channel = 0; in_channel < input_num_channels;\n               ++in_channel) {\n            // Loop through the output elements it will influence.\n            const int out_x_origin = (in_x * stride_width) - pad_width;\n            const int out_y_origin = (in_y * stride_height) - pad_height;\n            const int out_d_origin = (in_d * stride_depth) - pad_depth;\n            for (int filter_d = 0; filter_d < filter_depth; ++filter_d) {\n              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {\n                  for (int out_channel = 0; out_channel < output_num_channels;\n                       ++out_channel) {\n                    // Compute output element location.\n                    const int out_x =\n                        out_x_origin + params.dilation_width * filter_x;\n                    const int out_y =\n                        out_y_origin + params.dilation_height * filter_y;\n                    const int out_d =\n                        out_d_origin + params.dilation_depth * filter_d;\n                    // We cannot accumulate out of bounds.\n                    if ((out_x >= 0) && (out_x < output_width) &&\n                        (out_y >= 0) && (out_y < output_height) &&\n                        (out_d >= 0) && (out_d < output_depth)) {\n                      float input_value = input_data[Offset(\n                          input_shape, batch, in_d, in_y, in_x, in_channel)];\n                      float filter_value = filter_data[Offset(\n                          filter_shape, filter_d, filter_y, filter_x,\n                          out_channel, in_channel)];\n                      output_data[Offset(output_shape, batch, out_d, out_y,\n                                         out_x, out_channel)] +=\n                          input_value * filter_value;\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n\n  const float float_activation_min = params.float_activation_min;\n  const float float_activation_max = params.float_activation_max;\n  float* data_ptr = output_data;\n  if (bias_data) {\n    const int outer_size =\n        batches * output_depth * output_height * output_width;\n    const int num_channels = input_shape.Dims(4);\n    for (int n = 0; n < outer_size; ++n) {\n      for (int c = 0; c < output_num_channels; ++c) {\n        data_ptr[c] = ActivationFunctionWithMinMax(data_ptr[c] + bias_data[c],\n                                                   float_activation_min,\n                                                   float_activation_max);\n      }\n      data_ptr += num_channels;\n    }\n  } else {\n    const int flat_size = output_shape.FlatSize();\n    for (int i = 0; i < flat_size; ++i) {\n      data_ptr[i] = ActivationFunctionWithMinMax(\n          data_ptr[i], float_activation_min, float_activation_max);\n    }\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_password(self, password, old_password=None):\n        \"\"\"\n        Change the user's password. Raise a ValueError if the username or\n        the password are invalid.\n        \"\"\"\n        assert isinstance(password, str)\n        assert old_password is None or isinstance(old_password, str)\n        if not password:\n            raise ValueError(\"password can't be empty\")\n        cfg = cherrypy.tree.apps[''].cfg\n\n        # Cannot update admin-password if defined\n        if self.username == cfg.admin_user and cfg.admin_password:\n            raise ValueError(_(\"can't update admin-password defined in configuration file\"))\n\n        if old_password and not check_password(old_password, self.hash_password):\n            raise ValueError(_(\"Wrong password\"))\n\n        # Check password length\n        if cfg.password_min_length > len(password) or len(password) > cfg.password_max_length:\n            raise ValueError(\n                _('Password must have between %(min)d and %(max)d characters.')\n                % {'min': cfg.password_min_length, 'max': cfg.password_max_length}\n            )\n\n        # Verify password score using zxcvbn\n        stats = zxcvbn(password)\n        if stats.get('score') < cfg.password_score:\n            msg = _('Password too weak.')\n            warning = stats.get('feedback', {}).get('warning')\n            suggestions = stats.get('feedback', {}).get('suggestions')\n            if warning:\n                msg += ' ' + warning\n            if suggestions:\n                msg += ' ' + ' '.join(suggestions)\n            raise ValueError(msg)\n\n        logger.info(\"updating user password [%s]\", self.username)\n        self.hash_password = hash_password(password)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __post_init__(self) -> None:\n        super().__post_init__()\n        if self.default is not None:\n            self.default = f'\"{self.default}\"'\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  public Response render(ServeEvent serveEvent) {\n    ResponseDefinition responseDefinition = serveEvent.getResponseDefinition();\n    if (targetAddressProhibited(responseDefinition.getProxyUrl())) {\n      return response()\n          .status(500)\n          .headers(new HttpHeaders(new HttpHeader(\"Content-Type\", \"text/plain\")))\n          .body(\"The target proxy address is denied in WireMock's configuration.\")\n          .build();\n    }\n\n    HttpUriRequest httpRequest = getHttpRequestFor(responseDefinition);\n    addRequestHeaders(httpRequest, responseDefinition);\n\n    GlobalSettings settings = settingsStore.get();\n\n    Request originalRequest = responseDefinition.getOriginalRequest();\n    if ((originalRequest.getBody() != null && originalRequest.getBody().length > 0)\n        || originalRequest.containsHeader(CONTENT_LENGTH)) {\n      httpRequest.setEntity(buildEntityFrom(originalRequest));\n    }\n\n    CloseableHttpClient client = chooseClient(serveEvent.getRequest().isBrowserProxyRequest());\n\n    try {\n      return client.execute(\n          httpRequest,\n          httpResponse ->\n              response()\n                  .status(httpResponse.getCode())\n                  .headers(headersFrom(httpResponse, responseDefinition))\n                  .body(getEntityAsByteArray(httpResponse))\n                  .fromProxy(true)\n                  .configureDelay(\n                      settings.getFixedDelay(),\n                      settings.getDelayDistribution(),\n                      responseDefinition.getFixedDelayMilliseconds(),\n                      responseDefinition.getDelayDistribution())\n                  .chunkedDribbleDelay(responseDefinition.getChunkedDribbleDelay())\n                  .build());\n    } catch (SSLException e) {\n      return proxyResponseError(\"SSL\", httpRequest, e);\n    } catch (IOException e) {\n      return proxyResponseError(\"Network\", httpRequest, e);\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _parse_qsl(qs):\n    r = []\n    for pair in qs.replace(';','&').split('&'):\n        if not pair: continue\n        nv = pair.split('=', 1)\n        if len(nv) != 2: nv.append('')\n        key = urlunquote(nv[0].replace('+', ' '))\n        value = urlunquote(nv[1].replace('+', ' '))\n        r.append((key, value))\n    return r\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def edit(request, bookmark_id: int):\n    try:\n        bookmark = Bookmark.objects.get(pk=bookmark_id, owner=request.user)\n    except Bookmark.DoesNotExist:\n        raise Http404('Bookmark does not exist')\n\n    if request.method == 'POST':\n        form = BookmarkForm(request.POST, instance=bookmark)\n        return_url = form.data['return_url']\n        if form.is_valid():\n            tag_string = convert_tag_string(form.data['tag_string'])\n            update_bookmark(form.save(commit=False), tag_string, request.user)\n            return HttpResponseRedirect(return_url)\n    else:\n        return_url = request.GET.get('return_url')\n        form = BookmarkForm(instance=bookmark)\n\n    return_url = return_url if return_url else reverse('bookmarks:index')\n\n    form.initial['tag_string'] = build_tag_string(bookmark.tag_names, ' ')\n    form.initial['return_url'] = return_url\n\n    context = {\n        'form': form,\n        'bookmark_id': bookmark_id,\n        'return_url': return_url\n    }\n\n    return render(request, 'bookmarks/edit.html', context)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _test_setup_settings(pxe_once=1):\n\n    # rewrite modules.conf so we know we can use the testing module\n    # for xmlrpc rw testing (Makefile will put the user value back)\n   \n    import yaml\n    import Cheetah.Template as Template\n\n    MODULES_TEMPLATE = \"installer_templates/settings.template\"\n    DEFAULTS = \"installer_templates/defaults\"\n    fh = open(DEFAULTS)\n    data = yaml.load(fh.read())\n    fh.close()\n    data[\"pxe_once\"] = pxe_once\n\n    t = Template.Template(file=MODULES_TEMPLATE, searchList=[data])\n    open(\"/etc/cobbler/settings\",\"w+\").write(t.respond())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def check_table_permission(cls, user, table, permission):\n        \"\"\"Check if the provided user has the specified permission against the table.\"\"\"\n        # If the table does *not* require permissions\n        if table in cls.RULESET_IGNORE:\n            return True\n\n        # Work out which roles touch the given table\n        for role in cls.RULESET_NAMES:\n            if table in cls.RULESET_MODELS[role]:\n\n                if check_user_role(user, role, permission):\n                    return True\n\n        # Check for children models which inherits from parent role\n        for (parent, child) in cls.RULESET_CHANGE_INHERIT:\n            # Get child model name\n            parent_child_string = f'{parent}_{child}'\n\n            if parent_child_string == table:\n                # Check if parent role has change permission\n                if check_user_role(user, parent, 'change'):\n                    return True\n\n        # Print message instead of throwing an error\n        name = getattr(user, 'name', user.pk)\n\n        logger.info(f\"User '{name}' failed permission check for {table}.{permission}\")\n\n        return False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _cli_deploy_image(args):\n\n    docker_repository = _resolve_property(args, 'docker_repository')\n    vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)\n    vault_endpoint = _resolve_vault_endpoint(args)\n    image_tar_path = args.image_tar_path if args.image_tar_path else find_image_file()\n\n    bigflow.deploy.deploy_docker_image(\n        image_tar_path=image_tar_path,\n        auth_method=args.auth_method,\n        docker_repository=docker_repository,\n        vault_endpoint=vault_endpoint,\n        vault_secret=vault_secret,\n    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _configure_session(cls, app):\n        tempdir = app.config[\"PYLOAD_API\"].get_cachedir()\n        cache_path = os.path.join(tempdir, \"flask\")\n        os.makedirs(cache_path, exist_ok=True)\n\n        app.config[\"SESSION_FILE_DIR\"] = cache_path\n        app.config[\"SESSION_TYPE\"] = \"filesystem\"\n        app.config[\"SESSION_COOKIE_NAME\"] = \"pyload_session\"\n        app.config[\"SESSION_COOKIE_SAMESITE\"] = \"None\"\n        app.config[\"SESSION_COOKIE_SECURE\"] = app.config[\"PYLOAD_API\"].get_config_value(\"webui\", \"use_ssl\")\n        app.config[\"SESSION_PERMANENT\"] = False\n\n        session_lifetime = max(app.config[\"PYLOAD_API\"].get_config_value(\"webui\", \"session_lifetime\"), 1) * 60\n        app.config[\"PERMANENT_SESSION_LIFETIME\"] = session_lifetime\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "                        def get_content_length() -> Optional[int]:\n                            # payload length\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\n                            if length_hdr is None:\n                                return None\n\n                            try:\n                                length = int(length_hdr)\n                            except ValueError:\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            if length < 0:\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            return length\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _ensureSnapshotsFolder(self, subdir=None):\n        \"\"\" Ensure that the appropriate snapshot folder exists.\n        \"\"\"\n        path = ['snapshots', self._snapshot_id]\n\n        if subdir is not None:\n            path.extend(subdir.split('/'))\n\n        current = self._tool\n\n        for element in path:\n\n            if element not in current.objectIds():\n                # No Unicode IDs!\n                current._setObject(str(element), Folder(element))\n\n            current = current._getOb(element)\n\n        return current\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def load(self):\n        if self.saving_method == \"mongo\":\n            data = self._load_mongo()\n        elif self.saving_method == \"local\":\n            data = self._load_local()\n        data = data[\"members\"][self.sid]\n        self.member.data = GroupMemberData(**data)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def default(self, path=b\"\", action=None, **kwargs):\n        repo_obj = self.app.store.get_repo(path)\n        if kwargs.get('keepdays'):\n            return self._remove_older(repo_obj, **kwargs)\n        elif kwargs.get('new_encoding'):\n            return self._set_encoding(repo_obj, **kwargs)\n        elif kwargs.get('maxage'):\n            return self._set_maxage(repo_obj, **kwargs)\n        # Get page data.\n        params = {\n            'repo': repo_obj,\n            'keepdays': repo_obj.keepdays,\n        }\n        # Generate page.\n        return self._compile_template(\"settings.html\", **params)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_object_element(dataset, seq_no, rel_path, download_link):\n    \"\"\"If rel_path and download_link are not None, we are called from scope.\n    Otherwise we are called from ID and need to run SQL query to fetch these attrs.\"\"\"\n\n    if rel_path is None:\n        query = \"SELECT rel_path, download_link FROM \" + \\\n        dataset + \\\n        \" WHERE sequence_no = %s\"\n\n        cnx = mysql.connector.connect(user=DB_USER,\n                                    password=DB_PASSWORD,\n                                    host=DB_HOST,\n                                    database=DB_DBNAME,\n                                    port=DB_PORT)\n        cursor = cnx.cursor()\n        cursor.execute(query, (seq_no,))\n\n        row = cursor.fetchone()\n\n        if not row:\n            return None\n\n        rel_path, download_link = row[0], row[1]\n\n    if LOCAL_OBJ_URI:\n        src_uri = 'file://' + os.path.join(DATAROOT, dataset, rel_path)\n    else:\n        src_uri = url_for('.get_object_src_http', dataset=dataset, rel_path=rel_path)\n\n    return '<object id={} src={} hyperfind.external-link={} />' \\\n        .format(\n        quoteattr(url_for('.get_object_id', dataset=dataset, seq_no=seq_no)),\n        quoteattr(src_uri),\n        quoteattr(download_link))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const float input_min_float = ctx->input(1).flat<float>()(0);\n    const float input_max_float = ctx->input(2).flat<float>()(0);\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(2, TensorShape({}), &output_max));\n\n    // See QuantizationRangeOp as well, which has a copy of this logic.\n    auto input_array = input.flat<T1>();\n    const int32_t input_lowest_quantized =\n        static_cast<int32>(Eigen::NumTraits<T1>::lowest());\n    const int32_t input_highest_quantized =\n        static_cast<int32>(Eigen::NumTraits<T1>::highest());\n    T1 actual_min_quantized = input_highest_quantized;\n    T1 actual_max_quantized = input_lowest_quantized;\n    for (int i = 0; i < input_array.size(); ++i) {\n      const T1 value = input_array(i);\n      actual_min_quantized = std::min(actual_min_quantized, value);\n      actual_max_quantized = std::max(actual_max_quantized, value);\n    }\n    // We want to make sure that the minimum is no larger than zero, so that the\n    // convolution operation can run efficiently.\n    const float actual_min_float =\n        std::min(0.0f, QuantizedToFloat(actual_min_quantized, input_min_float,\n                                        input_max_float));\n    const float actual_max_float = QuantizedToFloat(\n        actual_max_quantized, input_min_float, input_max_float);\n\n#if 0\n    // This is the reference, non-eigen implementation:\n    auto output_array = output->flat<T2>();\n    RequantizeManyInNewRange<T1, T2>(input_array.data(), input_array.size(),\n                                     input_min_float, input_max_float,\n                                     actual_min_float, actual_max_float,\n                                     output_array.data());\n#endif\n\n    if (input_array.size() > 0) {\n      if (meta::IsSupportedAndEnabled() && std::is_same<T1, qint32>() &&\n          std::is_same<T2, quint8>()) {\n        auto input_i32_array = input.flat<qint32>();\n        meta::Requantize(ctx, input_i32_array.data(), input_i32_array.size(),\n                         input_min_float, input_max_float, actual_min_float,\n                         actual_max_float, output->flat<quint8>().data());\n      } else {\n        RequantizeManyInNewRangeUsingEigen<T1, T2>(\n            ctx->eigen_device<CPUDevice>(), input, input_min_float,\n            input_max_float, actual_min_float, actual_max_float, output);\n      }\n    }\n\n    output_min->flat<float>().setConstant(actual_min_float);\n    output_max->flat<float>().setConstant(actual_max_float);\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _ttl_action(self):\n        # type: () -> TtlActions\n        \"\"\"Determine the correct action to take based on the local resources and TTL.\n\n        :returns: decision\n        :rtype: TtlActions\n        \"\"\"\n        if self._version is None:\n            _LOGGER.debug(\"TTL Expired because no version is known\")\n            return TtlActions.EXPIRED\n\n        time_since_updated = time.time() - self._last_updated\n\n        if time_since_updated < self._version_ttl:\n            return TtlActions.LIVE\n\n        if time_since_updated < self._version_ttl + _GRACE_PERIOD:\n            return TtlActions.GRACE_PERIOD\n\n        _LOGGER.debug(\"TTL Expired because known version has expired\")\n        return TtlActions.EXPIRED\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def config():\n    config_disabled = (\n            app.config['CONFIG_DISABLE'] or\n            not valid_user_session(session))\n    if request.method == 'GET':\n        return json.dumps(g.user_config.__dict__)\n    elif request.method == 'PUT' and not config_disabled:\n        if 'name' in request.args:\n            config_pkl = os.path.join(\n                app.config['CONFIG_PATH'],\n                request.args.get('name'))\n            session['config'] = (pickle.load(open(config_pkl, 'rb'))\n                                 if os.path.exists(config_pkl)\n                                 else session['config'])\n            return json.dumps(session['config'])\n        else:\n            return json.dumps({})\n    elif not config_disabled:\n        config_data = request.form.to_dict()\n        if 'url' not in config_data or not config_data['url']:\n            config_data['url'] = g.user_config.url\n\n        # Save config by name to allow a user to easily load later\n        if 'name' in request.args:\n            pickle.dump(\n                config_data,\n                open(os.path.join(\n                    app.config['CONFIG_PATH'],\n                    request.args.get('name')), 'wb'))\n\n        session['config'] = config_data\n        return redirect(config_data['url'])\n    else:\n        return redirect(url_for('.index'), code=403)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def explain_query_prefix(self, format=None, **options):\n        prefix = super().explain_query_prefix(format)\n        extra = {}\n        if format:\n            extra[\"FORMAT\"] = format\n        if options:\n            extra.update(\n                {\n                    name.upper(): \"true\" if value else \"false\"\n                    for name, value in options.items()\n                }\n            )\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def put(self, name: str):\n        \"\"\"add new file\n        params in FormData:\n            - file\n            - original_file_name [optional]\n        \"\"\"\n\n        data = {}\n        mindsdb_file_name = name\n\n        existing_file_names = ca.file_controller.get_files_names()\n\n        def on_field(field):\n            name = field.field_name.decode()\n            value = field.value.decode()\n            data[name] = value\n\n        file_object = None\n\n        def on_file(file):\n            nonlocal file_object\n            data[\"file\"] = file.file_name.decode()\n            file_object = file.file_object\n\n        temp_dir_path = tempfile.mkdtemp(prefix=\"mindsdb_file_\")\n\n        if request.headers[\"Content-Type\"].startswith(\"multipart/form-data\"):\n            parser = multipart.create_form_parser(\n                headers=request.headers,\n                on_field=on_field,\n                on_file=on_file,\n                config={\n                    \"UPLOAD_DIR\": temp_dir_path.encode(),  # bytes required\n                    \"UPLOAD_KEEP_FILENAME\": True,\n                    \"UPLOAD_KEEP_EXTENSIONS\": True,\n                    \"MAX_MEMORY_FILE_SIZE\": 0,\n                },\n            )\n\n            while True:\n                chunk = request.stream.read(8192)\n                if not chunk:\n                    break\n                parser.write(chunk)\n            parser.finalize()\n            parser.close()\n\n            if file_object is not None and not file_object.closed:\n                file_object.close()\n        else:\n            data = request.json\n\n        if mindsdb_file_name in existing_file_names:\n            return http_error(\n                400,\n                \"File already exists\",\n                f\"File with name '{data['file']}' already exists\",\n            )\n\n        if data.get(\"source_type\") == \"url\":\n            url = data[\"source\"]\n            data[\"file\"] = data[\"name\"]\n\n            config = Config()\n            is_cloud = config.get(\"cloud\", False)\n            if is_cloud is True and ctx.user_class != 1:\n                info = requests.head(url)\n                file_size = info.headers.get(\"Content-Length\")\n                try:\n                    file_size = int(file_size)\n                except Exception:\n                    pass\n\n                if file_size is None:\n                    return http_error(\n                        400,\n                        \"Error getting file info\",\n                        \"\u0421an't determine remote file size\",\n                    )\n                if file_size > 1024 * 1024 * 100:\n                    return http_error(\n                        400, \"File is too big\", \"Upload limit for file is 100Mb\"\n                    )\n            with requests.get(url, stream=True) as r:\n                if r.status_code != 200:\n                    return http_error(\n                        400, \"Error getting file\", f\"Got status code: {r.status_code}\"\n                    )\n                file_path = os.path.join(temp_dir_path, data[\"file\"])\n                with open(file_path, \"wb\") as f:\n                    for chunk in r.iter_content(chunk_size=8192):\n                        f.write(chunk)\n\n        original_file_name = data.get(\"original_file_name\")\n\n        file_path = os.path.join(temp_dir_path, data[\"file\"])\n        lp = file_path.lower()\n        if lp.endswith((\".zip\", \".tar.gz\")):\n            if lp.endswith(\".zip\"):\n                with zipfile.ZipFile(file_path) as f:\n                    f.extractall(temp_dir_path)\n            elif lp.endswith(\".tar.gz\"):\n                with tarfile.open(file_path) as f:\n                    safe_extract(f, temp_dir_path)\n            os.remove(file_path)\n            files = os.listdir(temp_dir_path)\n            if len(files) != 1:\n                os.rmdir(temp_dir_path)\n                return http_error(\n                    400, \"Wrong content.\", \"Archive must contain only one data file.\"\n                )\n            file_path = os.path.join(temp_dir_path, files[0])\n            mindsdb_file_name = files[0]\n            if not os.path.isfile(file_path):\n                os.rmdir(temp_dir_path)\n                return http_error(\n                    400, \"Wrong content.\", \"Archive must contain data file in root.\"\n                )\n\n        ca.file_controller.save_file(\n            mindsdb_file_name, file_path, file_name=original_file_name\n        )\n\n        os.rmdir(temp_dir_path)\n\n        return \"\", 200\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def handle_html_static(self, request):\n        \"\"\"Handle static html.\"\"\"\n        if not (page := request.path[1:]):\n            page = \"index.html\"\n        file = os.path.join(self.web_path, page)\n        try:\n            with open(file, encoding=\"utf-8\"):\n                return web.FileResponse(file)\n        except (FileNotFoundError, IsADirectoryError) as exc:\n            raise web.HTTPNotFound(reason=\"File not found\") from exc\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_services(self, context):\n        service_list = self.catalog_api.list_services(context)\n        service_refs = [self.catalog_api.get_service(context, x)\n                        for x in service_list]\n        return {'OS-KSADM:services': service_refs}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _handle_carbon_received(self, msg):\n        self.xmpp.event('carbon_received', msg)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def render_markdown(value):\n    \"\"\"\n    Render text as Markdown\n\n    Example:\n        {{ text | render_markdown }}\n    \"\"\"\n    # Strip HTML tags\n    value = strip_tags(value)\n\n    # Sanitize Markdown links\n    schemes = \"|\".join(settings.ALLOWED_URL_SCHEMES)\n    pattern = rf\"\\[(.+)\\]\\((?!({schemes})).*:(.+)\\)\"\n    value = re.sub(pattern, \"[\\\\1](\\\\3)\", value, flags=re.IGNORECASE)\n\n    # Render Markdown\n    html = markdown(value, extensions=[\"fenced_code\", \"tables\"])\n\n    return mark_safe(html)  # noqa: S308\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static cache_accel_t *read_cache_accel(RBuffer *cache_buf, cache_hdr_t *hdr, cache_map_t *maps) {\n\tif (!cache_buf || !hdr || !hdr->accelerateInfoSize || !hdr->accelerateInfoAddr) {\n\t\treturn NULL;\n\t}\n\n\tut64 offset = va2pa (hdr->accelerateInfoAddr, hdr->mappingCount, maps, cache_buf, 0, NULL, NULL);\n\tif (!offset) {\n\t\treturn NULL;\n\t}\n\n\tut64 size = sizeof (cache_accel_t);\n\tcache_accel_t *accel = R_NEW0 (cache_accel_t);\n\tif (!accel) {\n\t\treturn NULL;\n\t}\n\n\tif (r_buf_fread_at (cache_buf, offset, (ut8*) accel, \"16il\", 1) != size) {\n\t\tR_FREE (accel);\n\t\treturn NULL;\n\t}\n\n\taccel->imagesExtrasOffset += offset;\n\taccel->bottomUpListOffset += offset;\n\taccel->dylibTrieOffset += offset;\n\taccel->initializersOffset += offset;\n\taccel->dofSectionsOffset += offset;\n\taccel->reExportListOffset += offset;\n\taccel->depListOffset += offset;\n\taccel->rangeTableOffset += offset;\n\n\treturn accel;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        dir_name, file_name = os.path.split(name)\n        if '..' in pathlib.PurePath(dir_name).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n        validate_file_name(file_name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, add an underscore and a random 7\n        # character alphanumeric string (before the file extension, if one\n        # exists) to the filename until the generated filename doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n        return name\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def settings(request):\n   \"\"\"\n   This page presents a list of all the settings to the user.  They are not editable.\n   \"\"\"\n   if not test_user_authenticated(request): return login(request, next=\"/cobbler_web/settings\")\n   settings = remote.get_settings()\n   skeys = settings.keys()\n   skeys.sort()\n\n   results = []\n   for k in skeys:\n      results.append([k,settings[k]])\n\n   t = get_template('settings.tmpl')\n   html = t.render(Context({\n        'settings' : results,\n        'version'  : remote.version(request.session['token']),\n        'username' : username,\n   }))\n   return HttpResponse(html)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _login(server, creds_file=None, api_key=None, silent=False):\n    \"\"\"Login main function.\"\"\"\n    # fetch_api_key and check_version expect server to end in /\n    if server[-1] != \"/\":\n        server = server + \"/\"\n\n    # creds file path setup\n    if creds_file is None:\n        creds_file = os.path.expanduser(\"~/.onecodex\")\n\n    # check if the creds file exists and is readable\n    if not os.path.exists(creds_file):\n        if silent:\n            return None\n\n        creds = {}\n    elif not os.access(creds_file, os.R_OK):\n        click.echo(\"Please check the permissions on {}\".format(collapse_user(creds_file)), err=True)\n        sys.exit(1)\n    else:\n        # it is, so let's read it!\n        with open(creds_file, \"r\") as fp:\n            try:\n                creds = json.load(fp)\n            except ValueError:\n                click.echo(\n                    \"Your ~/.onecodex credentials file appears to be corrupted. \"  # noqa\n                    \"Please delete it and re-authorize.\",\n                    err=True,\n                )\n                sys.exit(1)\n\n        # check for updates if logged in more than one day ago\n        last_update = creds.get(\"updated_at\") or creds.get(\"saved_at\")\n        last_update = last_update if last_update else datetime.datetime.now().strftime(DATE_FORMAT)\n        diff = datetime.datetime.now() - datetime.datetime.strptime(last_update, DATE_FORMAT)\n\n        if diff.days >= 1:\n            # if creds_file is old, check for updates\n            upgrade_required, msg = check_version(__version__, server)\n            creds[\"updated_at\"] = datetime.datetime.now().strftime(DATE_FORMAT)\n\n            try:\n                json.dump(creds, open(creds_file, \"w\"))\n            except Exception as e:\n                if e.errno == errno.EACCES:\n                    click.echo(\n                        \"Please check the permissions on {}\".format(collapse_user(creds_file)),\n                        err=True,\n                    )\n                    sys.exit(1)\n                else:\n                    raise\n\n            if upgrade_required:\n                click.echo(\"\\nWARNING: {}\\n\".format(msg), err=True)\n\n        # finally, give the user back what they want (whether silent or not)\n        if silent:\n            return creds.get(\"api_key\", None)\n\n        click.echo(\n            \"Credentials file already exists ({}). Logout first.\".format(collapse_user(creds_file)),\n            err=True,\n        )\n        return creds.get(\"email\", None)\n\n    # creds_file was not found and we're not silent, so prompt user to login\n    email, api_key = login_uname_pwd(server, api_key=api_key)\n\n    if api_key is None:\n        click.echo(\n            \"We could not verify your credentials. Please check your email and password again. \"\n            \"If you have single sign-on (SSO) enabled, you must login with `onecodex --api-key $API_KEY login`. \"\n            \"Please contact help@onecodex.com if you continue to experience problems.\"\n        )\n        sys.exit(1)\n\n    creds.update(\n        {\n            \"api_key\": api_key,\n            \"saved_at\": datetime.datetime.now().strftime(DATE_FORMAT),\n            \"updated_at\": None,\n            \"email\": email,\n        }\n    )\n\n    try:\n        json.dump(creds, open(creds_file, \"w\"))\n    except Exception as e:\n        if e.errno == errno.EACCES:\n            click.echo(\"Please check the permissions on {}\".format(creds_file), err=True)\n            sys.exit(1)\n        else:\n            raise\n\n    click.echo(\"Your ~/.onecodex credentials file was successfully created.\", err=True)\n\n    return email\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, verbose_name=None, name=None, verify_exists=True, **kwargs):\n        kwargs['max_length'] = kwargs.get('max_length', 200)\n        CharField.__init__(self, verbose_name, name, **kwargs)\n        self.validators.append(validators.URLValidator(verify_exists=verify_exists))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def from_unix_seconds(seconds: Union[int, float]) -> datetime:\n    while abs(seconds) > MS_WATERSHED:\n        seconds /= 1000\n    dt = EPOCH + timedelta(seconds=seconds)\n    return dt.replace(tzinfo=timezone.utc)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def do_template(self, data, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, disable_lookups=False):\n        if USE_JINJA2_NATIVE and not isinstance(data, string_types):\n            return data\n\n        # For preserving the number of input newlines in the output (used\n        # later in this method)\n        data_newlines = _count_newlines_from_end(data)\n\n        if fail_on_undefined is None:\n            fail_on_undefined = self._fail_on_undefined_errors\n\n        try:\n            # allows template header overrides to change jinja2 options.\n            if overrides is None:\n                myenv = self.environment.overlay()\n            else:\n                myenv = self.environment.overlay(overrides)\n\n            # Get jinja env overrides from template\n            if hasattr(data, 'startswith') and data.startswith(JINJA2_OVERRIDE):\n                eol = data.find('\\n')\n                line = data[len(JINJA2_OVERRIDE):eol]\n                data = data[eol + 1:]\n                for pair in line.split(','):\n                    (key, val) = pair.split(':')\n                    key = key.strip()\n                    setattr(myenv, key, ast.literal_eval(val.strip()))\n\n            # Adds Ansible custom filters and tests\n            myenv.filters.update(self._get_filters())\n            for k in myenv.filters:\n                if not getattr(myenv.filters[k], '__UNROLLED__', False):\n                    myenv.filters[k] = _unroll_iterator(myenv.filters[k])\n            myenv.tests.update(self._get_tests())\n\n            if escape_backslashes:\n                # Allow users to specify backslashes in playbooks as \"\\\\\" instead of as \"\\\\\\\\\".\n                data = _escape_backslashes(data, myenv)\n\n            try:\n                t = myenv.from_string(data)\n            except TemplateSyntaxError as e:\n                raise AnsibleError(\"template error while templating string: %s. String: %s\" % (to_native(e), to_native(data)))\n            except Exception as e:\n                if 'recursion' in to_native(e):\n                    raise AnsibleError(\"recursive loop detected in template string: %s\" % to_native(data))\n                else:\n                    return data\n\n            if disable_lookups:\n                t.globals['query'] = t.globals['q'] = t.globals['lookup'] = self._fail_lookup\n\n            jvars = AnsibleJ2Vars(self, t.globals)\n\n            self.cur_context = new_context = t.new_context(jvars, shared=True)\n            rf = t.root_render_func(new_context)\n\n            try:\n                res = j2_concat(rf)\n                if getattr(new_context, 'unsafe', False):\n                    res = wrap_var(res)\n            except TypeError as te:\n                if 'AnsibleUndefined' in to_native(te):\n                    errmsg = \"Unable to look up a name or access an attribute in template string (%s).\\n\" % to_native(data)\n                    errmsg += \"Make sure your variable name does not contain invalid characters like '-': %s\" % to_native(te)\n                    raise AnsibleUndefinedVariable(errmsg)\n                else:\n                    display.debug(\"failing because of a type error, template data is: %s\" % to_text(data))\n                    raise AnsibleError(\"Unexpected templating type error occurred on (%s): %s\" % (to_native(data), to_native(te)))\n\n            if USE_JINJA2_NATIVE and not isinstance(res, string_types):\n                return res\n\n            if preserve_trailing_newlines:\n                # The low level calls above do not preserve the newline\n                # characters at the end of the input data, so we use the\n                # calculate the difference in newlines and append them\n                # to the resulting output for parity\n                #\n                # jinja2 added a keep_trailing_newline option in 2.7 when\n                # creating an Environment.  That would let us make this code\n                # better (remove a single newline if\n                # preserve_trailing_newlines is False).  Once we can depend on\n                # that version being present, modify our code to set that when\n                # initializing self.environment and remove a single trailing\n                # newline here if preserve_newlines is False.\n                res_newlines = _count_newlines_from_end(res)\n                if data_newlines > res_newlines:\n                    res += self.environment.newline_sequence * (data_newlines - res_newlines)\n            return res\n        except (UndefinedError, AnsibleUndefinedVariable) as e:\n            if fail_on_undefined:\n                raise AnsibleUndefinedVariable(e)\n            else:\n                display.debug(\"Ignoring undefined failure: %s\" % to_text(e))\n                return data\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_python_instance_state(self, instance, state):\n        if hasattr(instance, '__setstate__'):\n            instance.__setstate__(state)\n        else:\n            slotstate = {}\n            if isinstance(state, tuple) and len(state) == 2:\n                state, slotstate = state\n            if hasattr(instance, '__dict__'):\n                instance.__dict__.update(state)\n            elif state:\n                slotstate.update(state)\n            for key, value in slotstate.items():\n                setattr(instance, key, value)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ModelWeights::Initialize(OpKernelContext* const context) {\n  OpInputList sparse_indices_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"sparse_indices\", &sparse_indices_inputs));\n  OpInputList sparse_weights_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"sparse_weights\", &sparse_weights_inputs));\n  OpInputList dense_weights_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"dense_weights\", &dense_weights_inputs));\n\n  OpOutputList sparse_weights_outputs;\n  TF_RETURN_IF_ERROR(context->output_list(\"out_delta_sparse_weights\",\n                                          &sparse_weights_outputs));\n\n  OpOutputList dense_weights_outputs;\n  TF_RETURN_IF_ERROR(\n      context->output_list(\"out_delta_dense_weights\", &dense_weights_outputs));\n\n  for (int i = 0; i < sparse_weights_inputs.size(); ++i) {\n    Tensor* delta_t;\n    TF_RETURN_IF_ERROR(sparse_weights_outputs.allocate(\n        i, sparse_weights_inputs[i].shape(), &delta_t));\n    // Convert the input vector to a row matrix in internal representation.\n    auto deltas = delta_t->shaped<float, 2>({1, delta_t->NumElements()});\n    deltas.setZero();\n    sparse_weights_.emplace_back(FeatureWeightsSparseStorage{\n        sparse_indices_inputs[i].flat<int64>(),\n        sparse_weights_inputs[i].shaped<float, 2>(\n            {1, sparse_weights_inputs[i].NumElements()}),\n        deltas});\n  }\n\n  // Reads in the weights, and allocates and initializes the delta weights.\n  const auto initialize_weights =\n      [&](const OpInputList& weight_inputs, OpOutputList* const weight_outputs,\n          std::vector<FeatureWeightsDenseStorage>* const feature_weights) {\n        for (int i = 0; i < weight_inputs.size(); ++i) {\n          Tensor* delta_t;\n          TF_RETURN_IF_ERROR(\n              weight_outputs->allocate(i, weight_inputs[i].shape(), &delta_t));\n          // Convert the input vector to a row matrix in internal\n          // representation.\n          auto deltas = delta_t->shaped<float, 2>({1, delta_t->NumElements()});\n          deltas.setZero();\n          feature_weights->emplace_back(FeatureWeightsDenseStorage{\n              weight_inputs[i].shaped<float, 2>(\n                  {1, weight_inputs[i].NumElements()}),\n              deltas});\n        }\n        return Status::OK();\n      };\n\n  return initialize_weights(dense_weights_inputs, &dense_weights_outputs,\n                            &dense_weights_);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def task_status(request, task_id):\n    \"\"\"\n    get task status info\n    :param request: request object\n    :param task_id: task id\n    :return:\n    \"\"\"\n    if request.method == 'GET':\n        result = []\n        task = Task.objects.get(id=task_id)\n        clients = clients_of_task(task)\n        for client in clients:\n            job_id = get_job_id(client, task)\n            jobs = DjangoJob.objects.filter(id=job_id)\n            logger.debug('jobs from djangojob %s', jobs)\n            # if job does not exist, for date mode exceed time\n            if not jobs:\n                continue\n            job = DjangoJob.objects.get(id=job_id)\n            executions = serialize(\n                'json', DjangoJobExecution.objects.filter(job=job))\n            result.append({\n                'client': model_to_dict(client),\n                'next': job.next_run_time,\n                'executions': json.loads(executions)\n            })\n        return JsonResponse({'data': result})\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _delete_user(content):\n    if ub.session.query(ub.User).filter(ub.User.role.op('&')(constants.ROLE_ADMIN) == constants.ROLE_ADMIN,\n                                        ub.User.id != content.id).count():\n        if content.name != \"Guest\":\n            # Delete all books in shelfs belonging to user, all shelfs of user, downloadstat of user, read status\n            # and user itself\n            ub.session.query(ub.ReadBook).filter(content.id == ub.ReadBook.user_id).delete()\n            ub.session.query(ub.Downloads).filter(content.id == ub.Downloads.user_id).delete()\n            for us in ub.session.query(ub.Shelf).filter(content.id == ub.Shelf.user_id):\n                ub.session.query(ub.BookShelf).filter(us.id == ub.BookShelf.shelf).delete()\n            ub.session.query(ub.Shelf).filter(content.id == ub.Shelf.user_id).delete()\n            ub.session.query(ub.Bookmark).filter(content.id == ub.Bookmark.user_id).delete()\n            ub.session.query(ub.User).filter(ub.User.id == content.id).delete()\n            ub.session.query(ub.ArchivedBook).filter(ub.ArchivedBook.user_id == content.id).delete()\n            ub.session.query(ub.RemoteAuthToken).filter(ub.RemoteAuthToken.user_id == content.id).delete()\n            ub.session.query(ub.User_Sessions).filter(ub.User_Sessions.user_id == content.id).delete()\n            ub.session.query(ub.KoboSyncedBooks).filter(ub.KoboSyncedBooks.user_id == content.id).delete()\n            # delete KoboReadingState and all it's children\n            kobo_entries = ub.session.query(ub.KoboReadingState).filter(ub.KoboReadingState.user_id == content.id).all()\n            for kobo_entry in kobo_entries:\n                ub.session.delete(kobo_entry)\n            ub.session_commit()\n            log.info(\"User {} deleted\".format(content.name))\n            return(_(\"User '%(nick)s' deleted\", nick=content.name))\n        else:\n            log.warning(_(\"Can't delete Guest User\"))\n            raise Exception(_(\"Can't delete Guest User\"))\n    else:\n        log.warning(\"No admin user remaining, can't delete user\")\n        raise Exception(_(\"No admin user remaining, can't delete user\"))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self) -> None:\n        \"\"\"\n        Run the REPL loop.\n        \"\"\"\n        if self.terminal_title:\n            set_title(self.terminal_title)\n\n        self._add_to_namespace()\n\n        try:\n            while True:\n                try:\n                    # Read.\n                    try:\n                        text = self.read()\n                    except EOFError:\n                        return\n\n                    # Eval.\n                    try:\n                        result = self.eval(text)\n                    except KeyboardInterrupt:  # KeyboardInterrupt doesn't inherit from Exception.\n                        raise\n                    except SystemExit:\n                        return\n                    except BaseException as e:\n                        self._handle_exception(e)\n                    else:\n                        # Print.\n                        if result is not None:\n                            self.show_result(result)\n\n                        # Loop.\n                        self.current_statement_index += 1\n                        self.signatures = []\n\n                except KeyboardInterrupt as e:\n                    # Handle all possible `KeyboardInterrupt` errors. This can\n                    # happen during the `eval`, but also during the\n                    # `show_result` if something takes too long.\n                    # (Try/catch is around the whole block, because we want to\n                    # prevent that a Control-C keypress terminates the REPL in\n                    # any case.)\n                    self._handle_keyboard_interrupt(e)\n        finally:\n            if self.terminal_title:\n                clear_title()\n            self._remove_from_namespace()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def authenticate_connection(self, username, password, command='login'):\n        success, result = OAuth2Helper.get_oauth2_credentials(username, password)\n        if success:\n            # send authentication command to server (response checked in ServerConnection)\n            # note: we only support single-trip authentication (SASL) without checking server capabilities - improve?\n            super().process_data(b'%s AUTHENTICATE XOAUTH2 ' % self.authentication_tag.encode('utf-8'))\n            super().process_data(OAuth2Helper.encode_oauth2_string(result), censor_server_log=True)\n            super().process_data(b'\\r\\n')\n\n            # because get_oauth2_credentials blocks, the server could have disconnected, and may no-longer exist\n            if self.server_connection:\n                self.server_connection.authenticated_username = username\n\n        else:\n            error_message = '%s NO %s %s\\r\\n' % (self.authentication_tag, command.upper(), result)\n            self.send(error_message.encode('utf-8'))\n            self.send(b'* BYE Autologout; authentication failed\\r\\n')\n            self.close()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def check_alias(self, alias):\n        if FORBIDDEN_ALIAS_PATTERN.search(alias):\n            raise ValueError(\n                \"Column aliases cannot contain whitespace characters, quotation marks, \"\n                \"semicolons, or SQL comments.\"\n            )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def initiate_flow(self, redirect_uri: str) -> dict:\n        flow = self._client.initiate_auth_code_flow(\n            scopes=[],  # confusingly, scopes=[] is the only way to get the openid, profile, and\n                        # offline_access scopes\n                        # https://github.com/AzureAD/microsoft-authentication-library-for-python/blob/dev/msal/application.py#L568-L580\n            redirect_uri=redirect_uri\n        )\n        return {\n            'flow': flow,\n            'authorization_url': flow['auth_uri'],\n            'state': flow['state'],\n        }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_header_value(self, data: bytes, start: int, end: int) -> None:\n        self._current_partial_header_value += data[start:end]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create_table(\n        self,\n        name: str,\n        obj: pd.DataFrame | pa.Table | ir.Table | None = None,\n        *,\n        schema: sch.Schema | None = None,\n        database: str | None = None,\n        catalog: str | None = None,\n        tbl_properties: dict | None = None,\n        watermark: Watermark | None = None,\n        temp: bool = False,\n        overwrite: bool = False,\n    ) -> ir.Table:\n        \"\"\"Create a new table in Flink.\n\n        Note that: in Flink, tables can be either virtual (VIEWS) or regular\n        (TABLES).\n\n        VIEWS can be created from an existing Table object, usually the result\n        of a Table API or SQL query. TABLES describe external data, such as a\n        file, database table, or message queue. In other words, TABLES refer\n        explicitly to tables constructed directly from source/sink connectors.\n\n        Parameters\n        ----------\n        name\n            Name of the new table.\n        obj\n            An Ibis table expression or pandas table that will be used to\n            extract the schema and the data of the new table. If not provided,\n            `schema` must be given.\n        schema\n            The schema for the new table. Only one of `schema` or `obj` can be\n            provided.\n        database\n            Name of the database where the table will be created, if not the\n            default.\n        catalog\n            Name of the catalog where the table will be created, if not the\n            default.\n        tbl_properties\n            Table properties used to create a table source/sink. The properties\n            are usually used to find and create the underlying connector. Accepts\n            dictionary of key-value pairs (key1=val1, key2=val2, ...).\n        watermark\n            Watermark strategy for the table, only applicable on sources.\n        temp\n            Whether a table is temporary or not.\n        overwrite\n            Whether to clobber existing data.\n\n        Returns\n        -------\n        Table\n            The table that was created.\n        \"\"\"\n        import pandas as pd\n        import pyarrow as pa\n        import pyarrow_hotfix  # noqa: F401\n\n        import ibis.expr.types as ir\n\n        if obj is None and schema is None:\n            raise exc.IbisError(\"The schema or obj parameter is required\")\n\n        if overwrite:\n            self.drop_table(name=name, catalog=catalog, database=database, force=True)\n\n        if isinstance(obj, pa.Table):\n            obj = obj.to_pandas()\n        if isinstance(obj, pd.DataFrame):\n            qualified_name = self._fully_qualified_name(name, database, catalog)\n            table = self._table_env.from_pandas(obj)\n            # in-memory data is created as views in `pyflink`\n            # TODO(chloeh13q): alternatively, we can do CREATE TABLE and then INSERT\n            # INTO ... VALUES to keep things consistent\n            self._table_env.create_temporary_view(qualified_name, table)\n        if isinstance(obj, ir.Table):\n            # TODO(chloeh13q): implement CREATE TABLE for expressions\n            raise NotImplementedError\n        if schema is not None:\n            if not tbl_properties:\n                raise exc.IbisError(\n                    \"tbl_properties is required when creating table with schema\"\n                )\n            statement = CreateTableFromConnector(\n                table_name=name,\n                schema=schema,\n                tbl_properties=tbl_properties,\n                watermark=watermark,\n                temp=temp,\n                database=database,\n                catalog=catalog,\n            )\n            self._exec_sql(statement.compile())\n\n        return self.table(name, database=database)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def check_alias(self, alias):\n        if FORBIDDEN_ALIAS_PATTERN.search(alias):\n            raise ValueError(\n                \"Column aliases cannot contain whitespace characters, quotation marks, \"\n                \"semicolons, or SQL comments.\"\n            )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _get_values(self, lst, list_columns):\n        \"\"\"\n        Get Values: formats values for list template.\n        returns [{'col_name':'col_value',....},{'col_name':'col_value',....}]\n\n        :param lst:\n            The list of item objects from query\n        :param list_columns:\n            The list of columns to include\n        \"\"\"\n        retlst = []\n        for item in lst:\n            retdict = {}\n            for col in list_columns:\n                retdict[col] = self._get_attr_value(item, col)\n            retlst.append(retdict)\n        return retlst\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get(self, pk, filter=None):\n        \"\"\"\n        return the record from key, you can optionally pass filters\n        if pk exits on the db but filters exclude it it will return none.\n        \"\"\"\n        pass\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_argument(self, *args, **kwargs):\n        value = super(BaseHandler, self).get_argument(*args, **kwargs)\n\n        try:\n            return bleach.clean(value)\n        except TypeError:\n            return value\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def auth_user_registration(self) -> bool:\n        return self.appbuilder.get_app.config[\"AUTH_USER_REGISTRATION\"]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "            def enqueue_on_commit():\n                task_wrapper.huey.enqueue(task)\n            transaction.on_commit(enqueue_on_commit)"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_environ(self):\n        # Strip all headers with underscores in the name before constructing\n        # the WSGI environ. This prevents header-spoofing based on ambiguity\n        # between underscores and dashes both normalized to underscores in WSGI\n        # env vars. Nginx and Apache 2.4+ both do this as well.\n        for k, v in self.headers.items():\n            if '_' in k:\n                del self.headers[k]\n\n        return super(WSGIRequestHandler, self).get_environ()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, **kwargs):\n        \"\"\"Configure default initial values for this field\"\"\"\n        kwargs['max_length'] = self.NOTES_MAX_LENGTH\n        kwargs['verbose_name'] = _('Notes')\n        kwargs['blank'] = True\n        kwargs['null'] = True\n\n        super().__init__(**kwargs)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _convert_object(cls, obj: Any) -> Any:\n        if isinstance(obj, ir.Table):\n            # Support memtables\n            assert isinstance(obj.op(), ops.InMemoryTable)\n            return obj.op().data.to_frame()\n        elif isinstance(obj, pa.Table):\n            return obj.to_pandas()\n        return cls.backend_table_type(obj)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def authenticate(self, username, password):\n        \"\"\"\n        Only verify the user's credentials using the database store.\n        \"\"\"\n        user = UserObject.query.filter_by(username=username).first()\n        if user and user.validate_password(password):\n            return username, {}\n        return False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_inner_cte_expression(self, tokens: TokenList) -> Optional[TokenList]:\n        for token in tokens:\n            if self._is_identifier(token):\n                for identifier_token in token.tokens:\n                    if (\n                        isinstance(identifier_token, Parenthesis)\n                        and identifier_token.is_group\n                    ):\n                        return identifier_token.tokens\n        return None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def getLinkContent(self, item):\n        title = get_value(item, 'Title')\n        return escape(safe_unicode(title))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_list_from_env(name, default=None):\n    \"\"\"Get list from environment variable.\n\n    Supports splitting on comma or white space.\n    Use the default as fallback only when the variable is not set.\n    So if the env variable is set to an empty string, this will ignore the\n    default and return an empty list.\n    \"\"\"\n    value = os.environ.get(name)\n    if value is None:\n        return default or []\n    value = value.strip()\n    if \",\" in value:\n        return value.split(\",\")\n    return value.split()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def job_cancel(request, client_id, project_name, job_id):\n    \"\"\"\n    cancel a job\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :param job_id: job id\n    :return: json of cancel\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        scrapyd = get_scrapyd(client)\n        result = scrapyd.cancel(project_name, job_id)\n        return JsonResponse(result)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def export_recording(recording_id):\n    \"\"\"Export a recording by creating a new database, importing the recording, and then restoring the previous state.\n\n    Args:\n        recording_id (int): The ID of the recording to export.\n\n    Returns:\n        str: The file path of the new database.\n    \"\"\"\n    sql, values = export_sql(recording_id)\n    t, db_file_path = create_db(recording_id, sql, values)\n    restore_db(t)\n    return db_file_path\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def import_obj(cls, dashboard_to_import, import_time=None):\n        \"\"\"Imports the dashboard from the object to the database.\n\n         Once dashboard is imported, json_metadata field is extended and stores\n         remote_id and import_time. It helps to decide if the dashboard has to\n         be overridden or just copies over. Slices that belong to this\n         dashboard will be wired to existing tables. This function can be used\n         to import/export dashboards between multiple superset instances.\n         Audit metadata isn't copied over.\n        \"\"\"\n        def alter_positions(dashboard, old_to_new_slc_id_dict):\n            \"\"\" Updates slice_ids in the position json.\n\n            Sample position json:\n            [{\n                \"col\": 5,\n                \"row\": 10,\n                \"size_x\": 4,\n                \"size_y\": 2,\n                \"slice_id\": \"3610\"\n            }]\n            \"\"\"\n            position_array = dashboard.position_array\n            for position in position_array:\n                if 'slice_id' not in position:\n                    continue\n                old_slice_id = int(position['slice_id'])\n                if old_slice_id in old_to_new_slc_id_dict:\n                    position['slice_id'] = '{}'.format(\n                        old_to_new_slc_id_dict[old_slice_id])\n            dashboard.position_json = json.dumps(position_array)\n\n        logging.info('Started import of the dashboard: {}'\n                     .format(dashboard_to_import.to_json()))\n        session = db.session\n        logging.info('Dashboard has {} slices'\n                     .format(len(dashboard_to_import.slices)))\n        # copy slices object as Slice.import_slice will mutate the slice\n        # and will remove the existing dashboard - slice association\n        slices = copy(dashboard_to_import.slices)\n        old_to_new_slc_id_dict = {}\n        new_filter_immune_slices = []\n        new_timed_refresh_immune_slices = []\n        new_expanded_slices = {}\n        i_params_dict = dashboard_to_import.params_dict\n        remote_id_slice_map = {\n            slc.params_dict['remote_id']: slc\n            for slc in session.query(Slice).all()\n            if 'remote_id' in slc.params_dict\n        }\n        for slc in slices:\n            logging.info('Importing slice {} from the dashboard: {}'.format(\n                slc.to_json(), dashboard_to_import.dashboard_title))\n            remote_slc = remote_id_slice_map.get(slc.id)\n            new_slc_id = Slice.import_obj(slc, remote_slc, import_time=import_time)\n            old_to_new_slc_id_dict[slc.id] = new_slc_id\n            # update json metadata that deals with slice ids\n            new_slc_id_str = '{}'.format(new_slc_id)\n            old_slc_id_str = '{}'.format(slc.id)\n            if ('filter_immune_slices' in i_params_dict and\n                    old_slc_id_str in i_params_dict['filter_immune_slices']):\n                new_filter_immune_slices.append(new_slc_id_str)\n            if ('timed_refresh_immune_slices' in i_params_dict and\n                    old_slc_id_str in\n                    i_params_dict['timed_refresh_immune_slices']):\n                new_timed_refresh_immune_slices.append(new_slc_id_str)\n            if ('expanded_slices' in i_params_dict and\n                    old_slc_id_str in i_params_dict['expanded_slices']):\n                new_expanded_slices[new_slc_id_str] = (\n                    i_params_dict['expanded_slices'][old_slc_id_str])\n\n        # override the dashboard\n        existing_dashboard = None\n        for dash in session.query(Dashboard).all():\n            if ('remote_id' in dash.params_dict and\n                    dash.params_dict['remote_id'] ==\n                    dashboard_to_import.id):\n                existing_dashboard = dash\n\n        dashboard_to_import.id = None\n        alter_positions(dashboard_to_import, old_to_new_slc_id_dict)\n        dashboard_to_import.alter_params(import_time=import_time)\n        if new_expanded_slices:\n            dashboard_to_import.alter_params(\n                expanded_slices=new_expanded_slices)\n        if new_filter_immune_slices:\n            dashboard_to_import.alter_params(\n                filter_immune_slices=new_filter_immune_slices)\n        if new_timed_refresh_immune_slices:\n            dashboard_to_import.alter_params(\n                timed_refresh_immune_slices=new_timed_refresh_immune_slices)\n\n        new_slices = session.query(Slice).filter(\n            Slice.id.in_(old_to_new_slc_id_dict.values())).all()\n\n        if existing_dashboard:\n            existing_dashboard.override(dashboard_to_import)\n            existing_dashboard.slices = new_slices\n            session.flush()\n            return existing_dashboard.id\n        else:\n            # session.add(dashboard_to_import) causes sqlachemy failures\n            # related to the attached users / slices. Creating new object\n            # allows to avoid conflicts in the sql alchemy state.\n            copied_dash = dashboard_to_import.copy()\n            copied_dash.slices = new_slices\n            session.add(copied_dash)\n            session.flush()\n            return copied_dash.id\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "export const useJobDetail = () => {\n  const params = useParams() as { id: string };\n  const [msg, setMsg] = useState(\"Loading the job detail\");\n  const [refreshing, setRefresh] = useState(true);\n  const { data: job, isLoading } = useSWR(\n    [\"useJobDetail\", params.id],\n    async ([_, jobId]) => {\n      try {\n        const rsp = await getJobDetail(jobId);\n        return rsp.data;\n      } catch (e) {\n        setMsg(\"Job Query Error Please Check JobId\");\n        setRefresh(false);\n      }\n    },\n    { refreshInterval: refreshing ? API_REFRESH_INTERVAL_MS : 0 },\n  );\n\n  return {\n    job,\n    isLoading,\n    msg,\n    params,\n  };\n};\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_absolute_url(self):\n        return reverse('download', kwargs={\"download_id\": self.pk})\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def flash(message, level='info'):\r\n    \"\"\"\r\n    Add a flashin message to the session.\r\n    \"\"\"\r\n    assert message\r\n    assert level in ['info', 'error', 'warning', 'success']\r\n    if 'flash' not in cherrypy.session:  # @UndefinedVariable\r\n        cherrypy.session['flash'] = []  # @UndefinedVariable\r\n    flash_message = FlashMessage(str(message), level)\r\n    cherrypy.session['flash'].append(flash_message)\r\n\r"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": ") => [number, number, TooltipContent] | undefined;\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, *, win_id, mode_manager, private, parent=None):\n        self.private = private\n        self.win_id = win_id\n        self.tab_id = next(tab_id_gen)\n        super().__init__(parent)\n\n        self.registry = objreg.ObjectRegistry()\n        tab_registry = objreg.get('tab-registry', scope='window',\n                                  window=win_id)\n        tab_registry[self.tab_id] = self\n        objreg.register('tab', self, registry=self.registry)\n\n        self.data = TabData()\n        self._layout = miscwidgets.WrapperLayout(self)\n        self._widget = None\n        self._progress = 0\n        self._mode_manager = mode_manager\n        self._load_status = usertypes.LoadStatus.none\n        self._mouse_event_filter = mouse.MouseEventFilter(\n            self, parent=self)\n        self.backend = None\n\n        # FIXME:qtwebengine  Should this be public api via self.hints?\n        #                    Also, should we get it out of objreg?\n        hintmanager = hints.HintManager(win_id, self.tab_id, parent=self)\n        objreg.register('hintmanager', hintmanager, scope='tab',\n                        window=self.win_id, tab=self.tab_id)\n\n        self.predicted_navigation.connect(self._on_predicted_navigation)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def setup_class(self):\n        self.conf = config_factory(\"sp\", dotname(\"server_conf\"))\n        self.ar = authn_response(self.conf, \"http://lingon.catalogix.se:8087/\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def datetime_trunc_sql(self, lookup_type, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        # https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/ROUND-and-TRUNC-Date-Functions.html\n        trunc_param = None\n        if lookup_type in (\"year\", \"month\"):\n            trunc_param = lookup_type.upper()\n        elif lookup_type == \"quarter\":\n            trunc_param = \"Q\"\n        elif lookup_type == \"week\":\n            trunc_param = \"IW\"\n        elif lookup_type == \"hour\":\n            trunc_param = \"HH24\"\n        elif lookup_type == \"minute\":\n            trunc_param = \"MI\"\n        elif lookup_type == \"day\":\n            return f\"TRUNC({sql})\", params\n        else:\n            # Cast to DATE removes sub-second precision.\n            return f\"CAST({sql} AS DATE)\", params\n        return f\"TRUNC({sql}, %s)\", (*params, trunc_param)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _should_force_download(self):\n        # If this returns True, the caller should set a\n        # Content-Disposition header with filename.\n        mimetype = self.content_type\n        if not mimetype:\n            return False\n        if self.use_denylist:\n            # We explicitly deny a few mimetypes, and allow the rest.\n            return mimetype in self.disallowed_inline_mimetypes\n        # Use the allowlist.\n        # We only explicitly allow a few mimetypes, and deny the rest.\n        return mimetype not in self.allowed_inline_mimetypes\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def model_from_config(config, custom_objects=None):\n  \"\"\"Instantiates a Keras model from its config.\n\n  Usage:\n  ```\n  # for a Functional API model\n  tf.keras.Model().from_config(model.get_config())\n\n  # for a Sequential model\n  tf.keras.Sequential().from_config(model.get_config())\n  ```\n\n  Arguments:\n      config: Configuration dictionary.\n      custom_objects: Optional dictionary mapping names\n          (strings) to custom classes or functions to be\n          considered during deserialization.\n\n  Returns:\n      A Keras model instance (uncompiled).\n\n  Raises:\n      TypeError: if `config` is not a dictionary.\n  \"\"\"\n  if isinstance(config, list):\n    raise TypeError('`model_from_config` expects a dictionary, not a list. '\n                    'Maybe you meant to use '\n                    '`Sequential.from_config(config)`?')\n  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\n  return deserialize(config, custom_objects=custom_objects)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def job_log(request, client_id, project_name, spider_name, job_id):\n    \"\"\"\n    get log of jog\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :param spider_name: spider name\n    :param job_id: job id\n    :return: log of job\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        # get log url\n        url = log_url(client.ip, client.port,\n                      project_name, spider_name, job_id)\n        # get last 1000 bytes of log\n        response = requests.get(url, timeout=5, headers={\n            'Range': 'bytes=-1000'\n        }, auth=(client.username, client.password) if client.auth else None)\n        # Get encoding\n        encoding = response.apparent_encoding\n        # log not found\n        if response.status_code == 404:\n            return JsonResponse({'message': 'Log Not Found'}, status=404)\n        # bytes to string\n        text = response.content.decode(encoding, errors='replace')\n        return HttpResponse(text)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def extract_audiences_from_token(self, audiences_from_token=[], scopes_from_token=[], clientid_from_token=None):\n        '''\n        Extracts Audience From Token\n        '''\n        audiences = []\n        token_audiences = audiences_from_token\n        for audience in token_audiences:\n            if audience.find(self.DOT) > -1:\n         # CF UAA derives the audiences from the scopes.\n         # In case the scopes contains namespaces, these needs to be removed.\n             audience = audience[0:audience.find(self.DOT)].strip()\n             if audience and (audience not in audiences):\n                audiences.append(audience)\n            else:\n                audiences.append(audience)\n\n        if len(audiences) == 0:\n\n            for scope in scopes_from_token:\n\n                if scope.find(self.DOT) > -1:\n                  audience = scope[0 :scope.find(self.DOT)].strip()\n                  if audience :\n                    if (audience not in audiences):\n                        audiences.append(audience)\n\n            if (clientid_from_token and (clientid_from_token not in audiences)):\n                audiences.append(clientid_from_token)\n\n        return audiences\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def dictsortreversed(value, arg):\n    \"\"\"\n    Given a list of dicts, return that list sorted in reverse order by the\n    property given in the argument.\n    \"\"\"\n    try:\n        return sorted(value, key=_property_resolver(arg), reverse=True)\n    except (AttributeError, TypeError):\n        return ''\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def test_uploading_real_exe_file_should_fail(self):\n        with open(\"tests/ui/data/reactos_csrss.exe\", \"rb\") as exe_file:\n            b64 = base64.b64encode(exe_file.read()).decode()\n\n            with self.assertRaisesRegex(\n                Fault, \"Uploading executable files is forbidden\"\n            ):\n                self.rpc_client.User.add_attachment(\"csrss.exe_from_reactos\", b64)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def session_commit(success=None, sess=None):\n    s = sess if sess else session\n    try:\n        s.commit()\n        if success:\n            log.info(success)\n    except (exc.OperationalError, exc.InvalidRequestError) as e:\n        s.rollback()\n        log.debug_or_exception(e)\n    return \"\"\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def read_config(self, config, **kwargs):\n        self.enable_registration = strtobool(\n            str(config.get(\"enable_registration\", False))\n        )\n        if \"disable_registration\" in config:\n            self.enable_registration = not strtobool(\n                str(config[\"disable_registration\"])\n            )\n\n        self.account_validity = AccountValidityConfig(\n            config.get(\"account_validity\") or {}, config\n        )\n\n        self.registrations_require_3pid = config.get(\"registrations_require_3pid\", [])\n        self.allowed_local_3pids = config.get(\"allowed_local_3pids\", [])\n        self.enable_3pid_lookup = config.get(\"enable_3pid_lookup\", True)\n        self.registration_shared_secret = config.get(\"registration_shared_secret\")\n\n        self.bcrypt_rounds = config.get(\"bcrypt_rounds\", 12)\n        self.trusted_third_party_id_servers = config.get(\n            \"trusted_third_party_id_servers\", [\"matrix.org\", \"vector.im\"]\n        )\n        account_threepid_delegates = config.get(\"account_threepid_delegates\") or {}\n        self.account_threepid_delegate_email = account_threepid_delegates.get(\"email\")\n        self.account_threepid_delegate_msisdn = account_threepid_delegates.get(\"msisdn\")\n\n        self.default_identity_server = config.get(\"default_identity_server\")\n        self.allow_guest_access = config.get(\"allow_guest_access\", False)\n\n        if config.get(\"invite_3pid_guest\", False):\n            raise ConfigError(\"invite_3pid_guest is no longer supported\")\n\n        self.auto_join_rooms = config.get(\"auto_join_rooms\", [])\n        for room_alias in self.auto_join_rooms:\n            if not RoomAlias.is_valid(room_alias):\n                raise ConfigError(\"Invalid auto_join_rooms entry %s\" % (room_alias,))\n\n        # Options for creating auto-join rooms if they do not exist yet.\n        self.autocreate_auto_join_rooms = config.get(\"autocreate_auto_join_rooms\", True)\n        self.autocreate_auto_join_rooms_federated = config.get(\n            \"autocreate_auto_join_rooms_federated\", True\n        )\n        self.autocreate_auto_join_room_preset = (\n            config.get(\"autocreate_auto_join_room_preset\")\n            or RoomCreationPreset.PUBLIC_CHAT\n        )\n        self.auto_join_room_requires_invite = self.autocreate_auto_join_room_preset in {\n            RoomCreationPreset.PRIVATE_CHAT,\n            RoomCreationPreset.TRUSTED_PRIVATE_CHAT,\n        }\n\n        # Pull the creator/inviter from the configuration, this gets used to\n        # send invites for invite-only rooms.\n        mxid_localpart = config.get(\"auto_join_mxid_localpart\")\n        self.auto_join_user_id = None\n        if mxid_localpart:\n            # Convert the localpart to a full mxid.\n            self.auto_join_user_id = UserID(\n                mxid_localpart, self.server_name\n            ).to_string()\n\n        if self.autocreate_auto_join_rooms:\n            # Ensure the preset is a known value.\n            if self.autocreate_auto_join_room_preset not in {\n                RoomCreationPreset.PUBLIC_CHAT,\n                RoomCreationPreset.PRIVATE_CHAT,\n                RoomCreationPreset.TRUSTED_PRIVATE_CHAT,\n            }:\n                raise ConfigError(\"Invalid value for autocreate_auto_join_room_preset\")\n            # If the preset requires invitations to be sent, ensure there's a\n            # configured user to send them from.\n            if self.auto_join_room_requires_invite:\n                if not mxid_localpart:\n                    raise ConfigError(\n                        \"The configuration option `auto_join_mxid_localpart` is required if \"\n                        \"`autocreate_auto_join_room_preset` is set to private_chat or trusted_private_chat, such that \"\n                        \"Synapse knows who to send invitations from. Please \"\n                        \"configure `auto_join_mxid_localpart`.\"\n                    )\n\n        self.auto_join_rooms_for_guests = config.get(\"auto_join_rooms_for_guests\", True)\n\n        self.enable_set_displayname = config.get(\"enable_set_displayname\", True)\n        self.enable_set_avatar_url = config.get(\"enable_set_avatar_url\", True)\n        self.enable_3pid_changes = config.get(\"enable_3pid_changes\", True)\n\n        self.disable_msisdn_registration = config.get(\n            \"disable_msisdn_registration\", False\n        )\n\n        session_lifetime = config.get(\"session_lifetime\")\n        if session_lifetime is not None:\n            session_lifetime = self.parse_duration(session_lifetime)\n        self.session_lifetime = session_lifetime\n\n        # The success template used during fallback auth.\n        self.fallback_success_template = self.read_template(\"auth_success.html\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, security_policy, limits: TransportLimits):\n        self._sequence_number = 0\n        self._peer_sequence_number = None\n        self._incoming_parts = []\n        self.security_policy = security_policy\n        self._policies = []\n        self._open = False\n        self.security_token = ua.ChannelSecurityToken()\n        self.next_security_token = ua.ChannelSecurityToken()\n        self.prev_security_token = ua.ChannelSecurityToken()\n        self.local_nonce = 0\n        self.remote_nonce = 0\n        self._allow_prev_token = False\n        self._limits = limits\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def simple(prefix=\"\"):\n    # PEP 503: require normalized prefix\n    normalized = core.normalize_pkgname_for_url(prefix)\n    if prefix != normalized:\n        return redirect('/simple/{0}/'.format(normalized), 301)\n\n    files = sorted(core.find_packages(packages(), prefix=prefix),\n                   key=lambda x: (x.parsed_version, x.relfn))\n    if not files:\n        if config.redirect_to_fallback:\n            return redirect(\"%s/%s/\" % (config.fallback_url.rstrip(\"/\"), prefix))\n        return HTTPError(404, 'Not Found (%s does not exist)\\n\\n' % normalized)\n\n    fp = request.fullpath\n    links = [(os.path.basename(f.relfn),\n              urljoin(fp, \"../../packages/%s\" % f.fname_and_hash(config.hash_algo)))\n             for f in files]\n    tmpl = \"\"\"\\\n    <html>\n        <head>\n            <title>Links for {{prefix}}</title>\n        </head>\n        <body>\n            <h1>Links for {{prefix}}</h1>\n            % for file, href in links:\n                 <a href=\"{{href}}\">{{file}}</a><br>\n            % end\n        </body>\n    </html>\n    \"\"\"\n    return template(tmpl, prefix=prefix, links=links)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _external_call_helper(\n    contract_address,\n    contract_sig,\n    args_ir,\n    context,\n    value=None,\n    gas=None,\n    skip_contract_check=None,\n    expr=None,\n):\n\n    if value is None:\n        value = 0\n    if gas is None:\n        gas = \"gas\"\n    if skip_contract_check is None:\n        skip_contract_check = False\n\n    # sanity check\n    assert len(contract_sig.base_args) <= len(args_ir) <= len(contract_sig.args)\n\n    if context.is_constant() and contract_sig.mutability not in (\"view\", \"pure\"):\n        # TODO is this already done in type checker?\n        raise StateAccessViolation(\n            f\"May not call state modifying function '{contract_sig.name}' \"\n            f\"within {context.pp_constancy()}.\",\n            expr,\n        )\n\n    sub = [\"seq\"]\n\n    buf, arg_packer, args_ofst, args_len = _pack_arguments(contract_sig, args_ir, context)\n\n    ret_unpacker, ret_ofst, ret_len = _unpack_returndata(\n        buf, contract_sig, skip_contract_check, context, expr\n    )\n\n    sub += arg_packer\n\n    if contract_sig.return_type is None and not skip_contract_check:\n        # if we do not expect return data, check that a contract exists at the\n        # target address. we must perform this check BEFORE the call because\n        # the contract might selfdestruct. on the other hand we can omit this\n        # when we _do_ expect return data because we later check\n        # `returndatasize` (that check works even if the contract\n        # selfdestructs).\n        sub.append([\"assert\", [\"extcodesize\", contract_address]])\n\n    if context.is_constant() or contract_sig.mutability in (\"view\", \"pure\"):\n        call_op = [\"staticcall\", gas, contract_address, args_ofst, args_len, ret_ofst, ret_len]\n    else:\n        call_op = [\"call\", gas, contract_address, value, args_ofst, args_len, ret_ofst, ret_len]\n\n    sub.append(check_external_call(call_op))\n\n    if contract_sig.return_type is not None:\n        sub += ret_unpacker\n\n    return IRnode.from_list(sub, typ=contract_sig.return_type, location=MEMORY)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def main() -> None:\n    try:\n        run(sys.argv)\n    except KeyboardInterrupt:\n        pass\n    except (OSError, TooManyNodesError, TypeError, ValueError) as e:\n        print(f\"Error: {e}\", file=sys.stderr)  # noqa: T201\n        sys.exit(1)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def auth_role_admin(self) -> str:\n        return self.appbuilder.get_app.config[\"AUTH_ROLE_ADMIN\"]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _result_handle(self, task):\n        return Result(self, task)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def create_user():\n    content_type = request.headers.get(\"Content-Type\")\n    if content_type == \"application/x-www-form-urlencoded\":\n        username = request.form[\"username\"]\n        password = request.form[\"password\"]\n\n        if store.has_user(username):\n            flash(f\"Username has already been taken: {username}\")\n            return alert(href=SIGNUP)\n\n        store.create_user(username, password)\n        flash(f\"Successfully signed up user: {username}\")\n        return alert(href=HOME)\n    elif content_type == \"application/json\":\n        username = _get_request_param(\"username\")\n        password = _get_request_param(\"password\")\n\n        user = store.create_user(username, password)\n        return make_response({\"user\": user.to_json()})\n    else:\n        message = (\n            \"Invalid content type. Must be one of: \"\n            \"application/x-www-form-urlencoded, application/json\"\n        )\n        return make_response(message, 400)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def from_string(self, string):\n        \"\"\"Creates a TokenSet from a string.\n\n        The string may contain one or more wildcard characters (*) that will\n        allow wildcard matching when intersecting with another TokenSet\n        \"\"\"\n        node = TokenSet()\n        root = node\n\n        # Iterates throough all characters in the passed string appending\n        # a node for each character.\n        # When a wildcard character is found then a self referencing edge\n        # is introduced to continually match any number of characters\n        for i, char in enumerate(string):\n            final = i == len(string) - 1\n            if char == '*':\n                node.edges[char] = node\n                node.final = final\n            else:\n                next_ = TokenSet()\n                next_.final = final\n                node.edges[char] = next_\n                node = next_\n\n        return root\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        void write_library_config(const py::object& lib_cfg, const LibraryPath& path, const StorageOverride& storage_override,\n                                  const bool validate) const {\n            SegmentInMemory segment;\n\n            arcticdb::proto::storage::LibraryConfig lib_cfg_proto;\n            google::protobuf::Any output = {};\n            python_util::pb_from_python(lib_cfg, lib_cfg_proto);\n\n            apply_storage_override(storage_override, lib_cfg_proto);\n\n            output.PackFrom(lib_cfg_proto);\n\n            if (validate) {\n                for (const auto &storage: lib_cfg_proto.storage_by_id()) {\n                    is_storage_config_ok(storage.second, BAD_CONFIG_IN_ATTEMPTED_WRITE, true);\n                }\n            }\n\n            segment.set_metadata(std::move(output));\n\n            store_->write_sync(\n                    entity::KeyType::LIBRARY_CONFIG,\n                    StreamId(path.to_delim_path()),\n                    std::move(segment)\n            );\n        }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, env):\n        Storage.__init__(self)\n        self.env = Storage(env)\n        self.env.web2py_path = global_settings.applications_parent\n        self.cookies = Cookie.SimpleCookie()\n        self._get_vars = None\n        self._post_vars = None\n        self._vars = None\n        self._body = None\n        self.folder = None\n        self.application = None\n        self.function = None\n        self.args = List()\n        self.extension = 'html'\n        self.now = datetime.datetime.now()\n        self.utcnow = datetime.datetime.utcnow()\n        self.is_restful = False\n        self.is_https = False\n        self.is_local = False\n        self._uuid = None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, user, **kwargs):\n        \"\"\"Add token field.\"\"\"\n        super().__init__(user, **kwargs)\n        self.fields['token'].widget.attrs.update(\n            {\n                'autofocus': 'autofocus',\n                'autocomplete': 'off',\n            }\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def time_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        # The implementation is similar to `datetime_trunc_sql` as both\n        # `DateTimeField` and `TimeField` are stored as TIMESTAMP where\n        # the date part of the later is ignored.\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        trunc_param = None\n        if lookup_type == \"hour\":\n            trunc_param = \"HH24\"\n        elif lookup_type == \"minute\":\n            trunc_param = \"MI\"\n        elif lookup_type == \"second\":\n            # Cast to DATE removes sub-second precision.\n            return f\"CAST({sql} AS DATE)\", params\n        return f\"TRUNC({sql}, %s)\", (*params, trunc_param)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def check_is_safe_zip(zip_file: ZipFile) -> None:\n    \"\"\"\n    Checks whether a ZIP file is safe, raises SupersetException if not.\n\n    :param zip_file:\n    :return:\n    \"\"\"\n    uncompress_size = 0\n    compress_size = 0\n    for zip_file_element in zip_file.infolist():\n        if zip_file_element.file_size > current_app.config[\"ZIPPED_FILE_MAX_SIZE\"]:\n            raise SupersetException(\"Found file with size above allowed threshold\")\n        uncompress_size += zip_file_element.file_size\n        compress_size += zip_file_element.compress_size\n    compress_ratio = uncompress_size / compress_size\n    if compress_ratio > current_app.config[\"ZIP_FILE_MAX_COMPRESS_RATIO\"]:\n        raise SupersetException(\"Zip compress ratio above allowed threshold\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def __init__(self, opcode, max_frame_length, decoder=None, decompressor=None):\n            self.decoder = decoder\n            self.data = []\n            self.finished = False\n            self.opcode = opcode\n            self.decompressor = decompressor\n            self.max_frame_length = max_frame_length\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def to_xml(self, data, options=None):\n        \"\"\"\n        Given some Python data, produces XML output.\n        \"\"\"\n        options = options or {}\n\n        if lxml is None:\n            raise ImproperlyConfigured(\"Usage of the XML aspects requires lxml.\")\n\n        return tostring(self.to_etree(data, options), xml_declaration=True, encoding='utf-8')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def delete_book_from_table(book_id, book_format, jsonResponse):\n    warning = {}\n    if current_user.role_delete_books():\n        book = calibre_db.get_book(book_id)\n        if book:\n            try:\n                result, error = helper.delete_book(book, config.config_calibre_dir, book_format=book_format.upper())\n                if not result:\n                    if jsonResponse:\n                        return json.dumps([{\"location\": url_for(\"editbook.edit_book\", book_id=book_id),\n                                           \"type\": \"danger\",\n                                           \"format\": \"\",\n                                           \"message\": error}])\n                    else:\n                        flash(error, category=\"error\")\n                        return redirect(url_for('editbook.edit_book', book_id=book_id))\n                if error:\n                    if jsonResponse:\n                        warning = {\"location\": url_for(\"editbook.edit_book\", book_id=book_id),\n                                                \"type\": \"warning\",\n                                                \"format\": \"\",\n                                                \"message\": error}\n                    else:\n                        flash(error, category=\"warning\")\n                if not book_format:\n                    delete_whole_book(book_id, book)\n                else:\n                    calibre_db.session.query(db.Data).filter(db.Data.book == book.id).\\\n                        filter(db.Data.format == book_format).delete()\n                    kobo_sync_status.remove_synced_book(book.id, True)\n                calibre_db.session.commit()\n            except Exception as ex:\n                log.debug_or_exception(ex)\n                calibre_db.session.rollback()\n                if jsonResponse:\n                    return json.dumps([{\"location\": url_for(\"editbook.edit_book\", book_id=book_id),\n                                        \"type\": \"danger\",\n                                        \"format\": \"\",\n                                        \"message\": ex}])\n                else:\n                    flash(str(ex), category=\"error\")\n                    return redirect(url_for('editbook.edit_book', book_id=book_id))\n\n        else:\n            # book not found\n            log.error('Book with id \"%s\" could not be deleted: not found', book_id)\n        return render_delete_book_result(book_format, jsonResponse, warning, book_id)\n    message = _(\"You are missing permissions to delete books\")\n    if jsonResponse:\n        return json.dumps({\"location\": url_for(\"editbook.edit_book\", book_id=book_id),\n                           \"type\": \"danger\",\n                           \"format\": \"\",\n                           \"message\": message})\n    else:\n        flash(message, category=\"error\")\n        return redirect(url_for('editbook.edit_book', book_id=book_id))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _generate_cmd(self, executable, cmd):\n        if executable:\n            ### TODO: Why was \"-c\" removed from here? (vs jail.py)\n            local_cmd = [self.zlogin_cmd, self.zone, executable, cmd]\n        else:\n            local_cmd = '%s \"%s\" %s' % (self.zlogin_cmd, self.zone, cmd)\n        return local_cmd\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self):\n        self.request = None\n        self.response = None\n        self.config = None\n        self.conn = None\n        self.event_json = None\n        self.event_tags = None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def formatter_none(error):\n    \"\"\"\n    Formatter that ignores the error.\n    This is useful / necessary when handling custom css/html\n    It outputs an html comment just so you don't go insane debugging.\n    \"\"\"\n    return '<!-- formatter_none -->'\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def populate_obj(self, userobj):\n        # Save password if defined\n        if self.password.data:\n            userobj.set_password(self.password.data)\n        userobj.role = self.role.data\n        userobj.fullname = self.fullname.data or ''\n        userobj.email = self.email.data or ''\n        userobj.user_root = self.user_root.data\n        if self.mfa.data and not userobj.email:\n            flash(_(\"User email is required to enabled Two-Factor Authentication\"), level='error')\n        else:\n            userobj.mfa = self.mfa.data\n        if not userobj.valid_user_root():\n            flash(_(\"User's root directory %s is not accessible!\") % userobj.user_root, level='error')\n            logger.warning(\"user's root directory %s is not accessible\" % userobj.user_root)\n        else:\n            userobj.refresh_repos(delete=True)\n        # Try to update disk quota if the human readable value changed.\n        # Report error using flash.\n        new_quota = self.disk_quota.data or 0\n        old_quota = humanfriendly.parse_size(humanfriendly.format_size(self.disk_quota.object_data or 0, binary=True))\n        if old_quota != new_quota:\n            userobj.disk_quota = new_quota\n            # Setting quota will silently fail. Check if quota was updated.\n            if userobj.disk_quota != new_quota:\n                flash(_(\"Setting user's quota is not supported\"), level='warning')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_related_interface(self, col_name):\n        \"\"\"\n        Returns a BaseInterface for the related model\n        of column name.\n\n        :param col_name: Column name with relation\n        :return: BaseInterface\n        \"\"\"\n        raise NotImplementedError\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def static(path):\n    \"\"\"\n    Create a page handler to serve static files. Disable authentication.\n    \"\"\"\n    assert isinstance(path, str)\n    assert os.path.exists(path), \"%r doesn't exists\" % path\n    content_type = None\n    if os.path.isfile(path):\n        # Set content-type based on filename extension\n        ext = \"\"\n        i = path.rfind('.')\n        if i != -1:\n            ext = path[i:].lower()\n        content_type = mimetypes.types_map.get(ext, None)  # @UndefinedVariable\n\n    @cherrypy.expose\n    @cherrypy.config(**{'tools.authform.on': False, 'tools.sessions.on': False})\n    def handler(*args, **kwargs):\n        if cherrypy.request.method not in ('GET', 'HEAD'):\n            return None\n        filename = os.path.join(path, *args)\n        assert filename.startswith(path)\n        if not os.path.isfile(filename):\n            return None\n        return serve_file(filename, content_type)\n\n    return handler\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_valid_client_secret(client_secret):\n    \"\"\"Validate that a given string matches the client_secret regex defined by the spec\n\n    :param client_secret: The client_secret to validate\n    :type client_secret: str\n\n    :return: Whether the client_secret is valid\n    :rtype: bool\n    \"\"\"\n    return client_secret_regex.match(client_secret) is not None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "                    message: _.sprintf(\n                        gettext('You are about to delete the user \"%(name)s\".'),\n                        {name: _.escape(user.name)}\n                    ),\n                    proceed: gettext(\"Delete\"),\n                    onproceed: function () {\n                        OctoPrint.access.users.delete(user.name).done(self.fromResponse);\n                    }\n                });\n            };\n\n            self.updateUser = function (user) {\n                if (!user) {\n                    throw OctoPrint.InvalidArgumentError(\"user must be set\");\n                }\n\n                return OctoPrint.access.users\n                    .update(\n                        user.name,\n                        user.active,\n                        user.admin,\n                        user.permissions,\n                        user.groups\n                    )\n                    .done(self.fromResponse);\n            };\n\n            self.updatePassword = function (username, password, current) {\n                return OctoPrint.access.users.changePassword(username, password, current);\n            };\n\n            self.generateApikey = function (username) {\n                return OctoPrint.access.users.generateApiKey(username).done(function () {\n                    self.requestData();\n                });\n            };\n\n            self.deleteApikey = function (username) {\n                return OctoPrint.access.users.resetApiKey(username);\n            };\n\n            return self;\n        })();\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def project(self, configured_project):\n        return configured_project\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def read_config(self, config: dict, config_dir_path: str, **kwargs):\n\n        acme_config = config.get(\"acme\", None)\n        if acme_config is None:\n            acme_config = {}\n\n        self.acme_enabled = acme_config.get(\"enabled\", False)\n\n        if self.acme_enabled:\n            logger.warning(ACME_SUPPORT_ENABLED_WARN)\n\n        # hyperlink complains on py2 if this is not a Unicode\n        self.acme_url = str(\n            acme_config.get(\"url\", \"https://acme-v01.api.letsencrypt.org/directory\")\n        )\n        self.acme_port = acme_config.get(\"port\", 80)\n        self.acme_bind_addresses = acme_config.get(\"bind_addresses\", [\"::\", \"0.0.0.0\"])\n        self.acme_reprovision_threshold = acme_config.get(\"reprovision_threshold\", 30)\n        self.acme_domain = acme_config.get(\"domain\", config.get(\"server_name\"))\n\n        self.acme_account_key_file = self.abspath(\n            acme_config.get(\"account_key_file\", config_dir_path + \"/client.key\")\n        )\n\n        self.tls_certificate_file = self.abspath(config.get(\"tls_certificate_path\"))\n        self.tls_private_key_file = self.abspath(config.get(\"tls_private_key_path\"))\n\n        if self.root.server.has_tls_listener():\n            if not self.tls_certificate_file:\n                raise ConfigError(\n                    \"tls_certificate_path must be specified if TLS-enabled listeners are \"\n                    \"configured.\"\n                )\n            if not self.tls_private_key_file:\n                raise ConfigError(\n                    \"tls_private_key_path must be specified if TLS-enabled listeners are \"\n                    \"configured.\"\n                )\n\n        self._original_tls_fingerprints = config.get(\"tls_fingerprints\", [])\n\n        if self._original_tls_fingerprints is None:\n            self._original_tls_fingerprints = []\n\n        self.tls_fingerprints = list(self._original_tls_fingerprints)\n\n        # Whether to verify certificates on outbound federation traffic\n        self.federation_verify_certificates = config.get(\n            \"federation_verify_certificates\", True\n        )\n\n        # Minimum TLS version to use for outbound federation traffic\n        self.federation_client_minimum_tls_version = str(\n            config.get(\"federation_client_minimum_tls_version\", 1)\n        )\n\n        if self.federation_client_minimum_tls_version not in [\"1\", \"1.1\", \"1.2\", \"1.3\"]:\n            raise ConfigError(\n                \"federation_client_minimum_tls_version must be one of: 1, 1.1, 1.2, 1.3\"\n            )\n\n        # Prevent people shooting themselves in the foot here by setting it to\n        # the biggest number blindly\n        if self.federation_client_minimum_tls_version == \"1.3\":\n            if getattr(SSL, \"OP_NO_TLSv1_3\", None) is None:\n                raise ConfigError(\n                    (\n                        \"federation_client_minimum_tls_version cannot be 1.3, \"\n                        \"your OpenSSL does not support it\"\n                    )\n                )\n\n        # Whitelist of domains to not verify certificates for\n        fed_whitelist_entries = config.get(\n            \"federation_certificate_verification_whitelist\", []\n        )\n        if fed_whitelist_entries is None:\n            fed_whitelist_entries = []\n\n        # Support globs (*) in whitelist values\n        self.federation_certificate_verification_whitelist = []  # type: List[Pattern]\n        for entry in fed_whitelist_entries:\n            try:\n                entry_regex = glob_to_regex(entry.encode(\"ascii\").decode(\"ascii\"))\n            except UnicodeEncodeError:\n                raise ConfigError(\n                    \"IDNA domain names are not allowed in the \"\n                    \"federation_certificate_verification_whitelist: %s\" % (entry,)\n                )\n\n            # Convert globs to regex\n            self.federation_certificate_verification_whitelist.append(entry_regex)\n\n        # List of custom certificate authorities for federation traffic validation\n        custom_ca_list = config.get(\"federation_custom_ca_list\", None)\n\n        # Read in and parse custom CA certificates\n        self.federation_ca_trust_root = None\n        if custom_ca_list is not None:\n            if len(custom_ca_list) == 0:\n                # A trustroot cannot be generated without any CA certificates.\n                # Raise an error if this option has been specified without any\n                # corresponding certificates.\n                raise ConfigError(\n                    \"federation_custom_ca_list specified without \"\n                    \"any certificate files\"\n                )\n\n            certs = []\n            for ca_file in custom_ca_list:\n                logger.debug(\"Reading custom CA certificate file: %s\", ca_file)\n                content = self.read_file(ca_file, \"federation_custom_ca_list\")\n\n                # Parse the CA certificates\n                try:\n                    cert_base = Certificate.loadPEM(content)\n                    certs.append(cert_base)\n                except Exception as e:\n                    raise ConfigError(\n                        \"Error parsing custom CA certificate file %s: %s\" % (ca_file, e)\n                    )\n\n            self.federation_ca_trust_root = trustRootFromCertificates(certs)\n\n        # This config option applies to non-federation HTTP clients\n        # (e.g. for talking to recaptcha, identity servers, and such)\n        # It should never be used in production, and is intended for\n        # use only when running tests.\n        self.use_insecure_ssl_client_just_for_testing_do_not_use = config.get(\n            \"use_insecure_ssl_client_just_for_testing_do_not_use\"\n        )\n\n        self.tls_certificate = None  # type: Optional[crypto.X509]\n        self.tls_private_key = None  # type: Optional[crypto.PKey]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status CalculateOutputIndexRowSplit(\n      const RowPartitionTensor& row_split,\n      const vector<INDEX_TYPE>& parent_output_index,\n      INDEX_TYPE output_index_multiplier, INDEX_TYPE output_size,\n      vector<INDEX_TYPE>* result) {\n    INDEX_TYPE row_split_size = row_split.size();\n    if (row_split_size > 0) {\n      result->reserve(row_split(row_split_size - 1));\n    }\n    for (INDEX_TYPE i = 0; i < row_split_size - 1; ++i) {\n      INDEX_TYPE row_length = row_split(i + 1) - row_split(i);\n      INDEX_TYPE real_length = std::min(output_size, row_length);\n      INDEX_TYPE parent_output_index_current = parent_output_index[i];\n\n      if (parent_output_index_current == -1) {\n        real_length = 0;\n      }\n      for (INDEX_TYPE j = 0; j < real_length; ++j) {\n        result->push_back(parent_output_index_current);\n        parent_output_index_current += output_index_multiplier;\n      }\n      for (INDEX_TYPE j = 0; j < row_length - real_length; ++j) {\n        result->push_back(-1);\n      }\n    }\n    if (row_split_size > 0 && result->size() != row_split(row_split_size - 1)) {\n      return errors::InvalidArgument(\"Invalid row split size.\");\n    }\n\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def content_security_policy(self):\n        # In case we're serving HTML/SVG, confine any Javascript to a unique\n        # origin so it can't interact with the notebook server.\n        return super(NbconvertFileHandler, self).content_security_policy + \\\n               \"; sandbox allow-scripts\"\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def test_empty(self):\n        self.assertIsNone(validate_editor(''))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _contains_display_name(self, display_name: str) -> bool:\n        if not display_name:\n            return False\n\n        body = self._event.content.get(\"body\", None)\n        if not body or not isinstance(body, str):\n            return False\n\n        # Similar to _glob_matches, but do not treat display_name as a glob.\n        r = regex_cache.get((display_name, False, True), None)\n        if not r:\n            r1 = re.escape(display_name)\n            r1 = re_word_boundary(r1)\n            r = re.compile(r1, flags=re.IGNORECASE)\n            regex_cache[(display_name, False, True)] = r\n\n        return bool(r.search(body))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, hs: \"HomeServer\"):\n        self.config = hs.config\n        self.clock = hs.get_clock()\n        self._instance_name = hs.get_instance_name()\n\n        # These are safe to load in monolith mode, but will explode if we try\n        # and use them. However we have guards before we use them to ensure that\n        # we don't route to ourselves, and in monolith mode that will always be\n        # the case.\n        self._get_query_client = ReplicationGetQueryRestServlet.make_client(hs)\n        self._send_edu = ReplicationFederationSendEduRestServlet.make_client(hs)\n\n        self.edu_handlers = (\n            {}\n        )  # type: Dict[str, Callable[[str, dict], Awaitable[None]]]\n        self.query_handlers = {}  # type: Dict[str, Callable[[dict], Awaitable[None]]]\n\n        # Map from type to instance name that we should route EDU handling to.\n        self._edu_type_to_instance = {}  # type: Dict[str, str]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def ListLogs(self, request, context):\n        \"\"\"\n        Lists all files in the active Ray logs directory.\n\n        Part of `LogService` gRPC.\n\n        NOTE: These RPCs are used by state_head.py, not log_head.py\n        \"\"\"\n        path = Path(self._dashboard_agent.log_dir)\n        if not path.exists():\n            raise FileNotFoundError(\n                f\"Could not find log dir at path: {self._dashboard_agent.log_dir}\"\n                \"It is unexpected. Please report an issue to Ray Github.\"\n            )\n        log_files = []\n        for p in path.glob(request.glob_filter):\n            log_files.append(str(p.relative_to(path)) + (\"/\" if p.is_dir() else \"\"))\n        return reporter_pb2.ListLogsReply(log_files=log_files)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _get_cte_tables(self, parsed: dict[str, Any]) -> list[dict[str, Any]]:\n        if \"with\" not in parsed:\n            return []\n        return parsed[\"with\"].get(\"cte_tables\", [])\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def date_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        fields = {\n            \"year\": \"%Y-01-01\",\n            \"month\": \"%Y-%m-01\",\n        }\n        if lookup_type in fields:\n            format_str = fields[lookup_type]\n            return f\"CAST(DATE_FORMAT({sql}, %s) AS DATE)\", (*params, format_str)\n        elif lookup_type == \"quarter\":\n            return (\n                f\"MAKEDATE(YEAR({sql}), 1) + \"\n                f\"INTERVAL QUARTER({sql}) QUARTER - INTERVAL 1 QUARTER\",\n                (*params, *params),\n            )\n        elif lookup_type == \"week\":\n            return f\"DATE_SUB({sql}, INTERVAL WEEKDAY({sql}) DAY)\", (*params, *params)\n        else:\n            return f\"DATE({sql})\", params\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_autocommit(  # type: ignore[override]\n        self, conn: Connection, autocommit: bool\n    ) -> NotImplementedError:\n        raise NotImplementedError(\"There are no transactions in Drill.\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,\n                                  TfLiteDepthwiseConvParams* params,\n                                  OpData* data, const TfLiteTensor* input,\n                                  const TfLiteTensor* filter,\n                                  const TfLiteTensor* bias,\n                                  TfLiteTensor* output) {\n  float output_activation_min, output_activation_max;\n  CalculateActivationRange(params->activation, &output_activation_min,\n                           &output_activation_max);\n  const int batch_size = SizeOfDimension(input, 0);\n  TF_LITE_ENSURE(context, batch_size != 0);\n  const int input_size = NumElements(input) / batch_size;\n  TfLiteTensor* input_quantized;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, data->input_quantized_index,\n                                     &input_quantized));\n  int8_t* quantized_input_ptr_batch = input_quantized->data.int8;\n  TfLiteTensor* scaling_factors_tensor;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, data->scaling_factors_index,\n                                     &scaling_factors_tensor));\n  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);\n  TfLiteTensor* input_offset_tensor;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, data->input_offset_index,\n                                     &input_offset_tensor));\n  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);\n\n  for (int b = 0; b < batch_size; ++b) {\n    const int offset = b * input_size;\n    tensor_utils::AsymmetricQuantizeFloats(\n        GetTensorData<float>(input) + offset, input_size,\n        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],\n        &input_offset_ptr[b]);\n  }\n\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.depth_multiplier = params->depth_multiplier;\n\n  op_params.weights_offset = 0;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  const auto* affine_quantization =\n      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);\n  if (kernel_type == kReference) {\n    reference_integer_ops::DepthwiseConvHybridPerChannel(\n        op_params, scaling_factors_ptr, GetTensorShape(input),\n        quantized_input_ptr_batch, GetTensorShape(filter),\n        GetTensorData<int8>(filter), GetTensorShape(bias),\n        GetTensorData<float>(bias), GetTensorShape(output),\n        GetTensorData<float>(output), affine_quantization->scale->data,\n        input_offset_ptr);\n  } else {\n    optimized_integer_ops::DepthwiseConvHybridPerChannel(\n        op_params, scaling_factors_ptr, GetTensorShape(input),\n        quantized_input_ptr_batch, GetTensorShape(filter),\n        GetTensorData<int8>(filter), GetTensorShape(bias),\n        GetTensorData<float>(bias), GetTensorShape(output),\n        GetTensorData<float>(output), affine_quantization->scale->data,\n        input_offset_ptr, CpuBackendContext::GetFromContext(context));\n  }\n\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def spider_list(request, client_id, project_name):\n    \"\"\"\n    get spider list from one client\n    :param request: request Object\n    :param client_id: client id\n    :param project_name: project name\n    :return: json\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        scrapyd = get_scrapyd(client)\n        spiders = scrapyd.list_spiders(project_name)\n        spiders = [{'name': spider, 'id': index + 1}\n                   for index, spider in enumerate(spiders)]\n        return JsonResponse(spiders)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def task_update(request, task_id):\n    \"\"\"\n    update task info\n    :param request: request object\n    :param task_id: task id\n    :return: json\n    \"\"\"\n    if request.method == 'POST':\n        task = Task.objects.filter(id=task_id)\n        data = json.loads(request.body)\n        data['clients'] = json.dumps(data.get('clients'), ensure_ascii=False)\n        data['configuration'] = json.dumps(\n            data.get('configuration'), ensure_ascii=False)\n        data['modified'] = 1\n        task.update(**data)\n        return JsonResponse(model_to_dict(Task.objects.get(id=task_id)))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_header_field(self, data: bytes, start: int, end: int) -> None:\n        self._current_partial_header_name += data[start:end]\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void register_bindings(py::module& storage, py::exception<arcticdb::ArcticException>& base_exception) {\n    py::enum_<KeyType>(storage, \"KeyType\")\n        .value(\"STREAM_GROUP\", KeyType::STREAM_GROUP)\n        .value(\"VERSION\", KeyType::VERSION)\n        .value(\"VERSION_JOURNAL\", KeyType::VERSION_JOURNAL)\n        .value(\"GENERATION\", KeyType::GENERATION)\n        .value(\"TABLE_DATA\", KeyType::TABLE_DATA)\n        .value(\"TABLE_INDEX\", KeyType::TABLE_INDEX)\n        .value(\"METRICS\", KeyType::METRICS)\n        .value(\"SNAPSHOT\", KeyType::SNAPSHOT)\n        .value(\"SYMBOL_LIST\", KeyType::SYMBOL_LIST)\n        .value(\"VERSION_REF\", KeyType::VERSION_REF)\n        .value(\"STORAGE_INFO\", KeyType::STORAGE_INFO)\n        .value(\"APPEND_REF\", KeyType::APPEND_REF)\n        .value(\"LOCK\", KeyType::LOCK)\n        .value(\"SNAPSHOT_REF\", KeyType::SNAPSHOT_REF)\n        .value(\"TOMBSTONE\", KeyType::TOMBSTONE)\n        .value(\"APPEND_DATA\", KeyType::APPEND_DATA)\n        .value(\"MULTI_KEY\", KeyType::MULTI_KEY)\n        .value(\"LOG\", KeyType::LOG)\n        .value(\"PARTITION\", KeyType::PARTITION)\n        .value(\"OFFSET\", KeyType::OFFSET)\n        .value(\"BACKUP_SNAPSHOT_REF\", KeyType::BACKUP_SNAPSHOT_REF)\n        .value(\"TOMBSTONE_ALL\", KeyType::TOMBSTONE_ALL)\n        .value(\"SNAPSHOT_TOMBSTONE\", KeyType::SNAPSHOT_TOMBSTONE)\n        .value(\"LOG_COMPACTED\", KeyType::LOG_COMPACTED)\n        .value(\"COLUMN_STATS\", KeyType::COLUMN_STATS)\n        ;\n\n    py::enum_<OpenMode>(storage, \"OpenMode\")\n        .value(\"READ\", OpenMode::READ)\n        .value(\"WRITE\", OpenMode::WRITE)\n        .value(\"DELETE\", OpenMode::DELETE);\n\n    storage.def(\"create_library_index\", &create_library_index);\n\n    storage.def(\"create_mem_config_resolver\", [](const py::object & env_config_map_py) -> std::shared_ptr<ConfigResolver> {\n        arcticdb::proto::storage::EnvironmentConfigsMap ecm;\n        pb_from_python(env_config_map_py, ecm);\n        auto resolver = std::make_shared<storage::details::InMemoryConfigResolver>();\n        for(auto &[env, cfg] :ecm.env_by_id()){\n            EnvironmentName env_name{env};\n            for(auto &[id, variant_storage]: cfg.storage_by_id()){\n                resolver->add_storage(env_name, StorageName{id}, variant_storage);\n            }\n            for(auto &[id, lib_desc]: cfg.lib_by_path()){\n                resolver->add_library(env_name, lib_desc);\n            }\n        }\n        return resolver;\n    });\n\n    py::class_<ConfigResolver, std::shared_ptr<ConfigResolver>>(storage, \"ConfigResolver\");\n\n    py::class_<Library, std::shared_ptr<Library>>(storage, \"Library\")\n        .def_property_readonly(\"library_path\", [](const Library &library){ return library.library_path().to_delim_path(); })\n        .def_property_readonly(\"open_mode\", [](const Library &library){ return library.open_mode(); })\n        .def_property_readonly(\"config\", [](const Library & library) {\n            return util::variant_match(library.config(),\n                                       [](const arcticdb::proto::storage::VersionStoreConfig & cfg){\n                                           return pb_to_python(cfg);\n                                       },\n                                       [](const std::monostate & ){\n                                            auto none = py::none{};\n                                            none.inc_ref();\n                                            return none.cast<py::object>();\n                                       });\n        })\n        ;\n\n    py::class_<S3Override>(storage, \"S3Override\")\n        .def(py::init<>())\n        .def_property(\"credential_name\", &S3Override::credential_name, &S3Override::set_credential_name)\n        .def_property(\"credential_key\", &S3Override::credential_key, &S3Override::set_credential_key)\n        .def_property(\"endpoint\", &S3Override::endpoint, &S3Override::set_endpoint)\n        .def_property(\"bucket_name\", &S3Override::bucket_name, &S3Override::set_bucket_name)\n        .def_property(\"region\", &S3Override::region, &S3Override::set_region)\n        .def_property(\n                \"use_virtual_addressing\", &S3Override::use_virtual_addressing, &S3Override::set_use_virtual_addressing);\n\n    py::class_<AzureOverride>(storage, \"AzureOverride\")\n        .def(py::init<>())\n        .def_property(\"container_name\", &AzureOverride::container_name, &AzureOverride::set_container_name)\n        .def_property(\"endpoint\", &AzureOverride::endpoint, &AzureOverride::set_endpoint)\n        .def_property(\"ca_cert_path\", &AzureOverride::ca_cert_path, &AzureOverride::set_ca_cert_path);\n\n    py::class_<StorageOverride>(storage, \"StorageOverride\")\n        .def(py::init<>())\n        .def(\"set_s3_override\", &StorageOverride::set_s3_override)\n        .def(\"set_azure_override\", &StorageOverride::set_azure_override);\n\n    py::class_<LibraryManager, std::shared_ptr<LibraryManager>>(storage, \"LibraryManager\")\n        .def(py::init<std::shared_ptr<storage::Library>>())\n        .def(\"write_library_config\", [](const LibraryManager& library_manager, py::object& lib_cfg,\n                                        std::string_view library_path, const StorageOverride& storage_override, const bool validate) {\n            LibraryPath lib_path{library_path, '.'};\n            return library_manager.write_library_config(lib_cfg, lib_path, storage_override, validate);\n        },\n             py::arg(\"lib_cfg\"),\n             py::arg(\"library_path\"),\n             py::arg(\"override\") = StorageOverride{},\n             py::arg(\"test_only_validation_toggle\") = false)\n        .def(\"get_library_config\", [](const LibraryManager& library_manager, std::string_view library_path, const StorageOverride& storage_override){\n            return library_manager.get_library_config(LibraryPath{library_path, '.'}, storage_override);\n        }, py::arg(\"library_path\"), py::arg(\"override\") = StorageOverride{})\n        .def(\"is_library_config_ok\", [](const LibraryManager& library_manager, std::string_view library_path, bool throw_on_failure) {\n            return library_manager.is_library_config_ok(LibraryPath{library_path, '.'}, throw_on_failure);\n        }, py::arg(\"library_path\"), py::arg(\"throw_on_failure\") = true)\n        .def(\"remove_library_config\", [](const LibraryManager& library_manager, std::string_view library_path){\n            return library_manager.remove_library_config(LibraryPath{library_path, '.'});\n        })\n        .def(\"get_library\", [](const LibraryManager& library_manager, std::string_view library_path, const StorageOverride& storage_override){\n            return library_manager.get_library(LibraryPath{library_path, '.'}, storage_override);\n        }, py::arg(\"library_path\"), py::arg(\"storage_override\") = StorageOverride{})\n        .def(\"has_library\", [](const LibraryManager& library_manager, std::string_view library_path){\n            return library_manager.has_library(LibraryPath{library_path, '.'});\n        })\n        .def(\"list_libraries\", [](const LibraryManager& library_manager){\n            std::vector<std::string> res;\n            for(auto & lp:library_manager.get_library_paths()){\n                res.push_back(lp.to_delim_path());\n            }\n            return res;\n        });\n\n    py::class_<LibraryIndex, std::shared_ptr<LibraryIndex>>(storage, \"LibraryIndex\")\n        .def(py::init<>([](const std::string &environment_name) {\n                 auto resolver = std::make_shared<details::InMemoryConfigResolver>();\n                 return std::make_unique<LibraryIndex>(EnvironmentName{environment_name}, resolver);\n             })\n        )\n        .def_static(\"create_from_resolver\", [](const std::string &environment_name, std::shared_ptr<ConfigResolver> resolver){\n            return std::make_shared<LibraryIndex>(EnvironmentName{environment_name}, resolver);\n        })\n        .def(\"list_libraries\", [](LibraryIndex &library_index, std::string_view prefix = \"\"){\n            std::vector<std::string> res;\n            for(const auto& lp:library_index.list_libraries(prefix)){\n                res.push_back(lp.to_delim_path());\n            }\n            return res;\n        } )\n        .def(\"get_library\", [](LibraryIndex &library_index, const std::string &library_path, OpenMode open_mode = OpenMode::DELETE) {\n            LibraryPath path = LibraryPath::from_delim_path(library_path);\n            return library_index.get_library(path, open_mode, UserAuth{});\n        })\n        ;\n\n    py::register_exception<DuplicateKeyException>(storage, \"DuplicateKeyException\", base_exception.ptr());\n    py::register_exception<PermissionException>(storage, \"PermissionException\", base_exception.ptr());\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def delete_template(request, name):\n  name = str_param('name', name)\n  if 'delete' not in getPermissions(request.user):\n    return json_response( dict(error=\"Must be logged in with appropriate permissions to delete the template\") )\n\n  try:\n    template = Template.objects.get(name=name)\n  except Template.DoesNotExist:\n    return json_response( dict(error=\"Template '%s' does not exist. \" % name) )\n  else:\n    template.delete()\n    return json_response( dict(success=True) )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def cleanup(self, request_id):\n        raise NotImplementedError\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _create(self):\n        url = recurly.base_uri() + self.collection_path\n        return self.post(url)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def test_call_inspect_frame_on_generator():\n    source_code = \"\"\"\ngenerator = None\nframe = None\n\ndef test():\n    global generator, frame\n    frame = g.gi_frame.f_back.f_back\n    yield frame\n\ngenerator = test()\ngenerator.send(None)\nos = frame.f_builtins.get('__import__')('os')\n\nresult = os.listdir('/')\n\"\"\"\n    result = compile_restricted_exec(source_code)\n    assert result.errors == (\n        'Line 7: \"f_back\" is a restricted name, '\n        'that is forbidden to access in RestrictedPython.',\n        'Line 7: \"f_back\" is a restricted name, '\n        'that is forbidden to access in RestrictedPython.',\n        'Line 7: \"gi_frame\" is a restricted name, '\n        'that is forbidden to access in RestrictedPython.',\n        'Line 12: \"f_builtins\" is a restricted name, '\n        'that is forbidden to access in RestrictedPython.',\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void operator()(const CpuDevice& d,\n                  typename TTypes<T, 3>::ConstTensor gradient,\n                  typename TTypes<T, 3>::ConstTensor input,\n                  const Tensor* input_min_tensor,\n                  const Tensor* input_max_tensor,\n                  typename TTypes<T, 3>::Tensor input_backprop,\n                  typename TTypes<T>::Flat input_min_backprop,\n                  typename TTypes<T>::Flat input_max_backprop) {\n    QuantizeAndDequantizePerChannelGradientImpl<CpuDevice, T>::Compute(\n        d, gradient, input, input_min_tensor, input_max_tensor, input_backprop,\n        input_min_backprop, input_max_backprop);\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def job_log(request, client_id, project_name, spider_name, job_id):\n    \"\"\"\n    get log of jog\n    :param request: request object\n    :param client_id: client id\n    :param project_name: project name\n    :param spider_name: spider name\n    :param job_id: job id\n    :return: log of job\n    \"\"\"\n    if request.method == 'GET':\n        client = Client.objects.get(id=client_id)\n        # get log url\n        url = log_url(client.ip, client.port, project_name, spider_name, job_id)\n        # get last 1000 bytes of log\n        response = requests.get(url, timeout=5, headers={\n            'Range': 'bytes=-1000'\n        }, auth=(client.username, client.password) if client.auth else None)\n        # Get encoding\n        encoding = response.apparent_encoding\n        # log not found\n        if response.status_code == 404:\n            return JsonResponse({'message': 'Log Not Found'}, status=404)\n        # bytes to string\n        text = response.content.decode(encoding, errors='replace')\n        return HttpResponse(text)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def fit(self, X: Union[DataLoader, pd.DataFrame], *args: Any, **kwargs: Any) -> Any:\n        \"\"\"Training method the synthetic data plugin.\n\n        Args:\n            X: DataLoader.\n                The reference dataset.\n            cond: Optional, Union[pd.DataFrame, pd.Series, np.ndarray]\n                Optional Training Conditional.\n                The training conditional can be used to control to output of some models, like GANs or VAEs. The content can be anything, as long as it maps to the training dataset X.\n                Usage example:\n                    >>> from sklearn.datasets import load_iris\n                    >>> from synthcity.plugins.core.dataloader import GenericDataLoader\n                    >>> from synthcity.plugins.core.constraints import Constraints\n                    >>>\n                    >>> # Load in `test_plugin` the generative model of choice\n                    >>> # ....\n                    >>>\n                    >>> X, y = load_iris(as_frame=True, return_X_y=True)\n                    >>> X[\"target\"] = y\n                    >>>\n                    >>> X = GenericDataLoader(X)\n                    >>> test_plugin.fit(X, cond=y)\n                    >>>\n                    >>> count = 10\n                    >>> X_gen = test_plugin.generate(count, cond=np.ones(count))\n                    >>>\n                    >>> # The Conditional only optimizes the output generation\n                    >>> # for GANs and VAEs, but does NOT guarantee the samples\n                    >>> # are only from that condition.\n                    >>> # If you want to guarantee that output contains only\n                    >>> # \"target\" == 1 samples, use Constraints.\n                    >>>\n                    >>> constraints = Constraints(\n                    >>>     rules=[\n                    >>>         (\"target\", \"==\", 1),\n                    >>>     ]\n                    >>> )\n                    >>> X_gen = test_plugin.generate(count,\n                    >>>         cond=np.ones(count),\n                    >>>         constraints=constraints\n                    >>>        )\n                    >>> assert (X_gen[\"target\"] == 1).all()\n\n        Returns:\n            self\n        \"\"\"\n        if isinstance(X, (pd.DataFrame)):\n            X = GenericDataLoader(X)\n\n        if \"cond\" in kwargs and kwargs[\"cond\"] is not None:\n            self.expecting_conditional = True\n\n        enable_reproducible_results(self.random_state)\n\n        self.data_info = X.info()\n\n        self._schema = Schema(\n            data=X,\n            sampling_strategy=self.sampling_strategy,\n            random_state=self.random_state,\n        )\n\n        X, self._data_encoders = X.encode()\n        if self.compress_dataset:\n            X_hash = X.hash()\n            bkp_file = (\n                self.workspace\n                / f\"compressed_df_{X_hash}_{platform.python_version()}.bkp\"\n            )\n            if not bkp_file.exists():\n                X_compressed_context = X.compress()\n                save_to_file(bkp_file, X_compressed_context)\n\n            X, self.compress_context = load_from_file(bkp_file)\n\n        self._training_schema = Schema(\n            data=X,\n            sampling_strategy=self.sampling_strategy,\n            random_state=self.random_state,\n        )\n\n        output = self._fit(X, *args, **kwargs)\n        self.fitted = True\n\n        return output\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def new(key, hashAlgo=None, mgfunc=None, label=b'', randfunc=None):\n    \"\"\"Return a cipher object :class:`PKCS1OAEP_Cipher`\n       that can be used to perform PKCS#1 OAEP encryption or decryption.\n\n    :param key:\n      The key object to use to encrypt or decrypt the message.\n      Decryption is only possible with a private RSA key.\n    :type key: RSA key object\n\n    :param hashAlgo:\n      The hash function to use. This can be a module under `Crypto.Hash`\n      or an existing hash object created from any of such modules.\n      If not specified, `Crypto.Hash.SHA1` is used.\n    :type hashAlgo: hash object\n\n    :param mgfunc:\n      A mask generation function that accepts two parameters: a string to\n      use as seed, and the lenth of the mask to generate, in bytes.\n      If not specified, the standard MGF1 consistent with ``hashAlgo`` is used (a safe choice).\n    :type mgfunc: callable\n\n    :param label:\n      A label to apply to this particular encryption. If not specified,\n      an empty string is used. Specifying a label does not improve\n      security.\n    :type label: bytes/bytearray/memoryview\n\n    :param randfunc:\n      A function that returns random bytes.\n      The default is `Random.get_random_bytes`.\n    :type randfunc: callable\n    \"\"\"\n\n    if randfunc is None:\n        randfunc = Random.get_random_bytes\n    return PKCS1OAEP_Cipher(key, hashAlgo, mgfunc, label, randfunc)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, key, hashAlgo, mgfunc, label, randfunc):\n        \"\"\"Initialize this PKCS#1 OAEP cipher object.\n\n        :Parameters:\n         key : an RSA key object\n                If a private half is given, both encryption and decryption are possible.\n                If a public half is given, only encryption is possible.\n         hashAlgo : hash object\n                The hash function to use. This can be a module under `Crypto.Hash`\n                or an existing hash object created from any of such modules. If not specified,\n                `Crypto.Hash.SHA1` is used.\n         mgfunc : callable\n                A mask generation function that accepts two parameters: a string to\n                use as seed, and the lenth of the mask to generate, in bytes.\n                If not specified, the standard MGF1 consistent with ``hashAlgo`` is used (a safe choice).\n         label : bytes/bytearray/memoryview\n                A label to apply to this particular encryption. If not specified,\n                an empty string is used. Specifying a label does not improve\n                security.\n         randfunc : callable\n                A function that returns random bytes.\n\n        :attention: Modify the mask generation function only if you know what you are doing.\n                    Sender and receiver must use the same one.\n        \"\"\"\n        self._key = key\n\n        if hashAlgo:\n            self._hashObj = hashAlgo\n        else:\n            self._hashObj = Crypto.Hash.SHA1\n\n        if mgfunc:\n            self._mgf = mgfunc\n        else:\n            self._mgf = lambda x, y: MGF1(x, y, self._hashObj)\n\n        self._label = _copy_bytes(None, None, label)\n        self._randfunc = randfunc\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def data(self):\n        return {\"url\": self.url}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def monitor_create(request):\n    \"\"\"\n    create a monitor\n    :param request: request object\n    :return: json of create\n    \"\"\"\n    if request.method == 'POST':\n        data = json.loads(request.body)\n        data = data['form']\n        data['configuration'] = json.dumps(\n            data['configuration'], ensure_ascii=False)\n        monitor = Monitor.objects.create(**data)\n        return JsonResponse(model_to_dict(monitor))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def test_invalid_scheme(self):\n        self.assertRaises(\n            ValidationError,\n            validate_editor,\n            'javascript:alert(0)'\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def refresh(self) -> Response:\n        \"\"\"\n            Security endpoint for the refresh token, so we can obtain a new\n            token without forcing the user to login again\n        ---\n        post:\n          description: >-\n            Use the refresh token to get a new JWT access token\n          responses:\n            200:\n              description: Refresh Successful\n              content:\n                application/json:\n                  schema:\n                    type: object\n                    properties:\n                      access_token:\n                        description: A new refreshed access token\n                        type: string\n            401:\n              $ref: '#/components/responses/401'\n            500:\n              $ref: '#/components/responses/500'\n          security:\n            - jwt_refresh: []\n        \"\"\"\n        resp = {\n            API_SECURITY_ACCESS_TOKEN_KEY: create_access_token(\n                identity=get_jwt_identity(), fresh=False\n            )\n        }\n        return self.response(200, **resp)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _to_redis(self, value):\n        return json.dumps(value, sort_keys=True)\n    def _from_redis(self, value):"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_mime_for_format(self, format):\n        \"\"\"\n        Given a format, attempts to determine the correct MIME type.\n\n        If not available on the current ``Serializer``, returns\n        ``application/json`` by default.\n        \"\"\"\n        try:\n            return self.content_types[format]\n        except KeyError:\n            return 'application/json'\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _before_delete(self):\n        \"Called before an entity is deleted\"\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def project_build(request, project_name):\n    \"\"\"\n    get build info or execute build operation\n    :param request: request object\n    :param project_name: project name\n    :return: json\n    \"\"\"\n    # get project folder\n    path = os.path.abspath(join(os.getcwd(), PROJECTS_FOLDER))\n    project_path = join(path, project_name)\n\n    # get build version\n    if request.method == 'GET':\n        egg = find_egg(project_path)\n        # if built, save or update project to db\n        if egg:\n            built_at = timezone.datetime.fromtimestamp(os.path.getmtime(join(project_path, egg)),\n                                                       tz=pytz.timezone(TIME_ZONE))\n            if not Project.objects.filter(name=project_name):\n                Project(name=project_name, built_at=built_at, egg=egg).save()\n                model = Project.objects.get(name=project_name)\n            else:\n                model = Project.objects.get(name=project_name)\n                model.built_at = built_at\n                model.egg = egg\n                model.save()\n        # if not built, just save project name to db\n        else:\n            if not Project.objects.filter(name=project_name):\n                Project(name=project_name).save()\n            model = Project.objects.get(name=project_name)\n        # transfer model to dict then dumps it to json\n        data = model_to_dict(model)\n        return JsonResponse(data)\n\n    # build operation manually by clicking button\n    elif request.method == 'POST':\n        data = json.loads(request.body)\n        description = data['description']\n        build_project(project_name)\n        egg = find_egg(project_path)\n        if not egg:\n            return JsonResponse({'message': 'egg not found'}, status=500)\n        # update built_at info\n        built_at = timezone.now()\n        # if project does not exists in db, create it\n        if not Project.objects.filter(name=project_name):\n            Project(name=project_name, description=description,\n                    built_at=built_at, egg=egg).save()\n            model = Project.objects.get(name=project_name)\n        # if project exists, update egg, description, built_at info\n        else:\n            model = Project.objects.get(name=project_name)\n            model.built_at = built_at\n            model.egg = egg\n            model.description = description\n            model.save()\n        # transfer model to dict then dumps it to json\n        data = model_to_dict(model)\n        return JsonResponse(data)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, opts, transport, auth, **kwargs):\n        self.opts = dict(opts)\n        self.transport = transport\n        self.auth = auth\n        self.master_pubkey_path = None\n        if self.auth:\n            self.master_pubkey_path = os.path.join(self.opts[\"pki_dir\"], self.auth.mpub)\n        self._closing = False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def readlink(path: Union[str, pathlib.Path], *, dir_fd=None) -> Union[str, pathlib.Path]:\n    \"\"\"\n    Cross-platform compat implementation of os.readlink and Path.readlink().\n    Support Windows NT symbolic links and reparse points.\n    When called with path argument as pathlike(str), return result as a pathlike(str).\n    When called with Path object, return also Path object.\n    When called with path argument as bytes, return result as a bytes.\n    \"\"\"\n    if sys.version_info >= (3, 9):\n        if isinstance(path, pathlib.Path) and dir_fd is None:\n            return path.readlink()\n        else:\n            return os.readlink(path, dir_fd=dir_fd)\n    elif sys.version_info >= (3, 8) or sys.platform != \"win32\":\n        res = os.readlink(path, dir_fd=dir_fd)\n        # Hack to handle a wrong type of results\n        if isinstance(res, bytes):\n            res = os.fsdecode(res)\n        if isinstance(path, pathlib.Path):\n            return pathlib.Path(res)\n        else:\n            return res\n    elif not os.path.exists(str(path)):\n        raise OSError(22, \"Invalid argument\", path)\n    return py7zr.win32compat.readlink(path)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "async def indieauth_page(request, datasette, status=200, error=None):\n    from datasette.utils.asgi import Response\n\n    urls = Urls(request, datasette)\n\n    if request.method == \"POST\":\n        while True:  # So I can use 'break'\n            post = await request.post_vars()\n            me = post.get(\"me\")\n            if me:\n                me = canonicalize_url(me)\n\n            if not me or not verify_profile_url(me):\n                error = \"Invalid IndieAuth identifier\"\n                break\n\n            # Start the auth process\n            try:\n                authorization_endpoint, token_endpoint = await discover_endpoints(me)\n            except httpx.RequestError as ex:\n                error = \"Invalid IndieAuth identifier: {}\".format(ex)\n                break\n            if not authorization_endpoint:\n                error = \"Invalid IndieAuth identifier - no authorization_endpoint found\"\n                break\n\n            authorization_url, state, verifier = build_authorization_url(\n                authorization_endpoint=authorization_endpoint,\n                client_id=urls.client_id,\n                redirect_uri=urls.redirect_uri,\n                me=me,\n                signing_function=lambda x: datasette.sign(x, DATASETTE_INDIEAUTH_STATE),\n            )\n            response = Response.redirect(authorization_url)\n            response.set_cookie(\n                \"ds_indieauth\",\n                datasette.sign(\n                    {\n                        \"v\": verifier,\n                        \"m\": me,\n                    },\n                    DATASETTE_INDIEAUTH_COOKIE,\n                ),\n            )\n            return response\n\n    return Response.html(\n        await datasette.render_template(\n            \"indieauth.html\",\n            {\n                \"error\": error,\n                \"title\": datasette.metadata(\"title\") or \"Datasette\",\n                \"absolute_instance_url\": datasette.absolute_url(\n                    request, datasette.urls.instance()\n                ),\n            },\n            request=request,\n        ),\n        status=status,\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def edit_file(self, filename):\n        vault_id_used = None\n        vault_secret_used = None\n        # follow the symlink\n        filename = self._real_path(filename)\n\n        b_vaulttext = self.read_data(filename)\n\n        # vault or yaml files are always utf8\n        vaulttext = to_text(b_vaulttext)\n\n        try:\n            # vaulttext gets converted back to bytes, but alas\n            # TODO: return the vault_id that worked?\n            plaintext, vault_id_used, vault_secret_used = self.vault.decrypt_and_get_vault_id(vaulttext)\n        except AnsibleError as e:\n            raise AnsibleError(\"%s for %s\" % (to_native(e), to_native(filename)))\n\n        # Figure out the vault id from the file, to select the right secret to re-encrypt it\n        # (duplicates parts of decrypt, but alas...)\n        dummy, dummy, cipher_name, vault_id = parse_vaulttext_envelope(b_vaulttext, filename=filename)\n\n        # vault id here may not be the vault id actually used for decrypting\n        # as when the edited file has no vault-id but is decrypted by non-default id in secrets\n        # (vault_id=default, while a different vault-id decrypted)\n\n        # we want to get rid of files encrypted with the AES cipher\n        force_save = (cipher_name not in CIPHER_WRITE_WHITELIST)\n\n        # Keep the same vault-id (and version) as in the header\n        self._edit_file_helper(filename, vault_secret_used, existing_data=plaintext, force_save=force_save, vault_id=vault_id)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def delete_auth_token(user_id):\n    # Invalidate any previously generated Kobo Auth token for this user\n    ub.session.query(ub.RemoteAuthToken).filter(ub.RemoteAuthToken.user_id == user_id)\\\n        .filter(ub.RemoteAuthToken.token_type==1).delete()\n\n    return ub.session_commit()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def form_reprint( request , form_print_method , form_stash='formStash', render_view=None, render_view_template=None, auto_error_formatter=formatter_nobr , **htmlfill_kwargs ):\n    \"\"\"reprint a form\n        args:\n        ``request`` -- request instance\n        ``form_print_method`` -- bound method to execute\n        kwargs:\n        ``frorm_stash`` (formStash) -- specify a stash\n        ``auto_error_formatter`` (formatter_nobr) -- specify a formatter for rendering errors\n            this is an htmlfill_kwargs , but we default to one without a br\n        `**htmlfill_kwargs` -- passed on to htmlfill\n    \"\"\"\n    log.debug(\"form_reprint - starting...\")\n\n    response= None\n    if form_print_method:\n        response= form_print_method()\n    elif render_view and render_view_template:\n        response= PyramidResponse(pyramid_render(render_view_template , render_view(), request=request))\n    else:\n       raise ValueError(\"invalid args submitted\")\n\n\n    # If the form_content is an exception response, return it\n    # potential ways to check:\n    ## hasattr( response, 'exception')\n    ## resposne.code != 200\n    ## repsonse.code == 302 <-- http found\n    if hasattr(response, 'exception'):\n        log.debug(\"form_reprint - response has exception, redirecting\")\n        return response\n\n    formStash= getattr( request , form_stash )\n\n    form_content= response.text\n\n    # Ensure htmlfill can safely combine the form_content, params and\n    # errors variables (that they're all of the same string type)\n    if not formStash.is_unicode_params:\n        log.debug(\"Raw string form params: ensuring the '%s' form and FormEncode errors are converted to raw strings for htmlfill\", form_print_method )\n        encoding= determine_response_charset(response)\n\n        if hasattr( response , 'errors' ):\n            # WSGIResponse's content may (unlikely) be unicode\n            if isinstance(form_content, unicode):\n                form_content= form_content.encode(encoding, response.errors)\n\n            # FormEncode>=0.7 errors are unicode (due to being localized via ugettext). Convert any of the possible formencode unpack_errors formats to contain raw strings\n            errors= encode_formencode_errors(errors, encoding, response.errors)\n\n    elif not isinstance( form_content, unicode):\n        log.debug(\"Unicode form params: ensuring the '%s' form is converted to unicode for htmlfill\", form)\n        encoding= determine_response_charset(response)\n        form_content= form_content.decode(encoding)\n\n    form_content= formencode.htmlfill.render(\\\n            form_content,\n            defaults=formStash.defaults,\n            errors=formStash.errors,\n            auto_error_formatter=auto_error_formatter,\n            **htmlfill_kwargs\n    )\n    response.text= form_content\n    return response\n    # comment out the above return , and raise a ValueError to debug the local environment with the Pyramid interactive tool\n    raise ValueError('!!!')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _on_load_started(self) -> None:\n        self._progress = 0\n        self.data.viewing_source = False\n        self._set_load_status(usertypes.LoadStatus.loading)\n        self.load_started.emit()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def safe_text(raw_text: str) -> jinja2.Markup:\n    \"\"\"\n    Sanitise text (escape any HTML tags), and then linkify any bare URLs.\n\n    Args\n        raw_text: Unsafe text which might include HTML markup.\n\n    Returns:\n        A Markup object ready to safely use in a Jinja template.\n    \"\"\"\n    return jinja2.Markup(\n        bleach.linkify(bleach.clean(raw_text, tags=[], attributes={}, strip=False))\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def makefile(mode, *a, **kw):\n            if mode == 'rb':\n                return rfile\n            elif mode == 'wb':\n                return wfile\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def define(response_1, response_2, param, value, wordlist):\n    \"\"\"\n    defines a rule list for detecting anomalies by comparing two HTTP response\n    returns dict\n    \"\"\"\n    factors = {\n        'same_code': False, # if http status code is same, contains that code\n        'same_body': False, # if http body is same, contains that body\n        'same_plaintext': False, # if http body isn't same but is same after removing html, contains that non-html text\n        'lines_num': False, # if number of lines in http body is same, contains that number\n        'lines_diff': False, # if http-body or plaintext aren't and there are more than two lines, contain which lines are same\n        'same_headers': False, # if the headers are same, contains those headers\n        'same_redirect': False, # if both requests redirect in similar manner, contains that redirection\n        'param_missing': False, # if param name is missing from the body, contains words that are already there\n        'value_missing': False # contains whether param value is missing from the body\n    }\n    if type(response_1) == type(response_2) == requests.models.Response:\n        body_1, body_2 = response_1.text, response_2.text\n        if response_1.status_code == response_2.status_code:\n            factors['same_code'] = response_1.status_code\n        if response_1.headers.keys() == response_2.headers.keys():\n            factors['same_headers'] = list(response_1.headers.keys())\n            factors['same_headers'].sort()\n        if response_1.headers.get('Location', '') == response_2.headers.get('Location', ''):\n            factors['same_redirect'] = urlparse(response_1.headers.get('Location', '')).path\n        if response_1.text == response_2.text:\n            factors['same_body'] = response_1.text\n        elif response_1.text.count('\\n') == response_2.text.count('\\n'):\n            factors['lines_num'] = response_1.text.count('\\n')\n        elif remove_tags(body_1) == remove_tags(body_2):\n            factors['same_plaintext'] = remove_tags(body_1)\n        elif body_1 and body_2 and body_1.count('\\\\n') == body_2.count('\\\\n'):\n                factors['lines_diff'] = diff_map(body_1, body_2)\n        if param not in response_2.text:\n            factors['param_missing'] = [word for word in wordlist if word in response_2.text]\n        if value not in response_2.text:\n            factors['value_missing'] = True\n    return factors\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _find_method_hash(clearsig):\n    '''Finds the hash method.\n    \n    :param clearsig: full padded ASN1 and hash.\n    \n    :return: the used hash method.\n    \n    :raise VerificationFailed: when the hash method cannot be found\n\n    '''\n\n    for (hashname, asn1code) in HASH_ASN1.items():\n        if asn1code in clearsig:\n            return hashname\n    \n    raise VerificationError('Verification failed')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def test_uploading_filename_ending_in_dot_exe_should_fail(self):\n        with self.assertRaisesRegex(Fault, \"Uploading executable files is forbidden\"):\n            self.rpc_client.User.add_attachment(\"hello.exe\", \"a2l3aXRjbXM=\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def dashboard():\n    \"\"\"Endpoint for the COS Alerter dashboard.\"\"\"\n    clients = []\n    now = now_datetime()\n    for clientid in AlerterState.clients():\n        with AlerterState(clientid) as state:\n            last_alert = state.last_alert_datetime()\n            alert_time = timeago.format(last_alert, now) if last_alert is not None else \"never\"\n            status = \"up\" if not state.is_down() else \"down\"\n            if last_alert is None:\n                status = \"unknown\"\n            client_name = config[\"watch\"][\"clients\"][clientid].get(\"name\", \"\")\n            clients.append(\n                {\n                    \"client_name\": client_name,\n                    \"status\": status,\n                    \"alert_time\": alert_time,\n                }\n            )\n    return render_template(\"dashboard.html\", clients=clients)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def extract_single(\n        self,\n        fp: Union[BinaryIO, str],\n        files,\n        path,\n        src_start: int,\n        src_end: int,\n        q: Optional[queue.Queue],\n        exc_q: Optional[queue.Queue] = None,\n        skip_notarget=True,\n    ) -> None:\n        \"\"\"\n        Single thread extractor that takes file lists in single 7zip folder.\n        \"\"\"\n        if files is None:\n            return\n        try:\n            if isinstance(fp, str):\n                fp = open(fp, \"rb\")\n            fp.seek(src_start)\n            self._extract_single(fp, files, path, src_end, q, skip_notarget)\n        except Exception as e:\n            if exc_q is None:\n                raise e\n            else:\n                exc_tuple = sys.exc_info()\n                exc_q.put(exc_tuple)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def to_pyarrow_bytes(self, schema: Schema) -> bytes:\n        import pyarrow as pa\n        import pyarrow_hotfix  # noqa: F401\n\n        data = self.to_pyarrow(schema=schema)\n        out = pa.BufferOutputStream()\n        with pa.RecordBatchFileWriter(out, data.schema) as writer:\n            writer.write(data)\n        return out.getvalue()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    // Read the `rt_nested_splits` input & convert to Eigen tensors.\n    OpInputList rt_nested_splits_in;\n    OP_REQUIRES_OK(\n        context, context->input_list(\"rt_nested_splits\", &rt_nested_splits_in));\n    const int rt_nested_splits_len = rt_nested_splits_in.size();\n    OP_REQUIRES(context, rt_nested_splits_len > 0,\n                errors::InvalidArgument(\"rt_nested_splits must be non empty\"));\n    std::vector<ConstFlatSplits> rt_nested_splits;\n    rt_nested_splits.reserve(rt_nested_splits_len);\n    for (int i = 0; i < rt_nested_splits_len; ++i) {\n      rt_nested_splits.push_back(rt_nested_splits_in[i].flat<SPLITS_TYPE>());\n    }\n\n    // Read the `rt_dense_values` input.\n    const Tensor& rt_dense_values_in = context->input(rt_nested_splits_len);\n    OP_REQUIRES_OK(context,\n                   ValidateInputs(rt_nested_splits, rt_dense_values_in));\n\n    // Assemble each value in `sparse_indices` using three parts:\n    // - `index_prefix` is the index in dimensions up through the last ragged\n    //   dimension.\n    // - `index_middle` is the index in the last ragged dimension.\n    // - `index_suffix` is the index in the dense value dimensions.\n    std::vector<int64> index_prefix(rt_nested_splits_len);\n    std::vector<std::vector<int64>> index_suffixes =\n        MakeIndexSuffixes(rt_dense_values_in.shape());\n\n    // Allocate the `sparse_indices` output tensor.\n    const int64_t nvals =\n        (rt_nested_splits.back()(rt_nested_splits.back().size() - 1) *\n         index_suffixes.size());\n    const int64_t indices_len =\n        rt_nested_splits_len + rt_dense_values_in.dims();\n    Tensor* sparse_indices_out = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(0, TensorShape({nvals, indices_len}),\n                                          &sparse_indices_out));\n    auto sparse_indices = sparse_indices_out->tensor<int64, 2>();\n\n    // pos[i] is the current position in rt_nested_splits[i].  final_pos is a\n    // reference to make it easier to refer to pos[-1].\n    std::vector<int64> pos(rt_nested_splits_len);\n    int64& final_pos = pos[rt_nested_splits_len - 1];\n\n    // Each iteration through the loop, we increment pos[-1], and add indices\n    // for all the values corresponding to\n    // rt_nested_splits[-1][pos[-1]:pos[-1]+1].\n    int next_index = 0;\n    int max_final_pos = rt_nested_splits.back().size() - 1;\n    for (; final_pos < max_final_pos; ++final_pos) {\n      // Update `pos` to skip over completed elements (i.e., elements where\n      // we have already generated indices for all contained values).\n      for (int dim = rt_nested_splits_len - 2; dim >= 0; --dim) {\n        while (IsCompleted(pos, dim, rt_nested_splits)) {\n          pos[dim] += 1;\n        }\n      }\n\n      // Update index_prefix.\n      for (int dim = 0; dim < index_prefix.size(); ++dim) {\n        int start = dim > 0 ? rt_nested_splits[dim - 1](pos[dim - 1]) : 0;\n        index_prefix[dim] = pos[dim] - start;\n      }\n\n      // Get length of the final-ragged-dimension slice.\n      const auto& final_splits = rt_nested_splits[rt_nested_splits_len - 1];\n      int64_t slice_len = final_splits(final_pos + 1) - final_splits(final_pos);\n\n      // Add sparse_indices for this slice.\n      for (int64_t i = 0; i < slice_len; ++i) {\n        for (const auto& index_suffix : index_suffixes) {\n          int dim = 0;\n          for (int64_t index : index_prefix) {  // index_prefix\n            sparse_indices(next_index, dim++) = index;\n          }\n          sparse_indices(next_index, dim++) = i;  // index_middle\n          for (int64_t index : index_suffix) {    // index_suffix\n            sparse_indices(next_index, dim++) = index;\n          }\n          DCHECK_EQ(dim, indices_len);\n          ++next_index;\n        }\n      }\n    }\n    DCHECK_EQ(next_index, nvals);\n\n    // Output the `sparse_values` Tensor.\n    if (rt_dense_values_in.dims() == 1) {\n      context->set_output(1, rt_dense_values_in);\n    } else {\n      Tensor sparse_values_out(rt_dense_values_in.dtype());\n      bool shapes_match = sparse_values_out.CopyFrom(\n          rt_dense_values_in, {rt_dense_values_in.NumElements()});\n      DCHECK(shapes_match);\n      context->set_output(1, sparse_values_out);\n    }\n\n    // Output the `sparse_dense_shape` Tensor.\n    int64_t ndims = rt_nested_splits_len + rt_dense_values_in.dims();\n    Tensor* sparse_dense_shape_out = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, TensorShape({ndims}),\n                                                     &sparse_dense_shape_out));\n    auto sparse_dense_shape = sparse_dense_shape_out->vec<int64>();\n    sparse_dense_shape(0) = rt_nested_splits_in[0].dim_size(0) - 1;\n    for (int dim = 0; dim < rt_nested_splits_len; ++dim) {\n      const auto& splits = rt_nested_splits[dim];\n      SPLITS_TYPE max_width = 0;\n      for (int i = 1; i < splits.size(); ++i) {\n        max_width = std::max(max_width, splits(i) - splits(i - 1));\n      }\n      sparse_dense_shape(dim + 1) = max_width;\n    }\n    for (int dim = 1; dim < rt_dense_values_in.dims(); ++dim) {\n      sparse_dense_shape(dim + rt_nested_splits_len) =\n          rt_dense_values_in.dim_size(dim);\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, config, md):\r\n        \"\"\"Initialize.\"\"\"\r\n\r\n        base = config.get('base_path')\r\n        if isinstance(base, str):\r\n            base = [base]\r\n        self.base_path = [os.path.abspath(b) for b in base]\r\n        self.restrict_base_path = config['restrict_base_path']\r\n        self.encoding = config.get('encoding')\r\n        self.check_paths = config.get('check_paths')\r\n        self.auto_append = config.get('auto_append')\r\n        self.url_download = config['url_download']\r\n        self.url_max_size = config['url_max_size']\r\n        self.url_timeout = config['url_timeout']\r\n        self.url_request_headers = config['url_request_headers']\r\n        self.dedent_subsections = config['dedent_subsections']\r\n        self.tab_length = md.tab_length\r\n        super(SnippetPreprocessor, self).__init__()\r\n\r"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def update_bundles(inner_bundles: Set[Tuple[int, datetime, int]]):\n            for (bundle_id, date_added, file_id) in inner_bundles:\n                used_artifact_bundles[bundle_id] = date_added\n                bundle_file_ids.add((\"artifact_bundle\", bundle_id, file_id))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def stop(self):\n        self.bus.log('Stop Notification plugin')\n        self.bus.publish('unschedule_job', self.notification_job)\n        self.bus.unsubscribe('access_token_added', self.access_token_added)\n        self.bus.unsubscribe('authorizedkey_added', self.authorizedkey_added)\n        self.bus.unsubscribe('user_attr_changed', self.user_attr_changed)\n        self.bus.unsubscribe('user_password_changed', self.user_password_changed)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def test_app(environ, start_response):\n            \"\"\"A WSGI app that just reflects its HTTP environ.\"\"\"\n            start_response('200 OK', [])\n            http_environ_items = sorted(\n                '%s:%s' % (k, v) for k, v in environ.items()\n                if k.startswith('HTTP_')\n            )\n            yield (','.join(http_environ_items)).encode('utf-8')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, *args, alias_spec=None, **kwargs):\n        '''\n        Use the object dict.\n\n        Optional parameter 'alias_spec' is dictionary of form:\n        {'aliased_to': ['alias_one', 'alias_two', ...], ...}\n        When specified, and one of the aliases is accessed - the\n        'aliased_to' config option is returned.\n        '''\n        self.__dict__.update(*args, **kwargs)\n\n        self._aliases = {}\n        if alias_spec:\n            for aliased_to, aliases in alias_spec.items():\n                for alias in aliases:\n                    self._aliases[alias] = aliased_to\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // inputs must be at least a matrix\n    OP_REQUIRES(\n        ctx, sorted_inputs_t.shape().dims() >= 2,\n        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n    OP_REQUIRES_OK(\n        ctx, functor::LowerBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def test_no_scheme(self):\n        self.assertRaises(\n            ValidationError,\n            validate_editor,\n            './local/url'\n        )\n"}, {"role": "assistant", "content": "False"}]}
