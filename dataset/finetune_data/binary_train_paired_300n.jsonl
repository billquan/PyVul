{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def failure(cls, request):\n        '''\n        Creates an RPCMessage instance containing exception information,\n        when the exception is from user-defined handlers or upper adaptation layers.\n\n        It must be called in an exception handler context, where ``sys.exc_info()``\n        returns a non-null tuple.\n        '''\n        exc_info = sys.exc_info()\n        return cls(\n            request.peer_id,\n            RPCMessageTypes.FAILURE,\n            request.method, request.order_key, request.seq_id,\n            ErrorMetadata(exc_info[0].__name__, traceback.format_exc()),\n            None,\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_send_leave_request(self, origin, content, room_id):\n        logger.debug(\"on_send_leave_request: content: %s\", content)\n\n        room_version = yield self.store.get_room_version(room_id)\n        format_ver = room_version_to_event_format(room_version)\n        pdu = event_from_pdu_json(content, format_ver)\n\n        origin_host, _ = parse_server_name(origin)\n        yield self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_leave_request: pdu sigs: %s\", pdu.signatures)\n        yield self.handler.on_send_leave_request(origin, pdu)\n        return 200, {}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& indices_tensor = ctx->input(0);\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(indices_tensor.shape()) ||\n                    TensorShapeUtils::IsScalar(indices_tensor.shape()),\n                errors::InvalidArgument(\n                    \"The indices can only be scalar or vector, got \\\"\",\n                    indices_tensor.shape().DebugString(), \"\\\"\"));\n\n    const Tensor& dims_tensor = ctx->input(1);\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(dims_tensor.shape()),\n        errors::InvalidArgument(\"The indices can only be 1-D, got \\\"\",\n                                dims_tensor.shape().DebugString(), \"\\\"\"));\n\n    auto dims = dims_tensor.vec<Tidx>();\n\n    // Chek to make sure indices is not out of boundary\n    Eigen::Tensor<Tidx, 0, Eigen::RowMajor> dims_prod_eigen = dims.prod();\n    Tidx dims_prod = dims_prod_eigen();\n    const Tidx* indices = indices_tensor.flat<Tidx>().data();\n    int64 size = indices_tensor.NumElements();\n    bool check = std::all_of(indices, indices + size,\n                             [&](Tidx index) { return index < dims_prod; });\n    OP_REQUIRES(ctx, check,\n                errors::InvalidArgument(\"index is out of bound as with dims\"));\n\n    Eigen::array<bool, 1> reverse({true});\n\n    Tensor strides_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<Tidx>::value,\n                                      TensorShape({dims_tensor.NumElements()}),\n                                      &strides_tensor));\n\n    auto strides = strides_tensor.vec<Tidx>();\n    strides = dims.reverse(reverse)\n                  .scan(0, Eigen::internal::ProdReducer<Tidx>(), false)\n                  .reverse(reverse);\n\n    Tensor strides_shifted_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<Tidx>::value,\n                                      TensorShape({dims_tensor.NumElements()}),\n                                      &strides_shifted_tensor));\n\n    auto strides_shifted = strides_shifted_tensor.vec<Tidx>();\n    strides_shifted = dims.reverse(reverse)\n                          .scan(0, Eigen::internal::ProdReducer<Tidx>(), true)\n                          .reverse(reverse);\n\n    Tensor* output_tensor = nullptr;\n    if (TensorShapeUtils::IsScalar(indices_tensor.shape())) {\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({dims_tensor.NumElements()}),\n                                    &output_tensor));\n\n      auto output = output_tensor->vec<Tidx>();\n\n      output = output.constant(indices_tensor.scalar<Tidx>()());\n      output = output.binaryExpr(strides, mod_op<Tidx>()) / strides_shifted;\n    } else {\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0,\n                                    TensorShape({dims_tensor.NumElements(),\n                                                 indices_tensor.NumElements()}),\n                                    &output_tensor));\n\n      auto output = output_tensor->matrix<Tidx>();\n\n      Eigen::array<Eigen::Index, 2> reshape{\n          {static_cast<Eigen::Index>(dims_tensor.NumElements()), 1}};\n      Eigen::array<Eigen::Index, 2> bcast(\n          {1, static_cast<Eigen::Index>(indices_tensor.NumElements())});\n      Eigen::array<Eigen::Index, 2> indices_reshape{\n          {1, static_cast<Eigen::Index>(indices_tensor.NumElements())}};\n      Eigen::array<Eigen::Index, 2> indices_bcast(\n          {static_cast<Eigen::Index>(dims_tensor.NumElements()), 1});\n\n      output = indices_tensor.vec<Tidx>()\n                   .reshape(indices_reshape)\n                   .broadcast(indices_bcast);\n      output = output.binaryExpr(strides.reshape(reshape).broadcast(bcast),\n                                 mod_op<Tidx>()) /\n               strides_shifted.reshape(reshape).broadcast(bcast);\n    }\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def callback(_, pdu, redacted):\n            with logcontext.PreserveLoggingContext(ctx):\n                if not check_event_content_hash(pdu):\n                    logger.warn(\n                        \"Event content has been tampered, redacting %s: %s\",\n                        pdu.event_id, pdu.get_pdu_json()\n                    )\n                    return redacted\n\n                if self.spam_checker.check_event_for_spam(pdu):\n                    logger.warn(\n                        \"Event contains spam, redacting %s: %s\",\n                        pdu.event_id, pdu.get_pdu_json()\n                    )\n                    return redacted\n\n                return pdu\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse(self):\n        \"\"\"\n        Parse the POST data and break it into a FILES MultiValueDict and a POST\n        MultiValueDict.\n\n        Return a tuple containing the POST and FILES dictionary, respectively.\n        \"\"\"\n        from django.http import QueryDict\n\n        encoding = self._encoding\n        handlers = self._upload_handlers\n\n        # HTTP spec says that Content-Length >= 0 is valid\n        # handling content-length == 0 before continuing\n        if self._content_length == 0:\n            return QueryDict(encoding=self._encoding), MultiValueDict()\n\n        # See if any of the handlers take care of the parsing.\n        # This allows overriding everything if need be.\n        for handler in handlers:\n            result = handler.handle_raw_input(\n                self._input_data,\n                self._meta,\n                self._content_length,\n                self._boundary,\n                encoding,\n            )\n            # Check to see if it was handled\n            if result is not None:\n                return result[0], result[1]\n\n        # Create the data structures to be used later.\n        self._post = QueryDict(mutable=True)\n        self._files = MultiValueDict()\n\n        # Instantiate the parser and stream:\n        stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n        # Whether or not to signal a file-completion at the beginning of the loop.\n        old_field_name = None\n        counters = [0] * len(handlers)\n\n        # Number of bytes that have been read.\n        num_bytes_read = 0\n        # To count the number of keys in the request.\n        num_post_keys = 0\n        # To limit the amount of data read from the request.\n        read_size = None\n\n        try:\n            for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                if old_field_name:\n                    # We run this at the beginning of the next loop\n                    # since we cannot be sure a file is complete until\n                    # we hit the next boundary/part of the multipart content.\n                    self.handle_file_complete(old_field_name, counters)\n                    old_field_name = None\n\n                try:\n                    disposition = meta_data['content-disposition'][1]\n                    field_name = disposition['name'].strip()\n                except (KeyError, IndexError, AttributeError):\n                    continue\n\n                transfer_encoding = meta_data.get('content-transfer-encoding')\n                if transfer_encoding is not None:\n                    transfer_encoding = transfer_encoding[0].strip()\n                field_name = force_text(field_name, encoding, errors='replace')\n\n                if item_type == FIELD:\n                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                    num_post_keys += 1\n                    if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and\n                            settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):\n                        raise TooManyFieldsSent(\n                            'The number of GET/POST parameters exceeded '\n                            'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'\n                        )\n\n                    # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                    if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                        read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n\n                    # This is a post field, we can just set it in the post\n                    if transfer_encoding == 'base64':\n                        raw_data = field_stream.read(size=read_size)\n                        num_bytes_read += len(raw_data)\n                        try:\n                            data = base64.b64decode(raw_data)\n                        except binascii.Error:\n                            data = raw_data\n                    else:\n                        data = field_stream.read(size=read_size)\n                        num_bytes_read += len(data)\n\n                    # Add two here to make the check consistent with the\n                    # x-www-form-urlencoded check that includes '&='.\n                    num_bytes_read += len(field_name) + 2\n                    if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and\n                            num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):\n                        raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')\n\n                    self._post.appendlist(field_name, force_text(data, encoding, errors='replace'))\n                elif item_type == FILE:\n                    # This is a file, use the handler...\n                    file_name = disposition.get('filename')\n                    if file_name:\n                        file_name = force_text(file_name, encoding, errors='replace')\n                        file_name = self.sanitize_file_name(file_name)\n                    if not file_name:\n                        continue\n\n                    content_type, content_type_extra = meta_data.get('content-type', ('', {}))\n                    content_type = content_type.strip()\n                    charset = content_type_extra.get('charset')\n\n                    try:\n                        content_length = int(meta_data.get('content-length')[0])\n                    except (IndexError, TypeError, ValueError):\n                        content_length = None\n\n                    counters = [0] * len(handlers)\n                    try:\n                        for handler in handlers:\n                            try:\n                                handler.new_file(\n                                    field_name, file_name, content_type,\n                                    content_length, charset, content_type_extra,\n                                )\n                            except StopFutureHandlers:\n                                break\n\n                        for chunk in field_stream:\n                            if transfer_encoding == 'base64':\n                                # We only special-case base64 transfer encoding\n                                # We should always decode base64 chunks by multiple of 4,\n                                # ignoring whitespace.\n\n                                stripped_chunk = b\"\".join(chunk.split())\n\n                                remaining = len(stripped_chunk) % 4\n                                while remaining != 0:\n                                    over_chunk = field_stream.read(4 - remaining)\n                                    stripped_chunk += b\"\".join(over_chunk.split())\n                                    remaining = len(stripped_chunk) % 4\n\n                                try:\n                                    chunk = base64.b64decode(stripped_chunk)\n                                except Exception as exc:\n                                    # Since this is only a chunk, any error is an unfixable error.\n                                    raise MultiPartParserError(\"Could not decode base64 data.\") from exc\n\n                            for i, handler in enumerate(handlers):\n                                chunk_length = len(chunk)\n                                chunk = handler.receive_data_chunk(chunk, counters[i])\n                                counters[i] += chunk_length\n                                if chunk is None:\n                                    # Don't continue if the chunk received by\n                                    # the handler is None.\n                                    break\n\n                    except SkipFile:\n                        self._close_files()\n                        # Just use up the rest of this file...\n                        exhaust(field_stream)\n                    else:\n                        # Handle file upload completions on next iteration.\n                        old_field_name = field_name\n                else:\n                    # If this is neither a FIELD or a FILE, just exhaust the stream.\n                    exhaust(stream)\n        except StopUpload as e:\n            self._close_files()\n            if not e.connection_reset:\n                exhaust(self._input_data)\n        else:\n            # Make sure that the request data is all fed\n            exhaust(self._input_data)\n\n        # Signal that the upload has completed.\n        # any() shortcircuits if a handler's upload_complete() returns a value.\n        any(handler.upload_complete() for handler in handlers)\n        self._post._mutable = False\n        return self._post, self._files\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def get_file(\n        self,\n        url: str,\n        output_stream: BinaryIO,\n        max_size: Optional[int] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:\n        \"\"\"GETs a file from a given URL\n        Args:\n            url: The URL to GET\n            output_stream: File to write the response body to.\n            headers: A map from header name to a list of values for that header\n        Returns:\n            A tuple of the file length, dict of the response\n            headers, absolute URI of the response and HTTP response code.\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            SynapseError: if the response is not a 2xx, the remote file is too large, or\n               another exception happens during the download.\n        \"\"\"\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", url, headers=Headers(actual_headers))\n\n        resp_headers = dict(response.headers.getAllRawHeaders())\n\n        if (\n            b\"Content-Length\" in resp_headers\n            and max_size\n            and int(resp_headers[b\"Content-Length\"][0]) > max_size\n        ):\n            logger.warning(\"Requested URL is too large > %r bytes\" % (max_size,))\n            raise SynapseError(\n                502,\n                \"Requested file is too large > %r bytes\" % (max_size,),\n                Codes.TOO_LARGE,\n            )\n\n        if response.code > 299:\n            logger.warning(\"Got %d when downloading %s\" % (response.code, url))\n            raise SynapseError(502, \"Got error %d\" % (response.code,), Codes.UNKNOWN)\n\n        # TODO: if our Content-Type is HTML or something, just read the first\n        # N bytes into RAM rather than saving it all to disk only to read it\n        # straight back in again\n\n        try:\n            length = await make_deferred_yieldable(\n                readBodyToFile(response, output_stream, max_size)\n            )\n        except SynapseError:\n            # This can happen e.g. because the body is too large.\n            raise\n        except Exception as e:\n            raise SynapseError(502, (\"Failed to download remote body: %s\" % e)) from e\n\n        return (\n            length,\n            resp_headers,\n            response.request.absoluteURI.decode(\"ascii\"),\n            response.code,\n        )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_safe_url(url, host=None):\n    \"\"\"\n    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to\n    a different host and uses a safe scheme).\n\n    Always returns ``False`` on an empty url.\n    \"\"\"\n    if url is not None:\n        url = url.strip()\n    if not url:\n        return False\n    # Chrome treats \\ completely as /\n    url = url.replace('\\\\', '/')\n    # Chrome considers any URL with more than two slashes to be absolute, but\n    # urlparse is not so flexible. Treat any url with three slashes as unsafe.\n    if url.startswith('///'):\n        return False\n    url_info = urlparse(url)\n    # Forbid URLs like http:///example.com - with a scheme, but without a hostname.\n    # In that URL, example.com is not the hostname but, a path component. However,\n    # Chrome will still consider example.com to be the hostname, so we must not\n    # allow this syntax.\n    if not url_info.netloc and url_info.scheme:\n        return False\n    # Forbid URLs that start with control characters. Some browsers (like\n    # Chrome) ignore quite a few control characters at the start of a\n    # URL and might consider the URL as scheme relative.\n    if unicodedata.category(url[0])[0] == 'C':\n        return False\n    return ((not url_info.netloc or url_info.netloc == host) and\n            (not url_info.scheme or url_info.scheme in ['http', 'https']))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n        key = self.make_key(key, version=version)\n        return self._cache.add(key, value, self.get_backend_timeout(timeout))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input_sizes = context->input(0);\n    const Tensor& filter = context->input(1);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(input_sizes.shape()),\n        errors::InvalidArgument(\n            \"Conv2DBackpropInput: input_sizes input must be 1-dim, not \",\n            input_sizes.dims()));\n    TensorShape input_shape;\n    const int32* in_sizes_data = input_sizes.template flat<int32>().data();\n\n    for (int i = 0; i < input_sizes.NumElements(); ++i) {\n      OP_REQUIRES(context, in_sizes_data[i] >= 0,\n                  errors::InvalidArgument(\"Dimension \", i,\n                                          \" of input_sizes must be >= 0\"));\n      input_shape.AddDim(in_sizes_data[i]);\n    }\n    const TensorShape& filter_shape = filter.shape();\n    EXTRACT_AND_VERIFY_DIMENSIONS(\"DepthwiseConv2DBackpropInput\");\n\n    Tensor* in_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, input_shape, &in_backprop));\n\n    // If there is nothing to compute, return.\n    if (input_shape.num_elements() == 0) {\n      return;\n    }\n\n    // If in_depth==1, this operation is just a standard convolution.\n    // Depthwise convolution is a special case of cuDNN's grouped convolution.\n    bool use_cudnn = std::is_same<Device, GPUDevice>::value &&\n                     (in_depth == 1 ||\n                      (use_cudnn_grouped_conv_ &&\n                       IsCudnnSupportedFilterSize(/*filter_rows=*/filter_rows,\n                                                  /*filter_cols=*/filter_cols,\n                                                  /*in_depth=*/in_depth,\n                                                  /*out_depth=*/out_depth)));\n\n    VLOG(2) << \"DepthwiseConv2dNativeBackpropInput: \"\n            << \" Input: [\" << batch << \", \" << input_rows << \", \" << input_cols\n            << \", \" << in_depth << \"]; Filter: [\" << filter_rows << \", \"\n            << filter_cols << \", \" << in_depth << \", \" << depth_multiplier\n            << \"]; Output: [\" << batch << \", \" << out_rows << \", \" << out_cols\n            << \", \" << out_depth << \"], stride = \" << stride_\n            << \", pad_rows = \" << pad_top << \", pad_cols = \" << pad_left\n            << \", Use cuDNN: \" << use_cudnn;\n\n    if (use_cudnn) {\n      // Reshape from TF depthwise filter to cuDNN grouped convolution filter:\n      //\n      //                  | TensorFlow       | cuDNN\n      // --------------------------------------------------------------------\n      // filter_out_depth | depth_multiplier | depth_multiplier * group_count\n      // filter_in_depth  | in_depth         | in_depth / group_count\n      //\n      // For depthwise convolution, we have group_count == in_depth.\n      int32_t filter_in_depth = 1;\n      TensorShape shape =\n          TensorShape{filter_rows, filter_cols, filter_in_depth, out_depth};\n      Tensor reshaped_filter(/*type=*/dtype_);\n      OP_REQUIRES(\n          context, reshaped_filter.CopyFrom(filter, shape),\n          errors::Internal(\n              \"Failed to reshape filter tensor for grouped convolution.\"));\n      // TODO(yangzihao): Send in arbitrary dilation rates after the dilated\n      // conv is supported.\n      launcher_(context, /*use_cudnn=*/true, cudnn_use_autotune_, out_backprop,\n                reshaped_filter, /*row_dilation=*/1, /*col_dilation=*/1,\n                stride_, stride_, padding_, explicit_paddings_, in_backprop,\n                data_format_);\n      return;\n    }\n\n    auto out_backprop_ptr = out_backprop.template flat<T>().data();\n    auto filter_ptr = filter.template flat<T>().data();\n    auto in_backprop_ptr = in_backprop->template flat<T>().data();\n    LaunchDepthwiseConvBackpropInputOp<Device, T>()(\n        context, args, out_backprop_ptr, filter_ptr, in_backprop_ptr,\n        data_format_);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def createRequest(self, op, req, outgoing_data):\n        \"\"\"Return a new QNetworkReply object.\n\n        Args:\n             op: Operation op\n             req: const QNetworkRequest & req\n             outgoing_data: QIODevice * outgoingData\n\n        Return:\n            A QNetworkReply.\n        \"\"\"\n        proxy_factory = objreg.get('proxy-factory', None)\n        if proxy_factory is not None:\n            proxy_error = proxy_factory.get_error()\n            if proxy_error is not None:\n                return networkreply.ErrorNetworkReply(\n                    req, proxy_error, QNetworkReply.UnknownProxyError,\n                    self)\n\n        scheme = req.url().scheme()\n        if scheme in self._scheme_handlers:\n            result = self._scheme_handlers[scheme](req)\n            if result is not None:\n                result.setParent(self)\n                return result\n\n        for header, value in shared.custom_headers(url=req.url()):\n            req.setRawHeader(header, value)\n\n        host_blocker = objreg.get('host-blocker')\n        if host_blocker.is_blocked(req.url()):\n            log.webview.info(\"Request to {} blocked by host blocker.\".format(\n                req.url().host()))\n            return networkreply.ErrorNetworkReply(\n                req, HOSTBLOCK_ERROR_STRING, QNetworkReply.ContentAccessDenied,\n                self)\n\n        # There are some scenarios where we can't figure out current_url:\n        # - There's a generic NetworkManager, e.g. for downloads\n        # - The download was in a tab which is now closed.\n        current_url = QUrl()\n\n        if self._tab_id is not None:\n            assert self._win_id is not None\n            try:\n                tab = objreg.get('tab', scope='tab', window=self._win_id,\n                                 tab=self._tab_id)\n                current_url = tab.url()\n            except (KeyError, RuntimeError):\n                # https://github.com/qutebrowser/qutebrowser/issues/889\n                # Catching RuntimeError because we could be in the middle of\n                # the webpage shutdown here.\n                current_url = QUrl()\n\n        if 'log-requests' in self._args.debug_flags:\n            operation = debug.qenum_key(QNetworkAccessManager, op)\n            operation = operation.replace('Operation', '').upper()\n            log.webview.debug(\"{} {}, first-party {}\".format(\n                operation,\n                req.url().toDisplayString(),\n                current_url.toDisplayString()))\n\n        self.set_referer(req, current_url)\n        return super().createRequest(op, req, outgoing_data)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_file_hash(self):\n        if self.file_hash == \"\":\n            with self.open_file() as f:\n                self._set_file_hash(f.read())\n\n            self.save(update_fields=[\"file_hash\"])\n\n        return self.file_hash\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void TensorSliceReader::LoadShard(int shard) const {\n  CHECK_LT(shard, sss_.size());\n  if (sss_[shard] || !status_.ok()) {\n    return;  // Already loaded, or invalid.\n  }\n  string value;\n  SavedTensorSlices sts;\n  const string fname = fnames_[shard];\n  VLOG(1) << \"Reading meta data from file \" << fname << \"...\";\n  Table* table;\n  Status s = open_function_(fname, &table);\n  if (!s.ok()) {\n    status_ = errors::DataLoss(\"Unable to open table file \", fname, \": \",\n                               s.ToString());\n    return;\n  }\n  sss_[shard].reset(table);\n  if (!(table->Get(kSavedTensorSlicesKey, &value) &&\n        ParseProtoUnlimited(&sts, value))) {\n    status_ = errors::Internal(\n        \"Failed to find the saved tensor slices at the beginning of the \"\n        \"checkpoint file: \",\n        fname);\n    return;\n  }\n  status_ = CheckVersions(sts.meta().versions(), TF_CHECKPOINT_VERSION,\n                          TF_CHECKPOINT_VERSION_MIN_PRODUCER, \"Checkpoint\",\n                          \"checkpoint\");\n  if (!status_.ok()) return;\n  for (const SavedSliceMeta& ssm : sts.meta().tensor()) {\n    TensorShape ssm_shape(ssm.shape());\n    for (const TensorSliceProto& tsp : ssm.slice()) {\n      TensorSlice ss_slice(tsp);\n      status_ = RegisterTensorSlice(ssm.name(), ssm_shape, ssm.type(), fname,\n                                    ss_slice, &tensors_);\n      if (!status_.ok()) return;\n    }\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus ResizeOutputTensors(TfLiteContext* context, TfLiteNode* node,\n                                 const TfLiteTensor* input,\n                                 const TfLiteTensor* size_splits,\n                                 const TfLiteTensor* axis) {\n  int axis_value = GetTensorData<int>(axis)[0];\n  if (axis_value < 0) {\n    axis_value += NumDimensions(input);\n  }\n\n  std::vector<int64_t> size_splits_vector;\n  if (size_splits->type == kTfLiteInt32) {\n    GetSizeSplitsVector<int32_t>(size_splits, &size_splits_vector);\n  } else if (size_splits->type == kTfLiteInt64) {\n    GetSizeSplitsVector<int64_t>(size_splits, &size_splits_vector);\n  } else {\n    context->ReportError(context, \"size_splits only support type int32|int64.\");\n    return kTfLiteError;\n  }\n\n  int minus_one_index = -1;\n  int64_t size_splits_sum = 0;\n\n  for (int i = 0; i < size_splits_vector.size(); ++i) {\n    if (size_splits_vector.at(i) == -1) {\n      if (minus_one_index == -1) {\n        minus_one_index = i;\n      } else {\n        context->ReportError(context,\n                             \"The size_splits contains more than one -1.\");\n      }\n    } else {\n      size_splits_sum += size_splits_vector.at(i);\n    }\n  }\n\n  const int input_size = SizeOfDimension(input, axis_value);\n\n  if (minus_one_index != -1) {\n    if (size_splits_sum > input_size) {\n      context->ReportError(\n          context,\n          \"The sum of size_splits must be less than the dimension of value.\");\n    } else {\n      size_splits_vector[minus_one_index] = input_size - size_splits_sum;\n    }\n  } else if (size_splits_sum != input_size) {\n    context->ReportError(\n        context,\n        \"The size_splits must sum to the dimension of value along axis.\");\n  }\n\n  for (int i = 0; i < NumOutputs(node); ++i) {\n    TfLiteIntArray* output_dims = TfLiteIntArrayCopy(input->dims);\n    output_dims->data[axis_value] = size_splits_vector.at(i);\n    TfLiteTensor* output;\n    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, i, &output));\n    TF_LITE_ENSURE_STATUS(context->ResizeTensor(context, output, output_dims));\n  }\n\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def create_trustee_and_trust(osc, cluster):\n    try:\n        password = utils.generate_password(length=18)\n        trustee = osc.keystone().create_trustee(\n            cluster.uuid,\n            password,\n        )\n        cluster.trustee_username = trustee.name\n        cluster.trustee_user_id = trustee.id\n        cluster.trustee_password = password\n        trust = osc.keystone().create_trust(trustee.id)\n        cluster.trust_id = trust.id\n    except Exception:\n        LOG.exception(\n            _LE('Failed to create trustee and trust for Cluster: %s'),\n            cluster.uuid)\n        raise exception.TrusteeOrTrustToClusterFailed(\n            cluster_uuid=cluster.uuid)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void SparseFillEmptyRowsOpImpl(OpKernelContext* context,\n                               AsyncOpKernel::DoneCallback done = nullptr) {\n  // Note that setting this empty lambda as the default parameter value directly\n  // can cause strange compiler/linker errors, so we do it like this instead.\n  if (!done) {\n    done = [] {};\n  }\n\n  const int kIndicesInput = 0;\n  const int kValuesInput = 1;\n  const int kDenseShapeInput = 2;\n  const int kDefaultValueInput = 3;\n\n  const Tensor& indices_t = context->input(kIndicesInput);\n  const Tensor& values_t = context->input(kValuesInput);\n  const Tensor& dense_shape_t = context->input(kDenseShapeInput);\n  const Tensor& default_value_t = context->input(kDefaultValueInput);\n\n  OP_REQUIRES_ASYNC(\n      context, TensorShapeUtils::IsVector(dense_shape_t.shape()),\n      errors::InvalidArgument(\"dense_shape must be a vector, saw: \",\n                              dense_shape_t.shape().DebugString()),\n      done);\n  OP_REQUIRES_ASYNC(context, TensorShapeUtils::IsMatrix(indices_t.shape()),\n                    errors::InvalidArgument(\"indices must be a matrix, saw: \",\n                                            indices_t.shape().DebugString()),\n                    done);\n  OP_REQUIRES_ASYNC(context, TensorShapeUtils::IsVector(values_t.shape()),\n                    errors::InvalidArgument(\"values must be a vector, saw: \",\n                                            values_t.shape().DebugString()),\n                    done);\n  OP_REQUIRES_ASYNC(\n      context, TensorShapeUtils::IsScalar(default_value_t.shape()),\n      errors::InvalidArgument(\"default_value must be a scalar, saw: \",\n                              default_value_t.shape().DebugString()),\n      done);\n  // TODO(ebrevdo): add shape checks between values, indices,\n  // Also add check that dense rank > 0.\n  OP_REQUIRES_ASYNC(context, dense_shape_t.NumElements() != 0,\n                    errors::InvalidArgument(\"Dense shape cannot be empty.\"),\n                    done);\n\n  using FunctorType = functor::SparseFillEmptyRows<Device, T, Tindex>;\n  OP_REQUIRES_OK_ASYNC(context,\n                       FunctorType()(context, default_value_t, indices_t,\n                                     values_t, dense_shape_t, done),\n                       done);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def clone(\n        self,\n        pdf_dest: Any,\n        force_duplicate: bool = False,\n        ignore_fields: Optional[Sequence[Union[str, int]]] = (),\n    ) -> \"ContentStream\":\n        \"\"\"\n        Clone object into pdf_dest.\n\n        Args:\n            pdf_dest:\n            force_duplicate:\n            ignore_fields:\n\n        Returns:\n            The cloned ContentStream\n        \"\"\"\n        try:\n            if self.indirect_reference.pdf == pdf_dest and not force_duplicate:  # type: ignore\n                return self\n        except Exception:\n            pass\n\n        d__ = cast(\n            \"ContentStream\",\n            self._reference_clone(\n                self.__class__(None, None), pdf_dest, force_duplicate\n            ),\n        )\n        if ignore_fields is None:\n            ignore_fields = []\n        d__._clone(self, pdf_dest, force_duplicate, ignore_fields)\n        return d__\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse(self):\n        \"\"\"\n        Parse the POST data and break it into a FILES MultiValueDict and a POST\n        MultiValueDict.\n\n        Return a tuple containing the POST and FILES dictionary, respectively.\n        \"\"\"\n        from django.http import QueryDict\n\n        encoding = self._encoding\n        handlers = self._upload_handlers\n\n        # HTTP spec says that Content-Length >= 0 is valid\n        # handling content-length == 0 before continuing\n        if self._content_length == 0:\n            return QueryDict(encoding=self._encoding), MultiValueDict()\n\n        # See if any of the handlers take care of the parsing.\n        # This allows overriding everything if need be.\n        for handler in handlers:\n            result = handler.handle_raw_input(\n                self._input_data,\n                self._meta,\n                self._content_length,\n                self._boundary,\n                encoding,\n            )\n            # Check to see if it was handled\n            if result is not None:\n                return result[0], result[1]\n\n        # Create the data structures to be used later.\n        self._post = QueryDict(mutable=True)\n        self._files = MultiValueDict()\n\n        # Instantiate the parser and stream:\n        stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n        # Whether or not to signal a file-completion at the beginning of the loop.\n        old_field_name = None\n        counters = [0] * len(handlers)\n\n        # Number of bytes that have been read.\n        num_bytes_read = 0\n        # To count the number of keys in the request.\n        num_post_keys = 0\n        # To limit the amount of data read from the request.\n        read_size = None\n        # Whether a file upload is finished.\n        uploaded_file = True\n\n        try:\n            for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                if old_field_name:\n                    # We run this at the beginning of the next loop\n                    # since we cannot be sure a file is complete until\n                    # we hit the next boundary/part of the multipart content.\n                    self.handle_file_complete(old_field_name, counters)\n                    old_field_name = None\n                    uploaded_file = True\n\n                try:\n                    disposition = meta_data['content-disposition'][1]\n                    field_name = disposition['name'].strip()\n                except (KeyError, IndexError, AttributeError):\n                    continue\n\n                transfer_encoding = meta_data.get('content-transfer-encoding')\n                if transfer_encoding is not None:\n                    transfer_encoding = transfer_encoding[0].strip()\n                field_name = force_str(field_name, encoding, errors='replace')\n\n                if item_type == FIELD:\n                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                    num_post_keys += 1\n                    if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and\n                            settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):\n                        raise TooManyFieldsSent(\n                            'The number of GET/POST parameters exceeded '\n                            'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'\n                        )\n\n                    # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                    if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                        read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n\n                    # This is a post field, we can just set it in the post\n                    if transfer_encoding == 'base64':\n                        raw_data = field_stream.read(size=read_size)\n                        num_bytes_read += len(raw_data)\n                        try:\n                            data = base64.b64decode(raw_data)\n                        except binascii.Error:\n                            data = raw_data\n                    else:\n                        data = field_stream.read(size=read_size)\n                        num_bytes_read += len(data)\n\n                    # Add two here to make the check consistent with the\n                    # x-www-form-urlencoded check that includes '&='.\n                    num_bytes_read += len(field_name) + 2\n                    if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and\n                            num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):\n                        raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')\n\n                    self._post.appendlist(field_name, force_str(data, encoding, errors='replace'))\n                elif item_type == FILE:\n                    # This is a file, use the handler...\n                    file_name = disposition.get('filename')\n                    if file_name:\n                        file_name = force_str(file_name, encoding, errors='replace')\n                        file_name = self.sanitize_file_name(file_name)\n                    if not file_name:\n                        continue\n\n                    content_type, content_type_extra = meta_data.get('content-type', ('', {}))\n                    content_type = content_type.strip()\n                    charset = content_type_extra.get('charset')\n\n                    try:\n                        content_length = int(meta_data.get('content-length')[0])\n                    except (IndexError, TypeError, ValueError):\n                        content_length = None\n\n                    counters = [0] * len(handlers)\n                    uploaded_file = False\n                    try:\n                        for handler in handlers:\n                            try:\n                                handler.new_file(\n                                    field_name, file_name, content_type,\n                                    content_length, charset, content_type_extra,\n                                )\n                            except StopFutureHandlers:\n                                break\n\n                        for chunk in field_stream:\n                            if transfer_encoding == 'base64':\n                                # We only special-case base64 transfer encoding\n                                # We should always decode base64 chunks by multiple of 4,\n                                # ignoring whitespace.\n\n                                stripped_chunk = b\"\".join(chunk.split())\n\n                                remaining = len(stripped_chunk) % 4\n                                while remaining != 0:\n                                    over_chunk = field_stream.read(4 - remaining)\n                                    stripped_chunk += b\"\".join(over_chunk.split())\n                                    remaining = len(stripped_chunk) % 4\n\n                                try:\n                                    chunk = base64.b64decode(stripped_chunk)\n                                except Exception as exc:\n                                    # Since this is only a chunk, any error is an unfixable error.\n                                    raise MultiPartParserError(\"Could not decode base64 data.\") from exc\n\n                            for i, handler in enumerate(handlers):\n                                chunk_length = len(chunk)\n                                chunk = handler.receive_data_chunk(chunk, counters[i])\n                                counters[i] += chunk_length\n                                if chunk is None:\n                                    # Don't continue if the chunk received by\n                                    # the handler is None.\n                                    break\n\n                    except SkipFile:\n                        self._close_files()\n                        # Just use up the rest of this file...\n                        exhaust(field_stream)\n                    else:\n                        # Handle file upload completions on next iteration.\n                        old_field_name = field_name\n                else:\n                    # If this is neither a FIELD or a FILE, just exhaust the stream.\n                    exhaust(stream)\n        except StopUpload as e:\n            self._close_files()\n            if not e.connection_reset:\n                exhaust(self._input_data)\n        else:\n            if not uploaded_file:\n                for handler in handlers:\n                    handler.upload_interrupted()\n            # Make sure that the request data is all fed\n            exhaust(self._input_data)\n\n        # Signal that the upload has completed.\n        # any() shortcircuits if a handler's upload_complete() returns a value.\n        any(handler.upload_complete() for handler in handlers)\n        self._post._mutable = False\n        return self._post, self._files\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_paths(base_path: pathlib.Path):\n    data_file = pathlib.Path(str(base_path) + \".data\")\n    metadata_file = pathlib.Path(str(base_path) + \".meta\")\n\n    return data_file, metadata_file\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  QInt32() {}\n  QInt32(const int8_t v) : value(v) {}"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    const findHeadingElement = (cell: Cell): void => {\n      model.getCellHeadings(cell).forEach(async heading => {\n        const elementId = await getIdForHeading(heading, this.parser!);\n\n        const selector = elementId\n          ? `h${heading.level}[id=\"${elementId}\"]`\n          : `h${heading.level}`;\n\n        if (heading.outputIndex !== undefined) {\n          // Code cell\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              (heading.cellRef as CodeCell).outputArea.widgets[\n                heading.outputIndex\n              ].node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        } else {\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              heading.cellRef.node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        }\n      });\n    };\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def gather_tree(ids, parents):\n    r\"\"\"\n    To be used after beam search. After beam search, we get selected ids at\n    each time step and the corresponding parents in the search tree. Both ids\n    and parents have the layout :attr:`[max_time, batch_size, beam_size]`. Then\n    :attr:`gather_tree` is used to backtrace from the last time step and\n    generate the full sequences by collecting selected ids.\n\n    Here is an example:\n\n    .. code-block:: text\n\n            Given:\n                ids = [[[2 2]\n                        [6 1]]\n                       [[3 9]\n                        [6 1]]\n                       [[0 1]\n                        [9 0]]]\n                parents = [[[0 0]\n                            [1 1]]\n                           [[1 0]\n                            [1 0]]\n                           [[0 0]\n                            [0 1]]]\n\n            Then:\n                gather_tree(ids, parents)\n                         = [[[2 2]\n                             [1 6]]\n                            [[3 3]\n                             [6 1]]\n                            [[0 1]\n                             [9 0]]]\n\n    Args:\n        ids(Tensor): A Tensor with shape :attr:`[length, batch_size, beam_size]`\n            and data type :attr:`int32` or :attr:`int64`. It contains the selected\n            ids of all time steps.\n        parents(Tensor): A Tensor with the same shape and data type as :attr:`ids`,\n            It contains the parents corresponding to selected ids when searching\n            among beams.\n\n    Returns:\n            A Tensor with the same shape and data type as :attr:`ids`. \\\n            It contains the full sequences. The sequences are collected from \\\n            :attr:`ids` by backtracing according to :attr:`parents`.\n\n    Examples:\n        .. code-block:: python\n\n            import paddle\n\n            ids = paddle.to_tensor([[[2, 2], [6, 1]], [[3, 9], [6, 1]], [[0, 1], [9, 0]]])\n\n            parents = paddle.to_tensor([[[0, 0], [1, 1]], [[1, 0], [1, 0]], [[0, 0], [0, 1]]])\n\n            final_sequences = paddle.nn.functional.gather_tree(ids, parents)\n            # [[[2, 2], [1, 6]], [[3, 3], [6, 1]], [[0, 1], [9, 0]]]\n\n    \"\"\"\n    if in_dygraph_mode():\n        return _C_ops.gather_tree(ids, parents)\n    else:\n        if _in_legacy_dygraph():\n            return _legacy_C_ops.gather_tree(ids, parents)\n        else:\n            helper = LayerHelper('gather_tree', **locals())\n            check_variable_and_dtype(ids, 'ids', ['int32', 'int64'],\n                                     'gather_tree')\n            check_variable_and_dtype(parents, 'parents', ['int32', 'int64'],\n                                     'gather_tree')\n            out = helper.create_variable_for_type_inference(dtype=ids.dtype)\n\n            helper.append_op(type=\"gather_tree\",\n                             inputs={\n                                 \"Ids\": ids,\n                                 \"Parents\": parents\n                             },\n                             outputs={\"Out\": out})\n\n            return out\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, *, openapi: GeneratorData) -> None:\n        self.openapi: GeneratorData = openapi\n        self.env: Environment = Environment(loader=PackageLoader(__package__), trim_blocks=True, lstrip_blocks=True)\n\n        self.project_name: str = self.project_name_override or f\"{utils.kebab_case(openapi.title).lower()}-client\"\n        self.project_dir: Path = Path.cwd() / self.project_name\n\n        self.package_name: str = self.package_name_override or self.project_name.replace(\"-\", \"_\")\n        self.package_dir: Path = self.project_dir / self.package_name\n        self.package_description: str = f\"A client library for accessing {self.openapi.title}\"\n        self.version: str = openapi.version\n\n        self.env.filters.update(self.TEMPLATE_FILTERS)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, instance, filename):\n        \"\"\"\n        Apply (if callable) or prepend (if a string) upload_to to the filename,\n        then delegate further processing of the name to the storage backend.\n        Until the storage layer, all file paths are expected to be Unix style\n        (with forward slashes).\n        \"\"\"\n        if callable(self.upload_to):\n            filename = self.upload_to(instance, filename)\n        else:\n            dirname = datetime.datetime.now().strftime(str(self.upload_to))\n            filename = posixpath.join(dirname, filename)\n        return self.storage.generate_filename(filename)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs(\n    OpKernelContext* context, TensorInputs* inputs,\n    TensorShapes* input_matrix_shapes, TensorShape* batch_shape) {\n  int input_rank = -1;\n  for (int i = 0; i < NumMatrixInputs(context); ++i) {\n    const Tensor& in = context->input(i);\n    if (i == 0) {\n      input_rank = in.dims();\n      OP_REQUIRES(\n          context, input_rank >= 2,\n          errors::InvalidArgument(\"Input tensor \", i,\n                                  \" must have rank >= 2, got \", input_rank));\n      // If the tensor rank is greater than 2, we consider the inner-most\n      // dimensions as matrices, and loop over all the other outer (\"batch\")\n      // dimensions to compute the results.\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        batch_shape->AddDim(in.dim_size(dim));\n      }\n    } else {\n      // Make sure that all inputs have the same rank and outer dimensions.\n      OP_REQUIRES(context, input_rank == in.dims(),\n                  errors::InvalidArgument(\n                      \"All input tensors must have the same rank.\"));\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        OP_REQUIRES(\n            context, in.dim_size(dim) == batch_shape->dim_size(dim),\n            errors::InvalidArgument(\n                \"All input tensors must have the same outer dimensions.\"));\n      }\n    }\n\n    const int row_dimension = input_rank - 2;\n    const int col_dimension = input_rank - 1;\n    const int64_t num_rows = in.dim_size(row_dimension);\n    const int64_t num_cols = in.dim_size(col_dimension);\n    input_matrix_shapes->emplace_back(\n        std::initializer_list<int64_t>({num_rows, num_cols}));\n    inputs->emplace_back(&in);\n  }\n  // Have the derived class validate that the inputs are as expected.\n  ValidateInputMatrixShapes(context, *input_matrix_shapes);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def include_is_allowed(filepath):\n    for root in settings.ALLOWED_INCLUDE_ROOTS:\n        if filepath.startswith(root):\n            return True\n    return False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def get_metric_variables(self) -> List[Any]:\n    \"\"\"Returns a list containing the metric variable values.\"\"\"\n    result = self._session.run(fetches=self._metric_variable_nodes)\n    return result\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def respond_error(self, context, exception):\n        context.respond_server_error()\n        stack = traceback.format_exc()\n        return \"\"\"\n        <html>\n            <body>\n\n                <style>\n                    body {\n                        font-family: sans-serif;\n                        color: #888;\n                        text-align: center;\n                    }\n\n                    body pre {\n                        width: 600px;\n                        text-align: left;\n                        margin: auto;\n                        font-family: monospace;\n                    }\n                </style>\n\n                <img src=\"/ajenti:static/main/error.jpeg\" />\n                <br/>\n                <p>\n                    Server error\n                </p>\n                <pre>\n%s\n                </pre>\n            </body>\n        </html>\n        \"\"\" % stack\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def resolve_rpsl_object_journal(rpsl_object, info: GraphQLResolveInfo):\n    \"\"\"\n    Resolve a journal subquery on an RPSL object.\n    \"\"\"\n    database_handler = info.context['request'].app.state.database_handler\n    access_list = f\"sources.{rpsl_object['source']}.nrtm_access_list\"\n    if not is_client_permitted(info.context['request'].client.host, access_list):\n        raise GraphQLError(f\"Access to journal denied for source {rpsl_object['source']}\")\n\n    query = RPSLDatabaseJournalQuery()\n    query.sources([rpsl_object['source']]).rpsl_pk(rpsl_object['rpslPk'])\n    for row in database_handler.execute_query(query, refresh_on_error=True):\n        response = {snake_to_camel_case(k): v for k, v in row.items()}\n        response['operation'] = response['operation'].name\n        if response['origin']:\n            response['origin'] = response['origin'].name\n        yield response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def reset_metric_variables(self) -> None:\n    \"\"\"Reset metric variable values to their initial values.\"\"\"\n    self._session.run(self._reset_variables_op)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& in0 = context->input(0);\n    const Tensor& in1 = context->input(1);\n    const int dims = in0.dims();\n    constexpr int kMinDims = 0;\n    constexpr int kMaxDims = 5;\n    OP_REQUIRES(context, kMinDims <= dims && dims <= kMaxDims,\n                errors::Unimplemented(\"inputs rank not in [\", kMinDims, \",\",\n                                      kMaxDims, \"]: \", dims));\n    OP_REQUIRES(\n        context,\n        TensorShapeUtils::IsMatrix(in1.shape()) && in1.dim_size(1) == 2,\n        errors::InvalidArgument(\"paddings must be a matrix with 2 columns: \",\n                                in1.shape().DebugString()));\n    OP_REQUIRES(\n        context, dims == in1.dim_size(0),\n        errors::InvalidArgument(\n            \"The first dimension of paddings must be the rank of inputs\",\n            in1.shape().DebugString(), \" \", in0.shape().DebugString()));\n\n    // Compute the shape of the output tensor, and allocate it.\n    TensorShape output_shape;\n    typename TTypes<Tpaddings>::ConstMatrix paddings = in1.matrix<Tpaddings>();\n    for (int d = 0; d < dims; ++d) {\n      const Tpaddings before = paddings(d, 0);  // Pad before existing elements.\n      const Tpaddings after = paddings(d, 1);   // Pad after existing elements.\n      OP_REQUIRES(context, before >= 0 && after >= 0,\n                  errors::InvalidArgument(\n                      \"Paddings must be non-negative: \", before, \", \", after));\n\n      const int64_t out_size = in0.dim_size(d) - (before + after);\n      if (offset_ == 0) {  // SYMMETRIC mode.\n        OP_REQUIRES(context, before <= out_size && after <= out_size,\n                    errors::InvalidArgument(\"paddings must be no greater \"\n                                            \"than the output dimension size: \",\n                                            before, \", \", after,\n                                            \" greater than \", out_size));\n      } else if (offset_ == 1) {  // REFLECT mode.\n        OP_REQUIRES(context, before < out_size && after < out_size,\n                    errors::InvalidArgument(\"paddings must be less than\"\n                                            \" the output dimension size: \",\n                                            before, \", \", after,\n                                            \" not less than \", out_size));\n      }\n      output_shape.AddDim(out_size);\n    }\n\n    if (output_shape == in0.shape()) {\n      context->set_output(0, in0);\n      return;\n    }\n\n    Tensor scratch;\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<T>::value,\n                                                   in0.shape(), &scratch));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &output));\n\n#define MIRROR_PAD_GRAD_CASE(k)                                           \\\n  case k: {                                                               \\\n    functor::MirrorPadGrad<Device, T, Tpaddings, k>()(                    \\\n        context->eigen_device<Device>(), To32Bit(output->tensor<T, k>()), \\\n        To32Bit(in0.tensor<T, k>()), paddings, offset_,                   \\\n        To32Bit(scratch.tensor<T, k>()));                                 \\\n    break;                                                                \\\n  }\n\n    // Invoke the dims-specific implementation.\n    switch (dims) {\n      MIRROR_PAD_GRAD_CASE(1);\n      MIRROR_PAD_GRAD_CASE(2);\n      MIRROR_PAD_GRAD_CASE(3);\n      MIRROR_PAD_GRAD_CASE(4);\n      MIRROR_PAD_GRAD_CASE(5);\n      default:\n        OP_REQUIRES(context, false,\n                    errors::InvalidArgument(\"Unsupported rank: \",\n                                            in0.shape().DebugString()));\n    }\n#undef MIRROR_PAD_GRAD_CASE\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render(self):\n        if len(self.params) == 1:\n            # Retrieve the user portrait\n            user = decleanId(self.params[0])\n            portrait = self.portal_membership.getPersonalPortrait(user)\n        elif len(self.params) == 0:\n            current_user_id = self.portal_membership.getAuthenticatedMember().getId()\n            portrait = self.portal_membership.getPersonalPortrait(current_user_id)\n        else:\n            raise Exception(\n                \"Must supply exactly zero (own portrait) or one parameter \" \"(user id)\"\n            )\n        # User uploaded portraits have a meta_type of \"Image\"\n        if not portrait or isDefaultPortrait(portrait):\n            self.request.response.setStatus(404)\n            return None\n\n        self.request.response.setStatus(200)\n        self.request.response.setHeader(\"Content-Type\", portrait.content_type)\n\n        return stream_data(portrait)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  explicit FractionalMaxPoolOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pooling_ratio\", &pooling_ratio_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"pseudo_random\", &pseudo_random_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n\n    OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                errors::InvalidArgument(\"pooling_ratio field must \"\n                                        \"specify 4 dimensions\"));\n\n    OP_REQUIRES(\n        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n        errors::Unimplemented(\"Fractional max pooling is not yet \"\n                              \"supported on the batch nor channel dimension.\"));\n\n    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed2\", &seed2_));\n    if (deterministic_) {\n      // If both seeds are not set when deterministic_ is true, force set seeds.\n      if ((seed_ == 0) && (seed2_ == 0)) {\n        seed_ = random::New64();\n        seed2_ = random::New64();\n      }\n    } else {\n      OP_REQUIRES(\n          context, (seed_ == 0) && (seed2_ == 0),\n          errors::InvalidArgument(\n              \"Both seed and seed2 should be 0 if deterministic is false.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ConstantFolding::EvaluateOneFoldable(const NodeDef& node,\n                                            std::vector<NodeDef>* outputs,\n                                            bool* result_too_large) {\n  TensorVector inputs;\n  TensorVector output_tensors;\n  auto inputs_cleanup = gtl::MakeCleanup([&inputs, &output_tensors] {\n    for (const auto& input : inputs) {\n      delete input.tensor;\n    }\n    for (const auto& output : output_tensors) {\n      if (output.tensor) {\n        delete output.tensor;\n      }\n    }\n  });\n\n  size_t total_inputs_size = 0;\n  for (const auto& input : node.input()) {\n    const TensorId input_tensor = ParseTensorName(input);\n    if (input_tensor.index() < 0) {\n      // Control dependency\n      break;\n    }\n    const NodeDef* input_node = node_map_->GetNode(input);\n    if (!IsReallyConstant(*input_node)) {\n      return Status(error::INVALID_ARGUMENT,\n                    strings::StrCat(\"Can't fold \", node.name(), \", its \", input,\n                                    \" isn't constant\"));\n    }\n    TF_RETURN_IF_ERROR(CheckAttrExists(*input_node, \"value\"));\n    const TensorProto& raw_val = input_node->attr().at(\"value\").tensor();\n    if (raw_val.dtype() == DT_INVALID) {\n      return Status(\n          error::INVALID_ARGUMENT,\n          strings::StrCat(\"A tensor in the input node, with TensorId of \",\n                          input_tensor.ToString(),\n                          \" has a dtype of DT_INVALID.\"));\n    }\n    Tensor* value = new Tensor(raw_val.dtype(), raw_val.tensor_shape());\n    if (!value->FromProto(raw_val)) {\n      delete (value);\n      return errors::InvalidArgument(\"Unable to make Tensor from proto for \",\n                                     node.name(), \" with shape \",\n                                     raw_val.tensor_shape().DebugString());\n    }\n    inputs.emplace_back(value);\n    total_inputs_size += value->TotalBytes();\n  }\n\n  TF_RETURN_IF_ERROR(EvaluateNode(node, inputs, &output_tensors));\n  if (output_tensors.empty()) {\n    return Status(error::INVALID_ARGUMENT, \"Expected at least one output.\");\n  }\n\n  outputs->resize(output_tensors.size());\n  for (size_t i = 0; i < output_tensors.size(); i++) {\n    string node_name = OptimizedNodeName(node, \"-folded\");\n    if (output_tensors.size() > 1) {\n      node_name = strings::StrCat(node_name, \"-\", i);\n    }\n    if (output_tensors[i].tensor) {\n      Status s = CreateNodeDef(node_name, output_tensors[i], &outputs->at(i),\n                               total_inputs_size);\n      if (!s.ok()) {\n        *result_too_large = true;\n        return s;\n      }\n    } else {\n      // Create an empty NodeDef to identify dead outputs (e.g. the output of a\n      // switch that's not selected by the switch predicate).\n      outputs->at(i) = NodeDef();\n    }\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def local_get_cobbler_xmlrpc_url():\n    # Load xmlrpc port\n    try:\n        fh = open(\"/etc/cobbler/settings\")\n        data = yaml.load(fh.read())\n        fh.close()\n    except:\n       traceback.print_exc()\n       raise CX(\"/etc/cobbler/settings is not a valid YAML file\")\n    return \"http://%s:%s\" % (\"127.0.0.1\",data.get(\"xmlrpc_port\",\"25151\"))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def to_yaml(self, **kwargs):\n    \"\"\"Returns a yaml string containing the network configuration.\n\n    To load a network from a yaml save file, use\n    `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n\n    `custom_objects` should be a dictionary mapping\n    the names of custom losses / layers / etc to the corresponding\n    functions / classes.\n\n    Args:\n        **kwargs: Additional keyword arguments\n            to be passed to `yaml.dump()`.\n\n    Returns:\n        A YAML string.\n\n    Raises:\n        ImportError: if yaml module is not found.\n    \"\"\"\n    if yaml is None:\n      raise ImportError(\n          'Requires yaml module installed (`pip install pyyaml`).')\n    return yaml.dump(self._updated_config(), **kwargs)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_conn(self) -> Connection:\n        \"\"\"Establish a connection to Drillbit.\"\"\"\n        conn_md = self.get_connection(getattr(self, self.conn_name_attr))\n        creds = f\"{conn_md.login}:{conn_md.password}@\" if conn_md.login else \"\"\n        if \"/\" in conn_md.host or \"&\" in conn_md.host:\n            raise ValueError(\"Drill host should not contain '/&' characters\")\n        engine = create_engine(\n            f'{conn_md.extra_dejson.get(\"dialect_driver\", \"drill+sadrill\")}://{creds}'\n            f\"{conn_md.host}:{conn_md.port}/\"\n            f'{conn_md.extra_dejson.get(\"storage_plugin\", \"dfs\")}'\n        )\n\n        self.log.info(\n            \"Connected to the Drillbit at %s:%s as user %s\", conn_md.host, conn_md.port, conn_md.login\n        )\n        return engine.raw_connection()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _dump(self, file=None, format=None):\n        import tempfile\n        if not file:\n            file = tempfile.mktemp()\n        self.load()\n        if not format or format == \"PPM\":\n            self.im.save_ppm(file)\n        else:\n            file = file + \".\" + format\n            self.save(file, format)\n        return file\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  static port::StatusOr<CudnnRnnSequenceTensorDescriptor> Create(\n      GpuExecutor* parent, int max_seq_length, int batch_size, int data_size,\n      cudnnDataType_t data_type) {\n    CHECK_GT(max_seq_length, 0);\n    int dims[] = {batch_size, data_size, 1};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    TensorDescriptor tensor_desc = CreateTensorDescriptor();\n    RETURN_IF_CUDNN_ERROR(cudnnSetTensorNdDescriptor(\n        /*tensorDesc=*/tensor_desc.get(), /*dataType=*/data_type,\n        /*nbDims=*/sizeof(dims) / sizeof(dims[0]), /*dimA=*/dims,\n        /*strideA=*/strides));\n    return CudnnRnnSequenceTensorDescriptor(parent, max_seq_length, batch_size,\n                                            data_size, data_type,\n                                            nullptr,\n                                            std::move(tensor_desc));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, handler):\n        self.handler = handler\n        self.protocol_version = None\n        self.support_legacy_versions = True\n        self.supported_protocols = []\n        self.origin_checker = None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "load_header (XwdLoader *loader) // gconstpointer in, gsize in_max_len, XwdHeader *header_out)\n{\n    XwdHeader *h = &loader->header;\n    XwdHeader in;\n    const guint32 *p = (const guint32 *) &in;\n\n    if (!file_mapping_taste (loader->mapping, &in, 0, sizeof (in)))\n        return FALSE;\n\n    h->header_size = g_ntohl (*(p++));\n    h->file_version = g_ntohl (*(p++));\n    h->pixmap_format = g_ntohl (*(p++));\n    h->pixmap_depth = g_ntohl (*(p++));\n    h->pixmap_width = g_ntohl (*(p++));\n    h->pixmap_height = g_ntohl (*(p++));\n    h->x_offset = g_ntohl (*(p++));\n    h->byte_order = g_ntohl (*(p++));\n    h->bitmap_unit = g_ntohl (*(p++));\n    h->bitmap_bit_order = g_ntohl (*(p++));\n    h->bitmap_pad = g_ntohl (*(p++));\n    h->bits_per_pixel = g_ntohl (*(p++));\n    h->bytes_per_line = g_ntohl (*(p++));\n    h->visual_class = g_ntohl (*(p++));\n    h->red_mask = g_ntohl (*(p++));\n    h->green_mask = g_ntohl (*(p++));\n    h->blue_mask = g_ntohl (*(p++));\n    h->bits_per_rgb = g_ntohl (*(p++));\n    h->color_map_entries = g_ntohl (*(p++));\n    h->n_colors = g_ntohl (*(p++));\n    h->window_width = g_ntohl (*(p++));\n    h->window_height = g_ntohl (*(p++));\n    h->window_x = g_ntohl (*(p++));\n    h->window_y = g_ntohl (*(p++));\n    h->window_border_width = g_ntohl (*(p++));\n\n    /* Only support the most common/useful subset of XWD files out there;\n     * namely, that corresponding to screen dumps from modern X.Org servers. */\n\n    ASSERT_HEADER (h->header_size >= sizeof (XwdHeader));\n    ASSERT_HEADER (h->file_version == 7);\n    ASSERT_HEADER (h->pixmap_depth == 24);\n\n    /* Xvfb sets bits_per_rgb to 8, but 'convert' uses 24 for the same image data. One\n     * of them is likely misunderstanding. Let's be lenient and accept either. */\n    ASSERT_HEADER (h->bits_per_rgb == 8 || h->bits_per_rgb == 24);\n\n    ASSERT_HEADER (h->bytes_per_line >= h->pixmap_width * (h->bits_per_pixel / 8));\n    ASSERT_HEADER (compute_pixel_type (loader) < CHAFA_PIXEL_MAX);\n\n    loader->file_data = file_mapping_get_data (loader->mapping, &loader->file_data_len);\n    if (!loader->file_data)\n        return FALSE;\n\n    ASSERT_HEADER (loader->file_data_len >= h->header_size\n                   + h->n_colors * sizeof (XwdColor)\n                   + h->pixmap_height * h->bytes_per_line);\n\n    loader->image_data = (const guint8 *) loader->file_data\n        + h->header_size + h->n_colors * sizeof (XwdColor);\n\n    return TRUE;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _read_lockfile(self, lockfile: Mapping[str, Any]) -> None:\n        root = self.environment.project.root\n        with cd(root):\n            for package in lockfile.get(\"package\", []):\n                version = package.get(\"version\")\n                if version:\n                    package[\"version\"] = f\"=={version}\"\n                package_name = package.pop(\"name\")\n                req_dict = {\n                    k: v for k, v in package.items() if k not in (\"dependencies\", \"requires_python\", \"summary\", \"files\")\n                }\n                req = Requirement.from_req_dict(package_name, req_dict)\n                if req.is_file_or_url and req.path and not req.url:  # type: ignore[attr-defined]\n                    req.url = path_to_url(posixpath.join(root, req.path))  # type: ignore[attr-defined]\n                can = make_candidate(req, name=package_name, version=version)\n                can.hashes = package.get(\"files\", [])\n                can_id = self._identify_candidate(can)\n                self.packages[can_id] = can\n                candidate_info: CandidateInfo = (\n                    package.get(\"dependencies\", []),\n                    package.get(\"requires_python\", \"\"),\n                    package.get(\"summary\", \"\"),\n                )\n                self.candidate_info[can_id] = candidate_info\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void ImmutableConstantOp::Compute(OpKernelContext* ctx) {\n  std::unique_ptr<MemmappedTensorAllocator> allocator(\n      new MemmappedTensorAllocator());\n\n  OP_REQUIRES_OK(ctx,\n                 allocator->InitializeFromRegion(region_name_, ctx->env()));\n  ctx->set_output(0, Tensor(allocator.get(), dtype_, shape_));\n  OP_REQUIRES_OK(ctx, allocator->allocation_status());\n  // Allocator is owned by the tensor from this point.\n  allocator.release()->set_delete_on_deallocate();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    constexpr int tensor_in_and_out_dims = 4;\n\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n      OP_REQUIRES(\n          context, pooling_ratio_[i] <= input_size[i],\n          errors::InvalidArgument(\n              \"Pooling ratio cannot be bigger than input tensor dim size.\"));\n    }\n    // Output size.\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n\n    // Generate pooling sequence.\n    std::vector<int64_t> row_cum_seq;\n    std::vector<int64_t> col_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    row_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                          &generator, pseudo_random_);\n    col_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                          &generator, pseudo_random_);\n\n    // Prepare output.\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_row_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\n                     1, TensorShape({static_cast<int64_t>(row_cum_seq.size())}),\n                     &output_row_seq_tensor));\n    Tensor* output_col_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\n                     2, TensorShape({static_cast<int64_t>(col_cum_seq.size())}),\n                     &output_col_seq_tensor));\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n    // out_count corresponds to number of elements in each pooling cell.\n    Eigen::Matrix<T, Eigen::Dynamic, 1> out_count(out_mat.cols());\n\n    // Initializes the output tensor and out_count with 0.\n    out_mat.setZero();\n    out_count.setZero();\n\n    auto output_row_seq_flat = output_row_seq_tensor->flat<int64_t>();\n    auto output_col_seq_flat = output_col_seq_tensor->flat<int64_t>();\n\n    // Set output tensors.\n    for (int i = 0; i < row_cum_seq.size(); ++i) {\n      output_row_seq_flat(i) = row_cum_seq[i];\n    }\n\n    for (int i = 0; i < col_cum_seq.size(); ++i) {\n      output_col_seq_flat(i) = col_cum_seq[i];\n    }\n\n    // For both input and output,\n    // 0: batch\n    // 1: row / row\n    // 2: col / col\n    // 3: depth / channel\n    const int64_t row_max = input_size[1] - 1;\n    const int64_t col_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // row sequence.\n      for (int64_t hs = 0; hs < row_cum_seq.size() - 1; ++hs) {\n        // row start and end.\n        const int64_t row_start = row_cum_seq[hs];\n        int64_t row_end =\n            overlapping_ ? row_cum_seq[hs + 1] : row_cum_seq[hs + 1] - 1;\n        row_end = std::min(row_end, row_max);\n\n        // col sequence.\n        for (int64_t ws = 0; ws < col_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // col start and end.\n          const int64_t col_start = col_cum_seq[ws];\n          int64_t col_end =\n              overlapping_ ? col_cum_seq[ws + 1] : col_cum_seq[ws + 1] - 1;\n          col_end = std::min(col_end, col_max);\n          for (int64_t h = row_start; h <= row_end; ++h) {\n            for (int64_t w = col_start; w <= col_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) += in_mat.col(in_offset);\n              out_count(out_offset)++;\n            }\n          }\n        }\n      }\n    }\n    DCHECK_GT(out_count.minCoeff(), 0);\n    out_mat.array().rowwise() /= out_count.transpose().array();\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def check_secret_key(self) -> None:\n        if self.config[\"SECRET_KEY\"] == CHANGE_ME_SECRET_KEY:\n            top_banner = 80 * \"-\" + \"\\n\" + 36 * \" \" + \"WARNING\\n\" + 80 * \"-\"\n            bottom_banner = 80 * \"-\" + \"\\n\" + 80 * \"-\"\n            logger.warning(top_banner)\n            logger.warning(\n                \"A Default SECRET_KEY was detected, please use superset_config.py \"\n                \"to override it.\\n\"\n                \"Use a strong complex alphanumeric string and use a tool to help\"\n                \" you generate \\n\"\n                \"a sufficiently random sequence, ex: openssl rand -base64 42\"\n            )\n            logger.warning(bottom_banner)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def cookies(self):\n        \"\"\"The session cookies.\"\"\"\n        return self._cookies\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_snippet_path(self, path):\r\n        \"\"\"Get snippet path.\"\"\"\r\n\r\n        snippet = None\r\n        for base in self.base_path:\r\n            if os.path.exists(base):\r\n                if os.path.isdir(base):\r\n                    filename = os.path.join(base, path)\r\n                    if os.path.exists(filename):\r\n                        snippet = filename\r\n                        break\r\n                else:\r\n                    basename = os.path.basename(base)\r\n                    dirname = os.path.dirname(base)\r\n                    if basename.lower() == path.lower():\r\n                        filename = os.path.join(dirname, path)\r\n                        if os.path.exists(filename):\r\n                            snippet = filename\r\n                            break\r\n        return snippet\r\n\r"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_package(self, name, links, dest=Destination.QUEUE):\n        \"\"\"\n        Adds a package, with links to desired destination.\n\n        :param name: name of the new package\n        :param links: list of urls\n        :param dest: `Destination`\n        :return: package id of the new package\n        \"\"\"\n        if self.pyload.config.get(\"general\", \"folder_per_package\"):\n            folder = name\n        else:\n            folder = \"\"\n\n        folder = (\n            folder.replace(\"http://\", \"\")\n            .replace(\"https://\", \"\")\n            .replace(\":\", \"\")\n            .replace(\"/\", \"_\")\n            .replace(\"\\\\\", \"_\")\n        )\n\n        package_id = self.pyload.files.add_package(name, folder, Destination(dest))\n\n        self.pyload.files.add_links(links, package_id)\n\n        self.pyload.log.info(\n            self._(\"Added package {name} containing {count:d} links\").format(\n                name=name, count=len(links)\n            )\n        )\n\n        self.pyload.files.save()\n\n        return package_id\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _keyify(key):\n    return _key_pattern.sub(' ', key.lower())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& a = context->input(0);\n    const Tensor& b = context->input(1);\n    const float min_a = context->input(2).flat<float>()(0);\n    const float max_a = context->input(3).flat<float>()(0);\n    const float min_b = context->input(4).flat<float>()(0);\n    const float max_b = context->input(5).flat<float>()(0);\n\n    // Make sure that we have valid quantization ranges for the input buffers.\n    // If the difference between the min and max is negative or zero, it makes\n    // it hard to do meaningful intermediate operations on the values.\n    OP_REQUIRES(context, (max_a > min_a),\n                errors::InvalidArgument(\"max_a must be larger than min_a.\"));\n    OP_REQUIRES(context, (max_b > min_b),\n                errors::InvalidArgument(\"max_b must be larger than min_b.\"));\n    const int32_t offset_a = FloatToQuantizedUnclamped<T1>(0.0f, min_a, max_a);\n    const int32_t offset_b = FloatToQuantizedUnclamped<T2>(0.0f, min_b, max_b);\n    const int32_t offset_c = 0;\n    const int32_t mult_c = 1;\n    const int32_t shift_c = 0;\n\n    // Check that the dimensions of the two matrices are valid.\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(a.shape()),\n                errors::InvalidArgument(\"In[0] is not a matrix\"));\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(b.shape()),\n                errors::InvalidArgument(\"In[1] is not a matrix\"));\n    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n    dim_pair[0].first = transpose_a_ ? 0 : 1;\n    dim_pair[0].second = transpose_b_ ? 1 : 0;\n\n    OP_REQUIRES(context,\n                a.dim_size(dim_pair[0].first) == b.dim_size(dim_pair[0].second),\n                errors::InvalidArgument(\"Matrix size-incompatible: In[0]: \",\n                                        a.shape().DebugString(),\n                                        \", In[1]: \", b.shape().DebugString()));\n\n    OP_REQUIRES(context, ((shift_c >= 0) && (shift_c <= 31)),\n                errors::InvalidArgument(\"shift_c must be between 0 and 31, \"\n                                        \"inclusive.\"));\n\n    int a_dim_remaining = 1 - dim_pair[0].first;\n    int b_dim_remaining = 1 - dim_pair[0].second;\n    TensorShape out_shape(\n        {a.dim_size(a_dim_remaining), b.dim_size(b_dim_remaining)});\n    Tensor* c = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &c));\n    CHECK(c);\n\n    const T1* a_data = a.flat<T1>().data();\n    const T2* b_data = b.flat<T2>().data();\n    Toutput* c_data = c->flat<Toutput>().data();\n\n    const bool transpose_c = false;\n    const size_t m = a.dim_size(a_dim_remaining);\n    const size_t n = b.dim_size(b_dim_remaining);\n    const size_t k = a.dim_size(dim_pair[0].first);\n    const size_t lda = a.dim_size(1);\n    const size_t ldb = b.dim_size(1);\n    const size_t ldc = n;\n\n    if (meta::IsSupportedAndEnabled() && std::is_same<T1, quint8>() &&\n        std::is_same<T2, quint8>() && std::is_same<Toutput, qint32>() &&\n        (offset_c == 0) && (mult_c == 1) && (shift_c == 0) &&\n        (transpose_c == false) && (k <= 2048)) {\n      // Gemmlowp/meta code path works on 32 & 64 bit Arm with NEON Simd and\n      // allows optimized quantized 8bit to 32bit gemm.\n      meta::QuantizedGemm(context, transpose_a_, transpose_b_, a_data, b_data,\n                          c_data, m, n, k, -offset_a, -offset_b, lda, ldb, ldc);\n    } else if (std::is_same<T1, quint8>() && std::is_same<T2, quint8>() &&\n               std::is_same<Toutput, qint32>() && (offset_c == 0) &&\n               (mult_c == 1) && (shift_c == 0) && (transpose_c == false)) {\n      // The gemmlowp optimized library only works for a particular set of data\n      // types, so check if we meet those requirements and fall back to a slower\n      // reference implementation if not.\n      if (transpose_a_) {\n        if (transpose_b_) {\n          GemmlowpMultiply<true, true, false>(context, a_data, b_data, c_data,\n                                              m, n, k, offset_a, offset_b, lda,\n                                              ldb, ldc);\n        } else {\n          GemmlowpMultiply<true, false, false>(context, a_data, b_data, c_data,\n                                               m, n, k, offset_a, offset_b, lda,\n                                               ldb, ldc);\n        }\n      } else {\n        if (transpose_b_) {\n          GemmlowpMultiply<false, true, false>(context, a_data, b_data, c_data,\n                                               m, n, k, offset_a, offset_b, lda,\n                                               ldb, ldc);\n        } else {\n          GemmlowpMultiply<false, false, false>(context, a_data, b_data, c_data,\n                                                m, n, k, offset_a, offset_b,\n                                                lda, ldb, ldc);\n        }\n      }\n    } else {\n      ReferenceGemm<T1, T2, Toutput>(\n          transpose_a_, transpose_b_, transpose_c, m, n, k, a_data, offset_a,\n          lda, b_data, offset_b, ldb, c_data, shift_c, offset_c, mult_c, ldc);\n    }\n\n    float min_c_value;\n    float max_c_value;\n    QuantizationRangeForMultiplication<T1, T2, Toutput>(\n        min_a, max_a, min_b, max_b, &min_c_value, &max_c_value);\n    Tensor* c_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &c_min));\n    c_min->flat<float>()(0) = min_c_value;\n\n    Tensor* c_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &c_max));\n    c_max->flat<float>()(0) = max_c_value;\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def bad_url(prefix):\n    p = request.fullpath\n    if p.endswith(\"/\"):\n        p = p[:-1]\n    p = p.rsplit('/', 1)[0]\n    p += \"/simple/%s/\" % prefix\n\n    return redirect(p)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def authenticate(template, title):\n    \"\"\"\n    Authentication management method.\n    :param template: the template name that should be rendered\n    :returns: rendered final page or a page with error message\n    \"\"\"\n    global AUTH_ISSUERS, SAML_SUPPORT, AUTH_TYPE, COOKIES\n    valid_token_dict = validate_webui_token()\n    if not valid_token_dict:\n        COOKIES.append({'key': 'rucio-requested-path', 'value': request.environ.get('REQUEST_URI')})\n    else:\n        return access_granted(valid_token_dict, template, title)\n\n    # login without any known server config\n    if not AUTH_TYPE:\n        return add_cookies(make_response(render_template(\"select_login_method.html\", oidc_issuers=AUTH_ISSUERS, saml_support=SAML_SUPPORT)))\n    # for AUTH_TYPE predefined by the server continue\n    else:\n        if AUTH_TYPE == 'userpass':\n            return redirect('/login', code=303)\n        elif AUTH_TYPE == 'x509':\n            return x509token_auth(None)\n        elif AUTH_TYPE == 'x509_userpass':\n            if request.environ.get('SSL_CLIENT_VERIFY') == 'SUCCESS':\n                return x509token_auth(None)\n            return render_template(\"no_certificate.html\")\n        elif AUTH_TYPE == 'oidc':\n            return oidc_auth(None, AUTH_ISSUER_WEBUI)\n        elif AUTH_TYPE == 'saml':\n            return saml_auth(\"GET\")\n        return render_template('problem.html', msg='Invalid auth type')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_send_leave_request(self, origin, pdu):\n        \"\"\" We have received a leave event for a room. Fully process it.\"\"\"\n        event = pdu\n\n        logger.debug(\n            \"on_send_leave_request: Got event: %s, signatures: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        event.internal_metadata.outlier = False\n\n        context = yield self._handle_new_event(origin, event)\n\n        event_allowed = yield self.third_party_event_rules.check_event_allowed(\n            event, context\n        )\n        if not event_allowed:\n            logger.info(\"Sending of leave %s forbidden by third-party rules\", event)\n            raise SynapseError(\n                403, \"This event is not allowed in this context\", Codes.FORBIDDEN\n            )\n\n        logger.debug(\n            \"on_send_leave_request: After _handle_new_event: %s, sigs: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        return None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, redirect_to):\n        super(HttpResponseRedirect, self).__init__()\n        self['Location'] = iri_to_uri(redirect_to)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    const findHeadingElement = (cell: Cell): void => {\n      model.getCellHeadings(cell).forEach(async heading => {\n        const elementId = await getIdForHeading(heading, this.parser!);\n\n        const selector = elementId\n          ? `h${heading.level}[id=\"${elementId}\"]`\n          : `h${heading.level}`;\n\n        if (heading.outputIndex !== undefined) {\n          // Code cell\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              (heading.cellRef as CodeCell).outputArea.widgets[\n                heading.outputIndex\n              ].node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        } else {\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              heading.cellRef.node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        }\n      });\n    };\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def event_from_pdu_json(pdu_json, outlier=False):\n    \"\"\"Construct a FrozenEvent from an event json received over federation\n\n    Args:\n        pdu_json (object): pdu as received over federation\n        outlier (bool): True to mark this event as an outlier\n\n    Returns:\n        FrozenEvent\n\n    Raises:\n        SynapseError: if the pdu is missing required fields\n    \"\"\"\n    # we could probably enforce a bunch of other fields here (room_id, sender,\n    # origin, etc etc)\n    assert_params_in_request(pdu_json, ('event_id', 'type'))\n    event = FrozenEvent(\n        pdu_json\n    )\n\n    event.internal_metadata.outlier = outlier\n\n    return event\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    const Tensor& tag = c->input(0);\n    const Tensor& tensor = c->input(1);\n    OP_REQUIRES(c, TensorShapeUtils::IsScalar(tag.shape()),\n                errors::InvalidArgument(\"Tag must be a scalar\"));\n    OP_REQUIRES(c, tensor.dims() >= 2 && tensor.dims() <= 3,\n                errors::InvalidArgument(\"Tensor must be 3-D or 2-D, got: \",\n                                        tensor.shape().DebugString()));\n    const string& base_tag = tag.scalar<tstring>()();\n\n    float sample_rate = sample_rate_attr_;\n    if (!has_sample_rate_attr_) {\n      const Tensor& sample_rate_tensor = c->input(2);\n      sample_rate = sample_rate_tensor.scalar<float>()();\n    }\n    OP_REQUIRES(c, sample_rate > 0.0f,\n                errors::InvalidArgument(\"sample_rate must be > 0\"));\n\n    const int batch_size = tensor.dim_size(0);\n    const int64_t length_frames = tensor.dim_size(1);\n    const int64_t num_channels =\n        tensor.dims() == 2 ? 1 : tensor.dim_size(tensor.dims() - 1);\n\n    Summary s;\n    const int N = std::min<int>(max_outputs_, batch_size);\n    for (int i = 0; i < N; ++i) {\n      Summary::Value* v = s.add_value();\n      if (max_outputs_ > 1) {\n        v->set_tag(strings::StrCat(base_tag, \"/audio/\", i));\n      } else {\n        v->set_tag(strings::StrCat(base_tag, \"/audio\"));\n      }\n\n      Summary::Audio* sa = v->mutable_audio();\n      sa->set_sample_rate(sample_rate);\n      sa->set_num_channels(num_channels);\n      sa->set_length_frames(length_frames);\n      sa->set_content_type(\"audio/wav\");\n\n      auto values =\n          tensor.shaped<float, 3>({batch_size, length_frames, num_channels});\n      const float* data =\n          tensor.NumElements() == 0 ? nullptr : &values(i, 0, 0);\n\n      size_t sample_rate_truncated = lrintf(sample_rate);\n      if (sample_rate_truncated == 0) {\n        sample_rate_truncated = 1;\n      }\n      OP_REQUIRES_OK(c, wav::EncodeAudioAsS16LEWav(\n                            data, sample_rate_truncated, num_channels,\n                            length_frames, sa->mutable_encoded_audio_string()));\n    }\n\n    Tensor* summary_tensor = nullptr;\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape({}), &summary_tensor));\n    CHECK(SerializeToTString(s, &summary_tensor->scalar<tstring>()()));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_header_end(self) -> None:\n        message = (MultiPartMessage.HEADER_END, b\"\")\n        self.messages.append(message)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create_code_for_user(cls, user, next=None):\n        if not user.is_active:\n            return None\n\n        code = cls.generate_code()\n        login_code = LoginCode(user=user, code=code)\n        if next is not None:\n            login_code.next = next\n        login_code.save()\n        return login_code\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    PartialTensorShape element_shape;\n    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(0), &element_shape));\n    int32 num_elements = c->input(1).scalar<int32>()();\n    TensorList output;\n    output.element_shape = element_shape;\n    output.element_dtype = element_dtype_;\n    output.tensors().resize(num_elements, Tensor(DT_INVALID));\n    Tensor* result;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape{}, &result, attr));\n    result->scalar<Variant>()() = std::move(output);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n    OP_REQUIRES_OK(\n        ctx, functor::UpperBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n\n    PoolParameters params{context,\n                          ksize_,\n                          stride_,\n                          padding_,\n                          /*explicit_paddings=*/{},\n                          FORMAT_NHWC,\n                          tensor_in.shape()};\n    if (!context->status().ok()) {\n      return;\n    }\n\n    TensorShape out_shape({params.tensor_in_batch, params.out_height,\n                           params.out_width, params.depth});\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n    Tensor* argmax = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, out_shape, &argmax));\n\n    LaunchMaxPoolingWithArgmax<Device, T, Targmax>::launch(\n        context, params, tensor_in, output, argmax, propagate_nans_,\n        include_batch_in_index_);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _wget_download(url, fullname):\n    # using wget to download url\n    tmp_fullname = fullname + \"_tmp\"\n    # \u2013user-agent\n    command = f'wget -O {tmp_fullname} -t {DOWNLOAD_RETRY_LIMIT} {url}'\n    subprc = subprocess.Popen(\n        command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n    )\n    _ = subprc.communicate()\n\n    if subprc.returncode != 0:\n        raise RuntimeError(\n            f'{command} failed. Please make sure `wget` is installed or {url} exists'\n        )\n\n    shutil.move(tmp_fullname, fullname)\n\n    return fullname\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def delete_shelf(shelf_id):\n    cur_shelf = ub.session.query(ub.Shelf).filter(ub.Shelf.id == shelf_id).first()\n    try:\n        delete_shelf_helper(cur_shelf)\n    except InvalidRequestError:\n        ub.session.rollback()\n        log.error(\"Settings DB is not Writeable\")\n        flash(_(\"Settings DB is not Writeable\"), category=\"error\")\n    return redirect(url_for('web.index'))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status TrySimplify(NodeDef* consumer, string* simplified_node_name) override {\n    NodeDef* producer;\n    TF_RETURN_IF_ERROR(GetInputNode(consumer->input(0), &producer));\n    const bool producer_is_cast = IsCastLike(*producer);\n    const bool can_optimize =\n        !IsCheckNumerics(*producer) &&\n        ((producer_is_cast && IsValuePreserving(*consumer)) ||\n         (IsValuePreserving(*producer) && IsCastLike(*consumer)));\n    if (!can_optimize || IsControlFlow(*producer) ||\n        IsInPreserveSet(*producer) ||\n        producer->device() != consumer->device()) {\n      return Status::OK();\n    }\n\n    const NodeDef* cast_like_node = producer_is_cast ? producer : consumer;\n    const OpDef* cast_like_op_def = nullptr;\n    TF_RETURN_IF_ERROR(OpRegistry::Global()->LookUpOpDef(cast_like_node->op(),\n                                                         &cast_like_op_def));\n    DataType cast_src_type;\n    TF_RETURN_IF_ERROR(InputTypeForNode(*cast_like_node, *cast_like_op_def, 0,\n                                        &cast_src_type));\n    DataType cast_dst_type;\n    TF_RETURN_IF_ERROR(OutputTypeForNode(*cast_like_node, *cast_like_op_def, 0,\n                                         &cast_dst_type));\n    if (!IsFixedSizeType(cast_src_type) || !IsFixedSizeType(cast_dst_type)) {\n      return Status::OK();\n    } else if (producer_is_cast &&\n               DataTypeSize(cast_dst_type) <= DataTypeSize(cast_src_type)) {\n      return Status::OK();\n    } else if (!producer_is_cast &&\n               DataTypeSize(cast_dst_type) >= DataTypeSize(cast_src_type)) {\n      return Status::OK();\n    }\n\n    // Check that nodes were not already optimized.\n    const string optimized_producer_name = OptimizedNodeName(\n        ParseNodeScopeAndName(producer->name()), DataTypeString(cast_dst_type));\n    const string optimized_consumer_name = OptimizedNodeName(\n        ParseNodeScopeAndName(consumer->name()), DataTypeString(cast_src_type));\n    const bool is_already_optimized =\n        ctx().node_map->NodeExists(optimized_consumer_name) ||\n        ctx().node_map->NodeExists(optimized_producer_name);\n    if (is_already_optimized) {\n      return Status::OK();\n    }\n\n    // Add copies of consumer and producer in reverse order.\n    NodeDef* input;\n    TF_RETURN_IF_ERROR(GetInputNode(producer->input(0), &input));\n    // Create new producer node.\n    NodeDef* new_producer = AddCopyNode(optimized_consumer_name, consumer);\n    new_producer->set_input(0, producer->input(0));\n    ctx().node_map->AddOutput(input->name(), new_producer->name());\n\n    // Create new consumer node.\n    NodeDef* new_consumer = AddCopyNode(optimized_producer_name, producer);\n    new_consumer->set_input(0, new_producer->name());\n\n    NodeDef* new_value_preserving =\n        producer_is_cast ? new_producer : new_consumer;\n    const DataType new_input_type =\n        producer_is_cast ? cast_src_type : cast_dst_type;\n    // Update the input type of the value-preserving node. The input and\n    // output types of the cast-like nodes remain the same.\n    TF_RETURN_IF_ERROR(SetInputType(new_input_type, new_value_preserving));\n    // Make sure there is a kernel registered for the value preserving op\n    // with the new input type.\n    TF_RETURN_IF_ERROR(IsKernelRegisteredForNode(*new_value_preserving));\n    ctx().node_map->AddOutput(new_producer->name(), new_consumer->name());\n\n    AddToOptimizationQueue(new_producer);\n    *simplified_node_name = new_consumer->name();\n\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def wrapper(self, *args, **kwargs):\n        try:\n            result = method(self, *args, **kwargs)\n        except web.HTTPError as e:\n            status = e.status_code\n            message = e.log_message\n            self.log.warn(message)\n            self.set_status(e.status_code)\n            self.finish(json.dumps(dict(message=message)))\n        except Exception:\n            self.log.error(\"Unhandled error in API request\", exc_info=True)\n            status = 500\n            message = \"Unknown server error\"\n            t, value, tb = sys.exc_info()\n            self.set_status(status)\n            tb_text = ''.join(traceback.format_exception(t, value, tb))\n            reply = dict(message=message, traceback=tb_text)\n            self.finish(json.dumps(reply))\n        else:\n            return result\n    return wrapper"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _init_file(self):\n        \"\"\"Init the password file, set the password for it.\n        \"\"\"\n\n        password = None\n        while 1:\n            if not password:\n                password = self._getpass(\"Please set a password for your new keyring\")\n                password2 = self._getpass('Password (again): ')\n                if password != password2:\n                    sys.stderr.write(\"Error: Your passwords didn't match\\n\")\n                    password = None\n                    continue\n            if '' == password.strip():\n                # forbid the blank password\n                sys.stderr.write(\"Error: blank passwords aren't allowed.\\n\")\n                password = None\n                continue\n            if len(password) > _BLOCK_SIZE:\n                # block size of AES is less than 32\n                sys.stderr.write(\"Error: password can't be longer than 32.\\n\")\n                password = None\n                continue\n            break\n\n        # hash the password\n        import crypt\n        self.crypted_password = crypt.crypt(password, password)\n\n        # write down the initialization\n        config = ConfigParser.RawConfigParser()\n        config.add_section(_KEYRING_SETTING)\n        config.set(_KEYRING_SETTING, _CRYPTED_PASSWORD, self.crypted_password)\n\n        config_file = open(self.file_path,'w')\n        config.write(config_file)\n\n        if config_file:\n            config_file.close()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    auto x = ctx->input(0);\n    auto i = ctx->input(1);\n    auto v = ctx->input(2);\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(i.shape()),\n                errors::InvalidArgument(\"i must be a vector. \",\n                                        i.shape().DebugString()));\n    OP_REQUIRES(ctx, x.dims() == v.dims(),\n                errors::InvalidArgument(\n                    \"x and v shape doesn't match (ranks differ): \",\n                    x.shape().DebugString(), \" vs. \", v.shape().DebugString()));\n    for (int i = 1; i < x.dims(); ++i) {\n      OP_REQUIRES(\n          ctx, x.dim_size(i) == v.dim_size(i),\n          errors::InvalidArgument(\"x and v shape doesn't match at index \", i,\n                                  \" : \", x.shape().DebugString(), \" vs. \",\n                                  v.shape().DebugString()));\n    }\n    OP_REQUIRES(ctx, i.dim_size(0) == v.dim_size(0),\n                errors::InvalidArgument(\n                    \"i and x shape doesn't match at index 0: \",\n                    i.shape().DebugString(), \" vs. \", v.shape().DebugString()));\n\n    Tensor y = x;  // This creates an alias intentionally.\n    // Skip processing if tensors are empty.\n    if (x.NumElements() > 0 || v.NumElements() > 0) {\n      OP_REQUIRES_OK(ctx, DoCompute(ctx, i, v, &y));\n    }\n    ctx->set_output(0, y);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def guard(dest, ev_args):\n    next(dest)\n    depth = 1\n    while True:\n        ev = yield\n        if ev[0] == \"start\":\n            depth += 1\n        elif ev[0] == \"end\":\n            depth -= 1\n        try:\n            dest.send(ev)\n        except StopIteration as exc:\n            return exc.value\n        except Exception as exc:\n            error = exc\n            break\n    while depth > 0:\n        ev_type, *_ = yield\n        if ev_type == \"end\":\n            depth -= 1\n    raise error\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_session_id_by_provider_id(self, provider_id):\n        \"\"\"\n        Return a tuple of logged-in session IDs by provider_id\n        \"\"\"\n        candidates = self._ss.get_user_sessions(self.user)\n\n        session_ids = []\n        for c in candidates:\n            key = c.keys()[0]\n            if c[key].get('provider_id') == provider_id:\n                samlsession = self._data_to_samlsession(key, c[key])\n                session_ids.append(samlsession.session_id.encode('utf-8'))\n\n        return tuple(session_ids)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def real_download(self, filename, info_dict):\n        self.report_destination(filename)\n        tmpfilename = self.temp_name(filename)\n\n        try:\n            started = time.time()\n            retval = self._call_downloader(tmpfilename, info_dict)\n        except KeyboardInterrupt:\n            if not info_dict.get('is_live'):\n                raise\n            # Live stream downloading cancellation should be considered as\n            # correct and expected termination thus all postprocessing\n            # should take place\n            retval = 0\n            self.to_screen('[%s] Interrupted by user' % self.get_basename())\n\n        if retval == 0:\n            status = {\n                'filename': filename,\n                'status': 'finished',\n                'elapsed': time.time() - started,\n            }\n            if filename != '-':\n                fsize = os.path.getsize(encodeFilename(tmpfilename))\n                self.try_rename(tmpfilename, filename)\n                status.update({\n                    'downloaded_bytes': fsize,\n                    'total_bytes': fsize,\n                })\n            self._hook_progress(status, info_dict)\n            return True\n        else:\n            self.to_stderr('\\n')\n            self.report_error('%s exited with code %d' % (\n                self.get_basename(), retval))\n            return False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) final {\n    const Tensor& superdiag = context->input(0);\n    const Tensor& maindiag = context->input(1);\n    const Tensor& subdiag = context->input(2);\n    const Tensor& rhs = context->input(3);\n\n    const int ndims = rhs.dims();\n    int64 batch_size = 1;\n    for (int i = 0; i < ndims - 2; i++) {\n      batch_size *= rhs.dim_size(i);\n    }\n    const int m = rhs.dim_size(ndims - 2);\n    const int n = rhs.dim_size(ndims - 1);\n\n    // Allocate output.\n    Tensor* output;\n    OP_REQUIRES_OK(context, context->allocate_output(0, rhs.shape(), &output));\n\n    const Eigen::GpuDevice& device = context->eigen_device<Eigen::GpuDevice>();\n    GpuLaunchConfig cfg = GetGpuLaunchConfig(1, device);\n    TF_CHECK_OK(GpuLaunchKernel(\n        TridiagonalMatMulKernel<Scalar>, cfg.block_count, cfg.thread_per_block,\n        0, device.stream(), batch_size, m, n, superdiag.flat<Scalar>().data(),\n        maindiag.flat<Scalar>().data(), subdiag.flat<Scalar>().data(),\n        rhs.flat<Scalar>().data(), output->flat<Scalar>().data()));\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def upload_template(filename, destination, context=None, use_jinja=False,\n    template_dir=None, use_sudo=False, backup=True, mirror_local_mode=False,\n    mode=None):\n    \"\"\"\n    Render and upload a template text file to a remote host.\n\n    ``filename`` should be the path to a text file, which may contain `Python\n    string interpolation formatting\n    <http://docs.python.org/release/2.5.4/lib/typesseq-strings.html>`_ and will\n    be rendered with the given context dictionary ``context`` (if given.)\n\n    Alternately, if ``use_jinja`` is set to True and you have the Jinja2\n    templating library available, Jinja will be used to render the template\n    instead. Templates will be loaded from the invoking user's current working\n    directory by default, or from ``template_dir`` if given.\n\n    The resulting rendered file will be uploaded to the remote file path\n    ``destination`` (which should include the desired remote filename.) If the\n    destination file already exists, it will be renamed with a ``.bak``\n    extension unless ``backup=False`` is specified.\n\n    By default, the file will be copied to ``destination`` as the logged-in\n    user; specify ``use_sudo=True`` to use `sudo` instead.\n\n    In some use cases, it is desirable to force a newly uploaded file to match\n    the mode of its local counterpart (such as when uploading executable\n    scripts). To do this, specify ``mirror_local_mode=True``.\n\n    Alternately, you may use the ``mode`` kwarg to specify an exact mode, in\n    the same vein as ``os.chmod`` or the Unix ``chmod`` command.\n    \"\"\"\n    basename = os.path.basename(filename)\n\n    # This temporary file should not be automatically deleted on close, as we\n    # need it there to upload it (Windows locks the file for reading while\n    # open).\n    tempdir = tempfile.mkdtemp()\n    tempfile_name = os.path.join(tempdir, basename)\n    output = open(tempfile_name, \"w+b\")\n    # Init\n    text = None\n    if use_jinja:\n        try:\n            from jinja2 import Environment, FileSystemLoader\n            jenv = Environment(loader=FileSystemLoader(template_dir or '.'))\n            text = jenv.get_template(filename).render(**context or {})\n        except ImportError, e:\n            abort(\"tried to use Jinja2 but was unable to import: %s\" % e)\n    else:\n        with open(filename) as inputfile:\n            text = inputfile.read()\n        if context:\n            text = text % context\n    output.write(text)\n    output.close()\n\n    # Back up any original file \n    func = use_sudo and sudo or run\n    # Back up any original file (need to do figure out ultimate destination)\n    if backup:\n        to_backup = destination\n        with settings(hide('everything'), warn_only=True):\n            # Is destination a directory?\n            if func('test -f %s' % to_backup).failed:\n                # If so, tack on the filename to get \"real\" destination\n                to_backup = destination + '/' + basename\n        if exists(to_backup):\n            func(\"cp %s %s.bak\" % (to_backup, to_backup))\n\n    # Upload the file.\n    put(\n        local_path=tempfile_name,\n        remote_path=destination,\n        use_sudo=use_sudo,\n        mirror_local_mode=mirror_local_mode,\n        mode=mode\n    )\n    output.close()\n    os.remove(tempfile_name)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Subgraph::Invoke() {\n  if (!consistent_) {\n    ReportError(\"Invoke called on model that is not consistent.\");\n    return kTfLiteError;\n  }\n\n  TfLiteStatus status = kTfLiteOk;\n  if (state_ == kStateUninvokable) {\n    ReportError(\"Invoke called on model that is not ready.\");\n    return kTfLiteError;\n  } else if (memory_planner_ && !memory_planner_->HasNonPersistentMemory()) {\n    ReportError(\"Non-persistent memory is not available.\");\n    return kTfLiteError;\n  }\n  TFLITE_SCOPED_TAGGED_DEFAULT_PROFILE(profiler_.get(), \"Invoke\");\n\n  // Invocations are always done in node order.\n  // Note that calling Invoke repeatedly will cause the original memory plan to\n  // be reused, unless either ResizeInputTensor() or AllocateTensors() has been\n  // called.\n  for (int execution_plan_index = 0;\n       execution_plan_index < execution_plan_.size(); execution_plan_index++) {\n    if (execution_plan_index == next_execution_plan_index_to_prepare_) {\n      TF_LITE_ENSURE_STATUS(PrepareOpsAndTensors());\n      TF_LITE_ENSURE(&context_, next_execution_plan_index_to_prepare_ >=\n                                    execution_plan_index);\n    }\n    int node_index = execution_plan_[execution_plan_index];\n    TfLiteNode& node = nodes_and_registration_[node_index].first;\n    const TfLiteRegistration& registration =\n        nodes_and_registration_[node_index].second;\n\n    const char* op_name = nullptr;\n    if (profiler_) op_name = GetTFLiteOpName(registration);\n    TFLITE_SCOPED_TAGGED_OPERATOR_PROFILE(profiler_.get(), op_name, node_index);\n\n    for (int i = 0; i < node.inputs->size; ++i) {\n      int tensor_index = node.inputs->data[i];\n      if (tensor_index == kTfLiteOptionalTensor) {\n        continue;\n      }\n      TfLiteTensor* tensor = &tensors_[tensor_index];\n      if (tensor->delegate && tensor->delegate != node.delegate &&\n          tensor->data_is_stale) {\n        TF_LITE_ENSURE_STATUS(EnsureTensorDataIsReadable(tensor_index));\n      }\n      if (tensor->data.raw == nullptr && tensor->bytes > 0) {\n        if (registration.builtin_code == kTfLiteBuiltinReshape && i == 1) {\n          // In general, having a tensor here with no buffer will be an error.\n          // However, for the reshape operator, the second input tensor is only\n          // used for the shape, not for the data. Thus, null buffer is ok.\n          continue;\n        } else {\n          // In all other cases, we need to return an error as otherwise we will\n          // trigger a null pointer dereference (likely).\n          ReportError(\"Input tensor %d lacks data\", tensor_index);\n          return kTfLiteError;\n        }\n      }\n    }\n\n    if (check_cancelled_func_ != nullptr &&\n        check_cancelled_func_(cancellation_data_)) {\n      ReportError(\"Client requested cancel during Invoke()\");\n      return kTfLiteError;\n    }\n\n    EnsureTensorsVectorCapacity();\n    tensor_resized_since_op_invoke_ = false;\n    if (OpInvoke(registration, &node) != kTfLiteOk) {\n      return ReportOpError(&context_, node, registration, node_index,\n                           \"failed to invoke\");\n    }\n\n    // Force execution prep for downstream ops if the latest op triggered the\n    // resize of a dynamic tensor.\n    if (tensor_resized_since_op_invoke_ &&\n        HasDynamicTensor(context_, node.outputs)) {\n      next_execution_plan_index_to_prepare_ = execution_plan_index + 1;\n\n      // This happens when an intermediate dynamic tensor is resized.\n      // We don't have to prepare all the ops, but we need to recompute\n      // the allocation plan.\n      if (next_execution_plan_index_to_plan_allocation_ >\n          next_execution_plan_index_to_prepare_) {\n        next_execution_plan_index_to_plan_allocation_ =\n            next_execution_plan_index_to_prepare_;\n        if (memory_planner_) {\n          TF_LITE_ENSURE_STATUS(memory_planner_->ResetAllocationsAfter(\n              next_execution_plan_index_to_plan_allocation_ - 1));\n        }\n      }\n    }\n  }\n\n  return status;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def do_template(self, data, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, disable_lookups=False):\n        if self.jinja2_native and not isinstance(data, string_types):\n            return data\n\n        # For preserving the number of input newlines in the output (used\n        # later in this method)\n        data_newlines = _count_newlines_from_end(data)\n\n        if fail_on_undefined is None:\n            fail_on_undefined = self._fail_on_undefined_errors\n\n        try:\n            # allows template header overrides to change jinja2 options.\n            if overrides is None:\n                myenv = self.environment\n            else:\n                myenv = self.environment.overlay(overrides)\n\n            # Get jinja env overrides from template\n            if hasattr(data, 'startswith') and data.startswith(JINJA2_OVERRIDE):\n                eol = data.find('\\n')\n                line = data[len(JINJA2_OVERRIDE):eol]\n                data = data[eol + 1:]\n                for pair in line.split(','):\n                    (key, val) = pair.split(':')\n                    key = key.strip()\n                    setattr(myenv, key, ast.literal_eval(val.strip()))\n\n            if escape_backslashes:\n                # Allow users to specify backslashes in playbooks as \"\\\\\" instead of as \"\\\\\\\\\".\n                data = _escape_backslashes(data, myenv)\n\n            try:\n                t = myenv.from_string(data)\n            except TemplateSyntaxError as e:\n                raise AnsibleError(\"template error while templating string: %s. String: %s\" % (to_native(e), to_native(data)))\n            except Exception as e:\n                if 'recursion' in to_native(e):\n                    raise AnsibleError(\"recursive loop detected in template string: %s\" % to_native(data))\n                else:\n                    return data\n\n            if disable_lookups:\n                t.globals['query'] = t.globals['q'] = t.globals['lookup'] = self._fail_lookup\n\n            jvars = AnsibleJ2Vars(self, t.globals)\n\n            self.cur_context = new_context = t.new_context(jvars, shared=True)\n            rf = t.root_render_func(new_context)\n\n            try:\n                if self.jinja2_native:\n                    res = ansible_native_concat(rf)\n                else:\n                    res = j2_concat(rf)\n                if getattr(new_context, 'unsafe', False):\n                    res = wrap_var(res)\n            except TypeError as te:\n                if 'AnsibleUndefined' in to_native(te):\n                    errmsg = \"Unable to look up a name or access an attribute in template string (%s).\\n\" % to_native(data)\n                    errmsg += \"Make sure your variable name does not contain invalid characters like '-': %s\" % to_native(te)\n                    raise AnsibleUndefinedVariable(errmsg)\n                else:\n                    display.debug(\"failing because of a type error, template data is: %s\" % to_text(data))\n                    raise AnsibleError(\"Unexpected templating type error occurred on (%s): %s\" % (to_native(data), to_native(te)))\n\n            if self.jinja2_native and not isinstance(res, string_types):\n                return res\n\n            if preserve_trailing_newlines:\n                # The low level calls above do not preserve the newline\n                # characters at the end of the input data, so we use the\n                # calculate the difference in newlines and append them\n                # to the resulting output for parity\n                #\n                # jinja2 added a keep_trailing_newline option in 2.7 when\n                # creating an Environment.  That would let us make this code\n                # better (remove a single newline if\n                # preserve_trailing_newlines is False).  Once we can depend on\n                # that version being present, modify our code to set that when\n                # initializing self.environment and remove a single trailing\n                # newline here if preserve_newlines is False.\n                res_newlines = _count_newlines_from_end(res)\n                if data_newlines > res_newlines:\n                    res += self.environment.newline_sequence * (data_newlines - res_newlines)\n            return res\n        except (UndefinedError, AnsibleUndefinedVariable) as e:\n            if fail_on_undefined:\n                raise AnsibleUndefinedVariable(e)\n            else:\n                display.debug(\"Ignoring undefined failure: %s\" % to_text(e))\n                return data\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_user_roles(self, context, user_id, tenant_id=None):\n        \"\"\"Get the roles for a user and tenant pair.\n\n        Since we're trying to ignore the idea of user-only roles we're\n        not implementing them in hopes that the idea will die off.\n\n        \"\"\"\n        if tenant_id is None:\n            raise exception.NotImplemented(message='User roles not supported: '\n                                                   'tenant ID required')\n\n        user = self.identity_api.get_user(context, user_id)\n        if user is None:\n            raise exception.UserNotFound(user_id=user_id)\n        tenant = self.identity_api.get_tenant(context, tenant_id)\n        if tenant is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        roles = self.identity_api.get_roles_for_user_and_tenant(\n                context, user_id, tenant_id)\n        return {'roles': [self.identity_api.get_role(context, x)\n                          for x in roles]}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_valid_filename(s):\n    \"\"\"\n    Return the given string converted to a string that can be used for a clean\n    filename. Remove leading and trailing spaces; convert other spaces to\n    underscores; and remove anything that is not an alphanumeric, dash,\n    underscore, or dot.\n    >>> get_valid_filename(\"john's portrait in 2004.jpg\")\n    'johns_portrait_in_2004.jpg'\n    \"\"\"\n    s = str(s).strip().replace(' ', '_')\n    return re.sub(r'(?u)[^-\\w.]', '', s)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def decorated(*args, **kwargs):\n            __tracebackhide__ = True  # Hide from pytest traceback.\n\n            appbuilder = current_app.appbuilder\n\n            dag_id = (\n                request.args.get(\"dag_id\")\n                or request.form.get(\"dag_id\")\n                or (request.is_json and request.json.get(\"dag_id\"))\n                or None\n            )\n            if appbuilder.sm.check_authorization(permissions, dag_id):\n                return func(*args, **kwargs)\n            elif not g.user.is_anonymous and not g.user.perms:\n                return (\n                    render_template(\n                        \"airflow/no_roles_permissions.html\",\n                        hostname=get_hostname()\n                        if conf.getboolean(\"webserver\", \"EXPOSE_HOSTNAME\")\n                        else \"redact\",\n                        logout_url=appbuilder.get_url_for_logout,\n                    ),\n                    403,\n                )\n            else:\n                access_denied = \"Access is Denied\"\n                flash(access_denied, \"danger\")\n            return redirect(\n                url_for(\n                    appbuilder.sm.auth_view.__class__.__name__ + \".login\",\n                    next=request.url,\n                )\n            )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def generate_nonce(length=8):\n    \"\"\"Generate pseudorandom number.\"\"\"\n    return ''.join([str(random.randint(0, 9)) for i in range(length)])\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "ImagingSgiRleDecode(Imaging im, ImagingCodecState state, UINT8 *buf, Py_ssize_t bytes) {\n    UINT8 *ptr;\n    SGISTATE *c;\n    int err = 0;\n    int status;\n\n    /* size check */\n    if (im->xsize > INT_MAX / im->bands || im->ysize > INT_MAX / im->bands) {\n        state->errcode = IMAGING_CODEC_MEMORY;\n        return -1;\n    }\n\n    /* Get all data from File descriptor */\n    c = (SGISTATE *)state->context;\n    _imaging_seek_pyFd(state->fd, 0L, SEEK_END);\n    c->bufsize = _imaging_tell_pyFd(state->fd);\n    c->bufsize -= SGI_HEADER_SIZE;\n\n    c->tablen = im->bands * im->ysize;\n    /* below, we populate the starttab and lentab into the bufsize,\n       each with 4 bytes per element of tablen\n       Check here before we allocate any memory\n    */\n    if (c->bufsize < 8 * c->tablen) {\n        state->errcode = IMAGING_CODEC_OVERRUN;\n        return -1;\n    }\n\n    ptr = malloc(sizeof(UINT8) * c->bufsize);\n    if (!ptr) {\n        state->errcode = IMAGING_CODEC_MEMORY;\n        return -1;\n    }\n    _imaging_seek_pyFd(state->fd, SGI_HEADER_SIZE, SEEK_SET);\n    _imaging_read_pyFd(state->fd, (char *)ptr, c->bufsize);\n\n    /* decoder initialization */\n    state->count = 0;\n    state->y = 0;\n    if (state->ystep < 0) {\n        state->y = im->ysize - 1;\n    } else {\n        state->ystep = 1;\n    }\n\n    /* Allocate memory for RLE tables and rows */\n    free(state->buffer);\n    state->buffer = NULL;\n    /* malloc overflow check above */\n    state->buffer = calloc(im->xsize * im->bands, sizeof(UINT8) * 2);\n    c->starttab = calloc(c->tablen, sizeof(UINT32));\n    c->lengthtab = calloc(c->tablen, sizeof(UINT32));\n    if (!state->buffer || !c->starttab || !c->lengthtab) {\n        err = IMAGING_CODEC_MEMORY;\n        goto sgi_finish_decode;\n    }\n    /* populate offsets table */\n    for (c->tabindex = 0, c->bufindex = 0; c->tabindex < c->tablen;\n         c->tabindex++, c->bufindex += 4) {\n        read4B(&c->starttab[c->tabindex], &ptr[c->bufindex]);\n    }\n    /* populate lengths table */\n    for (c->tabindex = 0, c->bufindex = c->tablen * sizeof(UINT32);\n         c->tabindex < c->tablen;\n         c->tabindex++, c->bufindex += 4) {\n        read4B(&c->lengthtab[c->tabindex], &ptr[c->bufindex]);\n    }\n\n    state->count += c->tablen * sizeof(UINT32) * 2;\n\n    /* read compressed rows */\n    for (c->rowno = 0; c->rowno < im->ysize; c->rowno++, state->y += state->ystep) {\n        for (c->channo = 0; c->channo < im->bands; c->channo++) {\n            c->rleoffset = c->starttab[c->rowno + c->channo * im->ysize];\n            c->rlelength = c->lengthtab[c->rowno + c->channo * im->ysize];\n            c->rleoffset -= SGI_HEADER_SIZE;\n\n            if (c->rleoffset + c->rlelength > c->bufsize) {\n                state->errcode = IMAGING_CODEC_OVERRUN;\n                goto sgi_finish_decode;\n            }\n\n            /* row decompression */\n            if (c->bpc == 1) {\n                status = expandrow(\n                    &state->buffer[c->channo],\n                    &ptr[c->rleoffset],\n                    c->rlelength,\n                    im->bands,\n                    im->xsize);\n            } else {\n                status = expandrow2(\n                    &state->buffer[c->channo * 2],\n                    &ptr[c->rleoffset],\n                    c->rlelength,\n                    im->bands,\n                    im->xsize);\n            }\n            if (status == -1) {\n                state->errcode = IMAGING_CODEC_OVERRUN;\n                goto sgi_finish_decode;\n            } else if (status == 1) {\n                goto sgi_finish_decode;\n            }\n\n            state->count += c->rlelength;\n        }\n\n        /* store decompressed data in image */\n        state->shuffle((UINT8 *)im->image[state->y], state->buffer, im->xsize);\n    }\n\n    c->bufsize++;\n\nsgi_finish_decode:;\n\n    free(c->starttab);\n    free(c->lengthtab);\n    free(ptr);\n    if (err != 0) {\n        state->errcode = err;\n        return -1;\n    }\n    return state->count - c->bufsize;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_initial_logout(self):\n        \"\"\"\n        Get the initial logout request.\n\n        Return None if no sessions in INIT_LOGOUT state.\n        \"\"\"\n        candidates = self._ss.get_user_sessions(self.user)\n\n        # FIXME: what does it mean if there are multiple in init? We\n        #        just return the first one for now. How do we know\n        #        it's the \"right\" one if multiple logouts are started\n        #        at the same time from different SPs?\n        for c in candidates:\n            key = c.keys()[0]\n            if int(c[key].get('logoutstate', 0)) == INIT_LOGOUT:\n                samlsession = self._data_to_samlsession(key, c[key])\n                return samlsession\n        return None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_image_dimensions(file_or_path, close=False):\n    \"\"\"\n    Returns the (width, height) of an image, given an open file or a path.  Set\n    'close' to True to close the file at the end if it is initially in an open\n    state.\n    \"\"\"\n    # Try to import PIL in either of the two ways it can end up installed.\n    try:\n        from PIL import ImageFile as PILImageFile\n    except ImportError:\n        import ImageFile as PILImageFile\n\n    p = PILImageFile.Parser()\n    if hasattr(file_or_path, 'read'):\n        file = file_or_path\n        file_pos = file.tell()\n        file.seek(0)\n    else:\n        file = open(file_or_path, 'rb')\n        close = True\n    try:\n        while 1:\n            data = file.read(1024)\n            if not data:\n                break\n            p.feed(data)\n            if p.image:\n                return p.image.size\n        return None\n    finally:\n        if close:\n            file.close()\n        else:\n            file.seek(file_pos)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "                repeatedPassword: ko.observable(undefined),\n                passwordMismatch: ko.pureComputed(function () {\n                    return self.editor.password() !== self.editor.repeatedPassword();\n                }),\n                apikey: ko.observable(undefined),\n                active: ko.observable(undefined),\n                permissionSelectable: function (permission) {\n                    return true;\n                },\n                permissionSelected: function (permission) {\n                    var index = self.editor.permissions().indexOf(permission);\n                    return index >= 0;\n                },\n                togglePermission: function (permission) {\n                    var permissions = self.editor.permissions();\n                    var index = permissions.indexOf(permission);\n                    if (index < 0) {\n                        permissions.push(permission);\n                    } else {\n                        permissions.splice(index, 1);\n                    }\n                    self.editor.permissions(permissions);\n                },\n                groupSelected: function (group) {\n                    var index = self.editor.groups().indexOf(group);\n                    return index >= 0;\n                },\n                toggleGroup: function (group) {\n                    var groups = self.editor.groups();\n                    var index = groups.indexOf(group);\n                    if (index < 0) {\n                        groups.push(group);\n                    } else {\n                        groups.splice(index, 1);\n                    }\n                    self.editor.groups(groups);\n                },\n                joinedGroupPermissions: function (group) {\n                    return access.permissionList(group);\n                },\n                header: ko.observable(undefined),\n                new: ko.observable(true),\n                confirm: undefined,\n                valid: ko.pureComputed(function () {\n                    return (\n                        self.editor.name() &&\n                        self.editor.name().trim() &&\n                        (!self.editor.new() ||\n                            (self.editor.password() &&\n                                self.editor.password().trim() &&\n                                !self.editor.passwordMismatch()))\n                    );\n                }),\n                dangerRestricted: function () {\n                    return false;\n                },\n                dangerRestrictedText: gettext(\n                    \"This user may not have dangerous permissions.\"\n                )\n            };\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def cli(configfile, observed_since, setup_only=False, troubleshoot=False):\n    '''\n    Tenable.io -> Jira Cloud Transformer & Ingester\n    '''\n    # Load the config, but ensure that any additional fields are additive to the\n    # basic field set.\n    config_from_file = yaml.load(configfile, Loader=yaml.Loader)\n    fields = config_from_file.pop('custom_fields', list())\n    config = dict_merge(base_config(), config_from_file)\n    config['fields'] = config['fields'] + fields\n\n    if config['tenable'].get('tio_transform_tags'):\n        attr_cache = config['tenable'].get('tio_asset_attr_cache', list())\n        tag_attrs = config['tenable'].get('tio_transform_tags', list())\n        config['tenable']['tio_asset_attr_cache'] = attr_cache + tag_attrs\n\n\n    # Get the logging definition and define any defaults as need be.\n    log = config.get('log', {})\n    log_lvls = {'debug': 10, 'info': 20, 'warn': 30, 'error': 40}\n    log['level'] = log_lvls[log.get('level', 'warn')]\n    log['format'] = log.get('format',\n        '%(asctime)-15s %(name)s %(levelname)s %(message)s')\n\n    # Configure the root logging facility\n    if troubleshoot:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=log['format'],\n            filename='tenable_debug.log'\n        )\n    else:\n        logging.basicConfig(**log)\n\n    # Output some basic information detailing the config file used and the\n    # python version & system arch.\n    logging.info('Tenable2JiraCloud Version {}'.format(__version__))\n    logging.info('Using configuration file {}'.format(configfile.name))\n    uname = platform.uname()\n    logging.info('Running on Python {} {}/{}'.format(\n        '.'.join([str(i) for i in sys.version_info][0:3]),\n        uname[0], uname[-2]))\n\n    # instantiate the Jira object\n    jira = Jira(\n        'https://{}/rest/api/3'.format(config['jira']['address']),\n        config['jira']['api_username'],\n        config['jira']['api_token']\n    )\n\n    # Initiate the Tenable.io API model, the Ingester model, and start the\n    # ingestion and data transformation.\n    if config['tenable'].get('platform') == 'tenable.io':\n        if not observed_since:\n            # if no since field is supplied, then look in the config file to see\n            # if an age was applied, if not, then use the default of 30 days.\n            observed_since = arrow.now()\\\n                .shift(days=-config['tenable'].get('tio_age', 30))\\\n                .floor('day').timestamp()\n\n        source = TenableIO(\n            access_key=config['tenable'].get('access_key'),\n            secret_key=config['tenable'].get('secret_key'),\n            vendor='Tenable',\n            product='JiraCloud',\n            build=__version__\n        )\n        if int(source.session.details().get('permissions')) < 64:\n            logging.error('API Keys tie to non-admin user.')\n    elif config['tenable'].get('platform') == 'tenable.sc':\n        source = TenableSC(\n            config['tenable'].get('address'),\n            port=int(config['tenable'].get('port', 443)),\n            username=config['tenable'].get('username'),\n            password=config['tenable'].get('password'),\n            access_key=config['tenable'].get('access_key'),\n            secret_key=config['tenable'].get('secret_key'),\n            vendor='Tenable',\n            product='JiraCloud',\n            build=__version__\n        )\n    else:\n        logging.error('No valid Tenable platform configuration defined.')\n        exit(1)\n    ingest = Tio2Jira(source, jira, config)\n\n    if troubleshoot:\n        # if the troubleshooting flag is set, then we will be collecting some\n        # basic information and outputting it to the screen in a format that\n        # Github issues would expect to format it all pretty.  This should help\n        # reduce the amount of time that is spent with back-and-forth debugging.\n        try:\n            ingest.ingest(int(observed_since))\n        except:\n            logging.exception('Caught the following Exception')\n\n        # Some basic redaction of sensitive data, such as API Keys, Usernames,\n        # Passwords, and hostnames.\n        addr = config_from_file['jira']['address']\n        sc_addr = 'NOTHING_TO_SEE_HERE_AT_ALL'\n        config_from_file['jira']['address'] = '<REDACTED>'\n        config_from_file['jira']['api_token'] = '<REDACTED>'\n        config_from_file['jira']['api_username'] = '<REDACTED>'\n        config_from_file['project']['leadAccountId'] = '<REDACTED>'\n        if config_from_file['tenable'].get('address'):\n            sc_addr = config_from_file['tenable']['address']\n            config_from_file['tenable']['address'] = '<REDACTED>'\n        if config_from_file['tenable'].get('access_key'):\n            config_from_file['tenable']['access_key'] = '<REDACTED>'\n        if config_from_file['tenable'].get('secret_key'):\n            config_from_file['tenable']['secret_key'] = '<REDACTED>'\n        if config_from_file['tenable'].get('username'):\n            config_from_file['tenable']['username'] = '<REDACTED>'\n        if config_from_file['tenable'].get('password'):\n            config_from_file['tenable']['password'] = '<REDACTED>'\n\n        output = troubleshooting.format(\n            configfile=yaml.dump(config_from_file, default_flow_style=False),\n            logging=open('tenable_debug.log').read() \\\n                .replace(addr, '<JIRA_CLOUD_HOST>') \\\n                .replace(sc_addr, '<TENABLE_SC_HOST>'),\n            issuetypes='\\n'.join(\n                [\n                    '{id}: {name}'.format(**a)\n                    for a in jira.issue_types.list()\n                    if a.get('name').lower() in ['task', 'subtask', 'sub-task']\n                ]\n            )\n        )\n        print(output)\n        print('\\n'.join([\n            '/-------------------------------NOTICE-----------------------------------\\\\',\n            '| The output above is helpful for us to troubleshoot exactly what is     |',\n            '| happening within the code and offer a diagnosis for how to correct.    |',\n            '| Please note that while some basic redaction has already been performed |',\n            '| that we ask you to review the information you\\'re about to send and     |',\n            '| ensure that nothing deemed sensitive is transmitted.                   |',\n            '| ---------------------------------------------------------------------- |',\n            '| -- Copy of output saved to \"issue_debug.md\"                            |',\n            '\\\\------------------------------------------------------------------------/'\n        ]))\n        with open('issue_debug.md', 'w') as reportfile:\n            print(output, file=reportfile)\n        os.remove('tenable_debug.log')\n    elif not setup_only:\n        ingest.ingest(observed_since)\n\n        # If we are expected to continually re-run the transformer, then we will\n        # need to track the passage of time and run every X hours, where X is\n        # defined by the user in the configuration.\n        if config.get('service', {}).get('interval', 0) > 0:\n            sleeper = int(config['service']['interval']) * 3600\n            while True:\n                last_run = int(time.time())\n                logging.info(\n                    'Sleeping for {}h'.format(sleeper/3600))\n                time.sleep(sleeper)\n                logging.info(\n                    'Initiating ingest with observed_since={}'.format(last_run))\n                ingest.ingest(last_run)\n    elif setup_only:\n        # In setup-only mode, the ingest will not run, and instead a config file\n        # will be generated that will have all of the JIRA identifiers baked in\n        # and will also inform the integration to ignore the screen builder.\n        # When using this config, if there are any changes to the code, then\n        # this config will need to be re-generated.\n        config['screen']['no_create'] = True\n        logging.info('Set to setup-only.  Will not run ingest.')\n        logging.info('The following is the updated config file from the setup.')\n        with open('generated_config.yaml', 'w') as outfile:\n            outfile.write(yaml.dump(config, Dumper=yaml.Dumper))\n        logging.info('Generated \"generated_config.yaml\" config file.')\n        logging.info('This config file should be updated for every new version of this integration.')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def unarchive(request, bookmark_id: int):\n    bookmark = Bookmark.objects.get(pk=bookmark_id)\n    unarchive_bookmark(bookmark)\n    return_url = request.GET.get('return_url')\n    return_url = return_url if return_url else reverse('bookmarks:archived')\n    return HttpResponseRedirect(return_url)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _get_most_recent_version(self, allow_local):\n        # type: (bool) -> CryptographicMaterialsProvider\n        \"\"\"Get the most recent version of the provider.\n\n        If allowing local and we cannot obtain the lock, just return the most recent local\n        version. Otherwise, wait for the lock and ask the provider store for the most recent\n        version of the provider.\n\n        :param bool allow_local: Should we allow returning the local version if we cannot obtain the lock?\n        :returns: version and corresponding cryptographic materials provider\n        :rtype: CryptographicMaterialsProvider\n        \"\"\"\n        acquired = self._lock.acquire(not allow_local)\n\n        if not acquired:\n            # We failed to acquire the lock.\n            # If blocking, we will never reach this point.\n            # If not blocking, we want whatever the latest local version is.\n            _LOGGER.debug(\"Failed to acquire lock. Returning the last cached version.\")\n            version = self._version\n            return self._cache.get(version)\n\n        try:\n            max_version = self._get_max_version()\n            try:\n                provider = self._cache.get(max_version)\n            except KeyError:\n                provider = self._get_provider(max_version)\n            received_version = self._provider_store.version_from_material_description(\n                provider._material_description  # pylint: disable=protected-access\n            )\n\n            _LOGGER.debug(\"Caching materials provider version %d\", received_version)\n            self._version = received_version  # pylint: disable=attribute-defined-outside-init\n            self._last_updated = time.time()  # pylint: disable=attribute-defined-outside-init\n            self._cache.put(received_version, provider)\n        finally:\n            self._lock.release()\n\n        _LOGGER.debug(\"New latest version is %d\", self._version)\n\n        return provider\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static int ReadHuffmanCodeLengths(\n    VP8LDecoder* const dec, const int* const code_length_code_lengths,\n    int num_symbols, int* const code_lengths) {\n  int ok = 0;\n  VP8LBitReader* const br = &dec->br_;\n  int symbol;\n  int max_symbol;\n  int prev_code_len = DEFAULT_CODE_LENGTH;\n  HuffmanCode table[1 << LENGTHS_TABLE_BITS];\n\n  if (!VP8LBuildHuffmanTable(table, LENGTHS_TABLE_BITS,\n                             code_length_code_lengths,\n                             NUM_CODE_LENGTH_CODES)) {\n    goto End;\n  }\n\n  if (VP8LReadBits(br, 1)) {    // use length\n    const int length_nbits = 2 + 2 * VP8LReadBits(br, 3);\n    max_symbol = 2 + VP8LReadBits(br, length_nbits);\n    if (max_symbol > num_symbols) {\n      goto End;\n    }\n  } else {\n    max_symbol = num_symbols;\n  }\n\n  symbol = 0;\n  while (symbol < num_symbols) {\n    const HuffmanCode* p;\n    int code_len;\n    if (max_symbol-- == 0) break;\n    VP8LFillBitWindow(br);\n    p = &table[VP8LPrefetchBits(br) & LENGTHS_TABLE_MASK];\n    VP8LSetBitPos(br, br->bit_pos_ + p->bits);\n    code_len = p->value;\n    if (code_len < kCodeLengthLiterals) {\n      code_lengths[symbol++] = code_len;\n      if (code_len != 0) prev_code_len = code_len;\n    } else {\n      const int use_prev = (code_len == kCodeLengthRepeatCode);\n      const int slot = code_len - kCodeLengthLiterals;\n      const int extra_bits = kCodeLengthExtraBits[slot];\n      const int repeat_offset = kCodeLengthRepeatOffsets[slot];\n      int repeat = VP8LReadBits(br, extra_bits) + repeat_offset;\n      if (symbol + repeat > num_symbols) {\n        goto End;\n      } else {\n        const int length = use_prev ? prev_code_len : 0;\n        while (repeat-- > 0) code_lengths[symbol++] = length;\n      }\n    }\n  }\n  ok = 1;\n\n End:\n  if (!ok) return VP8LSetError(dec, VP8_STATUS_BITSTREAM_ERROR);\n  return ok;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_headers_finished(self) -> None:\n        message = (MultiPartMessage.HEADERS_FINISHED, b\"\")\n        self.messages.append(message)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "inline void Conv3DTranspose(\n    const Conv3DTransposeParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& bias_shape,\n    const float* bias_data, const RuntimeShape& output_shape,\n    float* output_data) {\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int stride_depth = params.stride_depth;\n  const int pad_width = params.padding_values.width;\n  const int pad_height = params.padding_values.height;\n  const int pad_depth = params.padding_values.depth;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_num_channels = MatchingDim(input_shape, 4, filter_shape, 4);\n  const int output_num_channels = output_shape.Dims(4);\n  const int input_depth = input_shape.Dims(1);\n  const int input_height = input_shape.Dims(2);\n  const int input_width = input_shape.Dims(3);\n  const int filter_depth = filter_shape.Dims(0);\n  const int filter_height = filter_shape.Dims(1);\n  const int filter_width = filter_shape.Dims(2);\n  const int output_depth = output_shape.Dims(1);\n  const int output_height = output_shape.Dims(2);\n  const int output_width = output_shape.Dims(3);\n  if (bias_data) {\n    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_num_channels);\n  }\n\n  // Initializes the output array to zero.\n  const int num_elements = output_shape.FlatSize();\n  for (int i = 0; i < num_elements; i++) {\n    output_data[i] = 0.0f;\n  }\n\n  // Loop through input elements one at a time.\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int in_d = 0; in_d < input_depth; ++in_d) {\n      for (int in_y = 0; in_y < input_height; ++in_y) {\n        for (int in_x = 0; in_x < input_width; ++in_x) {\n          for (int in_channel = 0; in_channel < input_num_channels;\n               ++in_channel) {\n            // Loop through the output elements it will influence.\n            const int out_x_origin = (in_x * stride_width) - pad_width;\n            const int out_y_origin = (in_y * stride_height) - pad_height;\n            const int out_d_origin = (in_d * stride_depth) - pad_depth;\n            for (int filter_d = 0; filter_d < filter_depth; ++filter_d) {\n              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {\n                  for (int out_channel = 0; out_channel < output_num_channels;\n                       ++out_channel) {\n                    // Compute output element location.\n                    const int out_x =\n                        out_x_origin + params.dilation_width * filter_x;\n                    const int out_y =\n                        out_y_origin + params.dilation_height * filter_y;\n                    const int out_d =\n                        out_d_origin + params.dilation_depth * filter_d;\n                    // We cannot accumulate out of bounds.\n                    if ((out_x >= 0) && (out_x < output_width) &&\n                        (out_y >= 0) && (out_y < output_height) &&\n                        (out_d >= 0) && (out_d < output_depth)) {\n                      float input_value = input_data[Offset(\n                          input_shape, batch, in_d, in_y, in_x, in_channel)];\n                      float filter_value = filter_data[Offset(\n                          filter_shape, filter_d, filter_y, filter_x,\n                          out_channel, in_channel)];\n                      output_data[Offset(output_shape, batch, out_d, out_y,\n                                         out_x, out_channel)] +=\n                          input_value * filter_value;\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n\n  const float float_activation_min = params.float_activation_min;\n  const float float_activation_max = params.float_activation_max;\n  float* data_ptr = output_data;\n  if (bias_data) {\n    const int outer_size =\n        batches * output_depth * output_height * output_width;\n    const int num_channels = input_shape.Dims(4);\n    for (int n = 0; n < outer_size; ++n) {\n      for (int c = 0; c < output_num_channels; ++c) {\n        data_ptr[c] = ActivationFunctionWithMinMax(data_ptr[c] + bias_data[c],\n                                                   float_activation_min,\n                                                   float_activation_max);\n      }\n      data_ptr += num_channels;\n    }\n  } else {\n    const int flat_size = output_shape.FlatSize();\n    for (int i = 0; i < flat_size; ++i) {\n      data_ptr[i] = ActivationFunctionWithMinMax(\n          data_ptr[i], float_activation_min, float_activation_max);\n    }\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_password(self, password, old_password=None):\n        \"\"\"\n        Change the user's password. Raise a ValueError if the username or\n        the password are invalid.\n        \"\"\"\n        assert isinstance(password, str)\n        assert old_password is None or isinstance(old_password, str)\n        if not password:\n            raise ValueError(\"password can't be empty\")\n        cfg = cherrypy.tree.apps[''].cfg\n\n        # Cannot update admin-password if defined\n        if self.username == cfg.admin_user and cfg.admin_password:\n            raise ValueError(_(\"can't update admin-password defined in configuration file\"))\n\n        if old_password and not check_password(old_password, self.hash_password):\n            raise ValueError(_(\"Wrong password\"))\n\n        # Check password length\n        if cfg.password_min_length > len(password) or len(password) > cfg.password_max_length:\n            raise ValueError(\n                _('Password must have between %(min)d and %(max)d characters.')\n                % {'min': cfg.password_min_length, 'max': cfg.password_max_length}\n            )\n\n        # Verify password score using zxcvbn\n        stats = zxcvbn(password)\n        if stats.get('score') < cfg.password_score:\n            msg = _('Password too weak.')\n            warning = stats.get('feedback', {}).get('warning')\n            suggestions = stats.get('feedback', {}).get('suggestions')\n            if warning:\n                msg += ' ' + warning\n            if suggestions:\n                msg += ' ' + ' '.join(suggestions)\n            raise ValueError(msg)\n\n        logger.info(\"updating user password [%s]\", self.username)\n        self.hash_password = hash_password(password)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __post_init__(self) -> None:\n        super().__post_init__()\n        if self.default is not None:\n            self.default = f'\"{self.default}\"'\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  public Response render(ServeEvent serveEvent) {\n    ResponseDefinition responseDefinition = serveEvent.getResponseDefinition();\n    if (targetAddressProhibited(responseDefinition.getProxyUrl())) {\n      return response()\n          .status(500)\n          .headers(new HttpHeaders(new HttpHeader(\"Content-Type\", \"text/plain\")))\n          .body(\"The target proxy address is denied in WireMock's configuration.\")\n          .build();\n    }\n\n    HttpUriRequest httpRequest = getHttpRequestFor(responseDefinition);\n    addRequestHeaders(httpRequest, responseDefinition);\n\n    GlobalSettings settings = settingsStore.get();\n\n    Request originalRequest = responseDefinition.getOriginalRequest();\n    if ((originalRequest.getBody() != null && originalRequest.getBody().length > 0)\n        || originalRequest.containsHeader(CONTENT_LENGTH)) {\n      httpRequest.setEntity(buildEntityFrom(originalRequest));\n    }\n\n    CloseableHttpClient client = chooseClient(serveEvent.getRequest().isBrowserProxyRequest());\n\n    try {\n      return client.execute(\n          httpRequest,\n          httpResponse ->\n              response()\n                  .status(httpResponse.getCode())\n                  .headers(headersFrom(httpResponse, responseDefinition))\n                  .body(getEntityAsByteArray(httpResponse))\n                  .fromProxy(true)\n                  .configureDelay(\n                      settings.getFixedDelay(),\n                      settings.getDelayDistribution(),\n                      responseDefinition.getFixedDelayMilliseconds(),\n                      responseDefinition.getDelayDistribution())\n                  .chunkedDribbleDelay(responseDefinition.getChunkedDribbleDelay())\n                  .build());\n    } catch (SSLException e) {\n      return proxyResponseError(\"SSL\", httpRequest, e);\n    } catch (IOException e) {\n      return proxyResponseError(\"Network\", httpRequest, e);\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _parse_qsl(qs):\n    r = []\n    for pair in qs.replace(';','&').split('&'):\n        if not pair: continue\n        nv = pair.split('=', 1)\n        if len(nv) != 2: nv.append('')\n        key = urlunquote(nv[0].replace('+', ' '))\n        value = urlunquote(nv[1].replace('+', ' '))\n        r.append((key, value))\n    return r\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def edit(request, bookmark_id: int):\n    try:\n        bookmark = Bookmark.objects.get(pk=bookmark_id, owner=request.user)\n    except Bookmark.DoesNotExist:\n        raise Http404('Bookmark does not exist')\n\n    if request.method == 'POST':\n        form = BookmarkForm(request.POST, instance=bookmark)\n        return_url = form.data['return_url']\n        if form.is_valid():\n            tag_string = convert_tag_string(form.data['tag_string'])\n            update_bookmark(form.save(commit=False), tag_string, request.user)\n            return HttpResponseRedirect(return_url)\n    else:\n        return_url = request.GET.get('return_url')\n        form = BookmarkForm(instance=bookmark)\n\n    return_url = return_url if return_url else reverse('bookmarks:index')\n\n    form.initial['tag_string'] = build_tag_string(bookmark.tag_names, ' ')\n    form.initial['return_url'] = return_url\n\n    context = {\n        'form': form,\n        'bookmark_id': bookmark_id,\n        'return_url': return_url\n    }\n\n    return render(request, 'bookmarks/edit.html', context)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _test_setup_settings(pxe_once=1):\n\n    # rewrite modules.conf so we know we can use the testing module\n    # for xmlrpc rw testing (Makefile will put the user value back)\n   \n    import yaml\n    import Cheetah.Template as Template\n\n    MODULES_TEMPLATE = \"installer_templates/settings.template\"\n    DEFAULTS = \"installer_templates/defaults\"\n    fh = open(DEFAULTS)\n    data = yaml.load(fh.read())\n    fh.close()\n    data[\"pxe_once\"] = pxe_once\n\n    t = Template.Template(file=MODULES_TEMPLATE, searchList=[data])\n    open(\"/etc/cobbler/settings\",\"w+\").write(t.respond())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def check_table_permission(cls, user, table, permission):\n        \"\"\"Check if the provided user has the specified permission against the table.\"\"\"\n        # If the table does *not* require permissions\n        if table in cls.RULESET_IGNORE:\n            return True\n\n        # Work out which roles touch the given table\n        for role in cls.RULESET_NAMES:\n            if table in cls.RULESET_MODELS[role]:\n\n                if check_user_role(user, role, permission):\n                    return True\n\n        # Check for children models which inherits from parent role\n        for (parent, child) in cls.RULESET_CHANGE_INHERIT:\n            # Get child model name\n            parent_child_string = f'{parent}_{child}'\n\n            if parent_child_string == table:\n                # Check if parent role has change permission\n                if check_user_role(user, parent, 'change'):\n                    return True\n\n        # Print message instead of throwing an error\n        name = getattr(user, 'name', user.pk)\n\n        logger.info(f\"User '{name}' failed permission check for {table}.{permission}\")\n\n        return False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _cli_deploy_image(args):\n\n    docker_repository = _resolve_property(args, 'docker_repository')\n    vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)\n    vault_endpoint = _resolve_vault_endpoint(args)\n    image_tar_path = args.image_tar_path if args.image_tar_path else find_image_file()\n\n    bigflow.deploy.deploy_docker_image(\n        image_tar_path=image_tar_path,\n        auth_method=args.auth_method,\n        docker_repository=docker_repository,\n        vault_endpoint=vault_endpoint,\n        vault_secret=vault_secret,\n    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _configure_session(cls, app):\n        tempdir = app.config[\"PYLOAD_API\"].get_cachedir()\n        cache_path = os.path.join(tempdir, \"flask\")\n        os.makedirs(cache_path, exist_ok=True)\n\n        app.config[\"SESSION_FILE_DIR\"] = cache_path\n        app.config[\"SESSION_TYPE\"] = \"filesystem\"\n        app.config[\"SESSION_COOKIE_NAME\"] = \"pyload_session\"\n        app.config[\"SESSION_COOKIE_SAMESITE\"] = \"None\"\n        app.config[\"SESSION_COOKIE_SECURE\"] = app.config[\"PYLOAD_API\"].get_config_value(\"webui\", \"use_ssl\")\n        app.config[\"SESSION_PERMANENT\"] = False\n\n        session_lifetime = max(app.config[\"PYLOAD_API\"].get_config_value(\"webui\", \"session_lifetime\"), 1) * 60\n        app.config[\"PERMANENT_SESSION_LIFETIME\"] = session_lifetime\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "                        def get_content_length() -> Optional[int]:\n                            # payload length\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\n                            if length_hdr is None:\n                                return None\n\n                            try:\n                                length = int(length_hdr)\n                            except ValueError:\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            if length < 0:\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            return length\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _ensureSnapshotsFolder(self, subdir=None):\n        \"\"\" Ensure that the appropriate snapshot folder exists.\n        \"\"\"\n        path = ['snapshots', self._snapshot_id]\n\n        if subdir is not None:\n            path.extend(subdir.split('/'))\n\n        current = self._tool\n\n        for element in path:\n\n            if element not in current.objectIds():\n                # No Unicode IDs!\n                current._setObject(str(element), Folder(element))\n\n            current = current._getOb(element)\n\n        return current\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def load(self):\n        if self.saving_method == \"mongo\":\n            data = self._load_mongo()\n        elif self.saving_method == \"local\":\n            data = self._load_local()\n        data = data[\"members\"][self.sid]\n        self.member.data = GroupMemberData(**data)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def default(self, path=b\"\", action=None, **kwargs):\n        repo_obj = self.app.store.get_repo(path)\n        if kwargs.get('keepdays'):\n            return self._remove_older(repo_obj, **kwargs)\n        elif kwargs.get('new_encoding'):\n            return self._set_encoding(repo_obj, **kwargs)\n        elif kwargs.get('maxage'):\n            return self._set_maxage(repo_obj, **kwargs)\n        # Get page data.\n        params = {\n            'repo': repo_obj,\n            'keepdays': repo_obj.keepdays,\n        }\n        # Generate page.\n        return self._compile_template(\"settings.html\", **params)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_object_element(dataset, seq_no, rel_path, download_link):\n    \"\"\"If rel_path and download_link are not None, we are called from scope.\n    Otherwise we are called from ID and need to run SQL query to fetch these attrs.\"\"\"\n\n    if rel_path is None:\n        query = \"SELECT rel_path, download_link FROM \" + \\\n        dataset + \\\n        \" WHERE sequence_no = %s\"\n\n        cnx = mysql.connector.connect(user=DB_USER,\n                                    password=DB_PASSWORD,\n                                    host=DB_HOST,\n                                    database=DB_DBNAME,\n                                    port=DB_PORT)\n        cursor = cnx.cursor()\n        cursor.execute(query, (seq_no,))\n\n        row = cursor.fetchone()\n\n        if not row:\n            return None\n\n        rel_path, download_link = row[0], row[1]\n\n    if LOCAL_OBJ_URI:\n        src_uri = 'file://' + os.path.join(DATAROOT, dataset, rel_path)\n    else:\n        src_uri = url_for('.get_object_src_http', dataset=dataset, rel_path=rel_path)\n\n    return '<object id={} src={} hyperfind.external-link={} />' \\\n        .format(\n        quoteattr(url_for('.get_object_id', dataset=dataset, seq_no=seq_no)),\n        quoteattr(src_uri),\n        quoteattr(download_link))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const float input_min_float = ctx->input(1).flat<float>()(0);\n    const float input_max_float = ctx->input(2).flat<float>()(0);\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(2, TensorShape({}), &output_max));\n\n    // See QuantizationRangeOp as well, which has a copy of this logic.\n    auto input_array = input.flat<T1>();\n    const int32_t input_lowest_quantized =\n        static_cast<int32>(Eigen::NumTraits<T1>::lowest());\n    const int32_t input_highest_quantized =\n        static_cast<int32>(Eigen::NumTraits<T1>::highest());\n    T1 actual_min_quantized = input_highest_quantized;\n    T1 actual_max_quantized = input_lowest_quantized;\n    for (int i = 0; i < input_array.size(); ++i) {\n      const T1 value = input_array(i);\n      actual_min_quantized = std::min(actual_min_quantized, value);\n      actual_max_quantized = std::max(actual_max_quantized, value);\n    }\n    // We want to make sure that the minimum is no larger than zero, so that the\n    // convolution operation can run efficiently.\n    const float actual_min_float =\n        std::min(0.0f, QuantizedToFloat(actual_min_quantized, input_min_float,\n                                        input_max_float));\n    const float actual_max_float = QuantizedToFloat(\n        actual_max_quantized, input_min_float, input_max_float);\n\n#if 0\n    // This is the reference, non-eigen implementation:\n    auto output_array = output->flat<T2>();\n    RequantizeManyInNewRange<T1, T2>(input_array.data(), input_array.size(),\n                                     input_min_float, input_max_float,\n                                     actual_min_float, actual_max_float,\n                                     output_array.data());\n#endif\n\n    if (input_array.size() > 0) {\n      if (meta::IsSupportedAndEnabled() && std::is_same<T1, qint32>() &&\n          std::is_same<T2, quint8>()) {\n        auto input_i32_array = input.flat<qint32>();\n        meta::Requantize(ctx, input_i32_array.data(), input_i32_array.size(),\n                         input_min_float, input_max_float, actual_min_float,\n                         actual_max_float, output->flat<quint8>().data());\n      } else {\n        RequantizeManyInNewRangeUsingEigen<T1, T2>(\n            ctx->eigen_device<CPUDevice>(), input, input_min_float,\n            input_max_float, actual_min_float, actual_max_float, output);\n      }\n    }\n\n    output_min->flat<float>().setConstant(actual_min_float);\n    output_max->flat<float>().setConstant(actual_max_float);\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _ttl_action(self):\n        # type: () -> TtlActions\n        \"\"\"Determine the correct action to take based on the local resources and TTL.\n\n        :returns: decision\n        :rtype: TtlActions\n        \"\"\"\n        if self._version is None:\n            _LOGGER.debug(\"TTL Expired because no version is known\")\n            return TtlActions.EXPIRED\n\n        time_since_updated = time.time() - self._last_updated\n\n        if time_since_updated < self._version_ttl:\n            return TtlActions.LIVE\n\n        if time_since_updated < self._version_ttl + _GRACE_PERIOD:\n            return TtlActions.GRACE_PERIOD\n\n        _LOGGER.debug(\"TTL Expired because known version has expired\")\n        return TtlActions.EXPIRED\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def config():\n    config_disabled = (\n            app.config['CONFIG_DISABLE'] or\n            not valid_user_session(session))\n    if request.method == 'GET':\n        return json.dumps(g.user_config.__dict__)\n    elif request.method == 'PUT' and not config_disabled:\n        if 'name' in request.args:\n            config_pkl = os.path.join(\n                app.config['CONFIG_PATH'],\n                request.args.get('name'))\n            session['config'] = (pickle.load(open(config_pkl, 'rb'))\n                                 if os.path.exists(config_pkl)\n                                 else session['config'])\n            return json.dumps(session['config'])\n        else:\n            return json.dumps({})\n    elif not config_disabled:\n        config_data = request.form.to_dict()\n        if 'url' not in config_data or not config_data['url']:\n            config_data['url'] = g.user_config.url\n\n        # Save config by name to allow a user to easily load later\n        if 'name' in request.args:\n            pickle.dump(\n                config_data,\n                open(os.path.join(\n                    app.config['CONFIG_PATH'],\n                    request.args.get('name')), 'wb'))\n\n        session['config'] = config_data\n        return redirect(config_data['url'])\n    else:\n        return redirect(url_for('.index'), code=403)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def explain_query_prefix(self, format=None, **options):\n        prefix = super().explain_query_prefix(format)\n        extra = {}\n        if format:\n            extra[\"FORMAT\"] = format\n        if options:\n            extra.update(\n                {\n                    name.upper(): \"true\" if value else \"false\"\n                    for name, value in options.items()\n                }\n            )\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def put(self, name: str):\n        \"\"\"add new file\n        params in FormData:\n            - file\n            - original_file_name [optional]\n        \"\"\"\n\n        data = {}\n        mindsdb_file_name = name\n\n        existing_file_names = ca.file_controller.get_files_names()\n\n        def on_field(field):\n            name = field.field_name.decode()\n            value = field.value.decode()\n            data[name] = value\n\n        file_object = None\n\n        def on_file(file):\n            nonlocal file_object\n            data[\"file\"] = file.file_name.decode()\n            file_object = file.file_object\n\n        temp_dir_path = tempfile.mkdtemp(prefix=\"mindsdb_file_\")\n\n        if request.headers[\"Content-Type\"].startswith(\"multipart/form-data\"):\n            parser = multipart.create_form_parser(\n                headers=request.headers,\n                on_field=on_field,\n                on_file=on_file,\n                config={\n                    \"UPLOAD_DIR\": temp_dir_path.encode(),  # bytes required\n                    \"UPLOAD_KEEP_FILENAME\": True,\n                    \"UPLOAD_KEEP_EXTENSIONS\": True,\n                    \"MAX_MEMORY_FILE_SIZE\": 0,\n                },\n            )\n\n            while True:\n                chunk = request.stream.read(8192)\n                if not chunk:\n                    break\n                parser.write(chunk)\n            parser.finalize()\n            parser.close()\n\n            if file_object is not None and not file_object.closed:\n                file_object.close()\n        else:\n            data = request.json\n\n        if mindsdb_file_name in existing_file_names:\n            return http_error(\n                400,\n                \"File already exists\",\n                f\"File with name '{data['file']}' already exists\",\n            )\n\n        if data.get(\"source_type\") == \"url\":\n            url = data[\"source\"]\n            data[\"file\"] = data[\"name\"]\n\n            config = Config()\n            is_cloud = config.get(\"cloud\", False)\n            if is_cloud is True and ctx.user_class != 1:\n                info = requests.head(url)\n                file_size = info.headers.get(\"Content-Length\")\n                try:\n                    file_size = int(file_size)\n                except Exception:\n                    pass\n\n                if file_size is None:\n                    return http_error(\n                        400,\n                        \"Error getting file info\",\n                        \"\u0421an't determine remote file size\",\n                    )\n                if file_size > 1024 * 1024 * 100:\n                    return http_error(\n                        400, \"File is too big\", \"Upload limit for file is 100Mb\"\n                    )\n            with requests.get(url, stream=True) as r:\n                if r.status_code != 200:\n                    return http_error(\n                        400, \"Error getting file\", f\"Got status code: {r.status_code}\"\n                    )\n                file_path = os.path.join(temp_dir_path, data[\"file\"])\n                with open(file_path, \"wb\") as f:\n                    for chunk in r.iter_content(chunk_size=8192):\n                        f.write(chunk)\n\n        original_file_name = data.get(\"original_file_name\")\n\n        file_path = os.path.join(temp_dir_path, data[\"file\"])\n        lp = file_path.lower()\n        if lp.endswith((\".zip\", \".tar.gz\")):\n            if lp.endswith(\".zip\"):\n                with zipfile.ZipFile(file_path) as f:\n                    f.extractall(temp_dir_path)\n            elif lp.endswith(\".tar.gz\"):\n                with tarfile.open(file_path) as f:\n                    safe_extract(f, temp_dir_path)\n            os.remove(file_path)\n            files = os.listdir(temp_dir_path)\n            if len(files) != 1:\n                os.rmdir(temp_dir_path)\n                return http_error(\n                    400, \"Wrong content.\", \"Archive must contain only one data file.\"\n                )\n            file_path = os.path.join(temp_dir_path, files[0])\n            mindsdb_file_name = files[0]\n            if not os.path.isfile(file_path):\n                os.rmdir(temp_dir_path)\n                return http_error(\n                    400, \"Wrong content.\", \"Archive must contain data file in root.\"\n                )\n\n        ca.file_controller.save_file(\n            mindsdb_file_name, file_path, file_name=original_file_name\n        )\n\n        os.rmdir(temp_dir_path)\n\n        return \"\", 200\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def handle_html_static(self, request):\n        \"\"\"Handle static html.\"\"\"\n        if not (page := request.path[1:]):\n            page = \"index.html\"\n        file = os.path.join(self.web_path, page)\n        try:\n            with open(file, encoding=\"utf-8\"):\n                return web.FileResponse(file)\n        except (FileNotFoundError, IsADirectoryError) as exc:\n            raise web.HTTPNotFound(reason=\"File not found\") from exc\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_services(self, context):\n        service_list = self.catalog_api.list_services(context)\n        service_refs = [self.catalog_api.get_service(context, x)\n                        for x in service_list]\n        return {'OS-KSADM:services': service_refs}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _handle_carbon_received(self, msg):\n        self.xmpp.event('carbon_received', msg)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def render_markdown(value):\n    \"\"\"\n    Render text as Markdown\n\n    Example:\n        {{ text | render_markdown }}\n    \"\"\"\n    # Strip HTML tags\n    value = strip_tags(value)\n\n    # Sanitize Markdown links\n    schemes = \"|\".join(settings.ALLOWED_URL_SCHEMES)\n    pattern = rf\"\\[(.+)\\]\\((?!({schemes})).*:(.+)\\)\"\n    value = re.sub(pattern, \"[\\\\1](\\\\3)\", value, flags=re.IGNORECASE)\n\n    # Render Markdown\n    html = markdown(value, extensions=[\"fenced_code\", \"tables\"])\n\n    return mark_safe(html)  # noqa: S308\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static cache_accel_t *read_cache_accel(RBuffer *cache_buf, cache_hdr_t *hdr, cache_map_t *maps) {\n\tif (!cache_buf || !hdr || !hdr->accelerateInfoSize || !hdr->accelerateInfoAddr) {\n\t\treturn NULL;\n\t}\n\n\tut64 offset = va2pa (hdr->accelerateInfoAddr, hdr->mappingCount, maps, cache_buf, 0, NULL, NULL);\n\tif (!offset) {\n\t\treturn NULL;\n\t}\n\n\tut64 size = sizeof (cache_accel_t);\n\tcache_accel_t *accel = R_NEW0 (cache_accel_t);\n\tif (!accel) {\n\t\treturn NULL;\n\t}\n\n\tif (r_buf_fread_at (cache_buf, offset, (ut8*) accel, \"16il\", 1) != size) {\n\t\tR_FREE (accel);\n\t\treturn NULL;\n\t}\n\n\taccel->imagesExtrasOffset += offset;\n\taccel->bottomUpListOffset += offset;\n\taccel->dylibTrieOffset += offset;\n\taccel->initializersOffset += offset;\n\taccel->dofSectionsOffset += offset;\n\taccel->reExportListOffset += offset;\n\taccel->depListOffset += offset;\n\taccel->rangeTableOffset += offset;\n\n\treturn accel;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        dir_name, file_name = os.path.split(name)\n        if '..' in pathlib.PurePath(dir_name).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n        validate_file_name(file_name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, add an underscore and a random 7\n        # character alphanumeric string (before the file extension, if one\n        # exists) to the filename until the generated filename doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n        return name\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def settings(request):\n   \"\"\"\n   This page presents a list of all the settings to the user.  They are not editable.\n   \"\"\"\n   if not test_user_authenticated(request): return login(request, next=\"/cobbler_web/settings\")\n   settings = remote.get_settings()\n   skeys = settings.keys()\n   skeys.sort()\n\n   results = []\n   for k in skeys:\n      results.append([k,settings[k]])\n\n   t = get_template('settings.tmpl')\n   html = t.render(Context({\n        'settings' : results,\n        'version'  : remote.version(request.session['token']),\n        'username' : username,\n   }))\n   return HttpResponse(html)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _login(server, creds_file=None, api_key=None, silent=False):\n    \"\"\"Login main function.\"\"\"\n    # fetch_api_key and check_version expect server to end in /\n    if server[-1] != \"/\":\n        server = server + \"/\"\n\n    # creds file path setup\n    if creds_file is None:\n        creds_file = os.path.expanduser(\"~/.onecodex\")\n\n    # check if the creds file exists and is readable\n    if not os.path.exists(creds_file):\n        if silent:\n            return None\n\n        creds = {}\n    elif not os.access(creds_file, os.R_OK):\n        click.echo(\"Please check the permissions on {}\".format(collapse_user(creds_file)), err=True)\n        sys.exit(1)\n    else:\n        # it is, so let's read it!\n        with open(creds_file, \"r\") as fp:\n            try:\n                creds = json.load(fp)\n            except ValueError:\n                click.echo(\n                    \"Your ~/.onecodex credentials file appears to be corrupted. \"  # noqa\n                    \"Please delete it and re-authorize.\",\n                    err=True,\n                )\n                sys.exit(1)\n\n        # check for updates if logged in more than one day ago\n        last_update = creds.get(\"updated_at\") or creds.get(\"saved_at\")\n        last_update = last_update if last_update else datetime.datetime.now().strftime(DATE_FORMAT)\n        diff = datetime.datetime.now() - datetime.datetime.strptime(last_update, DATE_FORMAT)\n\n        if diff.days >= 1:\n            # if creds_file is old, check for updates\n            upgrade_required, msg = check_version(__version__, server)\n            creds[\"updated_at\"] = datetime.datetime.now().strftime(DATE_FORMAT)\n\n            try:\n                json.dump(creds, open(creds_file, \"w\"))\n            except Exception as e:\n                if e.errno == errno.EACCES:\n                    click.echo(\n                        \"Please check the permissions on {}\".format(collapse_user(creds_file)),\n                        err=True,\n                    )\n                    sys.exit(1)\n                else:\n                    raise\n\n            if upgrade_required:\n                click.echo(\"\\nWARNING: {}\\n\".format(msg), err=True)\n\n        # finally, give the user back what they want (whether silent or not)\n        if silent:\n            return creds.get(\"api_key\", None)\n\n        click.echo(\n            \"Credentials file already exists ({}). Logout first.\".format(collapse_user(creds_file)),\n            err=True,\n        )\n        return creds.get(\"email\", None)\n\n    # creds_file was not found and we're not silent, so prompt user to login\n    email, api_key = login_uname_pwd(server, api_key=api_key)\n\n    if api_key is None:\n        click.echo(\n            \"We could not verify your credentials. Please check your email and password again. \"\n            \"If you have single sign-on (SSO) enabled, you must login with `onecodex --api-key $API_KEY login`. \"\n            \"Please contact help@onecodex.com if you continue to experience problems.\"\n        )\n        sys.exit(1)\n\n    creds.update(\n        {\n            \"api_key\": api_key,\n            \"saved_at\": datetime.datetime.now().strftime(DATE_FORMAT),\n            \"updated_at\": None,\n            \"email\": email,\n        }\n    )\n\n    try:\n        json.dump(creds, open(creds_file, \"w\"))\n    except Exception as e:\n        if e.errno == errno.EACCES:\n            click.echo(\"Please check the permissions on {}\".format(creds_file), err=True)\n            sys.exit(1)\n        else:\n            raise\n\n    click.echo(\"Your ~/.onecodex credentials file was successfully created.\", err=True)\n\n    return email\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, verbose_name=None, name=None, verify_exists=True, **kwargs):\n        kwargs['max_length'] = kwargs.get('max_length', 200)\n        CharField.__init__(self, verbose_name, name, **kwargs)\n        self.validators.append(validators.URLValidator(verify_exists=verify_exists))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def from_unix_seconds(seconds: Union[int, float]) -> datetime:\n    while abs(seconds) > MS_WATERSHED:\n        seconds /= 1000\n    dt = EPOCH + timedelta(seconds=seconds)\n    return dt.replace(tzinfo=timezone.utc)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def do_template(self, data, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, disable_lookups=False):\n        if USE_JINJA2_NATIVE and not isinstance(data, string_types):\n            return data\n\n        # For preserving the number of input newlines in the output (used\n        # later in this method)\n        data_newlines = _count_newlines_from_end(data)\n\n        if fail_on_undefined is None:\n            fail_on_undefined = self._fail_on_undefined_errors\n\n        try:\n            # allows template header overrides to change jinja2 options.\n            if overrides is None:\n                myenv = self.environment.overlay()\n            else:\n                myenv = self.environment.overlay(overrides)\n\n            # Get jinja env overrides from template\n            if hasattr(data, 'startswith') and data.startswith(JINJA2_OVERRIDE):\n                eol = data.find('\\n')\n                line = data[len(JINJA2_OVERRIDE):eol]\n                data = data[eol + 1:]\n                for pair in line.split(','):\n                    (key, val) = pair.split(':')\n                    key = key.strip()\n                    setattr(myenv, key, ast.literal_eval(val.strip()))\n\n            # Adds Ansible custom filters and tests\n            myenv.filters.update(self._get_filters())\n            for k in myenv.filters:\n                if not getattr(myenv.filters[k], '__UNROLLED__', False):\n                    myenv.filters[k] = _unroll_iterator(myenv.filters[k])\n            myenv.tests.update(self._get_tests())\n\n            if escape_backslashes:\n                # Allow users to specify backslashes in playbooks as \"\\\\\" instead of as \"\\\\\\\\\".\n                data = _escape_backslashes(data, myenv)\n\n            try:\n                t = myenv.from_string(data)\n            except TemplateSyntaxError as e:\n                raise AnsibleError(\"template error while templating string: %s. String: %s\" % (to_native(e), to_native(data)))\n            except Exception as e:\n                if 'recursion' in to_native(e):\n                    raise AnsibleError(\"recursive loop detected in template string: %s\" % to_native(data))\n                else:\n                    return data\n\n            if disable_lookups:\n                t.globals['query'] = t.globals['q'] = t.globals['lookup'] = self._fail_lookup\n\n            jvars = AnsibleJ2Vars(self, t.globals)\n\n            self.cur_context = new_context = t.new_context(jvars, shared=True)\n            rf = t.root_render_func(new_context)\n\n            try:\n                res = j2_concat(rf)\n                if getattr(new_context, 'unsafe', False):\n                    res = wrap_var(res)\n            except TypeError as te:\n                if 'AnsibleUndefined' in to_native(te):\n                    errmsg = \"Unable to look up a name or access an attribute in template string (%s).\\n\" % to_native(data)\n                    errmsg += \"Make sure your variable name does not contain invalid characters like '-': %s\" % to_native(te)\n                    raise AnsibleUndefinedVariable(errmsg)\n                else:\n                    display.debug(\"failing because of a type error, template data is: %s\" % to_text(data))\n                    raise AnsibleError(\"Unexpected templating type error occurred on (%s): %s\" % (to_native(data), to_native(te)))\n\n            if USE_JINJA2_NATIVE and not isinstance(res, string_types):\n                return res\n\n            if preserve_trailing_newlines:\n                # The low level calls above do not preserve the newline\n                # characters at the end of the input data, so we use the\n                # calculate the difference in newlines and append them\n                # to the resulting output for parity\n                #\n                # jinja2 added a keep_trailing_newline option in 2.7 when\n                # creating an Environment.  That would let us make this code\n                # better (remove a single newline if\n                # preserve_trailing_newlines is False).  Once we can depend on\n                # that version being present, modify our code to set that when\n                # initializing self.environment and remove a single trailing\n                # newline here if preserve_newlines is False.\n                res_newlines = _count_newlines_from_end(res)\n                if data_newlines > res_newlines:\n                    res += self.environment.newline_sequence * (data_newlines - res_newlines)\n            return res\n        except (UndefinedError, AnsibleUndefinedVariable) as e:\n            if fail_on_undefined:\n                raise AnsibleUndefinedVariable(e)\n            else:\n                display.debug(\"Ignoring undefined failure: %s\" % to_text(e))\n                return data\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_python_instance_state(self, instance, state):\n        if hasattr(instance, '__setstate__'):\n            instance.__setstate__(state)\n        else:\n            slotstate = {}\n            if isinstance(state, tuple) and len(state) == 2:\n                state, slotstate = state\n            if hasattr(instance, '__dict__'):\n                instance.__dict__.update(state)\n            elif state:\n                slotstate.update(state)\n            for key, value in slotstate.items():\n                setattr(instance, key, value)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ModelWeights::Initialize(OpKernelContext* const context) {\n  OpInputList sparse_indices_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"sparse_indices\", &sparse_indices_inputs));\n  OpInputList sparse_weights_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"sparse_weights\", &sparse_weights_inputs));\n  OpInputList dense_weights_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"dense_weights\", &dense_weights_inputs));\n\n  OpOutputList sparse_weights_outputs;\n  TF_RETURN_IF_ERROR(context->output_list(\"out_delta_sparse_weights\",\n                                          &sparse_weights_outputs));\n\n  OpOutputList dense_weights_outputs;\n  TF_RETURN_IF_ERROR(\n      context->output_list(\"out_delta_dense_weights\", &dense_weights_outputs));\n\n  for (int i = 0; i < sparse_weights_inputs.size(); ++i) {\n    Tensor* delta_t;\n    TF_RETURN_IF_ERROR(sparse_weights_outputs.allocate(\n        i, sparse_weights_inputs[i].shape(), &delta_t));\n    // Convert the input vector to a row matrix in internal representation.\n    auto deltas = delta_t->shaped<float, 2>({1, delta_t->NumElements()});\n    deltas.setZero();\n    sparse_weights_.emplace_back(FeatureWeightsSparseStorage{\n        sparse_indices_inputs[i].flat<int64>(),\n        sparse_weights_inputs[i].shaped<float, 2>(\n            {1, sparse_weights_inputs[i].NumElements()}),\n        deltas});\n  }\n\n  // Reads in the weights, and allocates and initializes the delta weights.\n  const auto initialize_weights =\n      [&](const OpInputList& weight_inputs, OpOutputList* const weight_outputs,\n          std::vector<FeatureWeightsDenseStorage>* const feature_weights) {\n        for (int i = 0; i < weight_inputs.size(); ++i) {\n          Tensor* delta_t;\n          TF_RETURN_IF_ERROR(\n              weight_outputs->allocate(i, weight_inputs[i].shape(), &delta_t));\n          // Convert the input vector to a row matrix in internal\n          // representation.\n          auto deltas = delta_t->shaped<float, 2>({1, delta_t->NumElements()});\n          deltas.setZero();\n          feature_weights->emplace_back(FeatureWeightsDenseStorage{\n              weight_inputs[i].shaped<float, 2>(\n                  {1, weight_inputs[i].NumElements()}),\n              deltas});\n        }\n        return Status::OK();\n      };\n\n  return initialize_weights(dense_weights_inputs, &dense_weights_outputs,\n                            &dense_weights_);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def failure(cls, request):\n        '''\n        Creates an RPCMessage instance containing exception information,\n        when the exception is from user-defined handlers or upper adaptation layers.\n\n        It must be called in an exception handler context, where ``sys.exc_info()``\n        returns a non-null tuple.\n        '''\n        exc_info = sys.exc_info()\n        return cls(\n            request.peer_id,\n            RPCMessageTypes.FAILURE,\n            request.method, request.order_key, request.client_seq_id,\n            ErrorMetadata(exc_info[0].__name__, traceback.format_exc()),\n            None,\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_send_leave_request(self, origin, content, room_id):\n        logger.debug(\"on_send_leave_request: content: %s\", content)\n\n        room_version = yield self.store.get_room_version(room_id)\n        format_ver = room_version_to_event_format(room_version)\n        pdu = event_from_pdu_json(content, format_ver)\n\n        origin_host, _ = parse_server_name(origin)\n        yield self.check_server_matches_acl(origin_host, pdu.room_id)\n\n        logger.debug(\"on_send_leave_request: pdu sigs: %s\", pdu.signatures)\n\n        pdu = yield self._check_sigs_and_hash(room_version, pdu)\n\n        yield self.handler.on_send_leave_request(origin, pdu)\n        return 200, {}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& indices_tensor = ctx->input(0);\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(indices_tensor.shape()) ||\n                    TensorShapeUtils::IsScalar(indices_tensor.shape()),\n                errors::InvalidArgument(\n                    \"The indices can only be scalar or vector, got \\\"\",\n                    indices_tensor.shape().DebugString(), \"\\\"\"));\n\n    const Tensor& dims_tensor = ctx->input(1);\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(dims_tensor.shape()),\n        errors::InvalidArgument(\"The indices can only be 1-D, got \\\"\",\n                                dims_tensor.shape().DebugString(), \"\\\"\"));\n\n    auto dims = dims_tensor.vec<Tidx>();\n    // Make sure dims does not contain a zero\n    for (int i = 0; i < dims.size(); i++) {\n      OP_REQUIRES(\n          ctx, dims(i) != 0,\n          errors::InvalidArgument(\"Input dims cannot contain a dim of zero, \"\n                                  \"but dims contains zero at index \",\n                                  i));\n    }\n\n    // Chek to make sure indices is not out of boundary\n    Eigen::Tensor<Tidx, 0, Eigen::RowMajor> dims_prod_eigen = dims.prod();\n    Tidx dims_prod = dims_prod_eigen();\n    const Tidx* indices = indices_tensor.flat<Tidx>().data();\n    int64 size = indices_tensor.NumElements();\n    bool check = std::all_of(indices, indices + size,\n                             [&](Tidx index) { return index < dims_prod; });\n    OP_REQUIRES(ctx, check,\n                errors::InvalidArgument(\"index is out of bound as with dims\"));\n\n    Eigen::array<bool, 1> reverse({true});\n\n    Tensor strides_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<Tidx>::value,\n                                      TensorShape({dims_tensor.NumElements()}),\n                                      &strides_tensor));\n\n    auto strides = strides_tensor.vec<Tidx>();\n    strides = dims.reverse(reverse)\n                  .scan(0, Eigen::internal::ProdReducer<Tidx>(), false)\n                  .reverse(reverse);\n\n    Tensor strides_shifted_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<Tidx>::value,\n                                      TensorShape({dims_tensor.NumElements()}),\n                                      &strides_shifted_tensor));\n\n    auto strides_shifted = strides_shifted_tensor.vec<Tidx>();\n    strides_shifted = dims.reverse(reverse)\n                          .scan(0, Eigen::internal::ProdReducer<Tidx>(), true)\n                          .reverse(reverse);\n\n    Tensor* output_tensor = nullptr;\n    if (TensorShapeUtils::IsScalar(indices_tensor.shape())) {\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({dims_tensor.NumElements()}),\n                                    &output_tensor));\n\n      auto output = output_tensor->vec<Tidx>();\n\n      output = output.constant(indices_tensor.scalar<Tidx>()());\n      output = output.binaryExpr(strides, mod_op<Tidx>()) / strides_shifted;\n    } else {\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0,\n                                    TensorShape({dims_tensor.NumElements(),\n                                                 indices_tensor.NumElements()}),\n                                    &output_tensor));\n\n      auto output = output_tensor->matrix<Tidx>();\n\n      Eigen::array<Eigen::Index, 2> reshape{\n          {static_cast<Eigen::Index>(dims_tensor.NumElements()), 1}};\n      Eigen::array<Eigen::Index, 2> bcast(\n          {1, static_cast<Eigen::Index>(indices_tensor.NumElements())});\n      Eigen::array<Eigen::Index, 2> indices_reshape{\n          {1, static_cast<Eigen::Index>(indices_tensor.NumElements())}};\n      Eigen::array<Eigen::Index, 2> indices_bcast(\n          {static_cast<Eigen::Index>(dims_tensor.NumElements()), 1});\n\n      output = indices_tensor.vec<Tidx>()\n                   .reshape(indices_reshape)\n                   .broadcast(indices_bcast);\n      output = output.binaryExpr(strides.reshape(reshape).broadcast(bcast),\n                                 mod_op<Tidx>()) /\n               strides_shifted.reshape(reshape).broadcast(bcast);\n    }\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def callback(_, pdu):\n            with logcontext.PreserveLoggingContext(ctx):\n                if not check_event_content_hash(pdu):\n                    logger.warn(\n                        \"Event content has been tampered, redacting %s: %s\",\n                        pdu.event_id, pdu.get_pdu_json()\n                    )\n                    return prune_event(pdu)\n\n                if self.spam_checker.check_event_for_spam(pdu):\n                    logger.warn(\n                        \"Event contains spam, redacting %s: %s\",\n                        pdu.event_id, pdu.get_pdu_json()\n                    )\n                    return prune_event(pdu)\n\n                return pdu\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse(self):\n        \"\"\"\n        Parse the POST data and break it into a FILES MultiValueDict and a POST\n        MultiValueDict.\n\n        Return a tuple containing the POST and FILES dictionary, respectively.\n        \"\"\"\n        from django.http import QueryDict\n\n        encoding = self._encoding\n        handlers = self._upload_handlers\n\n        # HTTP spec says that Content-Length >= 0 is valid\n        # handling content-length == 0 before continuing\n        if self._content_length == 0:\n            return QueryDict(encoding=self._encoding), MultiValueDict()\n\n        # See if any of the handlers take care of the parsing.\n        # This allows overriding everything if need be.\n        for handler in handlers:\n            result = handler.handle_raw_input(\n                self._input_data,\n                self._meta,\n                self._content_length,\n                self._boundary,\n                encoding,\n            )\n            # Check to see if it was handled\n            if result is not None:\n                return result[0], result[1]\n\n        # Create the data structures to be used later.\n        self._post = QueryDict(mutable=True)\n        self._files = MultiValueDict()\n\n        # Instantiate the parser and stream:\n        stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n        # Whether or not to signal a file-completion at the beginning of the loop.\n        old_field_name = None\n        counters = [0] * len(handlers)\n\n        # Number of bytes that have been read.\n        num_bytes_read = 0\n        # To count the number of keys in the request.\n        num_post_keys = 0\n        # To limit the amount of data read from the request.\n        read_size = None\n\n        try:\n            for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                if old_field_name:\n                    # We run this at the beginning of the next loop\n                    # since we cannot be sure a file is complete until\n                    # we hit the next boundary/part of the multipart content.\n                    self.handle_file_complete(old_field_name, counters)\n                    old_field_name = None\n\n                try:\n                    disposition = meta_data['content-disposition'][1]\n                    field_name = disposition['name'].strip()\n                except (KeyError, IndexError, AttributeError):\n                    continue\n\n                transfer_encoding = meta_data.get('content-transfer-encoding')\n                if transfer_encoding is not None:\n                    transfer_encoding = transfer_encoding[0].strip()\n                field_name = force_text(field_name, encoding, errors='replace')\n\n                if item_type == FIELD:\n                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                    num_post_keys += 1\n                    if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and\n                            settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):\n                        raise TooManyFieldsSent(\n                            'The number of GET/POST parameters exceeded '\n                            'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'\n                        )\n\n                    # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                    if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                        read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n\n                    # This is a post field, we can just set it in the post\n                    if transfer_encoding == 'base64':\n                        raw_data = field_stream.read(size=read_size)\n                        num_bytes_read += len(raw_data)\n                        try:\n                            data = base64.b64decode(raw_data)\n                        except binascii.Error:\n                            data = raw_data\n                    else:\n                        data = field_stream.read(size=read_size)\n                        num_bytes_read += len(data)\n\n                    # Add two here to make the check consistent with the\n                    # x-www-form-urlencoded check that includes '&='.\n                    num_bytes_read += len(field_name) + 2\n                    if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and\n                            num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):\n                        raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')\n\n                    self._post.appendlist(field_name, force_text(data, encoding, errors='replace'))\n                elif item_type == FILE:\n                    # This is a file, use the handler...\n                    file_name = disposition.get('filename')\n                    if file_name:\n                        file_name = force_text(file_name, encoding, errors='replace')\n                        file_name = self.sanitize_file_name(file_name)\n                    if not file_name:\n                        continue\n\n                    content_type, content_type_extra = meta_data.get('content-type', ('', {}))\n                    content_type = content_type.strip()\n                    charset = content_type_extra.get('charset')\n\n                    try:\n                        content_length = int(meta_data.get('content-length')[0])\n                    except (IndexError, TypeError, ValueError):\n                        content_length = None\n\n                    counters = [0] * len(handlers)\n                    try:\n                        for handler in handlers:\n                            try:\n                                handler.new_file(\n                                    field_name, file_name, content_type,\n                                    content_length, charset, content_type_extra,\n                                )\n                            except StopFutureHandlers:\n                                break\n\n                        for chunk in field_stream:\n                            if transfer_encoding == 'base64':\n                                # We only special-case base64 transfer encoding\n                                # We should always decode base64 chunks by multiple of 4,\n                                # ignoring whitespace.\n\n                                stripped_chunk = b\"\".join(chunk.split())\n\n                                remaining = len(stripped_chunk) % 4\n                                while remaining != 0:\n                                    over_chunk = field_stream.read(4 - remaining)\n                                    if not over_chunk:\n                                        break\n                                    stripped_chunk += b\"\".join(over_chunk.split())\n                                    remaining = len(stripped_chunk) % 4\n\n                                try:\n                                    chunk = base64.b64decode(stripped_chunk)\n                                except Exception as exc:\n                                    # Since this is only a chunk, any error is an unfixable error.\n                                    raise MultiPartParserError(\"Could not decode base64 data.\") from exc\n\n                            for i, handler in enumerate(handlers):\n                                chunk_length = len(chunk)\n                                chunk = handler.receive_data_chunk(chunk, counters[i])\n                                counters[i] += chunk_length\n                                if chunk is None:\n                                    # Don't continue if the chunk received by\n                                    # the handler is None.\n                                    break\n\n                    except SkipFile:\n                        self._close_files()\n                        # Just use up the rest of this file...\n                        exhaust(field_stream)\n                    else:\n                        # Handle file upload completions on next iteration.\n                        old_field_name = field_name\n                else:\n                    # If this is neither a FIELD or a FILE, just exhaust the stream.\n                    exhaust(stream)\n        except StopUpload as e:\n            self._close_files()\n            if not e.connection_reset:\n                exhaust(self._input_data)\n        else:\n            # Make sure that the request data is all fed\n            exhaust(self._input_data)\n\n        # Signal that the upload has completed.\n        # any() shortcircuits if a handler's upload_complete() returns a value.\n        any(handler.upload_complete() for handler in handlers)\n        self._post._mutable = False\n        return self._post, self._files\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def get_file(\n        self,\n        url: str,\n        output_stream: BinaryIO,\n        max_size: Optional[int] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:\n        \"\"\"GETs a file from a given URL\n        Args:\n            url: The URL to GET\n            output_stream: File to write the response body to.\n            headers: A map from header name to a list of values for that header\n        Returns:\n            A tuple of the file length, dict of the response\n            headers, absolute URI of the response and HTTP response code.\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            SynapseError: if the response is not a 2xx, the remote file is too large, or\n               another exception happens during the download.\n        \"\"\"\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", url, headers=Headers(actual_headers))\n\n        resp_headers = dict(response.headers.getAllRawHeaders())\n\n        if (\n            b\"Content-Length\" in resp_headers\n            and max_size\n            and int(resp_headers[b\"Content-Length\"][0]) > max_size\n        ):\n            logger.warning(\"Requested URL is too large > %r bytes\" % (max_size,))\n            raise SynapseError(\n                502,\n                \"Requested file is too large > %r bytes\" % (max_size,),\n                Codes.TOO_LARGE,\n            )\n\n        if response.code > 299:\n            logger.warning(\"Got %d when downloading %s\" % (response.code, url))\n            raise SynapseError(502, \"Got error %d\" % (response.code,), Codes.UNKNOWN)\n\n        # TODO: if our Content-Type is HTML or something, just read the first\n        # N bytes into RAM rather than saving it all to disk only to read it\n        # straight back in again\n\n        try:\n            length = await make_deferred_yieldable(\n                read_body_with_max_size(response, output_stream, max_size)\n            )\n        except BodyExceededMaxSize:\n            SynapseError(\n                502,\n                \"Requested file is too large > %r bytes\" % (max_size,),\n                Codes.TOO_LARGE,\n            )\n        except Exception as e:\n            raise SynapseError(502, (\"Failed to download remote body: %s\" % e)) from e\n\n        return (\n            length,\n            resp_headers,\n            response.request.absoluteURI.decode(\"ascii\"),\n            response.code,\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def is_safe_url(url, host=None):\n    \"\"\"\n    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to\n    a different host and uses a safe scheme).\n\n    Always returns ``False`` on an empty url.\n    \"\"\"\n    if url is not None:\n        url = url.strip()\n    if not url:\n        return False\n    # Chrome treats \\ completely as / in paths but it could be part of some\n    # basic auth credentials so we need to check both URLs.\n    return _is_safe_url(url, host) and _is_safe_url(url.replace('\\\\', '/'), host)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n        key = self.make_key(key, version=version)\n        self.validate_key(key)\n        return self._cache.add(key, value, self.get_backend_timeout(timeout))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input_sizes = context->input(0);\n    const Tensor& filter = context->input(1);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(input_sizes.shape()),\n        errors::InvalidArgument(\n            \"Conv2DBackpropInput: input_sizes input must be 1-dim, not \",\n            input_sizes.dims()));\n    TensorShape input_shape;\n    const int32* in_sizes_data = input_sizes.template flat<int32>().data();\n\n    for (int i = 0; i < input_sizes.NumElements(); ++i) {\n      OP_REQUIRES(context, in_sizes_data[i] >= 0,\n                  errors::InvalidArgument(\"Dimension \", i,\n                                          \" of input_sizes must be >= 0\"));\n      OP_REQUIRES_OK(context, input_shape.AddDimWithStatus(in_sizes_data[i]));\n    }\n    const TensorShape& filter_shape = filter.shape();\n    EXTRACT_AND_VERIFY_DIMENSIONS(\"DepthwiseConv2DBackpropInput\");\n\n    Tensor* in_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, input_shape, &in_backprop));\n\n    // If there is nothing to compute, return.\n    if (input_shape.num_elements() == 0) {\n      return;\n    }\n\n    // If in_depth==1, this operation is just a standard convolution.\n    // Depthwise convolution is a special case of cuDNN's grouped convolution.\n    bool use_cudnn = std::is_same<Device, GPUDevice>::value &&\n                     (in_depth == 1 ||\n                      (use_cudnn_grouped_conv_ &&\n                       IsCudnnSupportedFilterSize(/*filter_rows=*/filter_rows,\n                                                  /*filter_cols=*/filter_cols,\n                                                  /*in_depth=*/in_depth,\n                                                  /*out_depth=*/out_depth)));\n\n    VLOG(2) << \"DepthwiseConv2dNativeBackpropInput: \"\n            << \" Input: [\" << batch << \", \" << input_rows << \", \" << input_cols\n            << \", \" << in_depth << \"]; Filter: [\" << filter_rows << \", \"\n            << filter_cols << \", \" << in_depth << \", \" << depth_multiplier\n            << \"]; Output: [\" << batch << \", \" << out_rows << \", \" << out_cols\n            << \", \" << out_depth << \"], stride = \" << stride_\n            << \", pad_rows = \" << pad_top << \", pad_cols = \" << pad_left\n            << \", Use cuDNN: \" << use_cudnn;\n\n    if (use_cudnn) {\n      // Reshape from TF depthwise filter to cuDNN grouped convolution filter:\n      //\n      //                  | TensorFlow       | cuDNN\n      // --------------------------------------------------------------------\n      // filter_out_depth | depth_multiplier | depth_multiplier * group_count\n      // filter_in_depth  | in_depth         | in_depth / group_count\n      //\n      // For depthwise convolution, we have group_count == in_depth.\n      int32_t filter_in_depth = 1;\n      TensorShape shape =\n          TensorShape{filter_rows, filter_cols, filter_in_depth, out_depth};\n      Tensor reshaped_filter(/*type=*/dtype_);\n      OP_REQUIRES(\n          context, reshaped_filter.CopyFrom(filter, shape),\n          errors::Internal(\n              \"Failed to reshape filter tensor for grouped convolution.\"));\n      // TODO(yangzihao): Send in arbitrary dilation rates after the dilated\n      // conv is supported.\n      launcher_(context, /*use_cudnn=*/true, cudnn_use_autotune_, out_backprop,\n                reshaped_filter, /*row_dilation=*/1, /*col_dilation=*/1,\n                stride_, stride_, padding_, explicit_paddings_, in_backprop,\n                data_format_);\n      return;\n    }\n\n    auto out_backprop_ptr = out_backprop.template flat<T>().data();\n    auto filter_ptr = filter.template flat<T>().data();\n    auto in_backprop_ptr = in_backprop->template flat<T>().data();\n    LaunchDepthwiseConvBackpropInputOp<Device, T>()(\n        context, args, out_backprop_ptr, filter_ptr, in_backprop_ptr,\n        data_format_);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def createRequest(self, op, req, outgoing_data):\n        \"\"\"Return a new QNetworkReply object.\n\n        Args:\n             op: Operation op\n             req: const QNetworkRequest & req\n             outgoing_data: QIODevice * outgoingData\n\n        Return:\n            A QNetworkReply.\n        \"\"\"\n        proxy_factory = objreg.get('proxy-factory', None)\n        if proxy_factory is not None:\n            proxy_error = proxy_factory.get_error()\n            if proxy_error is not None:\n                return networkreply.ErrorNetworkReply(\n                    req, proxy_error, QNetworkReply.UnknownProxyError,\n                    self)\n\n        for header, value in shared.custom_headers(url=req.url()):\n            req.setRawHeader(header, value)\n\n        host_blocker = objreg.get('host-blocker')\n        if host_blocker.is_blocked(req.url()):\n            log.webview.info(\"Request to {} blocked by host blocker.\".format(\n                req.url().host()))\n            return networkreply.ErrorNetworkReply(\n                req, HOSTBLOCK_ERROR_STRING, QNetworkReply.ContentAccessDenied,\n                self)\n\n        # There are some scenarios where we can't figure out current_url:\n        # - There's a generic NetworkManager, e.g. for downloads\n        # - The download was in a tab which is now closed.\n        current_url = QUrl()\n\n        if self._tab_id is not None:\n            assert self._win_id is not None\n            try:\n                tab = objreg.get('tab', scope='tab', window=self._win_id,\n                                 tab=self._tab_id)\n                current_url = tab.url()\n            except (KeyError, RuntimeError):\n                # https://github.com/qutebrowser/qutebrowser/issues/889\n                # Catching RuntimeError because we could be in the middle of\n                # the webpage shutdown here.\n                current_url = QUrl()\n\n        if 'log-requests' in self._args.debug_flags:\n            operation = debug.qenum_key(QNetworkAccessManager, op)\n            operation = operation.replace('Operation', '').upper()\n            log.webview.debug(\"{} {}, first-party {}\".format(\n                operation,\n                req.url().toDisplayString(),\n                current_url.toDisplayString()))\n\n        scheme = req.url().scheme()\n        if scheme in self._scheme_handlers:\n            result = self._scheme_handlers[scheme](req, op, current_url)\n            if result is not None:\n                result.setParent(self)\n                return result\n\n        self.set_referer(req, current_url)\n        return super().createRequest(op, req, outgoing_data)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_file_hash(self):\n        if self.file_hash == \"\":\n            self._set_file_hash()\n            self.save(update_fields=[\"file_hash\"])\n\n        return self.file_hash\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void TensorSliceReader::LoadShard(int shard) const {\n  CHECK_LT(shard, sss_.size());\n  if (sss_[shard] || !status_.ok()) {\n    return;  // Already loaded, or invalid.\n  }\n  string value;\n  SavedTensorSlices sts;\n  const string fname = fnames_[shard];\n  VLOG(1) << \"Reading meta data from file \" << fname << \"...\";\n  Table* table;\n  Status s = open_function_(fname, &table);\n  if (!s.ok()) {\n    status_ = errors::DataLoss(\"Unable to open table file \", fname, \": \",\n                               s.ToString());\n    return;\n  }\n  sss_[shard].reset(table);\n  if (!(table->Get(kSavedTensorSlicesKey, &value) &&\n        ParseProtoUnlimited(&sts, value))) {\n    status_ = errors::Internal(\n        \"Failed to find the saved tensor slices at the beginning of the \"\n        \"checkpoint file: \",\n        fname);\n    return;\n  }\n  status_ = CheckVersions(sts.meta().versions(), TF_CHECKPOINT_VERSION,\n                          TF_CHECKPOINT_VERSION_MIN_PRODUCER, \"Checkpoint\",\n                          \"checkpoint\");\n  if (!status_.ok()) return;\n  for (const SavedSliceMeta& ssm : sts.meta().tensor()) {\n    TensorShape ssm_shape;\n    status_ = TensorShape::BuildTensorShapeBase(ssm.shape(), &ssm_shape);\n    if (!status_.ok()) return;\n    for (const TensorSliceProto& tsp : ssm.slice()) {\n      TensorSlice ss_slice(tsp);\n      status_ = RegisterTensorSlice(ssm.name(), ssm_shape, ssm.type(), fname,\n                                    ss_slice, &tensors_);\n      if (!status_.ok()) return;\n    }\n  }\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus ResizeOutputTensors(TfLiteContext* context, TfLiteNode* node,\n                                 const TfLiteTensor* input,\n                                 const TfLiteTensor* size_splits,\n                                 const TfLiteTensor* axis) {\n  int axis_value = GetTensorData<int>(axis)[0];\n  if (axis_value < 0) {\n    axis_value += NumDimensions(input);\n  }\n\n  std::vector<int64_t> size_splits_vector;\n  if (size_splits->type == kTfLiteInt32) {\n    GetSizeSplitsVector<int32_t>(size_splits, &size_splits_vector);\n  } else if (size_splits->type == kTfLiteInt64) {\n    GetSizeSplitsVector<int64_t>(size_splits, &size_splits_vector);\n  } else {\n    context->ReportError(context, \"size_splits only support type int32|int64.\");\n    return kTfLiteError;\n  }\n\n  int minus_one_index = -1;\n  int64_t size_splits_sum = 0;\n\n  for (int i = 0; i < size_splits_vector.size(); ++i) {\n    if (size_splits_vector.at(i) == -1) {\n      if (minus_one_index == -1) {\n        minus_one_index = i;\n      } else {\n        context->ReportError(context,\n                             \"The size_splits contains more than one -1.\");\n      }\n    } else {\n      size_splits_sum += size_splits_vector.at(i);\n    }\n  }\n\n  TF_LITE_ENSURE(context, axis_value >= 0);\n  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\n  const int input_size = SizeOfDimension(input, axis_value);\n\n  if (minus_one_index != -1) {\n    if (size_splits_sum > input_size) {\n      context->ReportError(\n          context,\n          \"The sum of size_splits must be less than the dimension of value.\");\n    } else {\n      size_splits_vector[minus_one_index] = input_size - size_splits_sum;\n    }\n  } else if (size_splits_sum != input_size) {\n    context->ReportError(\n        context,\n        \"The size_splits must sum to the dimension of value along axis.\");\n  }\n\n  for (int i = 0; i < NumOutputs(node); ++i) {\n    TfLiteIntArray* output_dims = TfLiteIntArrayCopy(input->dims);\n    output_dims->data[axis_value] = size_splits_vector.at(i);\n    TfLiteTensor* output;\n    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, i, &output));\n    TF_LITE_ENSURE_STATUS(context->ResizeTensor(context, output, output_dims));\n  }\n\n  return kTfLiteOk;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def create_trustee_and_trust(osc, cluster):\n    try:\n        password = utils.generate_password(length=18)\n\n        trustee = osc.keystone().create_trustee(\n            \"%s_%s\" % (cluster.uuid, cluster.project_id),\n            password,\n        )\n\n        cluster.trustee_username = trustee.name\n        cluster.trustee_user_id = trustee.id\n        cluster.trustee_password = password\n\n        trust = osc.keystone().create_trust(\n            cluster.trustee_user_id)\n        cluster.trust_id = trust.id\n\n    except Exception:\n        LOG.exception(\n            _LE('Failed to create trustee and trust for Cluster: %s'),\n            cluster.uuid)\n        raise exception.TrusteeOrTrustToClusterFailed(\n            cluster_uuid=cluster.uuid)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void SparseFillEmptyRowsOpImpl(OpKernelContext* context,\n                               AsyncOpKernel::DoneCallback done = nullptr) {\n  // Note that setting this empty lambda as the default parameter value directly\n  // can cause strange compiler/linker errors, so we do it like this instead.\n  if (!done) {\n    done = [] {};\n  }\n\n  const int kIndicesInput = 0;\n  const int kValuesInput = 1;\n  const int kDenseShapeInput = 2;\n  const int kDefaultValueInput = 3;\n\n  const Tensor& indices_t = context->input(kIndicesInput);\n  const Tensor& values_t = context->input(kValuesInput);\n  const Tensor& dense_shape_t = context->input(kDenseShapeInput);\n  const Tensor& default_value_t = context->input(kDefaultValueInput);\n\n  OP_REQUIRES_ASYNC(\n      context, TensorShapeUtils::IsVector(dense_shape_t.shape()),\n      errors::InvalidArgument(\"dense_shape must be a vector, saw: \",\n                              dense_shape_t.shape().DebugString()),\n      done);\n  OP_REQUIRES_ASYNC(context, TensorShapeUtils::IsMatrix(indices_t.shape()),\n                    errors::InvalidArgument(\"indices must be a matrix, saw: \",\n                                            indices_t.shape().DebugString()),\n                    done);\n  OP_REQUIRES_ASYNC(context, TensorShapeUtils::IsVector(values_t.shape()),\n                    errors::InvalidArgument(\"values must be a vector, saw: \",\n                                            values_t.shape().DebugString()),\n                    done);\n  OP_REQUIRES_ASYNC(\n      context, indices_t.dim_size(0) == values_t.dim_size(0),\n      errors::InvalidArgument(\"The length of `values` (\", values_t.dim_size(0),\n                              \") must match the first dimension of `indices` (\",\n                              indices_t.dim_size(0), \").\"),\n      done);\n  OP_REQUIRES_ASYNC(\n      context, TensorShapeUtils::IsScalar(default_value_t.shape()),\n      errors::InvalidArgument(\"default_value must be a scalar, saw: \",\n                              default_value_t.shape().DebugString()),\n      done);\n  // TODO(ebrevdo): add shape checks between values, indices,\n  // Also add check that dense rank > 0.\n  OP_REQUIRES_ASYNC(context, dense_shape_t.NumElements() != 0,\n                    errors::InvalidArgument(\"Dense shape cannot be empty.\"),\n                    done);\n\n  using FunctorType = functor::SparseFillEmptyRows<Device, T, Tindex>;\n  OP_REQUIRES_OK_ASYNC(context,\n                       FunctorType()(context, default_value_t, indices_t,\n                                     values_t, dense_shape_t, done),\n                       done);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def clone(\n        self,\n        pdf_dest: Any,\n        force_duplicate: bool = False,\n        ignore_fields: Optional[Sequence[Union[str, int]]] = (),\n    ) -> \"ContentStream\":\n        \"\"\"\n        Clone object into pdf_dest.\n\n        Args:\n            pdf_dest:\n            force_duplicate:\n            ignore_fields:\n\n        Returns:\n            The cloned ContentStream\n        \"\"\"\n        try:\n            if self.indirect_reference.pdf == pdf_dest and not force_duplicate:  # type: ignore\n                return self\n        except Exception:\n            pass\n\n        visited: Set[Tuple[int, int]] = set()\n        d__ = cast(\n            \"ContentStream\",\n            self._reference_clone(\n                self.__class__(None, None), pdf_dest, force_duplicate\n            ),\n        )\n        if ignore_fields is None:\n            ignore_fields = []\n        d__._clone(self, pdf_dest, force_duplicate, ignore_fields, visited)\n        return d__\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def parse(self):\n        \"\"\"\n        Parse the POST data and break it into a FILES MultiValueDict and a POST\n        MultiValueDict.\n\n        Return a tuple containing the POST and FILES dictionary, respectively.\n        \"\"\"\n        from django.http import QueryDict\n\n        encoding = self._encoding\n        handlers = self._upload_handlers\n\n        # HTTP spec says that Content-Length >= 0 is valid\n        # handling content-length == 0 before continuing\n        if self._content_length == 0:\n            return QueryDict(encoding=self._encoding), MultiValueDict()\n\n        # See if any of the handlers take care of the parsing.\n        # This allows overriding everything if need be.\n        for handler in handlers:\n            result = handler.handle_raw_input(\n                self._input_data,\n                self._meta,\n                self._content_length,\n                self._boundary,\n                encoding,\n            )\n            # Check to see if it was handled\n            if result is not None:\n                return result[0], result[1]\n\n        # Create the data structures to be used later.\n        self._post = QueryDict(mutable=True)\n        self._files = MultiValueDict()\n\n        # Instantiate the parser and stream:\n        stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n        # Whether or not to signal a file-completion at the beginning of the loop.\n        old_field_name = None\n        counters = [0] * len(handlers)\n\n        # Number of bytes that have been read.\n        num_bytes_read = 0\n        # To count the number of keys in the request.\n        num_post_keys = 0\n        # To limit the amount of data read from the request.\n        read_size = None\n        # Whether a file upload is finished.\n        uploaded_file = True\n\n        try:\n            for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                if old_field_name:\n                    # We run this at the beginning of the next loop\n                    # since we cannot be sure a file is complete until\n                    # we hit the next boundary/part of the multipart content.\n                    self.handle_file_complete(old_field_name, counters)\n                    old_field_name = None\n                    uploaded_file = True\n\n                try:\n                    disposition = meta_data['content-disposition'][1]\n                    field_name = disposition['name'].strip()\n                except (KeyError, IndexError, AttributeError):\n                    continue\n\n                transfer_encoding = meta_data.get('content-transfer-encoding')\n                if transfer_encoding is not None:\n                    transfer_encoding = transfer_encoding[0].strip()\n                field_name = force_str(field_name, encoding, errors='replace')\n\n                if item_type == FIELD:\n                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                    num_post_keys += 1\n                    if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and\n                            settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):\n                        raise TooManyFieldsSent(\n                            'The number of GET/POST parameters exceeded '\n                            'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'\n                        )\n\n                    # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                    if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                        read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n\n                    # This is a post field, we can just set it in the post\n                    if transfer_encoding == 'base64':\n                        raw_data = field_stream.read(size=read_size)\n                        num_bytes_read += len(raw_data)\n                        try:\n                            data = base64.b64decode(raw_data)\n                        except binascii.Error:\n                            data = raw_data\n                    else:\n                        data = field_stream.read(size=read_size)\n                        num_bytes_read += len(data)\n\n                    # Add two here to make the check consistent with the\n                    # x-www-form-urlencoded check that includes '&='.\n                    num_bytes_read += len(field_name) + 2\n                    if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and\n                            num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):\n                        raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')\n\n                    self._post.appendlist(field_name, force_str(data, encoding, errors='replace'))\n                elif item_type == FILE:\n                    # This is a file, use the handler...\n                    file_name = disposition.get('filename')\n                    if file_name:\n                        file_name = force_str(file_name, encoding, errors='replace')\n                        file_name = self.sanitize_file_name(file_name)\n                    if not file_name:\n                        continue\n\n                    content_type, content_type_extra = meta_data.get('content-type', ('', {}))\n                    content_type = content_type.strip()\n                    charset = content_type_extra.get('charset')\n\n                    try:\n                        content_length = int(meta_data.get('content-length')[0])\n                    except (IndexError, TypeError, ValueError):\n                        content_length = None\n\n                    counters = [0] * len(handlers)\n                    uploaded_file = False\n                    try:\n                        for handler in handlers:\n                            try:\n                                handler.new_file(\n                                    field_name, file_name, content_type,\n                                    content_length, charset, content_type_extra,\n                                )\n                            except StopFutureHandlers:\n                                break\n\n                        for chunk in field_stream:\n                            if transfer_encoding == 'base64':\n                                # We only special-case base64 transfer encoding\n                                # We should always decode base64 chunks by multiple of 4,\n                                # ignoring whitespace.\n\n                                stripped_chunk = b\"\".join(chunk.split())\n\n                                remaining = len(stripped_chunk) % 4\n                                while remaining != 0:\n                                    over_chunk = field_stream.read(4 - remaining)\n                                    if not over_chunk:\n                                        break\n                                    stripped_chunk += b\"\".join(over_chunk.split())\n                                    remaining = len(stripped_chunk) % 4\n\n                                try:\n                                    chunk = base64.b64decode(stripped_chunk)\n                                except Exception as exc:\n                                    # Since this is only a chunk, any error is an unfixable error.\n                                    raise MultiPartParserError(\"Could not decode base64 data.\") from exc\n\n                            for i, handler in enumerate(handlers):\n                                chunk_length = len(chunk)\n                                chunk = handler.receive_data_chunk(chunk, counters[i])\n                                counters[i] += chunk_length\n                                if chunk is None:\n                                    # Don't continue if the chunk received by\n                                    # the handler is None.\n                                    break\n\n                    except SkipFile:\n                        self._close_files()\n                        # Just use up the rest of this file...\n                        exhaust(field_stream)\n                    else:\n                        # Handle file upload completions on next iteration.\n                        old_field_name = field_name\n                else:\n                    # If this is neither a FIELD or a FILE, just exhaust the stream.\n                    exhaust(stream)\n        except StopUpload as e:\n            self._close_files()\n            if not e.connection_reset:\n                exhaust(self._input_data)\n        else:\n            if not uploaded_file:\n                for handler in handlers:\n                    handler.upload_interrupted()\n            # Make sure that the request data is all fed\n            exhaust(self._input_data)\n\n        # Signal that the upload has completed.\n        # any() shortcircuits if a handler's upload_complete() returns a value.\n        any(handler.upload_complete() for handler in handlers)\n        self._post._mutable = False\n        return self._post, self._files\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_paths(root: str, sub_path: str) \\\n        -> typing.Tuple[pathlib.Path, pathlib.Path]:\n    base_path = flask.safe_join(root, sub_path)\n    data_file = pathlib.Path(base_path + \".data\")\n    metadata_file = pathlib.Path(base_path + \".meta\")\n\n    return data_file, metadata_file\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  QInt32() : value(0) {}\n  QInt32(const int8_t v) : value(v) {}"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    const findHeadingElement = (cell: Cell): void => {\n      model.getCellHeadings(cell).forEach(async heading => {\n        const elementId = await getIdForHeading(\n          heading,\n          this.parser!,\n          this.sanitizer\n        );\n\n        const selector = elementId\n          ? `h${heading.level}[id=\"${CSS.escape(elementId)}\"]`\n          : `h${heading.level}`;\n\n        if (heading.outputIndex !== undefined) {\n          // Code cell\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              (heading.cellRef as CodeCell).outputArea.widgets[\n                heading.outputIndex\n              ].node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        } else {\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              heading.cellRef.node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        }\n      });\n    };\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def gather_tree(ids, parents):\n    r\"\"\"\n    To be used after beam search. After beam search, we get selected ids at\n    each time step and the corresponding parents in the search tree. Both ids\n    and parents have the layout :attr:`[max_time, batch_size, beam_size]`. Then\n    :attr:`gather_tree` is used to backtrace from the last time step and\n    generate the full sequences by collecting selected ids.\n\n    Here is an example:\n\n    .. code-block:: text\n\n            Given:\n                ids = [[[2 2]\n                        [6 1]]\n                       [[3 9]\n                        [6 1]]\n                       [[0 1]\n                        [9 0]]]\n                parents = [[[0 0]\n                            [1 1]]\n                           [[1 0]\n                            [1 0]]\n                           [[0 0]\n                            [0 1]]]\n\n            Then:\n                gather_tree(ids, parents)\n                         = [[[2 2]\n                             [1 6]]\n                            [[3 3]\n                             [6 1]]\n                            [[0 1]\n                             [9 0]]]\n\n    Args:\n        ids(Tensor): A Tensor with shape :attr:`[length, batch_size, beam_size]`\n            and data type :attr:`int32` or :attr:`int64`. It contains the selected\n            ids of all time steps.\n        parents(Tensor): A Tensor with the same shape and data type as :attr:`ids`,\n            It contains the parents corresponding to selected ids when searching\n            among beams.\n\n    Returns:\n            A Tensor with the same shape and data type as :attr:`ids`. \\\n            It contains the full sequences. The sequences are collected from \\\n            :attr:`ids` by backtracing according to :attr:`parents`.\n\n    Examples:\n        .. code-block:: python\n\n            import paddle\n\n            ids = paddle.to_tensor([[[2, 2], [6, 1]], [[3, 9], [6, 1]], [[0, 1], [9, 0]]])\n\n            parents = paddle.to_tensor([[[0, 0], [1, 1]], [[1, 0], [1, 0]], [[0, 0], [0, 1]]])\n\n            final_sequences = paddle.nn.functional.gather_tree(ids, parents)\n            # [[[2, 2], [1, 6]], [[3, 3], [6, 1]], [[0, 1], [9, 0]]]\n\n    \"\"\"\n    if ids.ndim != 3:\n        raise ValueError(\n            \"The input ids must be a 3D tensor with shape [length, batch_size, beam_size]\"\n        )\n    if ids.ndim != parents.ndim:\n        raise ValueError(\"The ids's shape must be the same as parents' shape. \")\n\n    if in_dygraph_mode():\n        return _C_ops.gather_tree(ids, parents)\n    else:\n        if _in_legacy_dygraph():\n            return _legacy_C_ops.gather_tree(ids, parents)\n        else:\n            helper = LayerHelper('gather_tree', **locals())\n            check_variable_and_dtype(ids, 'ids', ['int32', 'int64'],\n                                     'gather_tree')\n            check_variable_and_dtype(parents, 'parents', ['int32', 'int64'],\n                                     'gather_tree')\n            out = helper.create_variable_for_type_inference(dtype=ids.dtype)\n\n            helper.append_op(type=\"gather_tree\",\n                             inputs={\n                                 \"Ids\": ids,\n                                 \"Parents\": parents\n                             },\n                             outputs={\"Out\": out})\n\n            return out\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, *, openapi: GeneratorData) -> None:\n        self.openapi: GeneratorData = openapi\n        self.env: Environment = Environment(loader=PackageLoader(__package__), trim_blocks=True, lstrip_blocks=True)\n\n        self.project_name: str = self.project_name_override or f\"{utils.kebab_case(openapi.title).lower()}-client\"\n        self.project_dir: Path = Path.cwd() / self.project_name\n\n        self.package_name: str = self.package_name_override or self.project_name.replace(\"-\", \"_\")\n        self.package_dir: Path = self.project_dir / self.package_name\n        self.package_description: str = utils.remove_string_escapes(\n            f\"A client library for accessing {self.openapi.title}\"\n        )\n        self.version: str = openapi.version\n\n        self.env.filters.update(self.TEMPLATE_FILTERS)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, instance, filename):\n        \"\"\"\n        Apply (if callable) or prepend (if a string) upload_to to the filename,\n        then delegate further processing of the name to the storage backend.\n        Until the storage layer, all file paths are expected to be Unix style\n        (with forward slashes).\n        \"\"\"\n        filename = validate_file_name(filename)\n        if callable(self.upload_to):\n            filename = self.upload_to(instance, filename)\n        else:\n            dirname = datetime.datetime.now().strftime(str(self.upload_to))\n            filename = posixpath.join(dirname, filename)\n        return self.storage.generate_filename(filename)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs(\n    OpKernelContext* context, TensorInputs* inputs,\n    TensorShapes* input_matrix_shapes, TensorShape* batch_shape) {\n  int input_rank = -1;\n  for (int i = 0; i < NumMatrixInputs(context); ++i) {\n    const Tensor& in = context->input(i);\n    if (i == 0) {\n      input_rank = in.dims();\n      OP_REQUIRES(\n          context, input_rank >= 2,\n          errors::InvalidArgument(\"Input tensor \", i,\n                                  \" must have rank >= 2, got \", input_rank));\n      // If the tensor rank is greater than 2, we consider the inner-most\n      // dimensions as matrices, and loop over all the other outer (\"batch\")\n      // dimensions to compute the results.\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        batch_shape->AddDim(in.dim_size(dim));\n      }\n    } else {\n      // Make sure that all inputs have the same rank and outer dimensions.\n      OP_REQUIRES(context, input_rank == in.dims(),\n                  errors::InvalidArgument(\n                      \"All input tensors must have the same rank.\"));\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        OP_REQUIRES(\n            context, in.dim_size(dim) == batch_shape->dim_size(dim),\n            errors::InvalidArgument(\n                \"All input tensors must have the same outer dimensions.\"));\n      }\n    }\n\n    const int row_dimension = input_rank - 2;\n    const int col_dimension = input_rank - 1;\n    const int64_t num_rows = in.dim_size(row_dimension);\n    const int64_t num_cols = in.dim_size(col_dimension);\n    input_matrix_shapes->emplace_back(\n        std::initializer_list<int64_t>({num_rows, num_cols}));\n    inputs->emplace_back(&in);\n    OP_REQUIRES(\n        context, in.dtype() == DataTypeToEnum<InputScalar>::v(),\n        errors::InvalidArgument(\"Invalid input dtype \", in.dtype(), \" vs \",\n                                DataTypeToEnum<InputScalar>::v()));\n  }\n  // Have the derived class validate that the inputs are as expected.\n  ValidateInputMatrixShapes(context, *input_matrix_shapes);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def include_is_allowed(filepath):\n    filepath = os.path.abspath(filepath)\n    for root in settings.ALLOWED_INCLUDE_ROOTS:\n        if filepath.startswith(root):\n            return True\n    return False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def get_metric_variables(self) -> List[Any]:\n    \"\"\"Returns a list containing the metric variable values.\"\"\"\n    with self._lock:\n      return self._get_metric_variables()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def respond_error(self, context, exception):\n        context.respond_server_error()\n        stack = traceback.format_exc()\n        return \"\"\"\n        <html>\n            <body>\n\n                <style>\n                    body {\n                        font-family: sans-serif;\n                        color: #888;\n                        text-align: center;\n                    }\n\n                    body pre {\n                        width: 600px;\n                        text-align: left;\n                        margin: auto;\n                        font-family: monospace;\n                    }\n                </style>\n\n                <img src=\"/ajenti:static/main/error.jpeg\" />\n                <br/>\n                <p>\n                    Server error\n                </p>\n                <pre>\n%s\n                </pre>\n            </body>\n        </html>\n        \"\"\" % cgi.escape(stack)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def resolve_rpsl_object_journal(rpsl_object, info: GraphQLResolveInfo):\n    \"\"\"\n    Resolve a journal subquery on an RPSL object.\n    \"\"\"\n    database_handler = info.context['request'].app.state.database_handler\n    access_list = f\"sources.{rpsl_object['source']}.nrtm_access_list\"\n    if not is_client_permitted(info.context['request'].client.host, access_list):\n        raise GraphQLError(f\"Access to journal denied for source {rpsl_object['source']}\")\n\n    query = RPSLDatabaseJournalQuery()\n    query.sources([rpsl_object['source']]).rpsl_pk(rpsl_object['rpslPk'])\n    for row in database_handler.execute_query(query, refresh_on_error=True):\n        response = {snake_to_camel_case(k): v for k, v in row.items()}\n        response['operation'] = response['operation'].name\n        if response['origin']:\n            response['origin'] = response['origin'].name\n        if response['objectText']:\n            response['objectText'] = remove_auth_hashes(response['objectText'])\n        yield response\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def reset_metric_variables(self) -> None:\n    \"\"\"Reset metric variable values to their initial values.\"\"\"\n    with self._lock:\n      self._reset_metric_variables()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& in0 = context->input(0);\n    const Tensor& in1 = context->input(1);\n    const int dims = in0.dims();\n    constexpr int kMinDims = 0;\n    constexpr int kMaxDims = 5;\n    OP_REQUIRES(context, kMinDims <= dims && dims <= kMaxDims,\n                errors::Unimplemented(\"inputs rank not in [\", kMinDims, \",\",\n                                      kMaxDims, \"]: \", dims));\n    OP_REQUIRES(\n        context,\n        TensorShapeUtils::IsMatrix(in1.shape()) && in1.dim_size(1) == 2,\n        errors::InvalidArgument(\"paddings must be a matrix with 2 columns: \",\n                                in1.shape().DebugString()));\n    OP_REQUIRES(\n        context, dims == in1.dim_size(0),\n        errors::InvalidArgument(\n            \"The first dimension of paddings must be the rank of inputs\",\n            in1.shape().DebugString(), \" \", in0.shape().DebugString()));\n\n    // Compute the shape of the output tensor, and allocate it.\n    TensorShape output_shape;\n    typename TTypes<Tpaddings>::ConstMatrix paddings = in1.matrix<Tpaddings>();\n    for (int d = 0; d < dims; ++d) {\n      const int64_t before = paddings(d, 0);  // Pad before existing elements.\n      const int64_t after = paddings(d, 1);   // Pad after existing elements.\n      OP_REQUIRES(context, before >= 0 && after >= 0,\n                  errors::InvalidArgument(\n                      \"Paddings must be non-negative: \", before, \", \", after));\n\n      const int64_t in_size = in0.dim_size(d);\n      const int64_t total_padding = before + after;\n      OP_REQUIRES(\n          context, total_padding < in_size && total_padding >= 0,\n          errors::InvalidArgument(\n              \"Total paddings must be less than the input dimension size: \",\n              total_padding, \" was not less than \", in_size));\n\n      const int64_t out_size = in_size - total_padding;\n      if (offset_ == 0) {  // SYMMETRIC mode.\n        OP_REQUIRES(context, before <= out_size && after <= out_size,\n                    errors::InvalidArgument(\"paddings must be no greater \"\n                                            \"than the output dimension size: \",\n                                            before, \", \", after,\n                                            \" greater than \", out_size));\n      } else if (offset_ == 1) {  // REFLECT mode.\n        OP_REQUIRES(context, before < out_size && after < out_size,\n                    errors::InvalidArgument(\"paddings must be less than\"\n                                            \" the output dimension size: \",\n                                            before, \", \", after,\n                                            \" not less than \", out_size));\n      }\n      output_shape.AddDim(out_size);\n    }\n\n    if (output_shape == in0.shape()) {\n      context->set_output(0, in0);\n      return;\n    }\n\n    Tensor scratch;\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<T>::value,\n                                                   in0.shape(), &scratch));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &output));\n\n#define MIRROR_PAD_GRAD_CASE(k)                                           \\\n  case k: {                                                               \\\n    functor::MirrorPadGrad<Device, T, Tpaddings, k>()(                    \\\n        context->eigen_device<Device>(), To32Bit(output->tensor<T, k>()), \\\n        To32Bit(in0.tensor<T, k>()), paddings, offset_,                   \\\n        To32Bit(scratch.tensor<T, k>()));                                 \\\n    break;                                                                \\\n  }\n\n    // Invoke the dims-specific implementation.\n    switch (dims) {\n      MIRROR_PAD_GRAD_CASE(1);\n      MIRROR_PAD_GRAD_CASE(2);\n      MIRROR_PAD_GRAD_CASE(3);\n      MIRROR_PAD_GRAD_CASE(4);\n      MIRROR_PAD_GRAD_CASE(5);\n      default:\n        OP_REQUIRES(context, false,\n                    errors::InvalidArgument(\"Unsupported rank: \",\n                                            in0.shape().DebugString()));\n    }\n#undef MIRROR_PAD_GRAD_CASE\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def render(self):\n        if len(self.params) == 1:\n            # Retrieve the user portrait\n            user = decleanId(self.params[0])\n            portrait = self.portal_membership.getPersonalPortrait(user)\n        elif len(self.params) == 0:\n            current_user_id = self.portal_membership.getAuthenticatedMember().getId()\n            portrait = self.portal_membership.getPersonalPortrait(current_user_id)\n        else:\n            raise Exception(\n                \"Must supply exactly zero (own portrait) or one parameter \" \"(user id)\"\n            )\n        # User uploaded portraits have a meta_type of \"Image\"\n        if not portrait or isDefaultPortrait(portrait):\n            self.request.response.setStatus(404)\n            return None\n\n        if self._should_force_download(portrait):\n            # We need a filename, even a dummy one if needed.\n            ext = portrait.content_type.split(\"/\")[-1].split(\"+\")[0]\n            filename = f\"{portrait.getId()}.{ext}\"\n            filename = quote(filename.encode(\"utf8\"))\n            self.request.response.setHeader(\n                \"Content-Disposition\", f\"attachment; filename*=UTF-8''{filename}\"\n            )\n\n        self.request.response.setStatus(200)\n        self.request.response.setHeader(\"Content-Type\", portrait.content_type)\n\n        return stream_data(portrait)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  explicit FractionalMaxPoolOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pooling_ratio\", &pooling_ratio_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"pseudo_random\", &pseudo_random_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n\n    OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                errors::InvalidArgument(\"pooling_ratio field must \"\n                                        \"specify 4 dimensions\"));\n    for (std::size_t i = 0; i < pooling_ratio_.size(); ++i) {\n      OP_REQUIRES(context, pooling_ratio_[i] >= 1,\n                  errors::InvalidArgument(\n                      \"pooling_ratio cannot be smaller than 1, got: \",\n                      pooling_ratio_[i]));\n    }\n\n    OP_REQUIRES(\n        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n        errors::Unimplemented(\"Fractional max pooling is not yet \"\n                              \"supported on the batch nor channel dimension.\"));\n\n    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed2\", &seed2_));\n    if (deterministic_) {\n      // If both seeds are not set when deterministic_ is true, force set seeds.\n      if ((seed_ == 0) && (seed2_ == 0)) {\n        seed_ = random::New64();\n        seed2_ = random::New64();\n      }\n    } else {\n      OP_REQUIRES(\n          context, (seed_ == 0) && (seed2_ == 0),\n          errors::InvalidArgument(\n              \"Both seed and seed2 should be 0 if deterministic is false.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ConstantFolding::EvaluateOneFoldable(const NodeDef& node,\n                                            std::vector<NodeDef>* outputs,\n                                            bool* result_too_large) {\n  TensorVector inputs;\n  TensorVector output_tensors;\n  auto inputs_cleanup = gtl::MakeCleanup([&inputs, &output_tensors] {\n    for (const auto& input : inputs) {\n      delete input.tensor;\n    }\n    for (const auto& output : output_tensors) {\n      if (output.tensor) {\n        delete output.tensor;\n      }\n    }\n  });\n\n  size_t total_inputs_size = 0;\n  for (const auto& input : node.input()) {\n    const TensorId input_tensor = ParseTensorName(input);\n    if (input_tensor.index() < 0) {\n      // Control dependency\n      break;\n    }\n    const NodeDef* input_node = node_map_->GetNode(input);\n    if (!IsReallyConstant(*input_node)) {\n      return Status(error::INVALID_ARGUMENT,\n                    strings::StrCat(\"Can't fold \", node.name(), \", its \", input,\n                                    \" isn't constant\"));\n    }\n    TF_RETURN_IF_ERROR(CheckAttrExists(*input_node, \"value\"));\n    const TensorProto& raw_val = input_node->attr().at(\"value\").tensor();\n    if (raw_val.dtype() == DT_INVALID) {\n      return Status(\n          error::INVALID_ARGUMENT,\n          strings::StrCat(\"A tensor in the input node, with TensorId of \",\n                          input_tensor.ToString(),\n                          \" has a dtype of DT_INVALID.\"));\n    }\n    if (IsRefType(raw_val.dtype())) {\n      return errors::InvalidArgument(\n          \"Not allowed to construct a tensor with reference dtype, got \",\n          DataTypeString(raw_val.dtype()));\n    }\n    Tensor* value = new Tensor(raw_val.dtype(), raw_val.tensor_shape());\n    if (!value->FromProto(raw_val)) {\n      delete (value);\n      return errors::InvalidArgument(\"Unable to make Tensor from proto for \",\n                                     node.name(), \" with shape \",\n                                     raw_val.tensor_shape().DebugString());\n    }\n    inputs.emplace_back(value);\n    total_inputs_size += value->TotalBytes();\n  }\n\n  TF_RETURN_IF_ERROR(EvaluateNode(node, inputs, &output_tensors));\n  if (output_tensors.empty()) {\n    return Status(error::INVALID_ARGUMENT, \"Expected at least one output.\");\n  }\n\n  outputs->resize(output_tensors.size());\n  for (size_t i = 0; i < output_tensors.size(); i++) {\n    string node_name = OptimizedNodeName(node, \"-folded\");\n    if (output_tensors.size() > 1) {\n      node_name = strings::StrCat(node_name, \"-\", i);\n    }\n    if (output_tensors[i].tensor) {\n      Status s = CreateNodeDef(node_name, output_tensors[i], &outputs->at(i),\n                               total_inputs_size);\n      if (!s.ok()) {\n        *result_too_large = true;\n        return s;\n      }\n    } else {\n      // Create an empty NodeDef to identify dead outputs (e.g. the output of a\n      // switch that's not selected by the switch predicate).\n      outputs->at(i) = NodeDef();\n    }\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def local_get_cobbler_xmlrpc_url():\n    # Load xmlrpc port\n    try:\n        fh = open(\"/etc/cobbler/settings\")\n        data = yaml.safe_load(fh.read())\n        fh.close()\n    except:\n       traceback.print_exc()\n       raise CX(\"/etc/cobbler/settings is not a valid YAML file\")\n    return \"http://%s:%s\" % (\"127.0.0.1\",data.get(\"xmlrpc_port\",\"25151\"))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  def to_yaml(self, **kwargs):\n    \"\"\"Returns a yaml string containing the network configuration.\n\n    Note: Since TF 2.6, this method is no longer supported and will raise a\n    RuntimeError.\n\n    To load a network from a yaml save file, use\n    `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n\n    `custom_objects` should be a dictionary mapping\n    the names of custom losses / layers / etc to the corresponding\n    functions / classes.\n\n    Args:\n        **kwargs: Additional keyword arguments\n            to be passed to `yaml.dump()`.\n\n    Returns:\n        A YAML string.\n\n    Raises:\n        RuntimeError: announces that the method poses a security risk\n    \"\"\"\n    raise RuntimeError(\n        'Method `model.to_yaml()` has been removed due to security risk of '\n        'arbitrary code execution. Please use `model.to_json()` instead.'\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_conn(self) -> Connection:\n        \"\"\"Establish a connection to Drillbit.\"\"\"\n        conn_md = self.get_connection(getattr(self, self.conn_name_attr))\n        creds = f\"{conn_md.login}:{conn_md.password}@\" if conn_md.login else \"\"\n        database_url = (\n            f\"{conn_md.extra_dejson.get('dialect_driver', 'drill+sadrill')}://{creds}\"\n            f\"{conn_md.host}:{conn_md.port}/\"\n            f'{conn_md.extra_dejson.get(\"storage_plugin\", \"dfs\")}'\n        )\n        if \"?\" in database_url:\n            raise ValueError(\"Drill database_url should not contain a '?'\")\n        engine = create_engine(database_url)\n\n        self.log.info(\n            \"Connected to the Drillbit at %s:%s as user %s\", conn_md.host, conn_md.port, conn_md.login\n        )\n        return engine.raw_connection()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _dump(self, file=None, format=None):\n        import tempfile, os\n        if not file:\n            f, file = tempfile.mkstemp(format or '')\n            os.close(f)\n            \n        self.load()\n        if not format or format == \"PPM\":\n            self.im.save_ppm(file)\n        else:\n            if file.endswith(format):\n                file = file + \".\" + format\n            self.save(file, format)\n        return file\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  static port::StatusOr<CudnnRnnSequenceTensorDescriptor> Create(\n      GpuExecutor* parent, int max_seq_length, int batch_size, int data_size,\n      cudnnDataType_t data_type) {\n    if (max_seq_length <= 0) {\n      return port::Status(port::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n    }\n    int dims[] = {batch_size, data_size, 1};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    TensorDescriptor tensor_desc = CreateTensorDescriptor();\n    RETURN_IF_CUDNN_ERROR(cudnnSetTensorNdDescriptor(\n        /*tensorDesc=*/tensor_desc.get(), /*dataType=*/data_type,\n        /*nbDims=*/sizeof(dims) / sizeof(dims[0]), /*dimA=*/dims,\n        /*strideA=*/strides));\n    return CudnnRnnSequenceTensorDescriptor(parent, max_seq_length, batch_size,\n                                            data_size, data_type,\n                                            nullptr,\n                                            std::move(tensor_desc));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, handler, max_frame_length=DEFAULT_MAX_FRAME_LENGTH):\n        self.handler = handler\n        self.protocol_version = None\n        self.support_legacy_versions = True\n        self.supported_protocols = []\n        self.origin_checker = None\n        self.max_frame_length = max_frame_length\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "load_header (XwdLoader *loader)\n{\n    XwdHeader *h = &loader->header;\n    XwdHeader in;\n    const XwdHeader *inp;\n\n    if (!file_mapping_taste (loader->mapping, &in, 0, sizeof (in)))\n        return FALSE;\n\n    inp = &in;\n\n    UNPACK_FIELD_U32 (h, inp, header_size);\n    UNPACK_FIELD_U32 (h, inp, file_version);\n    UNPACK_FIELD_U32 (h, inp, pixmap_format);\n    UNPACK_FIELD_U32 (h, inp, pixmap_depth);\n    UNPACK_FIELD_U32 (h, inp, pixmap_width);\n    UNPACK_FIELD_U32 (h, inp, pixmap_height);\n    UNPACK_FIELD_U32 (h, inp, x_offset);\n    UNPACK_FIELD_U32 (h, inp, byte_order);\n    UNPACK_FIELD_U32 (h, inp, bitmap_unit);\n    UNPACK_FIELD_U32 (h, inp, bitmap_bit_order);\n    UNPACK_FIELD_U32 (h, inp, bitmap_pad);\n    UNPACK_FIELD_U32 (h, inp, bits_per_pixel);\n    UNPACK_FIELD_U32 (h, inp, bytes_per_line);\n    UNPACK_FIELD_U32 (h, inp, visual_class);\n    UNPACK_FIELD_U32 (h, inp, red_mask);\n    UNPACK_FIELD_U32 (h, inp, green_mask);\n    UNPACK_FIELD_U32 (h, inp, blue_mask);\n    UNPACK_FIELD_U32 (h, inp, bits_per_rgb);\n    UNPACK_FIELD_U32 (h, inp, color_map_entries);\n    UNPACK_FIELD_U32 (h, inp, n_colors);\n    UNPACK_FIELD_U32 (h, inp, window_width);\n    UNPACK_FIELD_U32 (h, inp, window_height);\n    UNPACK_FIELD_S32 (h, inp, window_x);\n    UNPACK_FIELD_S32 (h, inp, window_y);\n    UNPACK_FIELD_U32 (h, inp, window_border_width);\n\n    /* Only support the most common/useful subset of XWD files out there;\n     * namely, that corresponding to screen dumps from modern X.Org servers.\n     * We could check visual_class == 5 too, but the other fields convey all\n     * the info we need. */\n\n    ASSERT_HEADER (h->header_size >= sizeof (XwdHeader));\n    ASSERT_HEADER (h->header_size <= 65535);\n    ASSERT_HEADER (h->file_version == 7);\n    ASSERT_HEADER (h->pixmap_depth == 24);\n\n    /* Should be zero for truecolor/directcolor. Cap it to prevent overflows. */\n    ASSERT_HEADER (h->color_map_entries <= 256);\n\n    /* Xvfb sets bits_per_rgb to 8, but 'convert' uses 24 for the same image data. One\n     * of them is likely misunderstanding. Let's be lenient and accept either. */\n    ASSERT_HEADER (h->bits_per_rgb == 8 || h->bits_per_rgb == 24);\n\n    /* These are the pixel formats we allow. */\n    ASSERT_HEADER (h->bits_per_pixel == 24 || h->bits_per_pixel == 32);\n\n    /* Enforce sane dimensions. */\n    ASSERT_HEADER (h->pixmap_width >= 1 && h->pixmap_width <= 65535);\n    ASSERT_HEADER (h->pixmap_height >= 1 && h->pixmap_height <= 65535);\n\n    /* Make sure rowstride can actually hold a row's worth of data but is not padded to\n     * something ridiculous. */\n    ASSERT_HEADER (h->bytes_per_line >= h->pixmap_width * (h->bits_per_pixel / 8));\n    ASSERT_HEADER (h->bytes_per_line <= h->pixmap_width * (h->bits_per_pixel / 8) + 1024);\n\n    /* Make sure the total allocation/map is not too big. */\n    ASSERT_HEADER (h->bytes_per_line * h->pixmap_height < (1UL << 31) - 65536 - 256 * 32);\n\n    ASSERT_HEADER (compute_pixel_type (loader) < CHAFA_PIXEL_MAX);\n\n    loader->file_data = file_mapping_get_data (loader->mapping, &loader->file_data_len);\n    if (!loader->file_data)\n        return FALSE;\n\n    ASSERT_HEADER (loader->file_data_len >= h->header_size\n                   + h->color_map_entries * sizeof (XwdColor)\n                   + h->pixmap_height * (gsize) h->bytes_per_line);\n\n    loader->image_data = (const guint8 *) loader->file_data\n        + h->header_size + h->color_map_entries * sizeof (XwdColor);\n\n    return TRUE;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _read_lockfile(self, lockfile: Mapping[str, Any]) -> None:\n        root = self.environment.project.root\n        static_urls = self.environment.project.lockfile.static_urls\n        with cd(root):\n            for package in lockfile.get(\"package\", []):\n                version = package.get(\"version\")\n                if version:\n                    package[\"version\"] = f\"=={version}\"\n                package_name = package.pop(\"name\")\n                req_dict = {\n                    k: v for k, v in package.items() if k not in (\"dependencies\", \"requires_python\", \"summary\", \"files\")\n                }\n                req = Requirement.from_req_dict(package_name, req_dict)\n                if req.is_file_or_url and req.path and not req.url:  # type: ignore[attr-defined]\n                    req.url = path_to_url(posixpath.join(root, req.path))  # type: ignore[attr-defined]\n                can = make_candidate(req, name=package_name, version=version)\n                can.hashes = package.get(\"files\", [])\n                if not static_urls and any(\"url\" in f for f in can.hashes):\n                    raise PdmException(\n                        \"Static URLs are not allowed in lockfile unless enabled by `pdm lock --static-urls`.\"\n                    )\n                can_id = self._identify_candidate(can)\n                self.packages[can_id] = can\n                candidate_info: CandidateInfo = (\n                    package.get(\"dependencies\", []),\n                    package.get(\"requires_python\", \"\"),\n                    package.get(\"summary\", \"\"),\n                )\n                self.candidate_info[can_id] = candidate_info\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void ImmutableConstantOp::Compute(OpKernelContext* ctx) {\n  std::unique_ptr<MemmappedTensorAllocator> allocator(\n      new MemmappedTensorAllocator());\n\n  OP_REQUIRES_OK(ctx,\n                 allocator->InitializeFromRegion(region_name_, ctx->env()));\n  OP_REQUIRES(ctx, dtype_ != DT_STRING,\n              errors::Unimplemented(\"Sorry, DT_STRING is not currently \"\n                                    \"supported for ImmutableConstOp.\"));\n  ctx->set_output(0, Tensor(allocator.get(), dtype_, shape_));\n  OP_REQUIRES_OK(ctx, allocator->allocation_status());\n  // Allocator is owned by the tensor from this point.\n  allocator.release()->set_delete_on_deallocate();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    constexpr int tensor_in_and_out_dims = 4;\n\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n      OP_REQUIRES(\n          context, input_size[i] >= pooling_ratio_[i],\n          errors::InvalidArgument(\"Pooling ratio is higher than input \"\n                                  \"dimension size for dimension \",\n                                  i, \". Input dim size: \", input_size[i],\n                                  \" pooling ratio: \", pooling_ratio_[i]));\n    }\n    // Output size.\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n\n    // Generate pooling sequence.\n    std::vector<int64_t> row_cum_seq;\n    std::vector<int64_t> col_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    row_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                          &generator, pseudo_random_);\n    col_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                          &generator, pseudo_random_);\n\n    // Prepare output.\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_row_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\n                     1, TensorShape({static_cast<int64_t>(row_cum_seq.size())}),\n                     &output_row_seq_tensor));\n    Tensor* output_col_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\n                     2, TensorShape({static_cast<int64_t>(col_cum_seq.size())}),\n                     &output_col_seq_tensor));\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n    // out_count corresponds to number of elements in each pooling cell.\n    Eigen::Matrix<T, Eigen::Dynamic, 1> out_count(out_mat.cols());\n\n    // Initializes the output tensor and out_count with 0.\n    out_mat.setZero();\n    out_count.setZero();\n\n    auto output_row_seq_flat = output_row_seq_tensor->flat<int64_t>();\n    auto output_col_seq_flat = output_col_seq_tensor->flat<int64_t>();\n\n    // Set output tensors.\n    for (int i = 0; i < row_cum_seq.size(); ++i) {\n      output_row_seq_flat(i) = row_cum_seq[i];\n    }\n\n    for (int i = 0; i < col_cum_seq.size(); ++i) {\n      output_col_seq_flat(i) = col_cum_seq[i];\n    }\n\n    // For both input and output,\n    // 0: batch\n    // 1: row / row\n    // 2: col / col\n    // 3: depth / channel\n    const int64_t row_max = input_size[1] - 1;\n    const int64_t col_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // row sequence.\n      for (int64_t hs = 0; hs < row_cum_seq.size() - 1; ++hs) {\n        // row start and end.\n        const int64_t row_start = row_cum_seq[hs];\n        int64_t row_end =\n            overlapping_ ? row_cum_seq[hs + 1] : row_cum_seq[hs + 1] - 1;\n        row_end = std::min(row_end, row_max);\n\n        // col sequence.\n        for (int64_t ws = 0; ws < col_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // col start and end.\n          const int64_t col_start = col_cum_seq[ws];\n          int64_t col_end =\n              overlapping_ ? col_cum_seq[ws + 1] : col_cum_seq[ws + 1] - 1;\n          col_end = std::min(col_end, col_max);\n          for (int64_t h = row_start; h <= row_end; ++h) {\n            for (int64_t w = col_start; w <= col_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) += in_mat.col(in_offset);\n              out_count(out_offset)++;\n            }\n          }\n        }\n      }\n    }\n    DCHECK_GT(out_count.minCoeff(), 0);\n    out_mat.array().rowwise() /= out_count.transpose().array();\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def check_secret_key(self) -> None:\n        def log_default_secret_key_warning() -> None:\n            top_banner = 80 * \"-\" + \"\\n\" + 36 * \" \" + \"WARNING\\n\" + 80 * \"-\"\n            bottom_banner = 80 * \"-\" + \"\\n\" + 80 * \"-\"\n            logger.warning(top_banner)\n            logger.warning(\n                \"A Default SECRET_KEY was detected, please use superset_config.py \"\n                \"to override it.\\n\"\n                \"Use a strong complex alphanumeric string and use a tool to help\"\n                \" you generate \\n\"\n                \"a sufficiently random sequence, ex: openssl rand -base64 42\"\n            )\n            logger.warning(bottom_banner)\n\n        if self.config[\"SECRET_KEY\"] == CHANGE_ME_SECRET_KEY:\n            if (\n                self.superset_app.debug\n                or self.superset_app.config[\"TESTING\"]\n                # There must be a better way\n                or \"pytest\" in sys.modules\n            ):\n                logger.warning(\"Debug mode identified with default secret key\")\n                log_default_secret_key_warning()\n                return\n            log_default_secret_key_warning()\n            logger.error(\"Refusing to start due to insecure SECRET_KEY\")\n            sys.exit(1)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def cookies(self):\n        \"\"\"The session cookies.\"\"\"\n        return self._cookie_jar.cookies\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_snippet_path(self, path):\r\n        \"\"\"Get snippet path.\"\"\"\r\n\r\n        snippet = None\r\n        for base in self.base_path:\r\n            if os.path.exists(base):\r\n                if os.path.isdir(base):\r\n                    if self.restrict_base_path:\r\n                        filename = os.path.abspath(os.path.join(base, path))\r\n                        # If the absolute path is no longer under the specified base path, reject the file\r\n                        if not os.path.samefile(base, os.path.dirname(filename)):\r\n                            continue\r\n                    else:\r\n                        filename = os.path.join(base, path)\r\n                    if os.path.exists(filename):\r\n                        snippet = filename\r\n                        break\r\n                else:\r\n                    dirname = os.path.dirname(base)\r\n                    filename = os.path.join(dirname, path)\r\n                    if os.path.exists(filename) and os.path.samefile(filename, base):\r\n                        snippet = filename\r\n                        break\r\n        return snippet\r\n\r"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_package(self, name, links, dest=Destination.QUEUE):\n        \"\"\"\n        Adds a package, with links to desired destination.\n\n        :param name: name of the new package\n        :param links: list of urls\n        :param dest: `Destination`\n        :return: package id of the new package\n        \"\"\"\n        if self.pyload.config.get(\"general\", \"folder_per_package\"):\n            folder = name\n        else:\n            folder = \"\"\n\n        folder = (\n            folder.replace(\"http://\", \"\")\n            .replace(\"https://\", \"\")\n            .replace(\"../\", \"_\")\n            .replace(\"..\\\\\", \"_\")\n            .replace(\":\", \"\")\n            .replace(\"/\", \"_\")\n            .replace(\"\\\\\", \"_\")\n        )\n\n        package_id = self.pyload.files.add_package(name, folder, Destination(dest))\n\n        self.pyload.files.add_links(links, package_id)\n\n        self.pyload.log.info(\n            self._(\"Added package {name} containing {count:d} links\").format(\n                name=name, count=len(links)\n            )\n        )\n\n        self.pyload.files.save()\n\n        return package_id\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _keyify(key):\n    key = escape(key.lower(), quote=True)\n    return _key_pattern.sub(' ', key)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& a = context->input(0);\n    const Tensor& b = context->input(1);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(2).shape()),\n                errors::InvalidArgument(\"min_a must be a scalar, but got shape\",\n                                        context->input(2).shape()));\n    const float min_a = context->input(2).flat<float>()(0);\n    OP_REQUIRES(context, context->input(3).NumElements() == 1,\n                errors::InvalidArgument(\"max_a must be a scalar, but got shape\",\n                                        context->input(3).shape()));\n    const float max_a = context->input(3).flat<float>()(0);\n    OP_REQUIRES(context, context->input(4).NumElements() == 1,\n                errors::InvalidArgument(\"min_b must be a scalar, but got shape\",\n                                        context->input(4).shape()));\n    const float min_b = context->input(4).flat<float>()(0);\n    OP_REQUIRES(context, context->input(5).NumElements() == 1,\n                errors::InvalidArgument(\"max_b must be a scalar, but got shape\",\n                                        context->input(5).shape()));\n    const float max_b = context->input(5).flat<float>()(0);\n\n    // Make sure that we have valid quantization ranges for the input buffers.\n    // If the difference between the min and max is negative or zero, it makes\n    // it hard to do meaningful intermediate operations on the values.\n    OP_REQUIRES(context, (max_a > min_a),\n                errors::InvalidArgument(\"max_a must be larger than min_a.\"));\n    OP_REQUIRES(context, (max_b > min_b),\n                errors::InvalidArgument(\"max_b must be larger than min_b.\"));\n    const int32_t offset_a = FloatToQuantizedUnclamped<T1>(0.0f, min_a, max_a);\n    const int32_t offset_b = FloatToQuantizedUnclamped<T2>(0.0f, min_b, max_b);\n    const int32_t offset_c = 0;\n    const int32_t mult_c = 1;\n    const int32_t shift_c = 0;\n\n    // Check that the dimensions of the two matrices are valid.\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(a.shape()),\n                errors::InvalidArgument(\"In[0] is not a matrix\"));\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(b.shape()),\n                errors::InvalidArgument(\"In[1] is not a matrix\"));\n    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n    dim_pair[0].first = transpose_a_ ? 0 : 1;\n    dim_pair[0].second = transpose_b_ ? 1 : 0;\n\n    OP_REQUIRES(context,\n                a.dim_size(dim_pair[0].first) == b.dim_size(dim_pair[0].second),\n                errors::InvalidArgument(\"Matrix size-incompatible: In[0]: \",\n                                        a.shape().DebugString(),\n                                        \", In[1]: \", b.shape().DebugString()));\n\n    OP_REQUIRES(context, ((shift_c >= 0) && (shift_c <= 31)),\n                errors::InvalidArgument(\"shift_c must be between 0 and 31, \"\n                                        \"inclusive.\"));\n\n    int a_dim_remaining = 1 - dim_pair[0].first;\n    int b_dim_remaining = 1 - dim_pair[0].second;\n    TensorShape out_shape(\n        {a.dim_size(a_dim_remaining), b.dim_size(b_dim_remaining)});\n    Tensor* c = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &c));\n    CHECK(c);\n\n    const T1* a_data = a.flat<T1>().data();\n    const T2* b_data = b.flat<T2>().data();\n    Toutput* c_data = c->flat<Toutput>().data();\n\n    const bool transpose_c = false;\n    const size_t m = a.dim_size(a_dim_remaining);\n    const size_t n = b.dim_size(b_dim_remaining);\n    const size_t k = a.dim_size(dim_pair[0].first);\n    const size_t lda = a.dim_size(1);\n    const size_t ldb = b.dim_size(1);\n    const size_t ldc = n;\n\n    if (meta::IsSupportedAndEnabled() && std::is_same<T1, quint8>() &&\n        std::is_same<T2, quint8>() && std::is_same<Toutput, qint32>() &&\n        (offset_c == 0) && (mult_c == 1) && (shift_c == 0) &&\n        (transpose_c == false) && (k <= 2048)) {\n      // Gemmlowp/meta code path works on 32 & 64 bit Arm with NEON Simd and\n      // allows optimized quantized 8bit to 32bit gemm.\n      meta::QuantizedGemm(context, transpose_a_, transpose_b_, a_data, b_data,\n                          c_data, m, n, k, -offset_a, -offset_b, lda, ldb, ldc);\n    } else if (std::is_same<T1, quint8>() && std::is_same<T2, quint8>() &&\n               std::is_same<Toutput, qint32>() && (offset_c == 0) &&\n               (mult_c == 1) && (shift_c == 0) && (transpose_c == false)) {\n      // The gemmlowp optimized library only works for a particular set of data\n      // types, so check if we meet those requirements and fall back to a slower\n      // reference implementation if not.\n      if (transpose_a_) {\n        if (transpose_b_) {\n          GemmlowpMultiply<true, true, false>(context, a_data, b_data, c_data,\n                                              m, n, k, offset_a, offset_b, lda,\n                                              ldb, ldc);\n        } else {\n          GemmlowpMultiply<true, false, false>(context, a_data, b_data, c_data,\n                                               m, n, k, offset_a, offset_b, lda,\n                                               ldb, ldc);\n        }\n      } else {\n        if (transpose_b_) {\n          GemmlowpMultiply<false, true, false>(context, a_data, b_data, c_data,\n                                               m, n, k, offset_a, offset_b, lda,\n                                               ldb, ldc);\n        } else {\n          GemmlowpMultiply<false, false, false>(context, a_data, b_data, c_data,\n                                                m, n, k, offset_a, offset_b,\n                                                lda, ldb, ldc);\n        }\n      }\n    } else {\n      ReferenceGemm<T1, T2, Toutput>(\n          transpose_a_, transpose_b_, transpose_c, m, n, k, a_data, offset_a,\n          lda, b_data, offset_b, ldb, c_data, shift_c, offset_c, mult_c, ldc);\n    }\n\n    float min_c_value;\n    float max_c_value;\n    QuantizationRangeForMultiplication<T1, T2, Toutput>(\n        min_a, max_a, min_b, max_b, &min_c_value, &max_c_value);\n    Tensor* c_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &c_min));\n    c_min->flat<float>()(0) = min_c_value;\n\n    Tensor* c_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &c_max));\n    c_max->flat<float>()(0) = max_c_value;\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def bad_url(prefix):\n    \"\"\"Redirect unknown root URLs to /simple/.\"\"\"\n    return redirect(core.get_bad_url_redirect_path(request, prefix))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def authenticate(template, title):\n    \"\"\"\n    Authentication management method.\n    :param template: the template name that should be rendered\n    :returns: rendered final page or a page with error message\n    \"\"\"\n    global AUTH_ISSUERS, SAML_SUPPORT, AUTH_TYPE\n    cookie = []\n    valid_token_dict = validate_webui_token()\n    if not valid_token_dict:\n        cookie.append({'key': 'rucio-requested-path', 'value': request.environ.get('REQUEST_URI')})\n    else:\n        return access_granted(valid_token_dict, template, title)\n\n    # login without any known server config\n    if not AUTH_TYPE:\n        return add_cookies(make_response(render_template(\"select_login_method.html\", oidc_issuers=AUTH_ISSUERS, saml_support=SAML_SUPPORT)), cookie)\n    # for AUTH_TYPE predefined by the server continue\n    else:\n        if AUTH_TYPE == 'userpass':\n            return redirect('/login', code=303)\n        elif AUTH_TYPE == 'x509':\n            return x509token_auth(None)\n        elif AUTH_TYPE == 'x509_userpass':\n            if request.environ.get('SSL_CLIENT_VERIFY') == 'SUCCESS':\n                return x509token_auth(None)\n            return render_template(\"no_certificate.html\")\n        elif AUTH_TYPE == 'oidc':\n            return oidc_auth(None, AUTH_ISSUER_WEBUI)\n        elif AUTH_TYPE == 'saml':\n            return saml_auth(\"GET\")\n        return render_template('problem.html', msg='Invalid auth type')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_send_leave_request(self, origin, pdu):\n        \"\"\" We have received a leave event for a room. Fully process it.\"\"\"\n        event = pdu\n\n        logger.debug(\n            \"on_send_leave_request: Got event: %s, signatures: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        if get_domain_from_id(event.sender) != origin:\n            logger.info(\n                \"Got /send_leave request for user %r from different origin %s\",\n                event.sender,\n                origin,\n            )\n            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)\n\n        event.internal_metadata.outlier = False\n\n        context = yield self._handle_new_event(origin, event)\n\n        event_allowed = yield self.third_party_event_rules.check_event_allowed(\n            event, context\n        )\n        if not event_allowed:\n            logger.info(\"Sending of leave %s forbidden by third-party rules\", event)\n            raise SynapseError(\n                403, \"This event is not allowed in this context\", Codes.FORBIDDEN\n            )\n\n        logger.debug(\n            \"on_send_leave_request: After _handle_new_event: %s, sigs: %s\",\n            event.event_id,\n            event.signatures,\n        )\n\n        return None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, redirect_to):\n        super(HttpResponseRedirectBase, self).__init__()\n        parsed = urlparse(redirect_to)\n        if parsed.scheme and parsed.scheme not in self.allowed_schemes:\n            raise SuspiciousOperation(\"Unsafe redirect to URL with scheme '%s'\" % parsed.scheme)\n        self['Location'] = iri_to_uri(redirect_to)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    const findHeadingElement = (cell: Cell): void => {\n      model.getCellHeadings(cell).forEach(async heading => {\n        const elementId = await getIdForHeading(\n          heading,\n          this.parser!,\n          this.sanitizer\n        );\n\n        const selector = elementId\n          ? `h${heading.level}[id=\"${CSS.escape(elementId)}\"]`\n          : `h${heading.level}`;\n\n        if (heading.outputIndex !== undefined) {\n          // Code cell\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              (heading.cellRef as CodeCell).outputArea.widgets[\n                heading.outputIndex\n              ].node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        } else {\n          headingToElement.set(\n            heading,\n            TableOfContentsUtils.addPrefix(\n              heading.cellRef.node,\n              selector,\n              heading.prefix ?? ''\n            )\n          );\n        }\n      });\n    };\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def event_from_pdu_json(pdu_json, outlier=False):\n    \"\"\"Construct a FrozenEvent from an event json received over federation\n\n    Args:\n        pdu_json (object): pdu as received over federation\n        outlier (bool): True to mark this event as an outlier\n\n    Returns:\n        FrozenEvent\n\n    Raises:\n        SynapseError: if the pdu is missing required fields or is otherwise\n            not a valid matrix event\n    \"\"\"\n    # we could probably enforce a bunch of other fields here (room_id, sender,\n    # origin, etc etc)\n    assert_params_in_request(pdu_json, ('event_id', 'type', 'depth'))\n\n    depth = pdu_json['depth']\n    if not isinstance(depth, six.integer_types):\n        raise SynapseError(400, \"Depth %r not an intger\" % (depth, ),\n                           Codes.BAD_JSON)\n\n    if depth < 0:\n        raise SynapseError(400, \"Depth too small\", Codes.BAD_JSON)\n    elif depth > MAX_DEPTH:\n        raise SynapseError(400, \"Depth too large\", Codes.BAD_JSON)\n\n    event = FrozenEvent(\n        pdu_json\n    )\n\n    event.internal_metadata.outlier = outlier\n\n    return event\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    const Tensor& tag = c->input(0);\n    const Tensor& tensor = c->input(1);\n    OP_REQUIRES(c, TensorShapeUtils::IsScalar(tag.shape()),\n                errors::InvalidArgument(\"Tag must be a scalar\"));\n    OP_REQUIRES(c, tensor.dims() >= 2 && tensor.dims() <= 3,\n                errors::InvalidArgument(\"Tensor must be 3-D or 2-D, got: \",\n                                        tensor.shape().DebugString()));\n    const string& base_tag = tag.scalar<tstring>()();\n\n    float sample_rate = sample_rate_attr_;\n    if (!has_sample_rate_attr_) {\n      const Tensor& sample_rate_tensor = c->input(2);\n      OP_REQUIRES(c,\n                  sample_rate_tensor.IsAligned() &&\n                      sample_rate_tensor.NumElements() == 1,\n                  errors::InvalidArgument(\n                      \"sample_rate must be rank-0 or contain a single value\"));\n      sample_rate = sample_rate_tensor.scalar<float>()();\n    }\n    OP_REQUIRES(c, sample_rate > 0.0f,\n                errors::InvalidArgument(\"sample_rate must be > 0\"));\n\n    const int batch_size = tensor.dim_size(0);\n    const int64_t length_frames = tensor.dim_size(1);\n    const int64_t num_channels =\n        tensor.dims() == 2 ? 1 : tensor.dim_size(tensor.dims() - 1);\n\n    Summary s;\n    const int N = std::min<int>(max_outputs_, batch_size);\n    for (int i = 0; i < N; ++i) {\n      Summary::Value* v = s.add_value();\n      if (max_outputs_ > 1) {\n        v->set_tag(strings::StrCat(base_tag, \"/audio/\", i));\n      } else {\n        v->set_tag(strings::StrCat(base_tag, \"/audio\"));\n      }\n\n      Summary::Audio* sa = v->mutable_audio();\n      sa->set_sample_rate(sample_rate);\n      sa->set_num_channels(num_channels);\n      sa->set_length_frames(length_frames);\n      sa->set_content_type(\"audio/wav\");\n\n      auto values =\n          tensor.shaped<float, 3>({batch_size, length_frames, num_channels});\n      const float* data =\n          tensor.NumElements() == 0 ? nullptr : &values(i, 0, 0);\n\n      size_t sample_rate_truncated = lrintf(sample_rate);\n      if (sample_rate_truncated == 0) {\n        sample_rate_truncated = 1;\n      }\n      OP_REQUIRES_OK(c, wav::EncodeAudioAsS16LEWav(\n                            data, sample_rate_truncated, num_channels,\n                            length_frames, sa->mutable_encoded_audio_string()));\n    }\n\n    Tensor* summary_tensor = nullptr;\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape({}), &summary_tensor));\n    CHECK(SerializeToTString(s, &summary_tensor->scalar<tstring>()()));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_header_end(self) -> None:\n        field = self._current_partial_header_name.lower()\n        if field == b\"content-disposition\":\n            self._current_part.content_disposition = self._current_partial_header_value\n        self._current_part.item_headers.append(\n            (field, self._current_partial_header_value)\n        )\n        self._current_partial_header_name = b\"\"\n        self._current_partial_header_value = b\"\"\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def create_code_for_user(cls, user, next=None):\n        if not user.is_active:\n            return None\n\n        login_code = LoginCode(user=user)\n        if next is not None:\n            login_code.next = next\n        login_code.save()\n        return login_code\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* c) override {\n    PartialTensorShape element_shape;\n    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(0), &element_shape));\n    int32 num_elements = c->input(1).scalar<int32>()();\n    OP_REQUIRES(c, num_elements >= 0,\n                errors::InvalidArgument(\"The num_elements to reserve must be a \"\n                                        \"non negative number, but got \",\n                                        num_elements));\n    TensorList output;\n    output.element_shape = element_shape;\n    output.element_dtype = element_dtype_;\n    output.tensors().resize(num_elements, Tensor(DT_INVALID));\n    Tensor* result;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape{}, &result, attr));\n    result->scalar<Variant>()() = std::move(output);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // inputs must be at least a matrix\n    OP_REQUIRES(\n        ctx, sorted_inputs_t.shape().dims() >= 2,\n        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n    OP_REQUIRES_OK(\n        ctx, functor::UpperBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == 4,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional (2)\"));\n    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n                errors::InvalidArgument(\"tensor_in must not be empty (2)\"));\n\n    PoolParameters params{context,\n                          ksize_,\n                          stride_,\n                          padding_,\n                          /*explicit_paddings=*/{},\n                          FORMAT_NHWC,\n                          tensor_in.shape()};\n    if (!context->status().ok()) {\n      return;\n    }\n\n    TensorShape out_shape({params.tensor_in_batch, params.out_height,\n                           params.out_width, params.depth});\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n    Tensor* argmax = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, out_shape, &argmax));\n\n    LaunchMaxPoolingWithArgmax<Device, T, Targmax>::launch(\n        context, params, tensor_in, output, argmax, propagate_nans_,\n        include_batch_in_index_);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _wget_download(url: str, fullname: str):\n    try:\n        assert urlparse(url).scheme in (\n            'http',\n            'https',\n        ), 'Only support https and http url'\n        # using wget to download url\n        tmp_fullname = fullname + \"_tmp\"\n        # \u2013user-agent\n        command = f'wget -O {tmp_fullname} -t {DOWNLOAD_RETRY_LIMIT} {url}'\n        subprc = subprocess.Popen(\n            command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        _ = subprc.communicate()\n\n        if subprc.returncode != 0:\n            raise RuntimeError(\n                f'{command} failed. Please make sure `wget` is installed or {url} exists'\n            )\n\n        shutil.move(tmp_fullname, fullname)\n\n    except Exception as e:  # requests.exceptions.ConnectionError\n        logger.info(f\"Downloading {url} failed with exception {str(e)}\")\n        return False\n\n    return fullname\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def delete_shelf(shelf_id):\n    cur_shelf = ub.session.query(ub.Shelf).filter(ub.Shelf.id == shelf_id).first()\n    try:\n        delete_shelf_helper(cur_shelf)\n        flash(_(\"Shelf successfully deleted\"), category=\"success\")\n    except InvalidRequestError:\n        ub.session.rollback()\n        log.error(\"Settings DB is not Writeable\")\n        flash(_(\"Settings DB is not Writeable\"), category=\"error\")\n    return redirect(url_for('web.index'))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status TrySimplify(NodeDef* consumer, string* simplified_node_name) override {\n    NodeDef* producer;\n\n    if (consumer->input_size() < 1) {\n      return errors::FailedPrecondition(\"Node \", simplified_node_name,\n                                        \" lacks inputs\");\n    }\n\n    TF_RETURN_IF_ERROR(GetInputNode(consumer->input(0), &producer));\n    const bool producer_is_cast = IsCastLike(*producer);\n    const bool can_optimize =\n        !IsCheckNumerics(*producer) &&\n        ((producer_is_cast && IsValuePreserving(*consumer)) ||\n         (IsValuePreserving(*producer) && IsCastLike(*consumer)));\n    if (!can_optimize || IsControlFlow(*producer) ||\n        IsInPreserveSet(*producer) ||\n        producer->device() != consumer->device()) {\n      return Status::OK();\n    }\n\n    const NodeDef* cast_like_node = producer_is_cast ? producer : consumer;\n    const OpDef* cast_like_op_def = nullptr;\n    TF_RETURN_IF_ERROR(OpRegistry::Global()->LookUpOpDef(cast_like_node->op(),\n                                                         &cast_like_op_def));\n    DataType cast_src_type;\n    TF_RETURN_IF_ERROR(InputTypeForNode(*cast_like_node, *cast_like_op_def, 0,\n                                        &cast_src_type));\n    DataType cast_dst_type;\n    TF_RETURN_IF_ERROR(OutputTypeForNode(*cast_like_node, *cast_like_op_def, 0,\n                                         &cast_dst_type));\n    if (!IsFixedSizeType(cast_src_type) || !IsFixedSizeType(cast_dst_type)) {\n      return Status::OK();\n    } else if (producer_is_cast &&\n               DataTypeSize(cast_dst_type) <= DataTypeSize(cast_src_type)) {\n      return Status::OK();\n    } else if (!producer_is_cast &&\n               DataTypeSize(cast_dst_type) >= DataTypeSize(cast_src_type)) {\n      return Status::OK();\n    }\n\n    // Check that nodes were not already optimized.\n    const string optimized_producer_name = OptimizedNodeName(\n        ParseNodeScopeAndName(producer->name()), DataTypeString(cast_dst_type));\n    const string optimized_consumer_name = OptimizedNodeName(\n        ParseNodeScopeAndName(consumer->name()), DataTypeString(cast_src_type));\n    const bool is_already_optimized =\n        ctx().node_map->NodeExists(optimized_consumer_name) ||\n        ctx().node_map->NodeExists(optimized_producer_name);\n    if (is_already_optimized) {\n      return Status::OK();\n    }\n\n    // Add copies of consumer and producer in reverse order.\n    NodeDef* input;\n    TF_RETURN_IF_ERROR(GetInputNode(producer->input(0), &input));\n    // Create new producer node.\n    NodeDef* new_producer = AddCopyNode(optimized_consumer_name, consumer);\n    new_producer->set_input(0, producer->input(0));\n    ctx().node_map->AddOutput(input->name(), new_producer->name());\n\n    // Create new consumer node.\n    NodeDef* new_consumer = AddCopyNode(optimized_producer_name, producer);\n    new_consumer->set_input(0, new_producer->name());\n\n    NodeDef* new_value_preserving =\n        producer_is_cast ? new_producer : new_consumer;\n    const DataType new_input_type =\n        producer_is_cast ? cast_src_type : cast_dst_type;\n    // Update the input type of the value-preserving node. The input and\n    // output types of the cast-like nodes remain the same.\n    TF_RETURN_IF_ERROR(SetInputType(new_input_type, new_value_preserving));\n    // Make sure there is a kernel registered for the value preserving op\n    // with the new input type.\n    TF_RETURN_IF_ERROR(IsKernelRegisteredForNode(*new_value_preserving));\n    ctx().node_map->AddOutput(new_producer->name(), new_consumer->name());\n\n    AddToOptimizationQueue(new_producer);\n    *simplified_node_name = new_consumer->name();\n\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def wrapper(self, *args, **kwargs):\n        try:\n            result = method(self, *args, **kwargs)\n        except web.HTTPError as e:\n            status = e.status_code\n            message = e.log_message\n            self.log.warn(message)\n            self.set_status(e.status_code)\n            self.set_header('Content-Type', 'application/json')\n            self.finish(json.dumps(dict(message=message)))\n        except Exception:\n            self.log.error(\"Unhandled error in API request\", exc_info=True)\n            status = 500\n            message = \"Unknown server error\"\n            t, value, tb = sys.exc_info()\n            self.set_status(status)\n            tb_text = ''.join(traceback.format_exception(t, value, tb))\n            reply = dict(message=message, traceback=tb_text)\n            self.set_header('Content-Type', 'application/json')\n            self.finish(json.dumps(reply))\n        else:\n            return result\n    return wrapper"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _init_file(self):\n        \"\"\"Init the password file, set the password for it.\n        \"\"\"\n\n        password = None\n        while password is None:\n            password = self._getpass(\"Please set a password for your new keyring\")\n            password2 = self._getpass('Password (again): ')\n            if password != password2:\n                sys.stderr.write(\"Error: Your passwords didn't match\\n\")\n                password = None\n                continue\n            if '' == password.strip():\n                # forbid the blank password\n                sys.stderr.write(\"Error: blank passwords aren't allowed.\\n\")\n                password = None\n\n        # write down the initialization\n        config = ConfigParser.RawConfigParser()\n        self._write_config(config, password)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    auto x = ctx->input(0);\n    auto i = ctx->input(1);\n    auto v = ctx->input(2);\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(i.shape()),\n                errors::InvalidArgument(\"i must be a vector. \",\n                                        i.shape().DebugString()));\n    OP_REQUIRES(ctx, x.dims() == v.dims(),\n                errors::InvalidArgument(\n                    \"x and v shape doesn't match (ranks differ): \",\n                    x.shape().DebugString(), \" vs. \", v.shape().DebugString()));\n    for (int i = 1; i < x.dims(); ++i) {\n      OP_REQUIRES(\n          ctx, x.dim_size(i) == v.dim_size(i),\n          errors::InvalidArgument(\"x and v shape doesn't match at index \", i,\n                                  \" : \", x.shape().DebugString(), \" vs. \",\n                                  v.shape().DebugString()));\n    }\n    OP_REQUIRES(ctx, i.dim_size(0) == v.dim_size(0),\n                errors::InvalidArgument(\n                    \"i and x shape doesn't match at index 0: \",\n                    i.shape().DebugString(), \" vs. \", v.shape().DebugString()));\n\n    Tensor y = x;  // This creates an alias intentionally.\n    // Skip processing if tensors are empty.\n    if (x.NumElements() > 0 && v.NumElements() > 0) {\n      OP_REQUIRES_OK(ctx, DoCompute(ctx, i, v, &y));\n    }\n    ctx->set_output(0, y);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def guard(dest, ev_args):\n    depth = 1\n    try:\n        next(dest)\n        while True:\n            ev = yield\n            if ev[0] == \"start\":\n                depth += 1\n            elif ev[0] == \"end\":\n                depth -= 1\n            try:\n                dest.send(ev)\n            except StopIteration as exc:\n                return exc.value\n    finally:\n        while depth > 0:\n            ev_type, *_ = yield\n            if ev_type == \"end\":\n                depth -= 1\n            elif ev_type == \"start\":\n                depth += 1\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_session_id_by_provider_id(self, provider_id, user):\n        \"\"\"\n        Return a tuple of logged-in session IDs by provider_id\n        \"\"\"\n        candidates = self._ss.get_user_sessions(user)\n\n        session_ids = []\n        for c in candidates:\n            key = c.keys()[0]\n            if c[key].get('provider_id') == provider_id:\n                samlsession = self._data_to_samlsession(key, c[key])\n                session_ids.append(samlsession.session_id.encode('utf-8'))\n\n        return tuple(session_ids)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def real_download(self, filename, info_dict):\n        self.report_destination(filename)\n        tmpfilename = self.temp_name(filename)\n        self._cookies_tempfile = None\n\n        try:\n            started = time.time()\n            retval = self._call_downloader(tmpfilename, info_dict)\n        except KeyboardInterrupt:\n            if not info_dict.get('is_live'):\n                raise\n            # Live stream downloading cancellation should be considered as\n            # correct and expected termination thus all postprocessing\n            # should take place\n            retval = 0\n            self.to_screen('[%s] Interrupted by user' % self.get_basename())\n        finally:\n            if self._cookies_tempfile:\n                self.try_remove(self._cookies_tempfile)\n\n        if retval == 0:\n            status = {\n                'filename': filename,\n                'status': 'finished',\n                'elapsed': time.time() - started,\n            }\n            if filename != '-':\n                fsize = os.path.getsize(encodeFilename(tmpfilename))\n                self.try_rename(tmpfilename, filename)\n                status.update({\n                    'downloaded_bytes': fsize,\n                    'total_bytes': fsize,\n                })\n            self._hook_progress(status, info_dict)\n            return True\n        else:\n            self.to_stderr('\\n')\n            self.report_error('%s exited with code %d' % (\n                self.get_basename(), retval))\n            return False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) final {\n    const Tensor& superdiag = context->input(0);\n    const Tensor& maindiag = context->input(1);\n    const Tensor& subdiag = context->input(2);\n    const Tensor& rhs = context->input(3);\n\n    const int ndims = rhs.dims();\n    OP_REQUIRES(\n        context, ndims >= 2,\n        errors::InvalidArgument(\"Input must have rank >= 2, but got \", ndims));\n    OP_REQUIRES_OK(context, ValidateInputTensor(superdiag, \"superdiag\", rhs));\n    OP_REQUIRES_OK(context, ValidateInputTensor(maindiag, \"maindiag\", rhs));\n    OP_REQUIRES_OK(context, ValidateInputTensor(subdiag, \"subdiag\", rhs));\n    int64 batch_size = 1;\n    for (int i = 0; i < ndims - 2; i++) {\n      batch_size *= rhs.dim_size(i);\n    }\n    const int m = rhs.dim_size(ndims - 2);\n    const int n = rhs.dim_size(ndims - 1);\n\n    // Allocate output.\n    Tensor* output;\n    OP_REQUIRES_OK(context, context->allocate_output(0, rhs.shape(), &output));\n\n    const Eigen::GpuDevice& device = context->eigen_device<Eigen::GpuDevice>();\n    GpuLaunchConfig cfg = GetGpuLaunchConfig(1, device);\n    TF_CHECK_OK(GpuLaunchKernel(\n        TridiagonalMatMulKernel<Scalar>, cfg.block_count, cfg.thread_per_block,\n        0, device.stream(), batch_size, m, n, superdiag.flat<Scalar>().data(),\n        maindiag.flat<Scalar>().data(), subdiag.flat<Scalar>().data(),\n        rhs.flat<Scalar>().data(), output->flat<Scalar>().data()));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def upload_template(filename, destination, context=None, use_jinja=False,\n    template_dir=None, use_sudo=False, backup=True, mirror_local_mode=False,\n    mode=None):\n    \"\"\"\n    Render and upload a template text file to a remote host.\n\n    ``filename`` should be the path to a text file, which may contain `Python\n    string interpolation formatting\n    <http://docs.python.org/release/2.5.4/lib/typesseq-strings.html>`_ and will\n    be rendered with the given context dictionary ``context`` (if given.)\n\n    Alternately, if ``use_jinja`` is set to True and you have the Jinja2\n    templating library available, Jinja will be used to render the template\n    instead. Templates will be loaded from the invoking user's current working\n    directory by default, or from ``template_dir`` if given.\n\n    The resulting rendered file will be uploaded to the remote file path\n    ``destination``.  If the destination file already exists, it will be\n    renamed with a ``.bak`` extension unless ``backup=False`` is specified.\n\n    By default, the file will be copied to ``destination`` as the logged-in\n    user; specify ``use_sudo=True`` to use `sudo` instead.\n\n    In some use cases, it is desirable to force a newly uploaded file to match\n    the mode of its local counterpart (such as when uploading executable\n    scripts). To do this, specify ``mirror_local_mode=True``.\n\n    Alternately, you may use the ``mode`` kwarg to specify an exact mode, in\n    the same vein as ``os.chmod`` or the Unix ``chmod`` command.\n    \"\"\"\n    func = use_sudo and sudo or run\n    # Normalize destination to be an actual filename, due to using StringIO\n    with settings(hide('everything'), warn_only=True):\n        if func('test -d %s' % destination).succeeded:\n            sep = \"\" if destination.endswith('/') else \"/\"\n            destination += sep + os.path.basename(filename)\n\n    # Process template\n    text = None\n    if use_jinja:\n        try:\n            from jinja2 import Environment, FileSystemLoader\n            jenv = Environment(loader=FileSystemLoader(template_dir or '.'))\n            text = jenv.get_template(filename).render(**context or {})\n        except ImportError, e:\n            abort(\"tried to use Jinja2 but was unable to import: %s\" % e)\n    else:\n        with open(filename) as inputfile:\n            text = inputfile.read()\n        if context:\n            text = text % context\n\n    # Back up original file\n    if backup and exists(destination):\n        func(\"cp %s{,.bak}\" % destination)\n\n    # Upload the file.\n    put(\n        local_path=StringIO(text),\n        remote_path=destination,\n        use_sudo=use_sudo,\n        mirror_local_mode=mirror_local_mode,\n        mode=mode\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Subgraph::Invoke() {\n  if (!consistent_) {\n    ReportError(\"Invoke called on model that is not consistent.\");\n    return kTfLiteError;\n  }\n\n  TfLiteStatus status = kTfLiteOk;\n  if (state_ == kStateUninvokable) {\n    ReportError(\"Invoke called on model that is not ready.\");\n    return kTfLiteError;\n  } else if (memory_planner_ && !memory_planner_->HasNonPersistentMemory()) {\n    ReportError(\"Non-persistent memory is not available.\");\n    return kTfLiteError;\n  }\n  TFLITE_SCOPED_TAGGED_DEFAULT_PROFILE(profiler_.get(), \"Invoke\");\n\n  // Invocations are always done in node order.\n  // Note that calling Invoke repeatedly will cause the original memory plan to\n  // be reused, unless either ResizeInputTensor() or AllocateTensors() has been\n  // called.\n  for (int execution_plan_index = 0;\n       execution_plan_index < execution_plan_.size(); execution_plan_index++) {\n    if (execution_plan_index == next_execution_plan_index_to_prepare_) {\n      TF_LITE_ENSURE_STATUS(PrepareOpsAndTensors());\n      TF_LITE_ENSURE(&context_, next_execution_plan_index_to_prepare_ >=\n                                    execution_plan_index);\n    }\n    int node_index = execution_plan_[execution_plan_index];\n    TfLiteNode& node = nodes_and_registration_[node_index].first;\n    const TfLiteRegistration& registration =\n        nodes_and_registration_[node_index].second;\n\n    const char* op_name = nullptr;\n    if (profiler_) op_name = GetTFLiteOpName(registration);\n    TFLITE_SCOPED_TAGGED_OPERATOR_PROFILE(profiler_.get(), op_name, node_index);\n\n    for (int i = 0; i < node.inputs->size; ++i) {\n      int tensor_index = node.inputs->data[i];\n      if (tensor_index == kTfLiteOptionalTensor) {\n        continue;\n      }\n      TfLiteTensor* tensor = &tensors_[tensor_index];\n      if (tensor->delegate && tensor->delegate != node.delegate &&\n          tensor->data_is_stale) {\n        TF_LITE_ENSURE_STATUS(EnsureTensorDataIsReadable(tensor_index));\n      }\n      if (tensor->data.raw == nullptr && tensor->bytes > 0) {\n        if (registration.builtin_code == kTfLiteBuiltinReshape && i == 1 &&\n            tensor->dims->size != 1) {\n          // In general, having a tensor here with no buffer will be an error.\n          // However, for the reshape operator, the second input tensor is\n          // sometimes only used for the shape, not for the data. Thus, null\n          // buffer is ok in this situation.\n          // The situation where null buffer is not ok for reshape operator is\n          // only when there are 2 inputs given to the node and the one\n          // corresponding to the shape (i == 1) is a vector that contains all\n          // dimensions. See `GetOutputShape()` function in\n          // `tensorflow/lite/kernels/reshape.cc`\n          continue;\n        } else {\n          // In all other cases, we need to return an error as otherwise we will\n          // trigger a null pointer dereference (likely).\n          ReportError(\"Input tensor %d lacks data\", tensor_index);\n          return kTfLiteError;\n        }\n      }\n    }\n\n    if (check_cancelled_func_ != nullptr &&\n        check_cancelled_func_(cancellation_data_)) {\n      ReportError(\"Client requested cancel during Invoke()\");\n      return kTfLiteError;\n    }\n\n    EnsureTensorsVectorCapacity();\n    tensor_resized_since_op_invoke_ = false;\n    if (OpInvoke(registration, &node) != kTfLiteOk) {\n      return ReportOpError(&context_, node, registration, node_index,\n                           \"failed to invoke\");\n    }\n\n    // Force execution prep for downstream ops if the latest op triggered the\n    // resize of a dynamic tensor.\n    if (tensor_resized_since_op_invoke_ &&\n        HasDynamicTensor(context_, node.outputs)) {\n      next_execution_plan_index_to_prepare_ = execution_plan_index + 1;\n\n      // This happens when an intermediate dynamic tensor is resized.\n      // We don't have to prepare all the ops, but we need to recompute\n      // the allocation plan.\n      if (next_execution_plan_index_to_plan_allocation_ >\n          next_execution_plan_index_to_prepare_) {\n        next_execution_plan_index_to_plan_allocation_ =\n            next_execution_plan_index_to_prepare_;\n        if (memory_planner_) {\n          TF_LITE_ENSURE_STATUS(memory_planner_->ResetAllocationsAfter(\n              next_execution_plan_index_to_plan_allocation_ - 1));\n        }\n      }\n    }\n  }\n\n  return status;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def do_template(self, data, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, disable_lookups=False):\n        if self.jinja2_native and not isinstance(data, string_types):\n            return data\n\n        # For preserving the number of input newlines in the output (used\n        # later in this method)\n        data_newlines = _count_newlines_from_end(data)\n\n        if fail_on_undefined is None:\n            fail_on_undefined = self._fail_on_undefined_errors\n\n        try:\n            # allows template header overrides to change jinja2 options.\n            if overrides is None:\n                myenv = self.environment\n            else:\n                myenv = self.environment.overlay(overrides)\n\n            # Get jinja env overrides from template\n            if hasattr(data, 'startswith') and data.startswith(JINJA2_OVERRIDE):\n                eol = data.find('\\n')\n                line = data[len(JINJA2_OVERRIDE):eol]\n                data = data[eol + 1:]\n                for pair in line.split(','):\n                    (key, val) = pair.split(':')\n                    key = key.strip()\n                    setattr(myenv, key, ast.literal_eval(val.strip()))\n\n            if escape_backslashes:\n                # Allow users to specify backslashes in playbooks as \"\\\\\" instead of as \"\\\\\\\\\".\n                data = _escape_backslashes(data, myenv)\n\n            try:\n                t = myenv.from_string(data)\n            except TemplateSyntaxError as e:\n                raise AnsibleError(\"template error while templating string: %s. String: %s\" % (to_native(e), to_native(data)))\n            except Exception as e:\n                if 'recursion' in to_native(e):\n                    raise AnsibleError(\"recursive loop detected in template string: %s\" % to_native(data))\n                else:\n                    return data\n\n            if disable_lookups:\n                t.globals['query'] = t.globals['q'] = t.globals['lookup'] = self._fail_lookup\n\n            jvars = AnsibleJ2Vars(self, t.globals)\n\n            self.cur_context = new_context = t.new_context(jvars, shared=True)\n            rf = t.root_render_func(new_context)\n\n            try:\n                if self.jinja2_native:\n                    res = ansible_native_concat(rf)\n                else:\n                    res = j2_concat(rf)\n                unsafe = getattr(new_context, 'unsafe', False)\n                if unsafe:\n                    res = wrap_var(res)\n            except TypeError as te:\n                if 'AnsibleUndefined' in to_native(te):\n                    errmsg = \"Unable to look up a name or access an attribute in template string (%s).\\n\" % to_native(data)\n                    errmsg += \"Make sure your variable name does not contain invalid characters like '-': %s\" % to_native(te)\n                    raise AnsibleUndefinedVariable(errmsg)\n                else:\n                    display.debug(\"failing because of a type error, template data is: %s\" % to_text(data))\n                    raise AnsibleError(\"Unexpected templating type error occurred on (%s): %s\" % (to_native(data), to_native(te)))\n\n            if self.jinja2_native and not isinstance(res, string_types):\n                return res\n\n            if preserve_trailing_newlines:\n                # The low level calls above do not preserve the newline\n                # characters at the end of the input data, so we use the\n                # calculate the difference in newlines and append them\n                # to the resulting output for parity\n                #\n                # jinja2 added a keep_trailing_newline option in 2.7 when\n                # creating an Environment.  That would let us make this code\n                # better (remove a single newline if\n                # preserve_trailing_newlines is False).  Once we can depend on\n                # that version being present, modify our code to set that when\n                # initializing self.environment and remove a single trailing\n                # newline here if preserve_newlines is False.\n                res_newlines = _count_newlines_from_end(res)\n                if data_newlines > res_newlines:\n                    res += self.environment.newline_sequence * (data_newlines - res_newlines)\n                    if unsafe:\n                        res = wrap_var(res)\n            return res\n        except (UndefinedError, AnsibleUndefinedVariable) as e:\n            if fail_on_undefined:\n                raise AnsibleUndefinedVariable(e)\n            else:\n                display.debug(\"Ignoring undefined failure: %s\" % to_text(e))\n                return data\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_user_roles(self, context, user_id, tenant_id=None):\n        \"\"\"Get the roles for a user and tenant pair.\n\n        Since we're trying to ignore the idea of user-only roles we're\n        not implementing them in hopes that the idea will die off.\n\n        \"\"\"\n        self.assert_admin(context)\n        if tenant_id is None:\n            raise exception.NotImplemented(message='User roles not supported: '\n                                                   'tenant ID required')\n\n        user = self.identity_api.get_user(context, user_id)\n        if user is None:\n            raise exception.UserNotFound(user_id=user_id)\n        tenant = self.identity_api.get_tenant(context, tenant_id)\n        if tenant is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        roles = self.identity_api.get_roles_for_user_and_tenant(\n                context, user_id, tenant_id)\n        return {'roles': [self.identity_api.get_role(context, x)\n                          for x in roles]}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_valid_filename(name):\n    \"\"\"\n    Return the given string converted to a string that can be used for a clean\n    filename. Remove leading and trailing spaces; convert other spaces to\n    underscores; and remove anything that is not an alphanumeric, dash,\n    underscore, or dot.\n    >>> get_valid_filename(\"john's portrait in 2004.jpg\")\n    'johns_portrait_in_2004.jpg'\n    \"\"\"\n    s = str(name).strip().replace(' ', '_')\n    s = re.sub(r'(?u)[^-\\w.]', '', s)\n    if s in {'', '.', '..'}:\n        raise SuspiciousFileOperation(\"Could not derive file name from '%s'\" % name)\n    return s\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "        def decorated(*args, **kwargs):\n            __tracebackhide__ = True  # Hide from pytest traceback.\n\n            appbuilder = current_app.appbuilder\n\n            dag_id = (\n                kwargs.get(\"dag_id\")\n                or request.args.get(\"dag_id\")\n                or request.form.get(\"dag_id\")\n                or (request.is_json and request.json.get(\"dag_id\"))\n                or None\n            )\n            if appbuilder.sm.check_authorization(permissions, dag_id):\n                return func(*args, **kwargs)\n            elif not g.user.is_anonymous and not g.user.perms:\n                return (\n                    render_template(\n                        \"airflow/no_roles_permissions.html\",\n                        hostname=get_hostname()\n                        if conf.getboolean(\"webserver\", \"EXPOSE_HOSTNAME\")\n                        else \"redact\",\n                        logout_url=appbuilder.get_url_for_logout,\n                    ),\n                    403,\n                )\n            else:\n                access_denied = \"Access is Denied\"\n                flash(access_denied, \"danger\")\n            return redirect(\n                url_for(\n                    appbuilder.sm.auth_view.__class__.__name__ + \".login\",\n                    next=request.url,\n                )\n            )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def generate_nonce(length=8):\n    \"\"\"Generate pseudorandom number.\"\"\"\n    return ''.join([str(random.SystemRandom().randint(0, 9)) for i in range(length)])\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "ImagingSgiRleDecode(Imaging im, ImagingCodecState state, UINT8 *buf, Py_ssize_t bytes) {\n    UINT8 *ptr;\n    SGISTATE *c;\n    int err = 0;\n    int status;\n\n    /* size check */\n    if (im->xsize > INT_MAX / im->bands || im->ysize > INT_MAX / im->bands) {\n        state->errcode = IMAGING_CODEC_MEMORY;\n        return -1;\n    }\n\n    /* Get all data from File descriptor */\n    c = (SGISTATE *)state->context;\n    _imaging_seek_pyFd(state->fd, 0L, SEEK_END);\n    c->bufsize = _imaging_tell_pyFd(state->fd);\n    c->bufsize -= SGI_HEADER_SIZE;\n\n    c->tablen = im->bands * im->ysize;\n    /* below, we populate the starttab and lentab into the bufsize,\n       each with 4 bytes per element of tablen\n       Check here before we allocate any memory\n    */\n    if (c->bufsize < 8 * c->tablen) {\n        state->errcode = IMAGING_CODEC_OVERRUN;\n        return -1;\n    }\n\n    ptr = malloc(sizeof(UINT8) * c->bufsize);\n    if (!ptr) {\n        state->errcode = IMAGING_CODEC_MEMORY;\n        return -1;\n    }\n    _imaging_seek_pyFd(state->fd, SGI_HEADER_SIZE, SEEK_SET);\n    if (_imaging_read_pyFd(state->fd, (char *)ptr, c->bufsize) != c->bufsize) {\n        state->errcode = IMAGING_CODEC_UNKNOWN;\n        return -1;\n    }\n\n\n    /* decoder initialization */\n    state->count = 0;\n    state->y = 0;\n    if (state->ystep < 0) {\n        state->y = im->ysize - 1;\n    } else {\n        state->ystep = 1;\n    }\n\n    /* Allocate memory for RLE tables and rows */\n    free(state->buffer);\n    state->buffer = NULL;\n    /* malloc overflow check above */\n    state->buffer = calloc(im->xsize * im->bands, sizeof(UINT8) * 2);\n    c->starttab = calloc(c->tablen, sizeof(UINT32));\n    c->lengthtab = calloc(c->tablen, sizeof(UINT32));\n    if (!state->buffer || !c->starttab || !c->lengthtab) {\n        err = IMAGING_CODEC_MEMORY;\n        goto sgi_finish_decode;\n    }\n    /* populate offsets table */\n    for (c->tabindex = 0, c->bufindex = 0; c->tabindex < c->tablen;\n         c->tabindex++, c->bufindex += 4) {\n        read4B(&c->starttab[c->tabindex], &ptr[c->bufindex]);\n    }\n    /* populate lengths table */\n    for (c->tabindex = 0, c->bufindex = c->tablen * sizeof(UINT32);\n         c->tabindex < c->tablen;\n         c->tabindex++, c->bufindex += 4) {\n        read4B(&c->lengthtab[c->tabindex], &ptr[c->bufindex]);\n    }\n\n    /* read compressed rows */\n    for (c->rowno = 0; c->rowno < im->ysize; c->rowno++, state->y += state->ystep) {\n        for (c->channo = 0; c->channo < im->bands; c->channo++) {\n            c->rleoffset = c->starttab[c->rowno + c->channo * im->ysize];\n            c->rlelength = c->lengthtab[c->rowno + c->channo * im->ysize];\n\n            // Check for underflow of rleoffset-SGI_HEADER_SIZE\n            if (c->rleoffset < SGI_HEADER_SIZE) {\n                state->errcode = IMAGING_CODEC_OVERRUN;\n                goto sgi_finish_decode;\n            }\n\n            c->rleoffset -= SGI_HEADER_SIZE;\n\n            /* row decompression */\n            if (c->bpc == 1) {\n                status = expandrow(\n                    &state->buffer[c->channo],\n                    &ptr[c->rleoffset],\n                    c->rlelength,\n                    im->bands,\n                    im->xsize,\n                    &ptr[c->bufsize-1]);\n            } else {\n                status = expandrow2(\n                    &state->buffer[c->channo * 2],\n                    &ptr[c->rleoffset],\n                    c->rlelength,\n                    im->bands,\n                    im->xsize,\n                    &ptr[c->bufsize-1]);\n            }\n            if (status == -1) {\n                state->errcode = IMAGING_CODEC_OVERRUN;\n                goto sgi_finish_decode;\n            } else if (status == 1) {\n                goto sgi_finish_decode;\n            }\n\n        }\n\n        /* store decompressed data in image */\n        state->shuffle((UINT8 *)im->image[state->y], state->buffer, im->xsize);\n    }\n\nsgi_finish_decode:;\n\n    free(c->starttab);\n    free(c->lengthtab);\n    free(ptr);\n    if (err != 0) {\n        state->errcode = err;\n        return -1;\n    }\n    return 0;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_initial_logout(self, user):\n        \"\"\"\n        Get the initial logout request.\n\n        Return None if no sessions in INIT_LOGOUT state.\n        \"\"\"\n        candidates = self._ss.get_user_sessions(user)\n\n        # FIXME: what does it mean if there are multiple in init? We\n        #        just return the first one for now. How do we know\n        #        it's the \"right\" one if multiple logouts are started\n        #        at the same time from different SPs?\n        for c in candidates:\n            key = c.keys()[0]\n            if int(c[key].get('logoutstate', 0)) == INIT_LOGOUT:\n                samlsession = self._data_to_samlsession(key, c[key])\n                return samlsession\n        return None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_image_dimensions(file_or_path, close=False):\n    \"\"\"\n    Returns the (width, height) of an image, given an open file or a path.  Set\n    'close' to True to close the file at the end if it is initially in an open\n    state.\n    \"\"\"\n    # Try to import PIL in either of the two ways it can end up installed.\n    try:\n        from PIL import ImageFile as PILImageFile\n    except ImportError:\n        import ImageFile as PILImageFile\n\n    p = PILImageFile.Parser()\n    if hasattr(file_or_path, 'read'):\n        file = file_or_path\n        file_pos = file.tell()\n        file.seek(0)\n    else:\n        file = open(file_or_path, 'rb')\n        close = True\n    try:\n        # Most of the time PIL only needs a small chunk to parse the image and\n        # get the dimensions, but with some TIFF files PIL needs to parse the\n        # whole file.\n        chunk_size = 1024\n        while 1:\n            data = file.read(chunk_size)\n            if not data:\n                break\n            p.feed(data)\n            if p.image:\n                return p.image.size\n            chunk_size = chunk_size*2\n        return None\n    finally:\n        if close:\n            file.close()\n        else:\n            file.seek(file_pos)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "                repeatedPassword: ko.observable(undefined),\n                passwordMismatch: ko.pureComputed(function () {\n                    return self.editor.password() !== self.editor.repeatedPassword();\n                }),\n                currentPasswordMismatch: ko.observable(false),\n                apikey: ko.observable(undefined),\n                active: ko.observable(undefined),\n                permissionSelectable: function (permission) {\n                    return true;\n                },\n                permissionSelected: function (permission) {\n                    var index = self.editor.permissions().indexOf(permission);\n                    return index >= 0;\n                },\n                togglePermission: function (permission) {\n                    var permissions = self.editor.permissions();\n                    var index = permissions.indexOf(permission);\n                    if (index < 0) {\n                        permissions.push(permission);\n                    } else {\n                        permissions.splice(index, 1);\n                    }\n                    self.editor.permissions(permissions);\n                },\n                groupSelected: function (group) {\n                    var index = self.editor.groups().indexOf(group);\n                    return index >= 0;\n                },\n                toggleGroup: function (group) {\n                    var groups = self.editor.groups();\n                    var index = groups.indexOf(group);\n                    if (index < 0) {\n                        groups.push(group);\n                    } else {\n                        groups.splice(index, 1);\n                    }\n                    self.editor.groups(groups);\n                },\n                joinedGroupPermissions: function (group) {\n                    return access.permissionList(group);\n                },\n                header: ko.observable(undefined),\n                new: ko.observable(true),\n                confirm: undefined,\n                valid: ko.pureComputed(function () {\n                    return (\n                        self.editor.name() &&\n                        self.editor.name().trim() &&\n                        (!self.editor.new() ||\n                            (self.editor.password() &&\n                                self.editor.password().trim() &&\n                                !self.editor.passwordMismatch()))\n                    );\n                }),\n                dangerRestricted: function () {\n                    return false;\n                },\n                dangerRestrictedText: gettext(\n                    \"This user may not have dangerous permissions.\"\n                )\n            };\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def cli(configfile, observed_since, setup_only=False, troubleshoot=False):\n    '''\n    Tenable.io -> Jira Cloud Transformer & Ingester\n    '''\n    # Load the config, but ensure that any additional fields are additive to the\n    # basic field set.\n    config_from_file = yaml.safe_load(configfile)\n    fields = config_from_file.pop('custom_fields', list())\n    config = dict_merge(base_config(), config_from_file)\n    config['fields'] = config['fields'] + fields\n\n    if config['tenable'].get('tio_transform_tags'):\n        attr_cache = config['tenable'].get('tio_asset_attr_cache', list())\n        tag_attrs = config['tenable'].get('tio_transform_tags', list())\n        config['tenable']['tio_asset_attr_cache'] = attr_cache + tag_attrs\n\n\n    # Get the logging definition and define any defaults as need be.\n    log = config.get('log', {})\n    log_lvls = {'debug': 10, 'info': 20, 'warn': 30, 'error': 40}\n    log['level'] = log_lvls[log.get('level', 'warn')]\n    log['format'] = log.get('format',\n        '%(asctime)-15s %(name)s %(levelname)s %(message)s')\n\n    # Configure the root logging facility\n    if troubleshoot:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=log['format'],\n            filename='tenable_debug.log'\n        )\n    else:\n        logging.basicConfig(**log)\n\n    # Output some basic information detailing the config file used and the\n    # python version & system arch.\n    logging.info('Tenable2JiraCloud Version {}'.format(__version__))\n    logging.info('Using configuration file {}'.format(configfile.name))\n    uname = platform.uname()\n    logging.info('Running on Python {} {}/{}'.format(\n        '.'.join([str(i) for i in sys.version_info][0:3]),\n        uname[0], uname[-2]))\n\n    # instantiate the Jira object\n    jira = Jira(\n        'https://{}/rest/api/3'.format(config['jira']['address']),\n        config['jira']['api_username'],\n        config['jira']['api_token']\n    )\n\n    # Initiate the Tenable.io API model, the Ingester model, and start the\n    # ingestion and data transformation.\n    if config['tenable'].get('platform') == 'tenable.io':\n        if not observed_since:\n            # if no since field is supplied, then look in the config file to see\n            # if an age was applied, if not, then use the default of 30 days.\n            observed_since = arrow.now()\\\n                .shift(days=-config['tenable'].get('tio_age', 30))\\\n                .floor('day').timestamp()\n\n        source = TenableIO(\n            access_key=config['tenable'].get('access_key'),\n            secret_key=config['tenable'].get('secret_key'),\n            vendor='Tenable',\n            product='JiraCloud',\n            build=__version__\n        )\n        if int(source.session.details().get('permissions')) < 64:\n            logging.error('API Keys tie to non-admin user.')\n    elif config['tenable'].get('platform') == 'tenable.sc':\n        source = TenableSC(\n            config['tenable'].get('address'),\n            port=int(config['tenable'].get('port', 443)),\n            username=config['tenable'].get('username'),\n            password=config['tenable'].get('password'),\n            access_key=config['tenable'].get('access_key'),\n            secret_key=config['tenable'].get('secret_key'),\n            vendor='Tenable',\n            product='JiraCloud',\n            build=__version__\n        )\n    else:\n        logging.error('No valid Tenable platform configuration defined.')\n        exit(1)\n    ingest = Tio2Jira(source, jira, config)\n\n    if troubleshoot:\n        # if the troubleshooting flag is set, then we will be collecting some\n        # basic information and outputting it to the screen in a format that\n        # Github issues would expect to format it all pretty.  This should help\n        # reduce the amount of time that is spent with back-and-forth debugging.\n        try:\n            ingest.ingest(int(observed_since))\n        except:\n            logging.exception('Caught the following Exception')\n\n        # Some basic redaction of sensitive data, such as API Keys, Usernames,\n        # Passwords, and hostnames.\n        addr = config_from_file['jira']['address']\n        sc_addr = 'NOTHING_TO_SEE_HERE_AT_ALL'\n        config_from_file['jira']['address'] = '<REDACTED>'\n        config_from_file['jira']['api_token'] = '<REDACTED>'\n        config_from_file['jira']['api_username'] = '<REDACTED>'\n        config_from_file['project']['leadAccountId'] = '<REDACTED>'\n        if config_from_file['tenable'].get('address'):\n            sc_addr = config_from_file['tenable']['address']\n            config_from_file['tenable']['address'] = '<REDACTED>'\n        if config_from_file['tenable'].get('access_key'):\n            config_from_file['tenable']['access_key'] = '<REDACTED>'\n        if config_from_file['tenable'].get('secret_key'):\n            config_from_file['tenable']['secret_key'] = '<REDACTED>'\n        if config_from_file['tenable'].get('username'):\n            config_from_file['tenable']['username'] = '<REDACTED>'\n        if config_from_file['tenable'].get('password'):\n            config_from_file['tenable']['password'] = '<REDACTED>'\n\n        output = troubleshooting.format(\n            configfile=yaml.dump(config_from_file, default_flow_style=False),\n            logging=open('tenable_debug.log').read() \\\n                .replace(addr, '<JIRA_CLOUD_HOST>') \\\n                .replace(sc_addr, '<TENABLE_SC_HOST>'),\n            issuetypes='\\n'.join(\n                [\n                    '{id}: {name}'.format(**a)\n                    for a in jira.issue_types.list()\n                    if a.get('name').lower() in ['task', 'subtask', 'sub-task']\n                ]\n            )\n        )\n        print(output)\n        print('\\n'.join([\n            '/-------------------------------NOTICE-----------------------------------\\\\',\n            '| The output above is helpful for us to troubleshoot exactly what is     |',\n            '| happening within the code and offer a diagnosis for how to correct.    |',\n            '| Please note that while some basic redaction has already been performed |',\n            '| that we ask you to review the information you\\'re about to send and     |',\n            '| ensure that nothing deemed sensitive is transmitted.                   |',\n            '| ---------------------------------------------------------------------- |',\n            '| -- Copy of output saved to \"issue_debug.md\"                            |',\n            '\\\\------------------------------------------------------------------------/'\n        ]))\n        with open('issue_debug.md', 'w') as reportfile:\n            print(output, file=reportfile)\n        os.remove('tenable_debug.log')\n    elif not setup_only:\n        ingest.ingest(observed_since)\n\n        # If we are expected to continually re-run the transformer, then we will\n        # need to track the passage of time and run every X hours, where X is\n        # defined by the user in the configuration.\n        if config.get('service', {}).get('interval', 0) > 0:\n            sleeper = int(config['service']['interval']) * 3600\n            while True:\n                last_run = int(time.time())\n                logging.info(\n                    'Sleeping for {}h'.format(sleeper/3600))\n                time.sleep(sleeper)\n                logging.info(\n                    'Initiating ingest with observed_since={}'.format(last_run))\n                ingest.ingest(last_run)\n    elif setup_only:\n        # In setup-only mode, the ingest will not run, and instead a config file\n        # will be generated that will have all of the JIRA identifiers baked in\n        # and will also inform the integration to ignore the screen builder.\n        # When using this config, if there are any changes to the code, then\n        # this config will need to be re-generated.\n        config['screen']['no_create'] = True\n        logging.info('Set to setup-only.  Will not run ingest.')\n        logging.info('The following is the updated config file from the setup.')\n        with open('generated_config.yaml', 'w') as outfile:\n            outfile.write(yaml.dump(config, Dumper=yaml.Dumper))\n        logging.info('Generated \"generated_config.yaml\" config file.')\n        logging.info('This config file should be updated for every new version of this integration.')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def unarchive(request, bookmark_id: int):\n    try:\n        bookmark = Bookmark.objects.get(pk=bookmark_id, owner=request.user)\n    except Bookmark.DoesNotExist:\n        raise Http404('Bookmark does not exist')\n\n    unarchive_bookmark(bookmark)\n    return_url = request.GET.get('return_url')\n    return_url = return_url if return_url else reverse('bookmarks:archived')\n    return HttpResponseRedirect(return_url)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _get_most_recent_version(self, ttl_action):\n        # type: (bool) -> CryptographicMaterialsProvider\n        \"\"\"Get the most recent version of the provider.\n\n        If allowing local and we cannot obtain the lock, just return the most recent local\n        version. Otherwise, wait for the lock and ask the provider store for the most recent\n        version of the provider.\n\n        :param TtlActions ttl_action: The ttl action to take for this version\n        :returns: version and corresponding cryptographic materials provider\n        :rtype: CryptographicMaterialsProvider\n        \"\"\"\n        blocking_wait = bool(ttl_action is TtlActions.EXPIRED)\n        acquired = self._lock.acquire(blocking_wait)\n\n        if not acquired:\n            # We failed to acquire the lock.\n            # If blocking, we will never reach this point.\n            # If not blocking, we want whatever the latest local version is.\n            _LOGGER.debug(\"Failed to acquire lock. Returning the last cached version.\")\n            version = self._version\n            _, provider = self._cache.get(version)\n            return provider\n\n        try:\n            # If the entry was expired then we blocked waiting for the lock, so it's possible some other thread already\n            # queried the provider store and re-populated the cache. If so, we don't want to re-query the provider\n            # store, so check if the entry is back in the cache first\n            if ttl_action is TtlActions.EXPIRED:\n                try:\n                    _, provider = self._cache.get(self._version)\n                    return provider\n                except KeyError:\n                    pass\n\n            max_version = self._get_max_version()\n            try:\n                _, provider = self._cache.get(max_version)\n            except KeyError:\n                provider = self._get_provider(max_version)\n            received_version = self._provider_store.version_from_material_description(\n                provider._material_description  # pylint: disable=protected-access\n            )\n\n            _LOGGER.debug(\"Caching materials provider version %d\", received_version)\n            self._version = received_version  # pylint: disable=attribute-defined-outside-init\n            self._last_updated = time.time()  # pylint: disable=attribute-defined-outside-init\n            self._cache.put(received_version, (self._last_updated, provider))\n        finally:\n            self._lock.release()\n\n        _LOGGER.debug(\"New latest version is %d\", self._version)\n\n        return provider\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static int ReadHuffmanCodeLengths(\n    VP8LDecoder* const dec, const int* const code_length_code_lengths,\n    int num_symbols, int* const code_lengths) {\n  int ok = 0;\n  VP8LBitReader* const br = &dec->br_;\n  int symbol;\n  int max_symbol;\n  int prev_code_len = DEFAULT_CODE_LENGTH;\n  HuffmanTables tables;\n\n  if (!VP8LHuffmanTablesAllocate(1 << LENGTHS_TABLE_BITS, &tables) ||\n      !VP8LBuildHuffmanTable(&tables, LENGTHS_TABLE_BITS,\n                             code_length_code_lengths, NUM_CODE_LENGTH_CODES)) {\n    goto End;\n  }\n\n  if (VP8LReadBits(br, 1)) {    // use length\n    const int length_nbits = 2 + 2 * VP8LReadBits(br, 3);\n    max_symbol = 2 + VP8LReadBits(br, length_nbits);\n    if (max_symbol > num_symbols) {\n      goto End;\n    }\n  } else {\n    max_symbol = num_symbols;\n  }\n\n  symbol = 0;\n  while (symbol < num_symbols) {\n    const HuffmanCode* p;\n    int code_len;\n    if (max_symbol-- == 0) break;\n    VP8LFillBitWindow(br);\n    p = &tables.curr_segment->start[VP8LPrefetchBits(br) & LENGTHS_TABLE_MASK];\n    VP8LSetBitPos(br, br->bit_pos_ + p->bits);\n    code_len = p->value;\n    if (code_len < kCodeLengthLiterals) {\n      code_lengths[symbol++] = code_len;\n      if (code_len != 0) prev_code_len = code_len;\n    } else {\n      const int use_prev = (code_len == kCodeLengthRepeatCode);\n      const int slot = code_len - kCodeLengthLiterals;\n      const int extra_bits = kCodeLengthExtraBits[slot];\n      const int repeat_offset = kCodeLengthRepeatOffsets[slot];\n      int repeat = VP8LReadBits(br, extra_bits) + repeat_offset;\n      if (symbol + repeat > num_symbols) {\n        goto End;\n      } else {\n        const int length = use_prev ? prev_code_len : 0;\n        while (repeat-- > 0) code_lengths[symbol++] = length;\n      }\n    }\n  }\n  ok = 1;\n\n End:\n  VP8LHuffmanTablesDeallocate(&tables);\n  if (!ok) return VP8LSetError(dec, VP8_STATUS_BITSTREAM_ERROR);\n  return ok;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def on_headers_finished(self) -> None:\n        disposition, options = parse_options_header(\n            self._current_part.content_disposition\n        )\n        try:\n            self._current_part.field_name = _user_safe_decode(\n                options[b\"name\"], self._charset\n            )\n        except KeyError:\n            raise MultiPartException(\n                'The Content-Disposition header field \"name\" must be ' \"provided.\"\n            )\n        if b\"filename\" in options:\n            self._current_files += 1\n            if self._current_files > self.max_files:\n                raise MultiPartException(\n                    f\"Too many files. Maximum number of files is {self.max_files}.\"\n                )\n            filename = _user_safe_decode(options[b\"filename\"], self._charset)\n            tempfile = SpooledTemporaryFile(max_size=self.max_file_size)\n            self._current_part.file = UploadFile(\n                file=tempfile,  # type: ignore[arg-type]\n                size=0,\n                filename=filename,\n                headers=Headers(raw=self._current_part.item_headers),\n            )\n        else:\n            self._current_fields += 1\n            if self._current_fields > self.max_fields:\n                raise MultiPartException(\n                    f\"Too many fields. Maximum number of fields is {self.max_fields}.\"\n                )\n            self._current_part.file = None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "inline void Conv3DTranspose(\n    const Conv3DTransposeParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& bias_shape,\n    const float* bias_data, const RuntimeShape& output_shape,\n    float* output_data) {\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int stride_depth = params.stride_depth;\n  const int pad_width = params.padding_values.width;\n  const int pad_height = params.padding_values.height;\n  const int pad_depth = params.padding_values.depth;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 5);\n\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int input_num_channels = MatchingDim(input_shape, 4, filter_shape, 4);\n  const int output_num_channels = output_shape.Dims(4);\n  const int input_depth = input_shape.Dims(1);\n  const int input_height = input_shape.Dims(2);\n  const int input_width = input_shape.Dims(3);\n  const int filter_depth = filter_shape.Dims(0);\n  const int filter_height = filter_shape.Dims(1);\n  const int filter_width = filter_shape.Dims(2);\n  const int output_depth = output_shape.Dims(1);\n  const int output_height = output_shape.Dims(2);\n  const int output_width = output_shape.Dims(3);\n  if (bias_data) {\n    TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_num_channels);\n  }\n\n  // Initializes the output array to zero.\n  const int num_elements = output_shape.FlatSize();\n  for (int i = 0; i < num_elements; i++) {\n    output_data[i] = 0.0f;\n  }\n\n  // Loop through input elements one at a time.\n  for (int batch = 0; batch < batches; ++batch) {\n    for (int in_d = 0; in_d < input_depth; ++in_d) {\n      for (int in_y = 0; in_y < input_height; ++in_y) {\n        for (int in_x = 0; in_x < input_width; ++in_x) {\n          for (int in_channel = 0; in_channel < input_num_channels;\n               ++in_channel) {\n            // Loop through the output elements it will influence.\n            const int out_x_origin = (in_x * stride_width) - pad_width;\n            const int out_y_origin = (in_y * stride_height) - pad_height;\n            const int out_d_origin = (in_d * stride_depth) - pad_depth;\n            for (int filter_d = 0; filter_d < filter_depth; ++filter_d) {\n              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {\n                  for (int out_channel = 0; out_channel < output_num_channels;\n                       ++out_channel) {\n                    // Compute output element location.\n                    const int out_x =\n                        out_x_origin + params.dilation_width * filter_x;\n                    const int out_y =\n                        out_y_origin + params.dilation_height * filter_y;\n                    const int out_d =\n                        out_d_origin + params.dilation_depth * filter_d;\n                    // We cannot accumulate out of bounds.\n                    if ((out_x >= 0) && (out_x < output_width) &&\n                        (out_y >= 0) && (out_y < output_height) &&\n                        (out_d >= 0) && (out_d < output_depth)) {\n                      float input_value = input_data[Offset(\n                          input_shape, batch, in_d, in_y, in_x, in_channel)];\n                      float filter_value = filter_data[Offset(\n                          filter_shape, filter_d, filter_y, filter_x,\n                          out_channel, in_channel)];\n                      output_data[Offset(output_shape, batch, out_d, out_y,\n                                         out_x, out_channel)] +=\n                          input_value * filter_value;\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n\n  const float float_activation_min = params.float_activation_min;\n  const float float_activation_max = params.float_activation_max;\n  float* data_ptr = output_data;\n  if (bias_data) {\n    const int outer_size =\n        batches * output_depth * output_height * output_width;\n    for (int n = 0; n < outer_size; ++n) {\n      for (int c = 0; c < output_num_channels; ++c) {\n        data_ptr[c] = ActivationFunctionWithMinMax(data_ptr[c] + bias_data[c],\n                                                   float_activation_min,\n                                                   float_activation_max);\n      }\n      data_ptr += output_num_channels;\n    }\n  } else {\n    const int flat_size = output_shape.FlatSize();\n    for (int i = 0; i < flat_size; ++i) {\n      data_ptr[i] = ActivationFunctionWithMinMax(\n          data_ptr[i], float_activation_min, float_activation_max);\n    }\n  }\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_password(self, password):\n        \"\"\"\n        Change the user's password. Raise a ValueError if the username or\n        the password are invalid.\n        \"\"\"\n        assert isinstance(password, str)\n        if not password:\n            raise ValueError(\"password can't be empty\")\n        cfg = cherrypy.tree.apps[''].cfg\n\n        # Cannot update admin-password if defined\n        if self.username == cfg.admin_user and cfg.admin_password:\n            raise ValueError(_(\"can't update admin-password defined in configuration file\"))\n\n        # Check password length\n        if cfg.password_min_length > len(password) or len(password) > cfg.password_max_length:\n            raise ValueError(\n                _('Password must have between %(min)d and %(max)d characters.')\n                % {'min': cfg.password_min_length, 'max': cfg.password_max_length}\n            )\n\n        # Verify password score using zxcvbn\n        stats = zxcvbn(password)\n        if stats.get('score') < cfg.password_score:\n            msg = _('Password too weak.')\n            warning = stats.get('feedback', {}).get('warning')\n            suggestions = stats.get('feedback', {}).get('suggestions')\n            if warning:\n                msg += ' ' + warning\n            if suggestions:\n                msg += ' ' + ' '.join(suggestions)\n            raise ValueError(msg)\n\n        logger.info(\"updating user password [%s]\", self.username)\n        self.hash_password = hash_password(password)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __post_init__(self) -> None:\n        self.python_name = utils.snake_case(self.name)\n        if self.default is not None:\n            self.default = self._validate_default(default=self.default)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  public Response render(ServeEvent serveEvent) {\n    ResponseDefinition responseDefinition = serveEvent.getResponseDefinition();\n    //    if (targetAddressProhibited(responseDefinition.getProxyUrl())) {\n    //      return response()\n    //          .status(500)\n    //          .headers(new HttpHeaders(new HttpHeader(\"Content-Type\", \"text/plain\")))\n    //          .body(\"The target proxy address is denied in WireMock's configuration.\")\n    //          .build();\n    //    }\n\n    HttpUriRequest httpRequest = getHttpRequestFor(responseDefinition);\n    addRequestHeaders(httpRequest, responseDefinition);\n\n    GlobalSettings settings = settingsStore.get();\n\n    Request originalRequest = responseDefinition.getOriginalRequest();\n    if ((originalRequest.getBody() != null && originalRequest.getBody().length > 0)\n        || originalRequest.containsHeader(CONTENT_LENGTH)) {\n      httpRequest.setEntity(buildEntityFrom(originalRequest));\n    }\n\n    CloseableHttpClient client = chooseClient(serveEvent.getRequest().isBrowserProxyRequest());\n\n    try {\n      return client.execute(\n          httpRequest,\n          httpResponse ->\n              response()\n                  .status(httpResponse.getCode())\n                  .headers(headersFrom(httpResponse, responseDefinition))\n                  .body(getEntityAsByteArray(httpResponse))\n                  .fromProxy(true)\n                  .configureDelay(\n                      settings.getFixedDelay(),\n                      settings.getDelayDistribution(),\n                      responseDefinition.getFixedDelayMilliseconds(),\n                      responseDefinition.getDelayDistribution())\n                  .chunkedDribbleDelay(responseDefinition.getChunkedDribbleDelay())\n                  .build());\n    } catch (ProhibitedNetworkAddressException e) {\n      return response()\n          .status(HTTP_INTERNAL_ERROR)\n          .headers(new HttpHeaders(new HttpHeader(\"Content-Type\", \"text/plain\")))\n          .body(\"The target proxy address is denied in WireMock's configuration.\")\n          .build();\n    } catch (SSLException e) {\n      return proxyResponseError(\"SSL\", httpRequest, e);\n    } catch (IOException e) {\n      return proxyResponseError(\"Network\", httpRequest, e);\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _parse_qsl(qs):\n    r = []\n    for pair in qs.split('&'):\n        if not pair: continue\n        nv = pair.split('=', 1)\n        if len(nv) != 2: nv.append('')\n        key = urlunquote(nv[0].replace('+', ' '))\n        value = urlunquote(nv[1].replace('+', ' '))\n        r.append((key, value))\n    return r\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def edit(request, bookmark_id: int):\n    try:\n        bookmark = Bookmark.objects.get(pk=bookmark_id, owner=request.user)\n    except Bookmark.DoesNotExist:\n        raise Http404('Bookmark does not exist')\n    return_url = get_safe_return_url(request.GET.get('return_url'), reverse('bookmarks:index'))\n\n    if request.method == 'POST':\n        form = BookmarkForm(request.POST, instance=bookmark)\n        if form.is_valid():\n            tag_string = convert_tag_string(form.data['tag_string'])\n            update_bookmark(form.save(commit=False), tag_string, request.user)\n            return HttpResponseRedirect(return_url)\n    else:\n        form = BookmarkForm(instance=bookmark)\n\n    form.initial['tag_string'] = build_tag_string(bookmark.tag_names, ' ')\n\n    context = {\n        'form': form,\n        'bookmark_id': bookmark_id,\n        'return_url': return_url\n    }\n\n    return render(request, 'bookmarks/edit.html', context)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _test_setup_settings(pxe_once=1):\n\n    # rewrite modules.conf so we know we can use the testing module\n    # for xmlrpc rw testing (Makefile will put the user value back)\n   \n    import yaml\n    import Cheetah.Template as Template\n\n    MODULES_TEMPLATE = \"installer_templates/settings.template\"\n    DEFAULTS = \"installer_templates/defaults\"\n    fh = open(DEFAULTS)\n    data = yaml.safe_load(fh.read())\n    fh.close()\n    data[\"pxe_once\"] = pxe_once\n\n    t = Template.Template(file=MODULES_TEMPLATE, searchList=[data])\n    open(\"/etc/cobbler/settings\",\"w+\").write(t.respond())\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def check_table_permission(cls, user, table, permission):\n        \"\"\"Check if the provided user has the specified permission against the table.\"\"\"\n\n        # Superuser knows no bounds\n        if user.is_superuser:\n            return True\n\n        # If the table does *not* require permissions\n        if table in cls.RULESET_IGNORE:\n            return True\n\n        # Work out which roles touch the given table\n        for role in cls.RULESET_NAMES:\n            if table in cls.RULESET_MODELS[role]:\n\n                if check_user_role(user, role, permission):\n                    return True\n\n        # Check for children models which inherits from parent role\n        for (parent, child) in cls.RULESET_CHANGE_INHERIT:\n            # Get child model name\n            parent_child_string = f'{parent}_{child}'\n\n            if parent_child_string == table:\n                # Check if parent role has change permission\n                if check_user_role(user, parent, 'change'):\n                    return True\n\n        # Print message instead of throwing an error\n        name = getattr(user, 'name', user.pk)\n\n        logger.info(f\"User '{name}' failed permission check for {table}.{permission}\")\n\n        return False\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _cli_deploy_image(args):\n\n    docker_repository = _resolve_property(args, 'docker_repository')\n    vault_secret = _resolve_property(args, 'vault_secret', ignore_value_error=True)\n    vault_endpoint = _resolve_vault_endpoint(args)\n    vault_endpoint_verify = _resolve_property(args, 'vault_endpoint_verify', ignore_value_error=True)\n    image_tar_path = args.image_tar_path if args.image_tar_path else find_image_file()\n\n    bigflow.deploy.deploy_docker_image(\n        image_tar_path=image_tar_path,\n        auth_method=args.auth_method,\n        docker_repository=docker_repository,\n        vault_endpoint=vault_endpoint,\n        vault_endpoint_verify=vault_endpoint_verify,\n        vault_secret=vault_secret,\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _configure_session(cls, app):\n        tempdir = app.config[\"PYLOAD_API\"].get_cachedir()\n        cache_path = os.path.join(tempdir, \"flask\")\n        os.makedirs(cache_path, exist_ok=True)\n\n        app.config[\"SESSION_FILE_DIR\"] = cache_path\n        app.config[\"SESSION_TYPE\"] = \"filesystem\"\n        app.config[\"SESSION_COOKIE_NAME\"] = \"pyload_session\"\n        app.config[\"SESSION_COOKIE_SAMESITE\"] = \"Strict\"\n        app.config[\"SESSION_COOKIE_SECURE\"] = app.config[\"PYLOAD_API\"].get_config_value(\"webui\", \"use_ssl\")\n        app.config[\"SESSION_PERMANENT\"] = False\n\n        session_lifetime = max(app.config[\"PYLOAD_API\"].get_config_value(\"webui\", \"session_lifetime\"), 1) * 60\n        app.config[\"PERMANENT_SESSION_LIFETIME\"] = session_lifetime\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "                        def get_content_length() -> Optional[int]:\n                            # payload length\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\n                            if length_hdr is None:\n                                return None\n\n                            # Shouldn't allow +/- or other number formats.\n                            # https://www.rfc-editor.org/rfc/rfc9110#section-8.6-2\n                            if not length_hdr.strip(\" \\t\").isdigit():\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            return int(length_hdr)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _ensureSnapshotsFolder(self, subdir=None):\n        \"\"\" Ensure that the appropriate snapshot folder exists.\n        \"\"\"\n        path = ['snapshots', self._snapshot_id]\n\n        if subdir is not None:\n            path.extend(subdir.split('/'))\n\n        current = self._tool\n\n        for element in path:\n\n            if element not in current.objectIds():\n                # No Unicode IDs!\n                current._setObject(str(element), Folder(element))\n                current = current._getOb(element)\n                current.manage_permission(view, ('Manager', 'Owner'), 0)\n            else:\n                current = current._getOb(element)\n\n        return current\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def load(self):\n        start = time.time()\n        end = start + 15\n        current = start\n        while current <= end:\n            try:\n                if self.saving_method == \"mongo\":\n                    data = self._load_mongo()\n                elif self.saving_method == \"local\":\n                    data = self._load_local()\n\n                data = data[\"members\"][self.sid]\n                self.member.data = GroupMemberData(**data)\n\n                return\n\n            except KeyError:\n                time.sleep(1)\n                current = time.time()\n                msg = \"Error while loading.\"\n                if current <= end:\n                    msg += \" Trying again.\"\n                else:\n                    msg += \" Timeout reached. Aborting.\"\n\n                self.exp.log.exception(msg)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def default(self, path=b\"\", action=None, **kwargs):\n        repo_obj = self.app.store.get_repo(path)\n        if cherrypy.request.method == 'POST':\n            if kwargs.get('keepdays'):\n                return self._remove_older(repo_obj, **kwargs)\n            elif kwargs.get('new_encoding'):\n                return self._set_encoding(repo_obj, **kwargs)\n            elif kwargs.get('maxage'):\n                return self._set_maxage(repo_obj, **kwargs)\n        # Get page data.\n        params = {\n            'repo': repo_obj,\n            'keepdays': repo_obj.keepdays,\n        }\n        # Generate page.\n        return self._compile_template(\"settings.html\", **params)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_object_element(dataset, seq_no, rel_path, download_link):\n    \"\"\"If rel_path and download_link are not None, we are called from scope.\n    Otherwise we are called from ID and need to run SQL query to fetch these attrs.\"\"\"\n\n    if rel_path is None:\n        query = \"SELECT rel_path, download_link FROM \" + \\\n        dataset + \\\n        \" WHERE sequence_no = %s\"\n\n        cnx = mysql.connector.connect(user=DB_USER,\n                                    password=DB_PASSWORD,\n                                    host=DB_HOST,\n                                    database=DB_DBNAME,\n                                    port=DB_PORT)\n        cursor = cnx.cursor()\n        cursor.execute(query, (seq_no,))\n\n        row = cursor.fetchone()\n\n        if not row:\n            return None\n\n        rel_path, download_link = row[0], row[1]\n\n    if LOCAL_OBJ_URI:\n        src_uri = 'file://' + _get_obj_absolute_path(dataset, rel_path)\n    else:\n        src_uri = url_for('.get_object_src_http', dataset=dataset, rel_path=rel_path)\n\n    return '<object id={} src={} hyperfind.external-link={} />' \\\n        .format(\n        quoteattr(url_for('.get_object_id', dataset=dataset, seq_no=seq_no)),\n        quoteattr(src_uri),\n        quoteattr(download_link))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const Tensor& input_min = ctx->input(1);\n    const Tensor& input_max = ctx->input(2);\n\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsScalar(input_min.shape()),\n        errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",\n                                input_min.dims()));\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsScalar(input_max.shape()),\n        errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",\n                                input_max.dims()));\n\n    const float input_min_float = input_min.scalar<float>()();\n    const float input_max_float = input_max.scalar<float>()();\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(2, TensorShape({}), &output_max));\n\n    // See QuantizationRangeOp as well, which has a copy of this logic.\n    auto input_array = input.flat<T1>();\n    const int32_t input_lowest_quantized =\n        static_cast<int32>(Eigen::NumTraits<T1>::lowest());\n    const int32_t input_highest_quantized =\n        static_cast<int32>(Eigen::NumTraits<T1>::highest());\n    T1 actual_min_quantized = input_highest_quantized;\n    T1 actual_max_quantized = input_lowest_quantized;\n    for (int i = 0; i < input_array.size(); ++i) {\n      const T1 value = input_array(i);\n      actual_min_quantized = std::min(actual_min_quantized, value);\n      actual_max_quantized = std::max(actual_max_quantized, value);\n    }\n    // We want to make sure that the minimum is no larger than zero, so that the\n    // convolution operation can run efficiently.\n    const float actual_min_float =\n        std::min(0.0f, QuantizedToFloat(actual_min_quantized, input_min_float,\n                                        input_max_float));\n    const float actual_max_float = QuantizedToFloat(\n        actual_max_quantized, input_min_float, input_max_float);\n\n#if 0\n    // This is the reference, non-eigen implementation:\n    auto output_array = output->flat<T2>();\n    RequantizeManyInNewRange<T1, T2>(input_array.data(), input_array.size(),\n                                     input_min_float, input_max_float,\n                                     actual_min_float, actual_max_float,\n                                     output_array.data());\n#endif\n\n    if (input_array.size() > 0) {\n      if (meta::IsSupportedAndEnabled() && std::is_same<T1, qint32>() &&\n          std::is_same<T2, quint8>()) {\n        auto input_i32_array = input.flat<qint32>();\n        meta::Requantize(ctx, input_i32_array.data(), input_i32_array.size(),\n                         input_min_float, input_max_float, actual_min_float,\n                         actual_max_float, output->flat<quint8>().data());\n      } else {\n        RequantizeManyInNewRangeUsingEigen<T1, T2>(\n            ctx->eigen_device<CPUDevice>(), input, input_min_float,\n            input_max_float, actual_min_float, actual_max_float, output);\n      }\n    }\n\n    output_min->flat<float>().setConstant(actual_min_float);\n    output_max->flat<float>().setConstant(actual_max_float);\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _ttl_action(self, version, action):\n        # type: (str) -> TtlActions\n        \"\"\"Determine the correct action to take based on the local resources and TTL.\n\n        :param action: The action being taken (encrypt or decrypt)\n\n        :returns: decision\n        :rtype: TtlActions\n        \"\"\"\n        try:\n            if action == _ENCRYPT_ACTION:\n                # On encrypt, always check the class-level variable indicating when we last checked for updates.\n                # The cache timestamps will be updated by calls to decrypt, so we don't want frequent decrypts to\n                # prevent us from re-checking for a newer encryption version\n                if self._last_updated is None:\n                    return TtlActions.EXPIRED\n                timestamp = self._last_updated\n            else:\n                timestamp, _ = self._cache.get(version)\n            time_since_updated = time.time() - timestamp\n\n            if time_since_updated < self._version_ttl:\n                return TtlActions.LIVE\n\n            if time_since_updated < self._version_ttl + self._grace_period:\n                return TtlActions.GRACE_PERIOD\n\n            _LOGGER.debug(\"TTL Expired because known version has expired\")\n            return TtlActions.EXPIRED\n        except KeyError:\n            _LOGGER.debug(\"TTL Expired because the requested version doesn't exist in the cache\")\n            return TtlActions.EXPIRED\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def config():\n    config_disabled = (\n            app.config['CONFIG_DISABLE'] or\n            not valid_user_session(session))\n\n    name = ''\n    if 'name' in request.args:\n        name = os.path.normpath(request.args.get('name'))\n        if not re.match(r'^[A-Za-z0-9_.+-]+$', name):\n            return make_response('Invalid config name', 400)\n\n    if request.method == 'GET':\n        return json.dumps(g.user_config.__dict__)\n    elif request.method == 'PUT' and not config_disabled:\n        if name:\n            config_pkl = os.path.join(app.config['CONFIG_PATH'], name)\n            session['config'] = (pickle.load(open(config_pkl, 'rb'))\n                                 if os.path.exists(config_pkl)\n                                 else session['config'])\n            return json.dumps(session['config'])\n        else:\n            return json.dumps({})\n    elif not config_disabled:\n        config_data = request.form.to_dict()\n        if 'url' not in config_data or not config_data['url']:\n            config_data['url'] = g.user_config.url\n\n        # Save config by name to allow a user to easily load later\n        if 'name' in request.args:\n            pickle.dump(\n                config_data,\n                open(os.path.join(\n                    app.config['CONFIG_PATH'],\n                    name), 'wb'))\n\n        session['config'] = config_data\n        return redirect(config_data['url'])\n    else:\n        return redirect(url_for('.index'), code=403)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def explain_query_prefix(self, format=None, **options):\n        extra = {}\n        # Normalize options.\n        if options:\n            options = {\n                name.upper(): \"true\" if value else \"false\"\n                for name, value in options.items()\n            }\n            for valid_option in self.explain_options:\n                value = options.pop(valid_option, None)\n                if value is not None:\n                    extra[valid_option.upper()] = value\n        prefix = super().explain_query_prefix(format, **options)\n        if format:\n            extra[\"FORMAT\"] = format\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def put(self, name: str):\n        \"\"\"add new file\n        params in FormData:\n            - file\n            - original_file_name [optional]\n        \"\"\"\n\n        data = {}\n        mindsdb_file_name = name\n\n        existing_file_names = ca.file_controller.get_files_names()\n\n        def on_field(field):\n            name = field.field_name.decode()\n            value = field.value.decode()\n            data[name] = value\n\n        file_object = None\n\n        def on_file(file):\n            nonlocal file_object\n            data[\"file\"] = clear_filename(file.file_name.decode())\n            file_object = file.file_object\n\n        temp_dir_path = tempfile.mkdtemp(prefix=\"mindsdb_file_\")\n\n        if request.headers[\"Content-Type\"].startswith(\"multipart/form-data\"):\n            parser = multipart.create_form_parser(\n                headers=request.headers,\n                on_field=on_field,\n                on_file=on_file,\n                config={\n                    \"UPLOAD_DIR\": temp_dir_path.encode(),  # bytes required\n                    \"UPLOAD_KEEP_FILENAME\": True,\n                    \"UPLOAD_KEEP_EXTENSIONS\": True,\n                    \"MAX_MEMORY_FILE_SIZE\": 0,\n                },\n            )\n\n            while True:\n                chunk = request.stream.read(8192)\n                if not chunk:\n                    break\n                parser.write(chunk)\n            parser.finalize()\n            parser.close()\n\n            if file_object is not None and not file_object.closed:\n                file_object.close()\n        else:\n            data = request.json\n\n        if mindsdb_file_name in existing_file_names:\n            return http_error(\n                400,\n                \"File already exists\",\n                f\"File with name '{data['file']}' already exists\",\n            )\n\n        if data.get(\"source_type\") == \"url\":\n            url = data[\"source\"]\n            data[\"file\"] = clear_filename(data[\"name\"])\n\n            config = Config()\n            is_cloud = config.get(\"cloud\", False)\n            if is_cloud and is_private_url(url):\n                return http_error(\n                    400, f'URL is private: {url}'\n                )\n\n            if is_cloud is True and ctx.user_class != 1:\n                info = requests.head(url)\n                file_size = info.headers.get(\"Content-Length\")\n                try:\n                    file_size = int(file_size)\n                except Exception:\n                    pass\n\n                if file_size is None:\n                    return http_error(\n                        400,\n                        \"Error getting file info\",\n                        \"\u0421an't determine remote file size\",\n                    )\n                if file_size > 1024 * 1024 * 100:\n                    return http_error(\n                        400, \"File is too big\", \"Upload limit for file is 100Mb\"\n                    )\n            with requests.get(url, stream=True) as r:\n                if r.status_code != 200:\n                    return http_error(\n                        400, \"Error getting file\", f\"Got status code: {r.status_code}\"\n                    )\n                file_path = os.path.join(temp_dir_path, data[\"file\"])\n                with open(file_path, \"wb\") as f:\n                    for chunk in r.iter_content(chunk_size=8192):\n                        f.write(chunk)\n\n        original_file_name = clear_filename(data.get(\"original_file_name\"))\n\n        file_path = os.path.join(temp_dir_path, data[\"file\"])\n        lp = file_path.lower()\n        if lp.endswith((\".zip\", \".tar.gz\")):\n            if lp.endswith(\".zip\"):\n                with zipfile.ZipFile(file_path) as f:\n                    f.extractall(temp_dir_path)\n            elif lp.endswith(\".tar.gz\"):\n                with tarfile.open(file_path) as f:\n                    safe_extract(f, temp_dir_path)\n            os.remove(file_path)\n            files = os.listdir(temp_dir_path)\n            if len(files) != 1:\n                os.rmdir(temp_dir_path)\n                return http_error(\n                    400, \"Wrong content.\", \"Archive must contain only one data file.\"\n                )\n            file_path = os.path.join(temp_dir_path, files[0])\n            mindsdb_file_name = files[0]\n            if not os.path.isfile(file_path):\n                os.rmdir(temp_dir_path)\n                return http_error(\n                    400, \"Wrong content.\", \"Archive must contain data file in root.\"\n                )\n\n        ca.file_controller.save_file(\n            mindsdb_file_name, file_path, file_name=original_file_name\n        )\n\n        os.rmdir(temp_dir_path)\n\n        return \"\", 200\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def handle_html_static(self, request):\n        \"\"\"Handle static html.\"\"\"\n        if not (page := request.path[1:]):\n            page = \"index.html\"\n        file = os.path.normpath(os.path.join(self.web_path, page))\n        if not file.startswith(self.web_path):\n            raise ValueError(f\"File access outside {self.web_path} not permitted.\")\n        try:\n            with open(file, encoding=\"utf-8\"):\n                return web.FileResponse(file)\n        except (FileNotFoundError, IsADirectoryError) as exc:\n            raise web.HTTPNotFound(reason=\"File not found\") from exc\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_services(self, context):\n        self.assert_admin(context)\n        service_list = self.catalog_api.list_services(context)\n        service_refs = [self.catalog_api.get_service(context, x)\n                        for x in service_list]\n        return {'OS-KSADM:services': service_refs}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _handle_carbon_received(self, msg):\n        if msg['from'].bare == self.xmpp.boundjid.bare:\n            self.xmpp.event('carbon_received', msg)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def render_markdown(value):\n    \"\"\"\n    Render text as Markdown\n\n    Example:\n        {{ text | render_markdown }}\n    \"\"\"\n    # Render Markdown\n    html = markdown(value, extensions=[\"fenced_code\", \"tables\"])\n\n    # Sanitize rendered HTML\n    html = clean_html(html)\n\n    return mark_safe(html)  # noqa: S308  # suspicious-mark-safe-usage, OK here since we sanitized the string earlier\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static cache_accel_t *read_cache_accel(RBuffer *cache_buf, cache_hdr_t *hdr, cache_map_t *maps, int n_maps) {\n\tif (!cache_buf || !hdr || !hdr->accelerateInfoSize || !hdr->accelerateInfoAddr) {\n\t\treturn NULL;\n\t}\n\tsize_t mc = R_MIN (hdr->mappingCount, n_maps);\n\tut64 offset = va2pa (hdr->accelerateInfoAddr, mc, maps, cache_buf, 0, NULL, NULL);\n\tif (!offset) {\n\t\treturn NULL;\n\t}\n\n\tut64 size = sizeof (cache_accel_t);\n\tcache_accel_t *accel = R_NEW0 (cache_accel_t);\n\tif (!accel) {\n\t\treturn NULL;\n\t}\n\n\tif (r_buf_fread_at (cache_buf, offset, (ut8*) accel, \"16il\", 1) != size) {\n\t\tR_FREE (accel);\n\t\treturn NULL;\n\t}\n\n\taccel->imagesExtrasOffset += offset;\n\taccel->bottomUpListOffset += offset;\n\taccel->dylibTrieOffset += offset;\n\taccel->initializersOffset += offset;\n\taccel->dofSectionsOffset += offset;\n\taccel->reExportListOffset += offset;\n\taccel->depListOffset += offset;\n\taccel->rangeTableOffset += offset;\n\n\treturn accel;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        name = str(name).replace('\\\\', '/')\n        dir_name, file_name = os.path.split(name)\n        if '..' in pathlib.PurePath(dir_name).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n        validate_file_name(file_name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, add an underscore and a random 7\n        # character alphanumeric string (before the file extension, if one\n        # exists) to the filename until the generated filename doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n        return name\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def settings(request):\n   \"\"\"\n   This page presents a list of all the settings to the user.  They are not editable.\n   \"\"\"\n   if not test_user_authenticated(request): return login(request, next=\"/cobbler_web/settings\")\n   settings = remote.get_settings()\n   skeys = settings.keys()\n   skeys.sort()\n\n   results = []\n   for k in skeys:\n      results.append([k,settings[k]])\n\n   t = get_template('settings.tmpl')\n   html = t.render(RequestContext(request,{\n        'settings' : results,\n        'version'  : remote.version(request.session['token']),\n        'username' : username,\n   }))\n   return HttpResponse(html)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _login(server, creds_file=None, api_key=None, silent=False):\n    \"\"\"Login main function.\"\"\"\n    # fetch_api_key and check_version expect server to end in /\n    if server[-1] != \"/\":\n        server = server + \"/\"\n\n    # creds file path setup\n    if creds_file is None:\n        creds_file = os.path.expanduser(\"~/.onecodex\")\n\n    # check if the creds file exists and is readable\n    if not os.path.exists(creds_file):\n        if silent:\n            return None\n\n        creds = {}\n    elif not os.access(creds_file, os.R_OK):\n        click.echo(\"Please check the permissions on {}\".format(collapse_user(creds_file)), err=True)\n        sys.exit(1)\n    else:\n        with filelock.FileLock(\"{}.lock\".format(creds_file)):\n            # it is, so let's read it!\n            with open(creds_file, \"r\") as fp:\n                try:\n                    creds = json.load(fp)\n                except ValueError:\n                    click.echo(\n                        \"Your ~/.onecodex credentials file appears to be corrupted. \"  # noqa\n                        \"Please delete it and re-authorize.\",\n                        err=True,\n                    )\n                    sys.exit(1)\n\n            # check for updates if logged in more than one day ago\n            last_update = creds.get(\"updated_at\") or creds.get(\"saved_at\")\n            last_update = (\n                last_update if last_update else datetime.datetime.now().strftime(DATE_FORMAT)\n            )\n            diff = datetime.datetime.now() - datetime.datetime.strptime(last_update, DATE_FORMAT)\n\n            if diff.days >= 1:\n                # if creds_file is old, check for updates\n                upgrade_required, msg = check_version(__version__, server)\n                creds[\"updated_at\"] = datetime.datetime.now().strftime(DATE_FORMAT)\n\n                try:\n                    json.dump(creds, open(creds_file, \"w\"))\n                except Exception as e:\n                    if e.errno == errno.EACCES:\n                        click.echo(\n                            \"Please check the permissions on {}\".format(collapse_user(creds_file)),\n                            err=True,\n                        )\n                        sys.exit(1)\n                    else:\n                        raise\n\n                if upgrade_required:\n                    click.echo(\"\\nWARNING: {}\\n\".format(msg), err=True)\n\n            # finally, give the user back what they want (whether silent or not)\n            if silent:\n                return creds.get(\"api_key\", None)\n\n            click.echo(\n                \"Credentials file already exists ({}). Logout first.\".format(\n                    collapse_user(creds_file)\n                ),\n                err=True,\n            )\n            return creds.get(\"email\", None)\n\n    # creds_file was not found and we're not silent, so prompt user to login\n    email, api_key = login_uname_pwd(server, api_key=api_key)\n\n    if api_key is None:\n        click.echo(\n            \"We could not verify your credentials. Please check your email and password again. \"\n            \"If you have single sign-on (SSO) enabled, you must login with `onecodex --api-key $API_KEY login`. \"\n            \"Please contact help@onecodex.com if you continue to experience problems.\"\n        )\n        sys.exit(1)\n\n    creds.update(\n        {\n            \"api_key\": api_key,\n            \"saved_at\": datetime.datetime.now().strftime(DATE_FORMAT),\n            \"updated_at\": None,\n            \"email\": email,\n        }\n    )\n\n    with filelock.FileLock(\"{}.lock\".format(creds_file)):\n        try:\n            json.dump(creds, open(creds_file, \"w\"))\n        except Exception as e:\n            if e.errno == errno.EACCES:\n                click.echo(\"Please check the permissions on {}\".format(creds_file), err=True)\n                sys.exit(1)\n            else:\n                raise\n\n    click.echo(\"Your ~/.onecodex credentials file was successfully created.\", err=True)\n\n    return email\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, verbose_name=None, name=None, verify_exists=False, **kwargs):\n        kwargs['max_length'] = kwargs.get('max_length', 200)\n        CharField.__init__(self, verbose_name, name, **kwargs)\n        self.validators.append(validators.URLValidator(verify_exists=verify_exists))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def from_unix_seconds(seconds: Union[int, float]) -> datetime:\n    if seconds > MAX_NUMBER:\n        return datetime.max\n    elif seconds < -MAX_NUMBER:\n        return datetime.min\n\n    while abs(seconds) > MS_WATERSHED:\n        seconds /= 1000\n    dt = EPOCH + timedelta(seconds=seconds)\n    return dt.replace(tzinfo=timezone.utc)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def do_template(self, data, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, disable_lookups=False):\n        if USE_JINJA2_NATIVE and not isinstance(data, string_types):\n            return data\n\n        # For preserving the number of input newlines in the output (used\n        # later in this method)\n        data_newlines = _count_newlines_from_end(data)\n\n        if fail_on_undefined is None:\n            fail_on_undefined = self._fail_on_undefined_errors\n\n        try:\n            # allows template header overrides to change jinja2 options.\n            if overrides is None:\n                myenv = self.environment.overlay()\n            else:\n                myenv = self.environment.overlay(overrides)\n\n            # Get jinja env overrides from template\n            if hasattr(data, 'startswith') and data.startswith(JINJA2_OVERRIDE):\n                eol = data.find('\\n')\n                line = data[len(JINJA2_OVERRIDE):eol]\n                data = data[eol + 1:]\n                for pair in line.split(','):\n                    (key, val) = pair.split(':')\n                    key = key.strip()\n                    setattr(myenv, key, ast.literal_eval(val.strip()))\n\n            # Adds Ansible custom filters and tests\n            myenv.filters.update(self._get_filters())\n            for k in myenv.filters:\n                if not getattr(myenv.filters[k], '__UNROLLED__', False):\n                    myenv.filters[k] = _unroll_iterator(myenv.filters[k])\n            myenv.tests.update(self._get_tests())\n\n            if escape_backslashes:\n                # Allow users to specify backslashes in playbooks as \"\\\\\" instead of as \"\\\\\\\\\".\n                data = _escape_backslashes(data, myenv)\n\n            try:\n                t = myenv.from_string(data)\n            except TemplateSyntaxError as e:\n                raise AnsibleError(\"template error while templating string: %s. String: %s\" % (to_native(e), to_native(data)))\n            except Exception as e:\n                if 'recursion' in to_native(e):\n                    raise AnsibleError(\"recursive loop detected in template string: %s\" % to_native(data))\n                else:\n                    return data\n\n            if disable_lookups:\n                t.globals['query'] = t.globals['q'] = t.globals['lookup'] = self._fail_lookup\n\n            jvars = AnsibleJ2Vars(self, t.globals)\n\n            self.cur_context = new_context = t.new_context(jvars, shared=True)\n            rf = t.root_render_func(new_context)\n\n            try:\n                res = j2_concat(rf)\n                unsafe = getattr(new_context, 'unsafe', False)\n                if unsafe:\n                    res = wrap_var(res)\n            except TypeError as te:\n                if 'AnsibleUndefined' in to_native(te):\n                    errmsg = \"Unable to look up a name or access an attribute in template string (%s).\\n\" % to_native(data)\n                    errmsg += \"Make sure your variable name does not contain invalid characters like '-': %s\" % to_native(te)\n                    raise AnsibleUndefinedVariable(errmsg)\n                else:\n                    display.debug(\"failing because of a type error, template data is: %s\" % to_text(data))\n                    raise AnsibleError(\"Unexpected templating type error occurred on (%s): %s\" % (to_native(data), to_native(te)))\n\n            if USE_JINJA2_NATIVE and not isinstance(res, string_types):\n                return res\n\n            if preserve_trailing_newlines:\n                # The low level calls above do not preserve the newline\n                # characters at the end of the input data, so we use the\n                # calculate the difference in newlines and append them\n                # to the resulting output for parity\n                #\n                # jinja2 added a keep_trailing_newline option in 2.7 when\n                # creating an Environment.  That would let us make this code\n                # better (remove a single newline if\n                # preserve_trailing_newlines is False).  Once we can depend on\n                # that version being present, modify our code to set that when\n                # initializing self.environment and remove a single trailing\n                # newline here if preserve_newlines is False.\n                res_newlines = _count_newlines_from_end(res)\n                if data_newlines > res_newlines:\n                    res += self.environment.newline_sequence * (data_newlines - res_newlines)\n                    if unsafe:\n                        res = wrap_var(res)\n            return res\n        except (UndefinedError, AnsibleUndefinedVariable) as e:\n            if fail_on_undefined:\n                raise AnsibleUndefinedVariable(e)\n            else:\n                display.debug(\"Ignoring undefined failure: %s\" % to_text(e))\n                return data\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def set_python_instance_state(self, instance, state, unsafe=False):\n        if hasattr(instance, '__setstate__'):\n            instance.__setstate__(state)\n        else:\n            slotstate = {}\n            if isinstance(state, tuple) and len(state) == 2:\n                state, slotstate = state\n            if hasattr(instance, '__dict__'):\n                if not unsafe and state:\n                    for key in state.keys():\n                        self.check_state_key(key)\n                instance.__dict__.update(state)\n            elif state:\n                slotstate.update(state)\n            for key, value in slotstate.items():\n                if not unsafe:\n                    self.check_state_key(key)\n                setattr(instance, key, value)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ModelWeights::Initialize(OpKernelContext* const context) {\n  OpInputList sparse_indices_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"sparse_indices\", &sparse_indices_inputs));\n  OpInputList sparse_weights_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"sparse_weights\", &sparse_weights_inputs));\n  if (sparse_indices_inputs.size() != sparse_weights_inputs.size())\n    return errors::InvalidArgument(\n        \"sparse_indices and sparse_weights must have the same length, got \",\n        sparse_indices_inputs.size(), \" and \", sparse_weights_inputs.size());\n  OpInputList dense_weights_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"dense_weights\", &dense_weights_inputs));\n\n  OpOutputList sparse_weights_outputs;\n  TF_RETURN_IF_ERROR(context->output_list(\"out_delta_sparse_weights\",\n                                          &sparse_weights_outputs));\n  if (sparse_weights_outputs.size() != sparse_weights_inputs.size())\n    return errors::InvalidArgument(\n        \"out_delta_sparse_weights and sparse_weights must have the same \"\n        \"length, got \",\n        sparse_weights_outputs.size(), \" and \", sparse_weights_inputs.size());\n\n  OpOutputList dense_weights_outputs;\n  TF_RETURN_IF_ERROR(\n      context->output_list(\"out_delta_dense_weights\", &dense_weights_outputs));\n  if (dense_weights_outputs.size() != dense_weights_inputs.size())\n    return errors::InvalidArgument(\n        \"out_delta_dense_weights and dense_weights must have the same length, \"\n        \"got \",\n        dense_weights_outputs.size(), \" and \", dense_weights_inputs.size());\n\n  for (int i = 0; i < sparse_weights_inputs.size(); ++i) {\n    Tensor* delta_t;\n    TF_RETURN_IF_ERROR(sparse_weights_outputs.allocate(\n        i, sparse_weights_inputs[i].shape(), &delta_t));\n    // Convert the input vector to a row matrix in internal representation.\n    auto deltas = delta_t->shaped<float, 2>({1, delta_t->NumElements()});\n    deltas.setZero();\n    sparse_weights_.emplace_back(FeatureWeightsSparseStorage{\n        sparse_indices_inputs[i].flat<int64>(),\n        sparse_weights_inputs[i].shaped<float, 2>(\n            {1, sparse_weights_inputs[i].NumElements()}),\n        deltas});\n  }\n\n  // Reads in the weights, and allocates and initializes the delta weights.\n  const auto initialize_weights =\n      [&](const OpInputList& weight_inputs, OpOutputList* const weight_outputs,\n          std::vector<FeatureWeightsDenseStorage>* const feature_weights) {\n        for (int i = 0; i < weight_inputs.size(); ++i) {\n          Tensor* delta_t;\n          TF_RETURN_IF_ERROR(\n              weight_outputs->allocate(i, weight_inputs[i].shape(), &delta_t));\n          // Convert the input vector to a row matrix in internal\n          // representation.\n          auto deltas = delta_t->shaped<float, 2>({1, delta_t->NumElements()});\n          deltas.setZero();\n          feature_weights->emplace_back(FeatureWeightsDenseStorage{\n              weight_inputs[i].shaped<float, 2>(\n                  {1, weight_inputs[i].NumElements()}),\n              deltas});\n        }\n        return Status::OK();\n      };\n\n  return initialize_weights(dense_weights_inputs, &dense_weights_outputs,\n                            &dense_weights_);\n}\n"}, {"role": "assistant", "content": "False"}]}
