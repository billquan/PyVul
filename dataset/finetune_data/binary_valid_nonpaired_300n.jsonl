{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def read_config(self, config, **kwargs):\n        # FIXME: federation_domain_whitelist needs sytests\n        self.federation_domain_whitelist = None  # type: Optional[dict]\n        federation_domain_whitelist = config.get(\"federation_domain_whitelist\", None)\n\n        if federation_domain_whitelist is not None:\n            # turn the whitelist into a hash for speed of lookup\n            self.federation_domain_whitelist = {}\n\n            for domain in federation_domain_whitelist:\n                self.federation_domain_whitelist[domain] = True\n\n        self.federation_ip_range_blacklist = config.get(\n            \"federation_ip_range_blacklist\", []\n        )\n\n        # Attempt to create an IPSet from the given ranges\n        try:\n            self.federation_ip_range_blacklist = IPSet(\n                self.federation_ip_range_blacklist\n            )\n\n            # Always blacklist 0.0.0.0, ::\n            self.federation_ip_range_blacklist.update([\"0.0.0.0\", \"::\"])\n        except Exception as e:\n            raise ConfigError(\n                \"Invalid range(s) provided in federation_ip_range_blacklist: %s\" % e\n            )\n\n        federation_metrics_domains = config.get(\"federation_metrics_domains\") or []\n        validate_config(\n            _METRICS_FOR_DOMAINS_SCHEMA,\n            federation_metrics_domains,\n            (\"federation_metrics_domains\",),\n        )\n        self.federation_metrics_domains = set(federation_metrics_domains)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status SparseTensorToCSRSparseMatrixCPUFunctor::operator()(\n    const int64 batch_size, const int num_rows,\n    TTypes<int64>::ConstMatrix indices, TTypes<int32>::Vec batch_ptr,\n    TTypes<int32>::Vec csr_row_ptr, TTypes<int32>::Vec csr_col_ind) {\n  // Validate inputs.\n  if (batch_ptr.size() != batch_size + 1) {\n    return errors::InvalidArgument(\n        \"Expected batch_ptr.size() == batch_size + 1. Got: \", batch_ptr.size(),\n        \" vs. \", batch_size + 1);\n  }\n  if (csr_row_ptr.size() != batch_size * (num_rows + 1)) {\n    return errors::InvalidArgument(\n        \"Expected csr_row_ptr.size() == batch_size * (num_rows + 1). Got: \",\n        csr_row_ptr.size(), \" vs. \", batch_size * (num_rows + 1));\n  }\n\n  const int64 total_nnz = indices.dimension(0);\n  const int rank = indices.dimension(1);\n  if (rank == 2 && batch_size != 1) {\n    return errors::InvalidArgument(\n        \"Expected batch_size == 1 when rank is 2. Got batch_size: \",\n        batch_size);\n  }\n  if (csr_col_ind.size() != total_nnz) {\n    return errors::InvalidArgument(\n        \"Expected csr_col_ind.size() == total_nnz. Got: \", csr_col_ind.size(),\n        \" vs. \", total_nnz);\n  }\n\n  int prev_batch = -1;\n  if (rank == 2) {\n    // For a single batch, the batch_ptrs are {0, total_nnz}.\n    batch_ptr(0) = 0;\n    ++prev_batch;\n\n    for (int64 i = 0; i < total_nnz; ++i) {\n      // For now, the rows pointers store the corresponding row counts.\n      csr_row_ptr(indices(i, 0) + 1) += 1;\n      csr_col_ind(i) = indices(i, 1);\n    }\n  } else {  // rank == 3\n    for (int64 i = 0; i < total_nnz; ++i) {\n      const int cur_batch = indices(i, 0);\n      // For now, the rows pointers store the corresponding row counts.\n      csr_row_ptr(cur_batch * (num_rows + 1) + indices(i, 1) + 1) += 1;\n      csr_col_ind(i) = indices(i, 2);\n\n      // We're at a new batch and might have skipped over empty batches.\n      while (prev_batch < cur_batch) {\n        // The previous batch ends at position i.\n        batch_ptr(prev_batch + 1) = i;\n        ++prev_batch;\n      }\n    }\n  }\n  // Set the last element of batch_ptr and account for trailing empty batches.\n  while (prev_batch < batch_size) {\n    batch_ptr(prev_batch + 1) = total_nnz;\n    ++prev_batch;\n  }\n\n  // Compute the cumulative row counts for each batch.\n  for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n    auto* row_ptr_batch = csr_row_ptr.data() + batch_idx * (num_rows + 1);\n    std::partial_sum(row_ptr_batch, row_ptr_batch + num_rows + 1,\n                     row_ptr_batch);\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _complete_multipart_upload_artifact(artifact_path):\n    \"\"\"\n    A request handler for `POST /mlflow-artifacts/mpu/complete` to complete a multipart upload\n    to `artifact_path` (a relative path from the root artifact directory).\n    \"\"\"\n    validate_path_is_safe(artifact_path)\n\n    request_message = _get_request_message(\n        CompleteMultipartUpload(),\n        schema={\n            \"path\": [_assert_required, _assert_string],\n            \"upload_id\": [_assert_string],\n            \"parts\": [_assert_required],\n        },\n    )\n    path = request_message.path\n    upload_id = request_message.upload_id\n    parts = [MultipartUploadPart.from_proto(part) for part in request_message.parts]\n\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    _validate_support_multipart_upload(artifact_repo)\n\n    artifact_repo.complete_multipart_upload(\n        path,\n        upload_id,\n        parts,\n        artifact_path,\n    )\n    return _wrap_response(CompleteMultipartUpload.Response())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def put_file(path):\n    try:\n        dest_path = sanitized_join(\n            path,\n            pathlib.Path(app.config[\"DATA_ROOT\"]),\n        )\n    except ValueError:\n        return flask.Response(\n            \"Not Found\",\n            404,\n            mimetype=\"text/plain\",\n        )\n\n    verification_key = flask.request.args.get(\"v\", \"\")\n    length = int(flask.request.headers.get(\"Content-Length\", 0))\n    hmac_input = \"{} {}\".format(path, length).encode(\"utf-8\")\n    key = app.config[\"SECRET_KEY\"]\n    mac = hmac.new(key, hmac_input, hashlib.sha256)\n    digest = mac.hexdigest()\n\n    if not hmac.compare_digest(digest, verification_key):\n        return flask.Response(\n            \"Invalid verification key\",\n            403,\n            mimetype=\"text/plain\",\n        )\n\n    content_type = flask.request.headers.get(\n        \"Content-Type\",\n        \"application/octet-stream\",\n    )\n\n    dest_path.parent.mkdir(parents=True, exist_ok=True, mode=0o770)\n    data_file, metadata_file = get_paths(dest_path)\n\n    try:\n        with write_file(data_file) as fout:\n            stream_file(flask.request.stream, fout, length)\n\n            with metadata_file.open(\"x\") as f:\n                json.dump(\n                    {\n                        \"headers\": {\"Content-Type\": content_type},\n                    },\n                    f,\n                )\n    except EOFError:\n        return flask.Response(\n            \"Bad Request\",\n            400,\n            mimetype=\"text/plain\",\n        )\n    except OSError as exc:\n        if exc.errno == errno.EEXIST:\n            return flask.Response(\n                \"Conflict\",\n                409,\n                mimetype=\"text/plain\",\n            )\n        raise\n\n    return flask.Response(\n        \"Created\",\n        201,\n        mimetype=\"text/plain\",\n    )\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def authenticate_by_pam(username, password):\n    @pam_conversation\n    def conv(\n        num_msg, msg, resp, appdata_ptr\n    ):  # pylint: disable=unused-argument\n        # it is: *resp = (pam_response *) calloc(num_msg, sizeof(pam_response))\n        resp[0] = cast(\n            calloc(num_msg, sizeof(pam_response)), POINTER(pam_response)\n        )\n        for i in range(num_msg):\n            if msg[i].contents.msg_style == PAM_PROMPT_ECHO_OFF:\n                resp.contents[i].resp = strdup(password.encode(\"utf8\"))\n                resp.contents[i].resp_retcode = 0\n        return 0\n\n    pamh = pam_handle()\n    conversation = pam_conv(conv)\n    returncode = pam_start(\n        PCSD_SERVICE.encode(\"utf8\"),\n        username.encode(\"utf8\"),\n        byref(conversation),\n        byref(pamh),\n    )\n    if returncode == PAM_SUCCESS:\n        returncode = pam_authenticate(pamh, 0)\n    pam_end(pamh, returncode)\n    return returncode == PAM_SUCCESS\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<T, 1>::ConstTensor& values,\n                        const typename TTypes<T, 1>::ConstTensor& value_range,\n                        int32_t nbins, typename TTypes<Tout, 1>::Tensor& out) {\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    Tensor index_to_bin_tensor;\n\n    TF_RETURN_IF_ERROR(context->forward_input_or_allocate_temp(\n        {0}, DataTypeToEnum<int32>::value, TensorShape({values.size()}),\n        &index_to_bin_tensor));\n    auto index_to_bin = index_to_bin_tensor.flat<int32>();\n\n    const double step = static_cast<double>(value_range(1) - value_range(0)) /\n                        static_cast<double>(nbins);\n    const double nbins_minus_1 = static_cast<double>(nbins - 1);\n\n    // The calculation is done by finding the slot of each value in `values`.\n    // With [a, b]:\n    //   step = (b - a) / nbins\n    //   (x - a) / step\n    // , then the entries are mapped to output.\n\n    // Bug fix: Switch the order of cwiseMin and int32-casting to avoid\n    // producing a negative index when casting an big int64 number to int32\n    index_to_bin.device(d) =\n        ((values.cwiseMax(value_range(0)) - values.constant(value_range(0)))\n             .template cast<double>() /\n         step)\n            .cwiseMin(nbins_minus_1)\n            .template cast<int32>();\n\n    out.setZero();\n    for (int32_t i = 0; i < index_to_bin.size(); i++) {\n      out(index_to_bin(i)) += Tout(1);\n    }\n    return Status::OK();\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(\n        self,\n        targets=\"127.0.0.1\",\n        options=\"-sT\",\n        event_callback=None,\n        safe_mode=True,\n        fqp=None,\n    ):\n        \"\"\"\n        Constructor of NmapProcess class.\n\n        :param targets: hosts to be scanned. Could be a string of hosts \\\n        separated with a coma or a python list of hosts/ip.\n        :type targets: string or list\n\n        :param options: list of nmap options to be applied to scan. \\\n        These options are all documented in nmap's man pages.\n\n        :param event_callback: callable function which will be ran \\\n        each time nmap process outputs data. This function will receive \\\n        two parameters:\n\n            1. the nmap process object\n            2. the data produced by nmap process. See readme for examples.\n\n        :param safe_mode: parameter to protect unsafe options like -oN, -oG, \\\n        -iL, -oA,...\n\n        :param fqp: full qualified path, if None, nmap will be searched \\\n        in the PATH\n\n        :return: NmapProcess object\n\n        \"\"\"\n        Thread.__init__(self)\n        unsafe_opts = set(\n            [\n                \"-oG\",\n                \"-oN\",\n                \"-iL\",\n                \"-oA\",\n                \"-oS\",\n                \"-oX\",\n                \"--iflist\",\n                \"--resume\",\n                \"--stylesheet\",\n                \"--datadir\",\n            ]\n        )\n        # more reliable than just using os.name() (cygwin)\n        self.__is_windows = platform.system() == \"Windows\"\n        if fqp:\n            if os.path.isfile(fqp) and os.access(fqp, os.X_OK):\n                self.__nmap_binary = fqp\n            else:\n                raise EnvironmentError(1, \"wrong path or not executable\", fqp)\n        else:\n            nmap_binary_name = \"nmap\"\n            self.__nmap_binary = self._whereis(nmap_binary_name)\n        self.__nmap_fixed_options = \"-oX - -vvv --stats-every 1s\"\n\n        if self.__nmap_binary is None:\n            em = \"nmap is not installed or could not be found in system path\"\n            raise EnvironmentError(1, em)\n\n        if isinstance(targets, str):\n            self.__nmap_targets = targets.replace(\" \", \"\").split(\",\")\n        elif isinstance(targets, list):\n            self.__nmap_targets = targets\n        else:\n            raise Exception(\n                \"Supplied target list should be either a string or a list\"\n            )\n\n        self._nmap_options = set(options.split())\n        if safe_mode and not self._nmap_options.isdisjoint(unsafe_opts):\n            raise Exception(\n                \"unsafe options activated while safe_mode is set True\"\n            )\n        self.__nmap_dynamic_options = options\n        self.__sudo_run = \"\"\n        self.__nmap_command_line = self.get_command_line()\n\n        if event_callback and callable(event_callback):\n            self.__nmap_event_callback = event_callback\n        else:\n            self.__nmap_event_callback = None\n        (\n            self.DONE,\n            self.READY,\n            self.RUNNING,\n            self.CANCELLED,\n            self.FAILED,\n        ) = range(5)\n        self._run_init()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& true_classes = context->input(0);\n    OP_REQUIRES(context, true_classes.dims() == 2,\n                errors::InvalidArgument(\"true_classes must be a matrix\"));\n    const int32_t batch_size = true_classes.dim_size(0);\n    OP_REQUIRES(\n        context, true_classes.dim_size(1) == num_true_,\n        errors::InvalidArgument(\"true_classes must have \"\n                                \"num_true columns, expected: \",\n                                true_classes.dim_size(1), \" was: \", num_true_));\n    CHECK(sampler_) << \"CandidateSamplerOp did not set sampler_\";\n\n    if (unique_) {\n      OP_REQUIRES(context, num_sampled_ <= sampler_->range(),\n                  errors::InvalidArgument(\"Sampler's range is too small.\"));\n    }\n\n    // Output candidates and expected_count.\n    Tensor* out_sampled_candidates = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({num_sampled_}),\n                                            &out_sampled_candidates));\n\n    Tensor* out_true_expected_count = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                1, TensorShape({batch_size, num_true_}),\n                                &out_true_expected_count));\n    Tensor* out_sampled_expected_count = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(2, TensorShape({num_sampled_}),\n                                            &out_sampled_expected_count));\n\n    gtl::ArraySlice<int64_t> true_candidate(\n        true_classes.matrix<int64_t>().data(), batch_size * num_true_);\n    gtl::MutableArraySlice<int64_t> sampled_candidate(\n        out_sampled_candidates->vec<int64_t>().data(), num_sampled_);\n    gtl::MutableArraySlice<float> true_expected_count(\n        out_true_expected_count->matrix<float>().data(),\n        batch_size * num_true_);\n    gtl::MutableArraySlice<float> sampled_expected_count(\n        out_sampled_expected_count->vec<float>().data(), num_sampled_);\n\n    // Approximately conservatively estimate the number of samples required.\n    // In cases where rejection sampling is used we may occasionally use more\n    // samples than expected, which will result in reused random bits.\n    const int64_t samples32 = 2048 * num_sampled_;\n\n    // Pick sampled candidates.\n    auto local_gen = generator_.ReserveSamples32(samples32);\n    random::SimplePhilox random(&local_gen);\n    sampler_->SampleBatchGetExpectedCount(&random, unique_, sampled_candidate,\n                                          sampled_expected_count,\n                                          true_candidate, true_expected_count);\n\n    if (sampler_->NeedsUpdates()) {\n      sampler_->Update(true_candidate);\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, instance, filename):\n        \"\"\"\n        Apply (if callable) or prepend (if a string) upload_to to the filename,\n        then delegate further processing of the name to the storage backend.\n        Until the storage layer, all file paths are expected to be Unix style\n        (with forward slashes).\n        \"\"\"\n        if callable(self.upload_to):\n            filename = self.upload_to(instance, filename)\n        else:\n            dirname = datetime.datetime.now().strftime(self.upload_to)\n            filename = posixpath.join(dirname, filename)\n        return self.storage.generate_filename(filename)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _rpsl_db_query_to_graphql_out(query: RPSLDatabaseQuery, info: GraphQLResolveInfo):\n    \"\"\"\n    Given an RPSL database query, execute it and clean up the output\n    to be suitable to return to GraphQL.\n\n    Main changes are:\n    - Enum handling\n    - Adding the asn and prefix fields if applicable\n    - Ensuring the right fields are returned as a list of strings or a string\n    \"\"\"\n    database_handler = info.context['request'].app.state.database_handler\n    if info.context.get('sql_trace'):\n        if 'sql_queries' not in info.context:\n            info.context['sql_queries'] = [repr(query)]\n        else:\n            info.context['sql_queries'].append(repr(query))\n\n    for row in database_handler.execute_query(query, refresh_on_error=True):\n        graphql_result = {snake_to_camel_case(k): v for k, v in row.items() if k != 'parsed_data'}\n        if 'object_text' in row:\n            graphql_result['objectText'] = remove_auth_hashes(row['object_text'])\n        if 'rpki_status' in row:\n            graphql_result['rpkiStatus'] = row['rpki_status']\n        if row.get('ip_first') is not None and row.get('prefix_length'):\n            graphql_result['prefix'] = row['ip_first'] + '/' + str(row['prefix_length'])\n        if row.get('asn_first') is not None and row.get('asn_first') == row.get('asn_last'):\n            graphql_result['asn'] = row['asn_first']\n\n        object_type = resolve_rpsl_object_type(row)\n        for key, value in row.get('parsed_data', dict()).items():\n            if key == 'auth':\n                value = remove_auth_hashes(value)\n            graphql_type = schema.graphql_types[object_type][key]\n            if graphql_type == 'String' and isinstance(value, list):\n                value = '\\n'.join(value)\n            graphql_result[snake_to_camel_case(key)] = value\n        yield graphql_result\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_annotation(self, annotation, alias, is_summary=False, select=True):\n        \"\"\"Add a single annotation expression to the Query.\"\"\"\n        annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,\n                                                   summarize=is_summary)\n        if select:\n            self.append_annotation_mask([alias])\n        else:\n            self.set_annotation_mask(set(self.annotation_select).difference({alias}))\n        self.annotations[alias] = annotation\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor x = context->input(0);\n    const Tensor& scale = context->input(1);\n    const Tensor& offset = context->input(2);\n    const Tensor& estimated_mean = context->input(3);\n    const Tensor& estimated_variance = context->input(4);\n    const Tensor* side_input = has_side_input_ ? &context->input(5) : nullptr;\n\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(context, offset.dims() == 1,\n                errors::InvalidArgument(\"offset must be 1-dimensional\",\n                                        offset.shape().DebugString()));\n    OP_REQUIRES(context, estimated_mean.dims() == 1,\n                errors::InvalidArgument(\"estimated_mean must be 1-dimensional\",\n                                        estimated_mean.shape().DebugString()));\n    OP_REQUIRES(\n        context, estimated_variance.dims() == 1,\n        errors::InvalidArgument(\"estimated_variance must be 1-dimensional\",\n                                estimated_variance.shape().DebugString()));\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64 in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64 in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64 in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64 in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64 in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    if (has_side_input_) {\n      OP_REQUIRES(context, side_input->shape() == x.shape(),\n                  errors::InvalidArgument(\n                      \"side_input shape must be equal to input shape: \",\n                      side_input->shape().DebugString(),\n                      \" != \", x.shape().DebugString()));\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity) {\n      // NOTE(ezhulenev): This requirement is coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx.\n      OP_REQUIRES(\n          context, !is_training_ || x.dim_size(3) % 4 == 0,\n          errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                  \"channel dimension to be a multiple of 4.\"));\n    }\n\n    Tensor* y = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, alloc_shape, &y));\n\n    Tensor* batch_mean = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {3}, 1, scale.shape(), &batch_mean));\n    Tensor* batch_var = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {4}, 2, scale.shape(), &batch_var));\n    Tensor* saved_mean = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(3, scale.shape(), &saved_mean));\n    Tensor* saved_maybe_inv_var = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(4, scale.shape(),\n                                                     &saved_maybe_inv_var));\n\n    if (is_training_) {\n      functor::FusedBatchNorm<Device, T, U, true>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    } else {\n      functor::FusedBatchNorm<Device, T, U, false>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, y->CopyFrom(*y, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def complete_reg(request):\n    \"\"\"Completes the registeration, called by API\"\"\"\n    try:\n        data = cbor.decode(request.body)\n\n        client_data = CollectedClientData(data['clientDataJSON'])\n        att_obj = AttestationObject((data['attestationObject']))\n        server = getServer()\n        auth_data = server.register_complete(\n            request.session['fido_state'],\n            client_data,\n            att_obj\n        )\n        encoded = websafe_encode(auth_data.credential_data)\n        uk = User_Keys()\n        uk.username = request.user.username\n        uk.properties = {\"device\": encoded, \"type\": att_obj.fmt, }\n        uk.owned_by_enterprise = getattr(settings, \"MFA_OWNED_BY_ENTERPRISE\", False)\n        uk.key_type = \"FIDO2\"\n        uk.save()\n        return HttpResponse(simplejson.dumps({'status': 'OK'}))\n    except Exception as exp:\n        import traceback\n        print(traceback.format_exc())\n        try:\n            from raven.contrib.django.raven_compat.models import client\n            client.captureException()\n        except:\n            pass\n        return HttpResponse(simplejson.dumps({'status': 'ERR', \"message\": \"Error on server, please try again later\"}))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def lookup_path(\n        self, path: str\n    ) -> typing.Tuple[str, typing.Optional[os.stat_result]]:\n        for directory in self.all_directories:\n            joined_path = os.path.join(directory, path)\n            if self.follow_symlink:\n                full_path = os.path.abspath(joined_path)\n            else:\n                full_path = os.path.realpath(joined_path)\n            directory = os.path.realpath(directory)\n            if os.path.commonprefix([full_path, directory]) != directory:\n                # Don't allow misbehaving clients to break out of the static files\n                # directory.\n                continue\n            try:\n                return full_path, os.stat(full_path)\n            except (FileNotFoundError, NotADirectoryError):\n                continue\n        return \"\", None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def element():\n    element_url = src_url = request.args.get('url')\n    if element_url.startswith('gAAAAA'):\n        try:\n            cipher_suite = Fernet(g.session_key)\n            src_url = cipher_suite.decrypt(element_url.encode()).decode()\n        except (InvalidSignature, InvalidToken) as e:\n            return render_template(\n                'error.html',\n                error_message=str(e)), 401\n\n    src_type = request.args.get('type')\n\n    try:\n        file_data = g.user_request.send(base_url=src_url).content\n        tmp_mem = io.BytesIO()\n        tmp_mem.write(file_data)\n        tmp_mem.seek(0)\n\n        return send_file(tmp_mem, mimetype=src_type)\n    except exceptions.RequestException:\n        pass\n\n    empty_gif = base64.b64decode(\n        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n    return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, expression, delimiter, **extra):\n        super().__init__(expression, delimiter=delimiter, **extra)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def encode(self, types: Iterable[TypeStr], args: Iterable[Any]) -> bytes:\n        \"\"\"\n        Encodes the python values in ``args`` as a sequence of binary values of\n        the ABI types in ``types`` via the head-tail mechanism.\n\n        :param types: A list or tuple of string representations of the ABI types\n            that will be used for encoding e.g.  ``('uint256', 'bytes[]',\n            '(int,int)')``\n        :param args: A list or tuple of python values to be encoded.\n\n        :returns: The head-tail encoded binary representation of the python\n            values in ``args`` as values of the ABI types in ``types``.\n        \"\"\"\n        validate_list_like_param(types, \"types\")\n        validate_list_like_param(args, \"args\")\n\n        encoders = [self._registry.get_encoder(type_str) for type_str in types]\n\n        encoder = TupleEncoder(encoders=encoders)\n\n        return encoder(args)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status KernelAndDeviceOp::Run(\n    ScopedStepContainer* step_container, const EagerKernelArgs& inputs,\n    std::vector<EagerKernelRet>* outputs,\n    CancellationManager* cancellation_manager,\n    const absl::optional<EagerRemoteFunctionParams>& remote_func_params) {\n  OpKernelContext::Params params;\n  params.device = device_;\n  params.frame_iter = FrameAndIter(0, 0);\n  params.inputs = inputs.GetTensorValues();\n  params.op_kernel = kernel_.get();\n  params.resource_manager = device_->resource_manager();\n  params.input_alloc_attrs = &input_alloc_attrs_;\n  params.output_attr_array = output_alloc_attrs_.data();\n  params.function_library = flr_;\n  params.slice_reader_cache = &slice_reader_cache_;\n  params.rendezvous = rendezvous_;\n  OpExecutionState* op_execution_state = nullptr;\n\n  CancellationManager default_cancellation_manager;\n  if (cancellation_manager) {\n    params.cancellation_manager = cancellation_manager;\n  } else if (kernel_->is_deferred()) {\n    op_execution_state = new OpExecutionState;\n    params.cancellation_manager = &op_execution_state->cancellation_manager;\n    params.inc_num_deferred_ops_function = [op_execution_state]() {\n      op_execution_state->Ref();\n    };\n    params.dec_num_deferred_ops_function = [op_execution_state]() {\n      op_execution_state->Unref();\n    };\n  } else {\n    params.cancellation_manager = &default_cancellation_manager;\n  }\n\n  params.log_memory = log_memory_;\n\n  params.runner = get_runner();\n\n  params.step_container =\n      step_container == nullptr ? &step_container_ : step_container;\n  auto step_container_cleanup = gtl::MakeCleanup([step_container, this] {\n    if (step_container == nullptr) {\n      this->step_container_.CleanUp();\n    }\n  });\n\n  params.collective_executor =\n      collective_executor_ ? collective_executor_->get() : nullptr;\n\n  OpKernelContext context(&params);\n\n  {\n    port::ScopedFlushDenormal flush;\n    port::ScopedSetRound round(FE_TONEAREST);\n    // 'AnnotatedTraceMe' will trace both scheduling time on host and execution\n    // time on device of the OpKernel.\n    profiler::AnnotatedTraceMe activity(\n        [&] { return kernel_->TraceString(context, /*verbose=*/false); },\n        profiler::TraceMeLevel::kInfo);\n    device_->Compute(kernel_.get(), &context);\n  }\n\n  // Clean up execution op_execution_state if deferred ops aren't running.\n  if (op_execution_state != nullptr) {\n    op_execution_state->Unref();\n  }\n\n  if (!context.status().ok()) return context.status();\n\n  if (outputs != nullptr) {\n    outputs->clear();\n    for (int i = 0; i < context.num_outputs(); ++i) {\n      outputs->push_back(Tensor(*context.mutable_output(i)));\n    }\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get(self, path: str) -> None:\n        parts = path.split(\"/\")\n        component_name = parts[0]\n        component_root = self._registry.get_component_path(component_name)\n        if component_root is None:\n            self.write(\"not found\")\n            self.set_status(404)\n            return\n\n        filename = \"/\".join(parts[1:])\n        abspath = os.path.join(component_root, filename)\n\n        LOGGER.debug(\"ComponentRequestHandler: GET: %s -> %s\", path, abspath)\n\n        try:\n            with open(abspath, \"rb\") as file:\n                contents = file.read()\n        except (OSError) as e:\n            LOGGER.error(f\"ComponentRequestHandler: GET {path} read error\", exc_info=e)\n            self.write(\"read error\")\n            self.set_status(404)\n            return\n\n        self.write(contents)\n        self.set_header(\"Content-Type\", self.get_content_type(abspath))\n\n        self.set_extra_headers(path)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def decrypt_data(token, secret, data):\n    \"\"\" Decrypt the data with the given secret key. \"\"\"\n    if data.startswith(ENCRYPT_MARKER):\n        try:\n            # encrypted data\n            encoded_data = data[len(ENCRYPT_MARKER):]\n            aes_key = generate_aes_key(token, secret)\n            decoded_data = base64.b64decode(encoded_data)\n            iv = decoded_data[:16]\n            encrypted_data = decoded_data[16:]\n            cipher = AES.new(aes_key, AES.MODE_CFB, iv)\n            decrypted_data = cipher.decrypt(encrypted_data)\n            return json.loads(decrypted_data)\n        except:\n            raise DecryptError('data appeared to be corrupted')\n    else:\n        # doesn't appear to be encrypted data\n        return data\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def add_extra(self, select, select_params, where, params, tables, order_by):\n        \"\"\"\n        Add data to the various extra_* attributes for user-created additions\n        to the query.\n        \"\"\"\n        if select:\n            # We need to pair any placeholder markers in the 'select'\n            # dictionary with their parameters in 'select_params' so that\n            # subsequent updates to the select dictionary also adjust the\n            # parameters appropriately.\n            select_pairs = {}\n            if select_params:\n                param_iter = iter(select_params)\n            else:\n                param_iter = iter([])\n            for name, entry in select.items():\n                entry = str(entry)\n                entry_params = []\n                pos = entry.find(\"%s\")\n                while pos != -1:\n                    if pos == 0 or entry[pos - 1] != \"%\":\n                        entry_params.append(next(param_iter))\n                    pos = entry.find(\"%s\", pos + 2)\n                select_pairs[name] = (entry, entry_params)\n            self.extra.update(select_pairs)\n        if where or params:\n            self.where.add(ExtraWhere(where, params), AND)\n        if tables:\n            self.extra_tables += tuple(tables)\n        if order_by:\n            self.extra_order_by = order_by\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def writestr(self, data: Union[str, bytes, bytearray, memoryview], arcname: str):\n        if not isinstance(arcname, str):\n            raise ValueError(\"Unsupported arcname\")\n        if isinstance(data, str):\n            self.writef(io.BytesIO(data.encode(\"UTF-8\")), arcname)\n        elif isinstance(data, bytes) or isinstance(data, bytearray) or isinstance(data, memoryview):\n            self.writef(io.BytesIO(data), arcname)\n        else:\n            raise ValueError(\"Unsupported data type.\")\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def run(self):\n        dag_id = request.form.get('dag_id')\n        task_id = request.form.get('task_id')\n        origin = request.form.get('origin')\n        dag = current_app.dag_bag.get_dag(dag_id)\n        task = dag.get_task(task_id)\n\n        execution_date = request.form.get('execution_date')\n        execution_date = timezone.parse(execution_date)\n        ignore_all_deps = request.form.get('ignore_all_deps') == \"true\"\n        ignore_task_deps = request.form.get('ignore_task_deps') == \"true\"\n        ignore_ti_state = request.form.get('ignore_ti_state') == \"true\"\n\n        executor = ExecutorLoader.get_default_executor()\n        valid_celery_config = False\n        valid_kubernetes_config = False\n\n        try:\n            from airflow.executors.celery_executor import CeleryExecutor\n            valid_celery_config = isinstance(executor, CeleryExecutor)\n        except ImportError:\n            pass\n\n        try:\n            from airflow.executors.kubernetes_executor import KubernetesExecutor\n            valid_kubernetes_config = isinstance(executor, KubernetesExecutor)\n        except ImportError:\n            pass\n\n        if not valid_celery_config and not valid_kubernetes_config:\n            flash(\"Only works with the Celery or Kubernetes executors, sorry\", \"error\")\n            return redirect(origin)\n\n        ti = models.TaskInstance(task=task, execution_date=execution_date)\n        ti.refresh_from_db()\n\n        # Make sure the task instance can be run\n        dep_context = DepContext(\n            deps=RUNNING_DEPS,\n            ignore_all_deps=ignore_all_deps,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state)\n        failed_deps = list(ti.get_failed_dep_statuses(dep_context=dep_context))\n        if failed_deps:\n            failed_deps_str = \", \".join(\n                [\"{}: {}\".format(dep.dep_name, dep.reason) for dep in failed_deps])\n            flash(\"Could not queue task instance for execution, dependencies not met: \"\n                  \"{}\".format(failed_deps_str),\n                  \"error\")\n            return redirect(origin)\n\n        executor.start()\n        executor.queue_task_instance(\n            ti,\n            ignore_all_deps=ignore_all_deps,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state)\n        executor.heartbeat()\n        flash(\n            \"Sent {} to the message queue, \"\n            \"it should start any moment now.\".format(ti))\n        return redirect(origin)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "m3ApiRawFunction(m3_wasi_generic_fd_read)\n{\n    m3ApiReturnType  (uint32_t)\n    m3ApiGetArg      (__wasi_fd_t          , fd)\n    m3ApiGetArgMem   (wasi_iovec_t *       , wasi_iovs)\n    m3ApiGetArg      (__wasi_size_t        , iovs_len)\n    m3ApiGetArgMem   (__wasi_size_t *      , nread)\n\n    m3ApiCheckMem(wasi_iovs,    iovs_len * sizeof(wasi_iovec_t));\n    m3ApiCheckMem(nread,        sizeof(__wasi_size_t));\n\n#if defined(HAS_IOVEC)\n    struct iovec iovs[iovs_len];\n    copy_iov_to_host(_mem, iovs, wasi_iovs, iovs_len);\n\n    ssize_t ret = readv(fd, iovs, iovs_len);\n    if (ret < 0) { m3ApiReturn(errno_to_wasi(errno)); }\n    m3ApiWriteMem32(nread, ret);\n    m3ApiReturn(__WASI_ERRNO_SUCCESS);\n#else\n    ssize_t res = 0;\n    for (__wasi_size_t i = 0; i < iovs_len; i++) {\n        void* addr = m3ApiOffsetToPtr(m3ApiReadMem32(&wasi_iovs[i].buf));\n        size_t len = m3ApiReadMem32(&wasi_iovs[i].buf_len);\n        if (len == 0) continue;\n\n        int ret = read (fd, addr, len);\n        if (ret < 0) m3ApiReturn(errno_to_wasi(errno));\n        res += ret;\n        if ((size_t)ret < len) break;\n    }\n    m3ApiWriteMem32(nread, res);\n    m3ApiReturn(__WASI_ERRNO_SUCCESS);\n#endif\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_users(self, email):\n        \"\"\"Given an email, return matching user(s) who should receive a reset.\n\n        This allows subclasses to more easily customize the default policies\n        that prevent inactive users and users with unusable passwords from\n        resetting their password.\n        \"\"\"\n        active_users = UserModel._default_manager.filter(**{\n            '%s__iexact' % UserModel.get_email_field_name(): email,\n            'is_active': True,\n        })\n        return (u for u in active_users if u.has_usable_password())\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool IsHostMemoryArg(const EagerOperation& op, const NodeDef* node_def,\n                     const Device* op_device, const KernelDef* kernel_def,\n                     const int port_id) {\n  if (op.is_function()) return false;\n  if (node_def == nullptr) return false;\n  if (kernel_def == nullptr || op_device == nullptr) return false;\n  const auto& host_memory_args = kernel_def->host_memory_arg();\n  const OpDef& op_def = OpRegistry::Global()->LookUp(op.Name())->op_def;\n  const int arg_id = OpPortIdToArgId(*node_def, op_def.input_arg(), port_id);\n  return std::find(host_memory_args.begin(), host_memory_args.end(),\n                   op_def.input_arg(arg_id).name()) != host_memory_args.end();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def authenticate(self, context, auth=None):\n        \"\"\"Authenticate credentials and return a token.\n\n        Accept auth as a dict that looks like::\n\n            {\n                \"auth\":{\n                    \"passwordCredentials\":{\n                        \"username\":\"test_user\",\n                        \"password\":\"mypass\"\n                    },\n                    \"tenantName\":\"customer-x\"\n                }\n            }\n\n        In this case, tenant is optional, if not provided the token will be\n        considered \"unscoped\" and can later be used to get a scoped token.\n\n        Alternatively, this call accepts auth with only a token and tenant\n        that will return a token that is scoped to that tenant.\n        \"\"\"\n\n        token_id = uuid.uuid4().hex\n        if 'passwordCredentials' in auth:\n            username = auth['passwordCredentials'].get('username', '')\n            password = auth['passwordCredentials'].get('password', '')\n            tenant_name = auth.get('tenantName', None)\n\n            user_id = auth['passwordCredentials'].get('userId', None)\n            if username:\n                user_ref = self.identity_api.get_user_by_name(\n                        context=context, user_name=username)\n                if user_ref:\n                    user_id = user_ref['id']\n\n            # more compat\n            tenant_id = auth.get('tenantId', None)\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                if tenant_ref:\n                    tenant_id = tenant_ref['id']\n\n            try:\n                auth_info = self.identity_api.authenticate(context=context,\n                                                           user_id=user_id,\n                                                           password=password,\n                                                           tenant_id=tenant_id)\n                (user_ref, tenant_ref, metadata_ref) = auth_info\n\n                # If the user is disabled don't allow them to authenticate\n                if not user_ref.get('enabled', True):\n                    LOG.warning('User %s is disabled' % user_id)\n                    raise exception.Unauthorized()\n            except AssertionError as e:\n                raise exception.Unauthorized(e.message)\n\n            token_ref = self.token_api.create_token(\n                    context, token_id, dict(id=token_id,\n                                            user=user_ref,\n                                            tenant=tenant_ref,\n                                            metadata=metadata_ref))\n            if tenant_ref:\n                catalog_ref = self.catalog_api.get_catalog(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'],\n                        metadata=metadata_ref)\n            else:\n                catalog_ref = {}\n\n        elif 'token' in auth:\n            token = auth['token'].get('id', None)\n\n            tenant_name = auth.get('tenantName')\n\n            # more compat\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                tenant_id = tenant_ref['id']\n            else:\n                tenant_id = auth.get('tenantId', None)\n\n            try:\n                old_token_ref = self.token_api.get_token(context=context,\n                                                         token_id=token)\n            except exception.NotFound:\n                raise exception.Unauthorized()\n\n            user_ref = old_token_ref['user']\n\n            # If the user is disabled don't allow them to authenticate\n            current_user_ref = self.identity_api.get_user(\n                                                    context=context,\n                                                    user_id=user_ref['id'])\n            if not current_user_ref.get('enabled', True):\n                LOG.warning('User %s is disabled' % user_ref['id'])\n                raise exception.Unauthorized()\n\n            tenants = self.identity_api.get_tenants_for_user(context,\n                                                             user_ref['id'])\n            if tenant_id:\n                assert tenant_id in tenants\n\n            tenant_ref = self.identity_api.get_tenant(context=context,\n                                                      tenant_id=tenant_id)\n            if tenant_ref:\n                metadata_ref = self.identity_api.get_metadata(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'])\n                catalog_ref = self.catalog_api.get_catalog(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'],\n                        metadata=metadata_ref)\n            else:\n                metadata_ref = {}\n                catalog_ref = {}\n\n            token_ref = self.token_api.create_token(\n                    context, token_id, dict(id=token_id,\n                                            user=user_ref,\n                                            tenant=tenant_ref,\n                                            metadata=metadata_ref,\n                                            expires=old_token_ref['expires']))\n\n        # TODO(termie): optimize this call at some point and put it into the\n        #               the return for metadata\n        # fill out the roles in the metadata\n        roles_ref = []\n        for role_id in metadata_ref.get('roles', []):\n            roles_ref.append(self.identity_api.get_role(context, role_id))\n        logging.debug('TOKEN_REF %s', token_ref)\n        return self._format_authenticate(token_ref, roles_ref, catalog_ref)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_url(url):\n    if os.path.exists(url):\n        with open(url, 'r') as opener:\n            content = opener.read()\n    else:\n        content = url\n\n    urls = []\n    for _url in content.splitlines():\n        if not _url:\n            continue\n\n        urls.append((_validate(_url), _url))\n    \n    return urls\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, sydent):\n        self.sydent = sydent\n\n        root = Resource()\n        matrix = Resource()\n        identity = Resource()\n        api = Resource()\n        v1 = self.sydent.servlets.v1\n        v2 = self.sydent.servlets.v2\n\n        validate = Resource()\n        validate_v2 = Resource()\n        email = Resource()\n        email_v2 = Resource()\n        msisdn = Resource()\n        msisdn_v2 = Resource()\n\n        threepid_v1 = Resource()\n        threepid_v2 = Resource()\n        unbind = self.sydent.servlets.threepidUnbind\n\n        pubkey = Resource()\n        ephemeralPubkey = Resource()\n\n        root.putChild(b'_matrix', matrix)\n        matrix.putChild(b'identity', identity)\n        identity.putChild(b'api', api)\n        identity.putChild(b'v2', v2)\n        api.putChild(b'v1', v1)\n\n        validate.putChild(b'email', email)\n        validate.putChild(b'msisdn', msisdn)\n\n        validate_v2.putChild(b'email', email_v2)\n        validate_v2.putChild(b'msisdn', msisdn_v2)\n\n        v1.putChild(b'validate', validate)\n\n        v1.putChild(b'lookup', self.sydent.servlets.lookup)\n        v1.putChild(b'bulk_lookup', self.sydent.servlets.bulk_lookup)\n\n        v1.putChild(b'pubkey', pubkey)\n        pubkey.putChild(b'isvalid', self.sydent.servlets.pubkeyIsValid)\n        pubkey.putChild(b'ed25519:0', self.sydent.servlets.pubkey_ed25519)\n        pubkey.putChild(b'ephemeral', ephemeralPubkey)\n        ephemeralPubkey.putChild(b'isvalid', self.sydent.servlets.ephemeralPubkeyIsValid)\n\n        threepid_v2.putChild(b'getValidated3pid', self.sydent.servlets.getValidated3pidV2)\n        threepid_v2.putChild(b'bind', self.sydent.servlets.threepidBindV2)\n        threepid_v2.putChild(b'unbind', unbind)\n\n        threepid_v1.putChild(b'getValidated3pid', self.sydent.servlets.getValidated3pid)\n        threepid_v1.putChild(b'unbind', unbind)\n        if self.sydent.enable_v1_associations:\n            threepid_v1.putChild(b'bind', self.sydent.servlets.threepidBind)\n\n        v1.putChild(b'3pid', threepid_v1)\n\n        email.putChild(b'requestToken', self.sydent.servlets.emailRequestCode)\n        email.putChild(b'submitToken', self.sydent.servlets.emailValidate)\n\n        email_v2.putChild(b'requestToken', self.sydent.servlets.emailRequestCodeV2)\n        email_v2.putChild(b'submitToken', self.sydent.servlets.emailValidateV2)\n\n        msisdn.putChild(b'requestToken', self.sydent.servlets.msisdnRequestCode)\n        msisdn.putChild(b'submitToken', self.sydent.servlets.msisdnValidate)\n\n        msisdn_v2.putChild(b'requestToken', self.sydent.servlets.msisdnRequestCodeV2)\n        msisdn_v2.putChild(b'submitToken', self.sydent.servlets.msisdnValidateV2)\n\n        v1.putChild(b'store-invite', self.sydent.servlets.storeInviteServlet)\n\n        v1.putChild(b'sign-ed25519', self.sydent.servlets.blindlySignStuffServlet)\n\n        # v2\n        # note v2 loses the /api so goes on 'identity' not 'api'\n        identity.putChild(b'v2', v2)\n\n        # v2 exclusive APIs\n        v2.putChild(b'terms', self.sydent.servlets.termsServlet)\n        account = self.sydent.servlets.accountServlet\n        v2.putChild(b'account', account)\n        account.putChild(b'register', self.sydent.servlets.registerServlet)\n        account.putChild(b'logout', self.sydent.servlets.logoutServlet)\n\n        # v2 versions of existing APIs\n        v2.putChild(b'validate', validate_v2)\n        v2.putChild(b'pubkey', pubkey)\n        v2.putChild(b'3pid', threepid_v2)\n        v2.putChild(b'store-invite', self.sydent.servlets.storeInviteServletV2)\n        v2.putChild(b'sign-ed25519', self.sydent.servlets.blindlySignStuffServletV2)\n        v2.putChild(b'lookup', self.sydent.servlets.lookup_v2)\n        v2.putChild(b'hash_details', self.sydent.servlets.hash_details)\n\n        self.factory = Site(root)\n        self.factory.displayTracebacks = False\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ValidatePaddingValues(absl::Span<const int64_t> input_dimensions,\n                             absl::Span<const int64_t> window_dimensions,\n                             absl::Span<const int64_t> window_strides) {\n  bool ok = input_dimensions.size() == window_dimensions.size() &&\n            input_dimensions.size() == window_strides.size();\n  if (!ok) {\n    return InvalidArgument(\n        \"Want input dimensions size %u = window dimensions size %u = window \"\n        \"strides size %u\",\n        input_dimensions.size(), window_dimensions.size(),\n        window_strides.size());\n  }\n  return OkStatus();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def test_get_inspect_frame_on_generator():\n    source_code = \"\"\"\ngenerator = (statement.gi_frame for _ in (1,))\ngenerator_element = [elem for elem in generator][0]\n\n\"\"\"\n    result = compile_restricted_exec(source_code)\n    assert result.errors == (\n        'Line 2: \"gi_frame\" is a restricted name, '\n        'that is forbidden to access in RestrictedPython.',\n    )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def runTest(self):\n\n        ## ECB mode\n        mode = self.module.MODE_ECB\n        encryption_cipher = self.module.new(a2b_hex(self.key), mode)\n        ciphertext = encryption_cipher.encrypt(self.plaintext)\n        decryption_cipher = self.module.new(a2b_hex(self.key), mode)\n        decrypted_plaintext = decryption_cipher.decrypt(ciphertext)\n        self.assertEqual(self.plaintext, decrypted_plaintext)\n\n        ## OPENPGP mode\n        mode = self.module.MODE_OPENPGP\n        encryption_cipher = self.module.new(a2b_hex(self.key), mode, self.iv)\n        eiv_ciphertext = encryption_cipher.encrypt(self.plaintext)\n        eiv = eiv_ciphertext[:self.module.block_size+2]\n        ciphertext = eiv_ciphertext[self.module.block_size+2:]\n        decryption_cipher = self.module.new(a2b_hex(self.key), mode, eiv)\n        decrypted_plaintext = decryption_cipher.decrypt(ciphertext)\n        self.assertEqual(self.plaintext, decrypted_plaintext)\n\n        ## All other non-AEAD modes (but CTR)\n        for mode in (self.module.MODE_CBC, self.module.MODE_CFB, self.module.MODE_OFB):\n            encryption_cipher = self.module.new(a2b_hex(self.key), mode, self.iv)\n            ciphertext = encryption_cipher.encrypt(self.plaintext)\n            decryption_cipher = self.module.new(a2b_hex(self.key), mode, self.iv)\n            decrypted_plaintext = decryption_cipher.decrypt(ciphertext)\n            self.assertEqual(self.plaintext, decrypted_plaintext)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def c2p(mml, simple=False):\n    \"\"\"Transforms a document in MathML content (like the one that sympy produces)\n    in one document in MathML presentation, more suitable for printing, and more\n    widely accepted\n\n    Examples\n    ========\n\n    >>> from sympy.utilities.mathml import c2p\n    >>> mml = '<apply> <exp/> <cn>2</cn> </apply>'\n    >>> c2p(mml,simple=True) != c2p(mml,simple=False)\n    True\n\n    \"\"\"\n\n    if not mml.startswith('<math'):\n        mml = add_mathml_headers(mml)\n\n    if simple:\n        return apply_xsl(mml, 'mathml/data/simple_mmlctop.xsl')\n\n    return apply_xsl(mml, 'mathml/data/mmlctop.xsl')\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _set_same_site(self):\n        # Awaiting bug fix in cherrypy\n        # https://github.com/cherrypy/cherrypy/issues/1767\n        # Force SameSite to Lax\n        cookie = cherrypy.serving.response.cookie.get('session_id', None)\n        if cookie:\n            cookie['samesite'] = 'Lax'\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_readable_dags() -> list[str]:\n    return get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  constructor(\n    tracker: IWidgetTracker<MarkdownDocument>,\n    protected parser: IMarkdownParser | null,\n    protected sanitizer: IRenderMime.ISanitizer\n  ) {\n    super(tracker);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _build_file_header(self, headers):\n        if headers is None:\n            headers = {}\n        headers['Accept'] = 'application/json'\n        if self.api_version is not None:\n            headers['Omise-Version'] = self.api_version\n        headers['User-Agent'] = 'OmisePython/%s' % version.__VERSION__\n        return headers\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "BCastList<N>::BCastList(const BCastList::Vec (&x)[N],\n                        const bool fewer_dims_optimization,\n                        const bool return_flattened_batch_indices) {\n  typedef BCastList::Vec Vec;\n\n  // Safely multiplies dimensions taking into account symbolic shapes.\n  auto mul_dims = [](int64_t dim1, int64_t dim2) -> int64_t {\n    return dim1 != 0 && dim2 != 0 && (dim1 < 0 || dim2 < 0) ? -1 : dim1 * dim2;\n  };\n\n  bool all_equal = true;\n  size_t largest_rank = 0;\n  output_batch_size_ = 1;\n  for (int i = 0; i < N; ++i) {\n    if (x[i] != x[0]) {\n      all_equal = false;\n    }\n    if (x[i].size() > largest_rank) {\n      largest_rank = x[i].size();\n    }\n  }\n  if (all_equal) {\n    broadcasting_required_ = false;\n  }\n  if (all_equal && TF_PREDICT_TRUE(fewer_dims_optimization)) {\n    // Fast path for common case of identical shapes.\n    int64_t elements = 1;\n    const int rank = x[0].size();\n    output_.resize(rank);\n    for (int i = 0; i < rank; i++) {\n      const int64_t dim = x[0][i];\n      elements = mul_dims(elements, dim);\n      output_[i] = dim;\n    }\n    result_.push_back(elements);\n    output_batch_size_ = elements;\n    for (int i = 0; i < N; ++i) {\n      reshape_[i].push_back(elements);\n      bcast_[i].push_back(1);\n    }\n    // grad_reduce_ is left as empty\n    return;\n  }\n\n  // Reverse all the shapes for convenience\n  // After the reverse, 0-th is the inner-most dimension.\n  Vec copy[N];\n  for (int i = 0; i < N; ++i) {\n    copy[i] = x[i];\n    Reverse(&copy[i]);\n  }\n\n  // 1-extend and align all vectors.\n  for (int i = 0; i < N; ++i) {\n    if (copy[i].size() < largest_rank) {\n      copy[i].resize(largest_rank, 1);\n    }\n  }\n  // Going through each dimension starting from the inner-most\n  // dimension, compares dimension of x and y. They are compatible if\n  // they are equal or either is 1.\n\n  // indices of j-th component of each input.\n  bool prev_is_one[N];\n  bool current_is_one[N];\n  for (int i = 0; i < N; ++i) {\n    prev_is_one[i] = false;\n    current_is_one[i] = false;\n  }\n  Vec output;\n  bool output_dim_set = false;\n  int64_t output_dim = -1;\n  bool none_is_one = true;\n  bool set_one = false;\n  for (int j = 0; j < largest_rank; ++j) {\n    output_dim = -1;\n    output_dim_set = false;\n    none_is_one = true;\n    // Find which indices are 1.\n    for (int i = 0; i < N; ++i) {\n      // Keep track of which indices are 1.\n      if (copy[i][j] == 1) {\n        current_is_one[i] = true;\n        none_is_one = false;\n      } else {\n        current_is_one[i] = false;\n        if (!output_dim_set || copy[i][j] == output_dim) {\n          output_dim = copy[i][j];\n          output_dim_set = true;\n        } else {\n          valid_ = false;\n          return;\n        }\n      }\n    }\n    output_.push_back(output_dim_set ? output_dim : 1);\n    output_batch_size_ = mul_dims(output_batch_size_, output_.back());\n    // All dimensions are 1.\n    if (!output_dim_set) {\n      if (!TF_PREDICT_TRUE(fewer_dims_optimization)) {\n        for (int i = 0; i < N; ++i) {\n          bcast_[i].push_back(1);\n          reshape_[i].push_back(1);\n        }\n        result_.push_back(1);\n      }\n      for (int i = 0; i < N; ++i) {\n        grad_reduce_idx_[i].push_back(largest_rank - 1 - j);\n      }\n      // This will skip updating the previous state to the current one. We'll\n      // explain why this is safe below.\n      // Consider the previous state P, current state C and the next state N.\n      // In the case where N also is all ones (N == C), we'll do the same\n      // optimization here (push back one dimensions if we need to), which is\n      // safe and is expected.\n      //\n      // When N != C, we'll continue as usual. However, we might trigger the\n      // next block if N == P (because we didn't update the previous state).\n      // We trigger the next block if `fewer_dims_optimization` is true.\n      // This means that we did not modify and broadcast / reshapes in this\n      // block (we skipped updating, since the one dimensions can be ignored).\n      // In essence, we only need to check whether the previous non-one state is\n      // equal to the current non-one state.\n\n      continue;\n    } else if (TF_PREDICT_TRUE(fewer_dims_optimization) &&\n               std::equal(current_is_one, current_is_one + N, prev_is_one) &&\n               set_one) {\n      // It is a run of the same broadcasting case as last time.\n      // We can reshape the input so that fewer dimensions\n      // are involved in the intermediate computation.\n      result_.back() = mul_dims(result_.back(), output_dim);\n      for (int i = 0; i < N; ++i) {\n        reshape_[i].back() = mul_dims(reshape_[i].back(), copy[i][j]);\n        bcast_[i].back() =\n            mul_dims(bcast_[i].back(), current_is_one[i] ? output_dim : 1);\n        if (current_is_one[i] && !none_is_one) {\n          grad_reduce_idx_[i].push_back(largest_rank - 1 - j);\n        }\n      }\n    } else {\n      result_.push_back(output_dim);\n      for (int i = 0; i < N; ++i) {\n        reshape_[i].push_back(copy[i][j]);\n        bcast_[i].push_back(current_is_one[i] ? output_dim : 1);\n        if (current_is_one[i] && !none_is_one) {\n          grad_reduce_idx_[i].push_back(largest_rank - 1 - j);\n        }\n      }\n    }\n    set_one = true;\n    for (int i = 0; i < N; ++i) {\n      prev_is_one[i] = current_is_one[i];\n    }\n  }\n  if (result_.empty()) {\n    result_.push_back(1);\n    for (int i = 0; i < N; ++i) {\n      reshape_[i].push_back(1);\n      bcast_[i].push_back(1);\n    }\n  }\n  // Do something about batches.\n  for (int i = 0; i < N; ++i) {\n    Reverse(&reshape_[i]);\n    Reverse(&bcast_[i]);\n    Reverse(&grad_reduce_idx_[i]);\n  }\n  Reverse(&result_);\n  Reverse(&output_);\n  // Only compute batch indices when we need broadcasting, and we aren't doing\n  // needless work (when the output size is 0 or the\n  // return_flattened_batch_indices isn't enabled).\n  if (return_flattened_batch_indices && broadcasting_required_ &&\n      output_batch_size_ > 0) {\n    for (int i = 0; i < N; ++i) {\n      ComputeBatchIndices(output_batch_size_, reshape_[i], bcast_[i],\n                          &batch_indices_[i]);\n    }\n  }\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n\n    const Tensor& x_min = context->input(1);\n    const Tensor& x_max = context->input(2);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_min.shape()),\n                errors::InvalidArgument(\"`x_min` must be rank 0 but is rank \",\n                                        x_min.dims()));\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_max.shape()),\n                errors::InvalidArgument(\"`x_max` must be rank 0 but is rank \",\n                                        x_max.dims()));\n    float input_min = x_min.scalar<float>()();\n    float input_max = x_max.scalar<float>()();\n    float input_scale = (input_max - input_min) / 255.0f;\n\n    OP_REQUIRES(context, input_min < input_max,\n                errors::InvalidArgument(\n                    \"input_min must be less than input_max : \", input_min,\n                    \" >= \", input_max));\n\n    auto input_tensor = input.tensor<quint8, 4>();\n    auto N = input_tensor.dimension(0);\n    auto H = input_tensor.dimension(1);\n    auto W = input_tensor.dimension(2);\n    auto C = input_tensor.dimension(3);\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &output));\n\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));\n\n    typedef TTypes<float>::Tensor::Index Index;\n\n    const Eigen::IndexList<Eigen::type2index<1>, Eigen::type2index<2>>\n        reduction_indices;\n    Eigen::IndexList<Eigen::type2index<1>, Index, Index, Eigen::type2index<1>>\n        broadcast_spec;\n    broadcast_spec.set(1, H);\n    broadcast_spec.set(2, W);\n    Eigen::IndexList<Index, Eigen::type2index<1>, Eigen::type2index<1>, Index>\n        expand_spec;\n    expand_spec.set(0, N);\n    expand_spec.set(3, C);\n\n    Eigen::Tensor<float, 2, Eigen::RowMajor> float_mean(N, C);\n    Eigen::Tensor<float, 2, Eigen::RowMajor> float_variance(N, C);\n\n#ifdef USE_NEON\n    if (N == 1 && (C % 16 == 0)) {\n      VLOG(2) << \"Calling optimized\";\n      ColMeanAndVariance(reinterpret_cast<const uint8_t*>(input_tensor.data()),\n                         H * W, C, float_mean.data(), float_variance.data());\n\n      float minimum = given_y_min_, maximum = given_y_max_;\n      if (!output_range_given_) {\n        MinAndMax(reinterpret_cast<const uint8_t*>(input_tensor.data()), H * W,\n                  C, float_mean.data(), float_variance.data(),\n                  variance_epsilon_, &minimum, &maximum);\n      }\n\n      if (maximum - minimum < min_separation_) {\n        maximum = minimum + min_separation_;\n      }\n\n      InstanceNorm(reinterpret_cast<const uint8_t*>(input_tensor.data()), H * W,\n                   C, float_mean.data(), float_variance.data(),\n                   variance_epsilon_, minimum, maximum,\n                   reinterpret_cast<uint8_t*>(output->flat<quint8>().data()));\n      output_min->scalar<float>()() = minimum;\n      output_max->scalar<float>()() = maximum;\n    } else  // NOLINT(readability/braces)\n#endif\n    {\n      VLOG(2) << \"Calling unoptimized\";\n      float_mean = input_tensor.cast<float>().reduce(\n          reduction_indices, Eigen::internal::MeanReducer<float>());\n\n      float_variance =\n          (input_scale *\n           ((input_tensor.cast<float>() -\n             float_mean.reshape(expand_spec).broadcast(broadcast_spec))))\n              .square()\n              .reduce(reduction_indices, Eigen::internal::MeanReducer<float>());\n\n      Eigen::Tensor<float, 4, Eigen::RowMajor> instance_normed =\n          input_scale *\n          (input_tensor.cast<float>() -\n           float_mean.reshape(expand_spec).broadcast(broadcast_spec)) *\n          (float_variance + variance_epsilon_)\n              .rsqrt()\n              .reshape(expand_spec)\n              .broadcast(broadcast_spec);\n\n      Eigen::Tensor<float, 0, Eigen::RowMajor> normed_min;\n      Eigen::Tensor<float, 0, Eigen::RowMajor> normed_max;\n\n      if (!output_range_given_) {\n        normed_min = instance_normed.minimum();\n        normed_max = instance_normed.maximum();\n      } else {\n        normed_min() = given_y_min_;\n        normed_max() = given_y_max_;\n      }\n\n      if (normed_max() - normed_min() < min_separation_) {\n        normed_max() = normed_min() + min_separation_;\n      }\n\n      FloatToQuantizedStruct<quint8> output_f2q(normed_min(), normed_max());\n      auto instance_normed_quantized =\n          QUANTIZE_WITH_EIGEN(instance_normed, output_f2q, quint8);\n\n      output->tensor<quint8, 4>().device(\n          context->template eigen_device<CPUDevice>()) =\n          instance_normed_quantized;\n      output_min->flat<float>()(0) = normed_min();\n      output_max->flat<float>()(0) = normed_max();\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_pyinfo():\n    try:\n        import distro\n        yield _('OS Version'), '%s %s (%s %s)' % (platform.system(), platform.release(), distro.linux_distribution()[0].capitalize(), distro.linux_distribution()[1])\n    except:\n        yield _('OS Version'), '%s %s' % (platform.system(), platform.release())\n    if hasattr(os, 'path'):\n        yield _('OS Path'), os.environ['PATH']\n    if hasattr(sys, 'version'):\n        yield _('Python Version'), ''.join(sys.version)\n    if hasattr(sys, 'subversion'):\n        yield _('Python Subversion'), ', '.join(sys.subversion)\n    if hasattr(sys, 'prefix'):\n        yield _('Python Prefix'), sys.prefix\n    if hasattr(sys, 'executable'):\n        yield _('Python Executable'), sys.executable\n    if hasattr(sys, 'path'):\n        yield _('Python Path'), ', '.join(sys.path)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def crypted_transfer_decode_dictentry(\n        self,\n        load,\n        dictkey=None,\n        timeout=60,\n    ):\n        nonce = uuid.uuid4().hex\n        load[\"nonce\"] = nonce\n        if not self.auth.authenticated:\n            yield self.auth.authenticate()\n        ret = yield self.transport.send(\n            self._package_load(self.auth.crypticle.dumps(load)),\n            timeout=timeout,\n        )\n        key = self.auth.get_keys()\n        if \"key\" not in ret:\n            # Reauth in the case our key is deleted on the master side.\n            yield self.auth.authenticate()\n            ret = yield self.transport.send(\n                self._package_load(self.auth.crypticle.dumps(load)),\n                timeout=timeout,\n            )\n        if HAS_M2:\n            aes = key.private_decrypt(ret[\"key\"], RSA.pkcs1_oaep_padding)\n        else:\n            cipher = PKCS1_OAEP.new(key)\n            aes = cipher.decrypt(ret[\"key\"])\n\n        # Decrypt using the public key.\n        pcrypt = salt.crypt.Crypticle(self.opts, aes)\n        signed_msg = pcrypt.loads(ret[dictkey])\n\n        # Validate the master's signature.\n        if not self.verify_signature(signed_msg[\"data\"], signed_msg[\"sig\"]):\n            raise salt.crypt.AuthenticationError(\n                \"Pillar payload signature failed to validate.\"\n            )\n\n        # Make sure the signed key matches the key we used to decrypt the data.\n        data = salt.payload.loads(signed_msg[\"data\"])\n        if data[\"key\"] != ret[\"key\"]:\n            raise salt.crypt.AuthenticationError(\"Key verification failed.\")\n\n        # Validate the nonce.\n        if data[\"nonce\"] != nonce:\n            raise salt.crypt.AuthenticationError(\"Pillar nonce verification failed.\")\n        raise salt.ext.tornado.gen.Return(data[\"pillar\"])\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def group_title(value: str) -> str:\n    value = re.sub(r\"([A-Z]{2,})([A-Z][a-z]|[ \\-_]|$)\", lambda m: m.group(1).title() + m.group(2), value.strip())\n    value = re.sub(r\"(^|[ _-])([A-Z])\", lambda m: m.group(1) + m.group(2).lower(), value)\n    return value\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def datetime_cast_date_sql(self, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"TRUNC({sql})\", params\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "static void ClearMetadata(VP8LMetadata* const hdr) {\n  assert(hdr != NULL);\n\n  WebPSafeFree(hdr->huffman_image_);\n  VP8LHuffmanTablesDeallocate(&hdr->huffman_tables_);\n  VP8LHtreeGroupsFree(hdr->htree_groups_);\n  VP8LColorCacheClear(&hdr->color_cache_);\n  VP8LColorCacheClear(&hdr->saved_color_cache_);\n  InitMetadata(hdr);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def load():\n    global conn\n    global cursor\n    database_name = f\"{autocomplete_filepath}{os.path.sep}{RTXConfig.autocomplete_path.split('/')[-1]}\"\n    conn = sqlite3.connect(database_name)\n    cursor = conn.cursor()\n    #print(f\"INFO: Connected to {database_name}\",file=sys.stderr)\n    return True\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_search_columns_list(self):\n        \"\"\"\n        Returns a list of searchable columns names\n        \"\"\"\n        return []\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def test_verify_signed_xsw(self, mock_validate_on_or_after):\n        self.ar.issue_instant_ok = Mock(return_value=True)\n\n        with open(XML_RESPONSE_XSW) as fp:\n            xml_response = fp.read()\n\n        self.ar.outstanding_queries = {\"id12\": \"http://localhost:8088/sso\"}\n        self.ar.timeslack = 10000\n        self.ar.loads(xml_response, decode=False)\n\n        assert self.ar.came_from == 'http://localhost:8088/sso'\n        assert self.ar.session_id() == \"id12\"\n        assert self.ar.issuer() == 'urn:mace:example.com:saml:roland:idp'\n\n        with raises(SignatureError):\n            self.ar.verify()\n\n        assert self.ar.ava is None\n        assert self.ar.name_id is None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def __call__(self, scope, receive, send):\n        \"\"\"\n        Async entrypoint - uses the sync_to_async wrapper to run things in a\n        threadpool.\n        \"\"\"\n        if scope[\"type\"] != \"http\":\n            raise ValueError(\n                \"The AsgiHandler can only handle HTTP connections, not %s\"\n                % scope[\"type\"]\n            )\n\n        # Receive the HTTP request body as a stream object.\n        try:\n            body_stream = await self.read_body(receive)\n        except RequestAborted:\n            return\n        # Launch into body handling (and a synchronous subthread).\n        await self.handle(scope, async_to_sync(send), body_stream)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def testzip(self) -> Optional[str]:\n        self.fp.seek(self.afterheader)\n        self.worker = Worker(self.files, self.afterheader, self.header, self.mp)\n        for f in self.files:\n            self.worker.register_filelike(f.id, None)\n        try:\n            self.worker.extract(\n                self.fp, None, parallel=(not self.password_protected), skip_notarget=False\n            )  # TODO: print progress\n        except CrcError as crce:\n            return crce.args[2]\n        else:\n            return None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def process_response(self, request, response):\n        if settings.DEBUG:\n            response.headers[\"X-Frame-Options\"] = \"DENY\"\n            response.headers[\"X-Content-Type-Options\"] = \"nosniff\"\n            response.headers[\n                \"Content-Security-Policy\"\n            ] = \"script-src 'self' cdn.crowdin.com;\"\n\n        return response\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _convert_ebook_format(self):\n        error_message = None\n        local_db = db.CalibreDB(expire_on_commit=False)\n        file_path = self.file_path\n        book_id = self.bookid\n        format_old_ext = u'.' + self.settings['old_book_format'].lower()\n        format_new_ext = u'.' + self.settings['new_book_format'].lower()\n\n        # check to see if destination format already exists - or if book is in database\n        # if it does - mark the conversion task as complete and return a success\n        # this will allow send to kindle workflow to continue to work\n        if os.path.isfile(file_path + format_new_ext) or\\\n            local_db.get_book_format(self.bookid, self.settings['new_book_format']):\n            log.info(\"Book id %d already converted to %s\", book_id, format_new_ext)\n            cur_book = local_db.get_book(book_id)\n            self.title = cur_book.title\n            self.results['path'] = file_path\n            self.results['title'] = self.title\n            self._handleSuccess()\n            local_db.session.close()\n            return os.path.basename(file_path + format_new_ext)\n        else:\n            log.info(\"Book id %d - target format of %s does not exist. Moving forward with convert.\",\n                     book_id,\n                     format_new_ext)\n\n        if config.config_kepubifypath and format_old_ext == '.epub' and format_new_ext == '.kepub':\n            check, error_message = self._convert_kepubify(file_path,\n                                                          format_old_ext,\n                                                          format_new_ext)\n        else:\n            # check if calibre converter-executable is existing\n            if not os.path.exists(config.config_converterpath):\n                # ToDo Text is not translated\n                self._handleError(_(u\"Calibre ebook-convert %(tool)s not found\", tool=config.config_converterpath))\n                return\n            check, error_message = self._convert_calibre(file_path, format_old_ext, format_new_ext)\n\n        if check == 0:\n            cur_book = local_db.get_book(book_id)\n            if os.path.isfile(file_path + format_new_ext):\n                new_format = db.Data(name=cur_book.data[0].name,\n                                         book_format=self.settings['new_book_format'].upper(),\n                                         book=book_id, uncompressed_size=os.path.getsize(file_path + format_new_ext))\n                try:\n                    local_db.session.merge(new_format)\n                    local_db.session.commit()\n                    if self.settings['new_book_format'].upper() in ['KEPUB', 'EPUB', 'EPUB3']:\n                        ub_session = ini()\n                        remove_synced_book(book_id, True, ub_session)\n                        ub_session.close()\n                except SQLAlchemyError as e:\n                    local_db.session.rollback()\n                    log.error(\"Database error: %s\", e)\n                    local_db.session.close()\n                    self._handleError(error_message)\n                    return\n                self.results['path'] = cur_book.path\n                self.title = cur_book.title\n                self.results['title'] = self.title\n                if not config.config_use_google_drive:\n                    self._handleSuccess()\n                return os.path.basename(file_path + format_new_ext)\n            else:\n                error_message = _('%(format)s format not found on disk', format=format_new_ext.upper())\n        local_db.session.close()\n        log.info(\"ebook converter failed with error while converting book\")\n        if not error_message:\n            error_message = _('Ebook converter failed with unknown error')\n        self._handleError(error_message)\n        return\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def encrypt(self, password):\n        raise NotImplementedError()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def marshall_table_full(table_dict: Dict) -> Dict:\n    \"\"\"\n    Forms the full version of a table Dict, with additional and sanitized fields\n    :param table_dict: Table Dict from metadata service\n    :return: Table Dict with sanitized fields\n    \"\"\"\n\n    schema = TableSchema(strict=True)\n    # TODO: consider migrating to validate() instead of roundtripping\n    table: Table = schema.load(table_dict).data\n    results: Dict[str, Any] = schema.dump(table).data\n\n    results['is_editable'] = is_table_editable(results['schema'], results['name'])\n\n    # TODO - Cleanup https://github.com/lyft/amundsen/issues/296\n    #  This code will try to supplement some missing data since the data here is incomplete.\n    #  Once the metadata service response provides complete user objects we can remove this.\n    results['owners'] = [_map_user_object_to_schema(owner) for owner in results['owners']]\n    readers = results['table_readers']\n    for reader_object in readers:\n        reader_object['user'] = _map_user_object_to_schema(reader_object['user'])\n\n    columns = results['columns']\n    for col in columns:\n        # Set editable state\n        col['is_editable'] = is_editable\n        # If order is provided, we sort the column based on the pre-defined order\n        if app.config['COLUMN_STAT_ORDER']:\n            # the stat_type isn't defined in COLUMN_STAT_ORDER, we just use the max index for sorting\n            col['stats'].sort(key=lambda x: app.config['COLUMN_STAT_ORDER'].\n                              get(x['stat_type'], len(app.config['COLUMN_STAT_ORDER'])))\n\n    # TODO: Add the 'key' or 'id' to the base TableSchema\n    results['key'] = f'{table.database}://{table.cluster}.{table.schema}/{table.name}'\n    # Temp code to make 'partition_key' and 'partition_value' part of the table\n    results['partition'] = _get_partition_data(results['watermarks'])\n\n    # We follow same style as column stat order for arranging the programmatic descriptions\n    prog_descriptions = results['programmatic_descriptions']\n    results['programmatic_descriptions'] = _convert_prog_descriptions(prog_descriptions)\n\n    return results\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def _populate_template(\n        self,\n        template_path: str,\n        heading: Optional[str] = None,\n        description: Optional[str] = None,\n        data: Optional[Dict[str, Any]] = None,\n    ) -> str:\n        \"\"\"Generates a file from the template and data\"\"\"\n        report_data = {\n            \"heading\": heading,\n            \"description\": description,\n            \"data\": data,\n            \"request\": self.request_data,\n        }\n        report_data.update(self.template_data)\n        template = self.template_loader.get_template(template_path)\n        rendered_template = template.render(report_data)\n        return rendered_template\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def confirm_sns_subscription(notification):\n    logger.info(\n        \"Received subscription confirmation: TopicArn: %s\",\n        notification.get(\"TopicArn\"),\n        extra={\n            \"notification\": notification,\n        },\n    )\n\n    # Get the subscribe url and hit the url to confirm the subscription.\n    subscribe_url = notification.get(\"SubscribeURL\")\n    try:\n        urlopen(subscribe_url).read()\n    except URLError as e:\n        # Some kind of error occurred when confirming the request.\n        logger.error(\n            'Could not confirm subscription: \"%s\"',\n            e,\n            extra={\n                \"notification\": notification,\n            },\n            exc_info=True,\n        )\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def handler(request, _operation, _current_url):\n    \"\"\"Handler for a file:// URL.\n\n    Args:\n        request: QNetworkRequest to answer to.\n        _operation: The HTTP operation being done.\n        _current_url: The page we're on currently.\n\n    Return:\n        A QNetworkReply for directories, None for files.\n    \"\"\"\n    path = request.url().toLocalFile()\n    try:\n        if os.path.isdir(path):\n            data = dirbrowser_html(path)\n            return networkreply.FixedDataNetworkReply(\n                request, data, 'text/html')\n        return None\n    except UnicodeEncodeError:\n        return None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, database_url):\n        self._ss = SAML2SessionStore(database_url=database_url)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n\n    const Tensor& input_min = ctx->input(1);\n    const Tensor& input_max = ctx->input(2);\n    const Tensor& requested_output_min = ctx->input(3);\n    const Tensor& requested_output_max = ctx->input(4);\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsScalar(input_min.shape()),\n        errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",\n                                input_min.dims()));\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsScalar(input_max.shape()),\n        errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",\n                                input_max.dims()));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_min.shape()),\n                errors::InvalidArgument(\n                    \"`requested_output_min` must be rank 0 but is rank \",\n                    requested_output_min.dims()));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_max.shape()),\n                errors::InvalidArgument(\n                    \"`requested_output_max` must be rank 0 but is rank \",\n                    requested_output_max.dims()));\n\n    const float input_min_float = input_min.flat<float>()(0);\n    const float input_max_float = input_max.flat<float>()(0);\n    const float requested_output_min_float =\n        requested_output_min.flat<float>()(0);\n    const float requested_output_max_float =\n        requested_output_max.flat<float>()(0);\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(2, TensorShape({}), &output_max));\n\n    OP_REQUIRES(\n        ctx, requested_output_min_float <= 0.0f,\n        errors::InvalidArgument(\"requested_output_min must be <= 0, but got \",\n                                requested_output_min_float));\n    OP_REQUIRES(\n        ctx, requested_output_max_float >= requested_output_min_float,\n        errors::InvalidArgument(\n            \"requested_output_max must be >= requested_output_min, but got \",\n            requested_output_max_float, \" and \", requested_output_min_float));\n\n    auto input_array = input.flat<T1>();\n\n#if 0\n    // This is the reference, non-eigen implementation:\n    auto output_array = output->flat<T2>();\n    RequantizeManyInNewRange<T1, T2>(\n        input_array.data(), input_array.size(),\n        input_min_float, input_max_float,\n        requested_output_min_float, requested_output_max_float,\n        output_array.data());\n#endif\n\n    if (input_array.size() > 0) {\n      if (meta::IsSupportedAndEnabled() && std::is_same<T1, qint32>() &&\n          std::is_same<T2, quint8>()) {\n        auto input_i32_array = input.flat<qint32>();\n        meta::Requantize(ctx, input_i32_array.data(), input_i32_array.size(),\n                         input_min_float, input_max_float,\n                         requested_output_min_float, requested_output_max_float,\n                         output->flat<quint8>().data());\n      } else {\n        RequantizeManyInNewRangeUsingEigen<T1, T2>(\n            ctx->eigen_device<CPUDevice>(), input, input_min_float,\n            input_max_float, requested_output_min_float,\n            requested_output_max_float, output);\n      }\n    }\n\n    output_min->flat<float>().setConstant(requested_output_min_float);\n    output_max->flat<float>().setConstant(requested_output_max_float);\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def stable_request(url, headers):\n    \"\"\"\n    guarantees crash-proof HTTP(S) requests\n    returns None in case of failure, returns a \"response\" object otherwise\n    \"\"\"\n    parsed = urlparse(url)\n    redirects_allowed = False if mem.var['disable_redirects'] else True\n    scheme, host, path = parsed.scheme, parsed.netloc, parsed.path\n    schemes = (['https', 'http'] if scheme == 'https' else ['http', 'https'])\n    for scheme in schemes:\n        try:\n            response = requests.get(\n                scheme + '://' + host + path,\n                headers=headers,\n                verify=False,\n                timeout=10,\n                allow_redirects=redirects_allowed)\n            return response.url\n        except Exception as e:\n            if 'ConnectionError' not in str(e):\n                continue\n        return None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __init__(self, hs):\n        self.hs = hs\n        self.auth = hs.get_auth()\n        self.client = hs.get_federation_http_client()\n        self.clock = hs.get_clock()\n        self.server_name = hs.hostname\n        self.store = hs.get_datastore()\n        self.max_upload_size = hs.config.max_upload_size\n        self.max_image_pixels = hs.config.max_image_pixels\n\n        self.primary_base_path = hs.config.media_store_path\n        self.filepaths = MediaFilePaths(self.primary_base_path)\n\n        self.dynamic_thumbnails = hs.config.dynamic_thumbnails\n        self.thumbnail_requirements = hs.config.thumbnail_requirements\n\n        self.remote_media_linearizer = Linearizer(name=\"media_remote\")\n\n        self.recently_accessed_remotes = set()\n        self.recently_accessed_locals = set()\n\n        self.federation_domain_whitelist = hs.config.federation_domain_whitelist\n\n        # List of StorageProviders where we should search for media and\n        # potentially upload to.\n        storage_providers = []\n\n        for clz, provider_config, wrapper_config in hs.config.media_storage_providers:\n            backend = clz(hs, provider_config)\n            provider = StorageProviderWrapper(\n                backend,\n                store_local=wrapper_config.store_local,\n                store_remote=wrapper_config.store_remote,\n                store_synchronous=wrapper_config.store_synchronous,\n            )\n            storage_providers.append(provider)\n\n        self.media_storage = MediaStorage(\n            self.hs, self.primary_base_path, self.filepaths, storage_providers\n        )\n\n        self.clock.looping_call(\n            self._start_update_recently_accessed, UPDATE_RECENTLY_ACCESSED_TS\n        )\n"}, {"role": "assistant", "content": "False"}]}
