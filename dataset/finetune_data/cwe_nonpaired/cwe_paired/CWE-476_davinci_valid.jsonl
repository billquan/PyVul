{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& val = ctx->input(0);\n    int64 id = ctx->session_state()->GetNewId();\n    TensorStore::TensorAndKey tk{val, id, requested_device()};\n    OP_REQUIRES_OK(ctx, ctx->tensor_store()->AddTensor(name(), tk));\n\n    Tensor* handle = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({}), &handle));\n    if (ctx->expected_output_dtype(0) == DT_RESOURCE) {\n      ResourceHandle resource_handle = MakeResourceHandle<Tensor>(\n          ctx, SessionState::kTensorHandleResourceTypeName,\n          tk.GetHandle(name()));\n      resource_handle.set_maybe_type_name(\n          SessionState::kTensorHandleResourceTypeName);\n      handle->scalar<ResourceHandle>()() = resource_handle;\n    } else {\n      // Legacy behavior in V1.\n      handle->flat<tstring>().setConstant(tk.GetHandle(name()));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs(\n    OpKernelContext* context, TensorInputs* inputs,\n    TensorShapes* input_matrix_shapes, TensorShape* batch_shape) {\n  int input_rank = -1;\n  for (int i = 0; i < NumMatrixInputs(context); ++i) {\n    const Tensor& in = context->input(i);\n    if (i == 0) {\n      input_rank = in.dims();\n      OP_REQUIRES(\n          context, input_rank >= 2,\n          errors::InvalidArgument(\"Input tensor \", i,\n                                  \" must have rank >= 2, got \", input_rank));\n      // If the tensor rank is greater than 2, we consider the inner-most\n      // dimensions as matrices, and loop over all the other outer (\"batch\")\n      // dimensions to compute the results.\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        batch_shape->AddDim(in.dim_size(dim));\n      }\n    } else {\n      // Make sure that all inputs have the same rank and outer dimensions.\n      OP_REQUIRES(context, input_rank == in.dims(),\n                  errors::InvalidArgument(\n                      \"All input tensors must have the same rank.\"));\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        OP_REQUIRES(\n            context, in.dim_size(dim) == batch_shape->dim_size(dim),\n            errors::InvalidArgument(\n                \"All input tensors must have the same outer dimensions.\"));\n      }\n    }\n\n    const int row_dimension = input_rank - 2;\n    const int col_dimension = input_rank - 1;\n    const int64_t num_rows = in.dim_size(row_dimension);\n    const int64_t num_cols = in.dim_size(col_dimension);\n    input_matrix_shapes->emplace_back(\n        std::initializer_list<int64_t>({num_rows, num_cols}));\n    inputs->emplace_back(&in);\n  }\n  // Have the derived class validate that the inputs are as expected.\n  ValidateInputMatrixShapes(context, *input_matrix_shapes);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const float min_input = context->input(1).flat<float>()(0);\n    const float max_input = context->input(2).flat<float>()(0);\n    MaxPoolingOp<Device, T>::Compute(context);\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));\n    output_min->flat<float>()(0) = min_input;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));\n    output_max->flat<float>()(0) = max_input;\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status KernelAndDeviceOp::Run(\n    ScopedStepContainer* step_container, const EagerKernelArgs& inputs,\n    std::vector<EagerKernelRet>* outputs,\n    CancellationManager* cancellation_manager,\n    const absl::optional<EagerRemoteFunctionParams>& remote_func_params) {\n  OpKernelContext::Params params;\n  params.device = device_;\n  params.frame_iter = FrameAndIter(0, 0);\n  params.inputs = inputs.GetTensorValues();\n  params.op_kernel = kernel_.get();\n  params.resource_manager = device_->resource_manager();\n  params.input_alloc_attrs = &input_alloc_attrs_;\n  params.output_attr_array = output_alloc_attrs_.data();\n  params.function_library = flr_;\n  params.slice_reader_cache = &slice_reader_cache_;\n  params.rendezvous = rendezvous_;\n  OpExecutionState* op_execution_state = nullptr;\n\n  CancellationManager default_cancellation_manager;\n  if (cancellation_manager) {\n    params.cancellation_manager = cancellation_manager;\n  } else if (kernel_->is_deferred()) {\n    op_execution_state = new OpExecutionState;\n    params.cancellation_manager = &op_execution_state->cancellation_manager;\n    params.inc_num_deferred_ops_function = [op_execution_state]() {\n      op_execution_state->Ref();\n    };\n    params.dec_num_deferred_ops_function = [op_execution_state]() {\n      op_execution_state->Unref();\n    };\n  } else {\n    params.cancellation_manager = &default_cancellation_manager;\n  }\n\n  params.log_memory = log_memory_;\n\n  params.runner = get_runner();\n\n  params.step_container =\n      step_container == nullptr ? &step_container_ : step_container;\n  auto step_container_cleanup = gtl::MakeCleanup([step_container, this] {\n    if (step_container == nullptr) {\n      this->step_container_.CleanUp();\n    }\n  });\n\n  params.collective_executor =\n      collective_executor_ ? collective_executor_->get() : nullptr;\n\n  OpKernelContext context(&params);\n\n  {\n    port::ScopedFlushDenormal flush;\n    port::ScopedSetRound round(FE_TONEAREST);\n    // 'AnnotatedTraceMe' will trace both scheduling time on host and execution\n    // time on device of the OpKernel.\n    profiler::AnnotatedTraceMe activity(\n        [&] { return kernel_->TraceString(context, /*verbose=*/false); },\n        profiler::TraceMeLevel::kInfo);\n    device_->Compute(kernel_.get(), &context);\n  }\n\n  // Clean up execution op_execution_state if deferred ops aren't running.\n  if (op_execution_state != nullptr) {\n    op_execution_state->Unref();\n  }\n\n  if (!context.status().ok()) return context.status();\n\n  if (outputs != nullptr) {\n    outputs->clear();\n    for (int i = 0; i < context.num_outputs(); ++i) {\n      outputs->push_back(Tensor(*context.mutable_output(i)));\n    }\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor x = context->input(0);\n    const Tensor& scale = context->input(1);\n    const Tensor& offset = context->input(2);\n    const Tensor& estimated_mean = context->input(3);\n    const Tensor& estimated_variance = context->input(4);\n    const Tensor* side_input = has_side_input_ ? &context->input(5) : nullptr;\n\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(context, offset.dims() == 1,\n                errors::InvalidArgument(\"offset must be 1-dimensional\",\n                                        offset.shape().DebugString()));\n    OP_REQUIRES(context, estimated_mean.dims() == 1,\n                errors::InvalidArgument(\"estimated_mean must be 1-dimensional\",\n                                        estimated_mean.shape().DebugString()));\n    OP_REQUIRES(\n        context, estimated_variance.dims() == 1,\n        errors::InvalidArgument(\"estimated_variance must be 1-dimensional\",\n                                estimated_variance.shape().DebugString()));\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64 in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64 in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64 in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64 in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64 in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    if (has_side_input_) {\n      OP_REQUIRES(context, side_input->shape() == x.shape(),\n                  errors::InvalidArgument(\n                      \"side_input shape must be equal to input shape: \",\n                      side_input->shape().DebugString(),\n                      \" != \", x.shape().DebugString()));\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity) {\n      // NOTE(ezhulenev): This requirement is coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx.\n      OP_REQUIRES(\n          context, !is_training_ || x.dim_size(3) % 4 == 0,\n          errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                  \"channel dimension to be a multiple of 4.\"));\n    }\n\n    Tensor* y = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, alloc_shape, &y));\n\n    Tensor* batch_mean = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {3}, 1, scale.shape(), &batch_mean));\n    Tensor* batch_var = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {4}, 2, scale.shape(), &batch_var));\n    Tensor* saved_mean = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(3, scale.shape(), &saved_mean));\n    Tensor* saved_maybe_inv_var = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(4, scale.shape(),\n                                                     &saved_maybe_inv_var));\n\n    if (is_training_) {\n      functor::FusedBatchNorm<Device, T, U, true>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    } else {\n      functor::FusedBatchNorm<Device, T, U, false>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, y->CopyFrom(*y, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // inputs must be at least a matrix\n    OP_REQUIRES(\n        ctx, sorted_inputs_t.shape().dims() >= 2,\n        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n    OP_REQUIRES_OK(\n        ctx, functor::LowerBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "string SummarizeArray<bool>(int64_t limit, int64_t num_elts,\n                            const TensorShape& tensor_shape, const char* data,\n                            const bool print_v2) {\n  // We first convert all chars to be 0/1 to not get InvalidEnumValue sanitizer\n  // error\n  auto mutable_data = std::unique_ptr<char[]>(new char[num_elts]);\n  for (int64_t i = 0; i < num_elts; ++i)\n    mutable_data.get()[i] = data[i] ? 1 : 0;\n  bool* array = reinterpret_cast<bool*>(mutable_data.get());\n  return SummarizeArrayInternal<bool>(limit, num_elts, tensor_shape, array,\n                                      print_v2);\n}\n}  // namespace"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) final {\n    // Extract inputs and validate shapes and types.\n    const CSRSparseMatrix* input_matrix;\n    OP_REQUIRES_OK(ctx, ExtractVariantFromInput(ctx, 0, &input_matrix));\n    const Tensor& input_permutation_indices = ctx->input(1);\n\n    int64 num_rows;\n    int batch_size;\n    ValidateInputs(ctx, *input_matrix, input_permutation_indices, &batch_size,\n                   &num_rows);\n\n    // Allocate batch pointers.\n    Tensor batch_ptr(cpu_allocator(), DT_INT32, TensorShape({batch_size + 1}));\n    auto batch_ptr_vec = batch_ptr.vec<int32>();\n    batch_ptr_vec(0) = 0;\n\n    // Temporary vector of Eigen SparseMatrices to store the Sparse Cholesky\n    // factors.\n    // Note: we use column-compressed (CSC) SparseMatrix because SimplicialLLT\n    // returns the factors in column major format. Since our input should be\n    // symmetric, column major and row major is identical in storage. We just\n    // have to switch to reading the upper triangular part of the input, which\n    // corresponds to the lower triangular part in row major format.\n    std::vector<SparseMatrix> sparse_cholesky_factors(batch_size);\n\n    // TODO(anudhyan): Tune the cost per unit based on benchmarks.\n    const double nnz_per_row =\n        (input_matrix->total_nnz() / batch_size) / num_rows;\n    const int64 sparse_cholesky_cost_per_batch =\n        nnz_per_row * nnz_per_row * num_rows;\n    // Perform sparse Cholesky factorization of each batch in parallel.\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n    std::atomic<int64> invalid_input_index(-1);\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          sparse_cholesky_cost_per_batch,\n          [&](int64 batch_begin, int64 batch_end) {\n            for (int64 batch_index = batch_begin; batch_index < batch_end;\n                 ++batch_index) {\n              // Define an Eigen SparseMatrix Map to operate on the\n              // CSRSparseMatrix component without copying the data.\n              Eigen::Map<const SparseMatrix> sparse_matrix(\n                  num_rows, num_rows, input_matrix->nnz(batch_index),\n                  input_matrix->row_pointers_vec(batch_index).data(),\n                  input_matrix->col_indices_vec(batch_index).data(),\n                  input_matrix->values_vec<T>(batch_index).data());\n\n              Eigen::SimplicialLLT<SparseMatrix, Eigen::Upper,\n                                   Eigen::NaturalOrdering<int>>\n                  solver;\n              auto permutation_indices_flat =\n                  input_permutation_indices.flat<int32>().data();\n\n              // Invert the fill-in reducing ordering and apply it to the input\n              // sparse matrix.\n              Eigen::Map<\n                  Eigen::PermutationMatrix<Eigen::Dynamic, Eigen::Dynamic, int>>\n                  permutation(permutation_indices_flat + batch_index * num_rows,\n                              num_rows);\n              auto permutation_inverse = permutation.inverse();\n\n              SparseMatrix permuted_sparse_matrix;\n              permuted_sparse_matrix.template selfadjointView<Eigen::Upper>() =\n                  sparse_matrix.template selfadjointView<Eigen::Upper>()\n                      .twistedBy(permutation_inverse);\n\n              // Compute the Cholesky decomposition.\n              solver.compute(permuted_sparse_matrix);\n              if (solver.info() != Eigen::Success) {\n                invalid_input_index = batch_index;\n                return;\n              }\n\n              // Get the upper triangular factor, which would end up in the\n              // lower triangular part of the output CSRSparseMatrix when\n              // interpreted in row major format.\n              sparse_cholesky_factors[batch_index] =\n                  std::move(solver.matrixU());\n              // For now, batch_ptr contains the number of nonzeros in each\n              // batch.\n              batch_ptr_vec(batch_index + 1) =\n                  sparse_cholesky_factors[batch_index].nonZeros();\n            }\n          });\n\n    // Check for invalid input.\n    OP_REQUIRES(\n        ctx, invalid_input_index == -1,\n        errors::InvalidArgument(\n            \"Sparse Cholesky factorization failed for batch index \",\n            invalid_input_index.load(), \". The input might not be valid.\"));\n\n    // Compute a cumulative sum to obtain the batch pointers.\n    std::partial_sum(batch_ptr_vec.data(),\n                     batch_ptr_vec.data() + batch_size + 1,\n                     batch_ptr_vec.data());\n\n    // Allocate output Tensors.\n    const int64 total_nnz = batch_ptr_vec(batch_size);\n    Tensor output_row_ptr(cpu_allocator(), DT_INT32,\n                          TensorShape({(num_rows + 1) * batch_size}));\n    Tensor output_col_ind(cpu_allocator(), DT_INT32, TensorShape({total_nnz}));\n    Tensor output_values(cpu_allocator(), DataTypeToEnum<T>::value,\n                         TensorShape({total_nnz}));\n    auto output_row_ptr_ptr = output_row_ptr.flat<int32>().data();\n    auto output_col_ind_ptr = output_col_ind.flat<int32>().data();\n    auto output_values_ptr = output_values.flat<T>().data();\n\n    // Copy the output matrices from each batch into the CSRSparseMatrix\n    // Tensors.\n    // TODO(b/129906419): Factor out the copy from Eigen SparseMatrix to\n    // CSRSparseMatrix into common utils. This is also used in\n    // SparseMatrixSparseMatMul.\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          (3 * total_nnz) / batch_size /* cost per unit */,\n          [&](int64 batch_begin, int64 batch_end) {\n            for (int64 batch_index = batch_begin; batch_index < batch_end;\n                 ++batch_index) {\n              const SparseMatrix& cholesky_factor =\n                  sparse_cholesky_factors[batch_index];\n              const int64 nnz = cholesky_factor.nonZeros();\n\n              std::copy(cholesky_factor.outerIndexPtr(),\n                        cholesky_factor.outerIndexPtr() + num_rows + 1,\n                        output_row_ptr_ptr + batch_index * (num_rows + 1));\n              std::copy(cholesky_factor.innerIndexPtr(),\n                        cholesky_factor.innerIndexPtr() + nnz,\n                        output_col_ind_ptr + batch_ptr_vec(batch_index));\n              std::copy(cholesky_factor.valuePtr(),\n                        cholesky_factor.valuePtr() + nnz,\n                        output_values_ptr + batch_ptr_vec(batch_index));\n            }\n          });\n\n    // Create the CSRSparseMatrix instance from its component Tensors and\n    // prepare the Variant output Tensor.\n    CSRSparseMatrix output_csr_matrix;\n    OP_REQUIRES_OK(\n        ctx,\n        CSRSparseMatrix::CreateCSRSparseMatrix(\n            DataTypeToEnum<T>::value, input_matrix->dense_shape(), batch_ptr,\n            output_row_ptr, output_col_ind, output_values, &output_csr_matrix));\n    Tensor* output_csr_matrix_tensor;\n    AllocatorAttributes cpu_alloc;\n    cpu_alloc.set_on_host(true);\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, TensorShape({}), &output_csr_matrix_tensor,\n                                  cpu_alloc));\n    output_csr_matrix_tensor->scalar<Variant>()() =\n        std::move(output_csr_matrix);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status Examples::CreateSparseFeatureRepresentation(\n    const DeviceBase::CpuWorkerThreads& worker_threads, const int num_examples,\n    const int num_sparse_features, const ModelWeights& weights,\n    const OpInputList& sparse_example_indices_inputs,\n    const OpInputList& sparse_feature_indices_inputs,\n    const OpInputList& sparse_feature_values_inputs,\n    std::vector<Example>* const examples) {\n  mutex mu;\n  Status result;  // Guarded by mu\n  auto parse_partition = [&](const int64 begin, const int64 end) {\n    // The static_cast here is safe since begin and end can be at most\n    // num_examples which is an int.\n    for (int i = static_cast<int>(begin); i < end; ++i) {\n      auto example_indices =\n          sparse_example_indices_inputs[i].template flat<int64>();\n      auto feature_indices =\n          sparse_feature_indices_inputs[i].template flat<int64>();\n\n      // Parse features for each example. Features for a particular example\n      // are at the offsets (start_id, end_id]\n      int start_id = -1;\n      int end_id = 0;\n      for (int example_id = 0; example_id < num_examples; ++example_id) {\n        start_id = end_id;\n        while (end_id < example_indices.size() &&\n               example_indices(end_id) == example_id) {\n          ++end_id;\n        }\n        Example::SparseFeatures* const sparse_features =\n            &(*examples)[example_id].sparse_features_[i];\n        if (start_id < example_indices.size() &&\n            example_indices(start_id) == example_id) {\n          sparse_features->indices.reset(new UnalignedInt64Vector(\n              &(feature_indices(start_id)), end_id - start_id));\n          if (sparse_feature_values_inputs.size() > i) {\n            auto feature_weights =\n                sparse_feature_values_inputs[i].flat<float>();\n            sparse_features->values.reset(new UnalignedFloatVector(\n                &(feature_weights(start_id)), end_id - start_id));\n          }\n          // If features are non empty.\n          if (end_id - start_id > 0) {\n            // TODO(sibyl-Aix6ihai): Write this efficiently using vectorized\n            // operations from eigen.\n            for (int64 k = 0; k < sparse_features->indices->size(); ++k) {\n              const int64 feature_index = (*sparse_features->indices)(k);\n              if (!weights.SparseIndexValid(i, feature_index)) {\n                mutex_lock l(mu);\n                result = errors::InvalidArgument(\n                    \"Found sparse feature indices out of valid range: \",\n                    (*sparse_features->indices)(k));\n                return;\n              }\n            }\n          }\n        } else {\n          // Add a Tensor that has size 0.\n          sparse_features->indices.reset(\n              new UnalignedInt64Vector(&(feature_indices(0)), 0));\n          // If values exist for this feature group.\n          if (sparse_feature_values_inputs.size() > i) {\n            auto feature_weights =\n                sparse_feature_values_inputs[i].flat<float>();\n            sparse_features->values.reset(\n                new UnalignedFloatVector(&(feature_weights(0)), 0));\n          }\n        }\n      }\n    }\n  };\n  // For each column, the cost of parsing it is O(num_examples). We use\n  // num_examples here, as empirically Shard() creates the right amount of\n  // threads based on the problem size.\n  // TODO(sibyl-Aix6ihai): Tune this as a function of dataset size.\n  const int64 kCostPerUnit = num_examples;\n  Shard(worker_threads.num_threads, worker_threads.workers, num_sparse_features,\n        kCostPerUnit, parse_partition);\n  return result;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& val = ctx->input(0);\n    auto session_state = ctx->session_state();\n    OP_REQUIRES(ctx, session_state != nullptr,\n                errors::FailedPrecondition(\n                    \"GetSessionHandle called on null session state\"));\n    int64 id = session_state->GetNewId();\n    TensorStore::TensorAndKey tk{val, id, requested_device()};\n    OP_REQUIRES_OK(ctx, ctx->tensor_store()->AddTensor(name(), tk));\n\n    Tensor* handle = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({}), &handle));\n    if (ctx->expected_output_dtype(0) == DT_RESOURCE) {\n      ResourceHandle resource_handle = MakeResourceHandle<Tensor>(\n          ctx, SessionState::kTensorHandleResourceTypeName,\n          tk.GetHandle(name()));\n      resource_handle.set_maybe_type_name(\n          SessionState::kTensorHandleResourceTypeName);\n      handle->scalar<ResourceHandle>()() = resource_handle;\n    } else {\n      // Legacy behavior in V1.\n      handle->flat<tstring>().setConstant(tk.GetHandle(name()));\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs(\n    OpKernelContext* context, TensorInputs* inputs,\n    TensorShapes* input_matrix_shapes, TensorShape* batch_shape) {\n  int input_rank = -1;\n  for (int i = 0; i < NumMatrixInputs(context); ++i) {\n    const Tensor& in = context->input(i);\n    if (i == 0) {\n      input_rank = in.dims();\n      OP_REQUIRES(\n          context, input_rank >= 2,\n          errors::InvalidArgument(\"Input tensor \", i,\n                                  \" must have rank >= 2, got \", input_rank));\n      // If the tensor rank is greater than 2, we consider the inner-most\n      // dimensions as matrices, and loop over all the other outer (\"batch\")\n      // dimensions to compute the results.\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        batch_shape->AddDim(in.dim_size(dim));\n      }\n    } else {\n      // Make sure that all inputs have the same rank and outer dimensions.\n      OP_REQUIRES(context, input_rank == in.dims(),\n                  errors::InvalidArgument(\n                      \"All input tensors must have the same rank.\"));\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        OP_REQUIRES(\n            context, in.dim_size(dim) == batch_shape->dim_size(dim),\n            errors::InvalidArgument(\n                \"All input tensors must have the same outer dimensions.\"));\n      }\n    }\n\n    const int row_dimension = input_rank - 2;\n    const int col_dimension = input_rank - 1;\n    const int64_t num_rows = in.dim_size(row_dimension);\n    const int64_t num_cols = in.dim_size(col_dimension);\n    input_matrix_shapes->emplace_back(\n        std::initializer_list<int64_t>({num_rows, num_cols}));\n    inputs->emplace_back(&in);\n    OP_REQUIRES(\n        context, in.dtype() == DataTypeToEnum<InputScalar>::v(),\n        errors::InvalidArgument(\"Invalid input dtype \", in.dtype(), \" vs \",\n                                DataTypeToEnum<InputScalar>::v()));\n  }\n  // Have the derived class validate that the inputs are as expected.\n  ValidateInputMatrixShapes(context, *input_matrix_shapes);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    auto min_input_tensor = context->input(1);\n    auto max_input_tensor = context->input(2);\n    OP_REQUIRES(\n        context, min_input_tensor.NumElements() == 1,\n        errors::InvalidArgument(\n            \"min_input must be a scalar float value, got tensor with shape \",\n            min_input_tensor.shape()));\n    OP_REQUIRES(\n        context, max_input_tensor.NumElements() == 1,\n        errors::InvalidArgument(\n            \"max_input must be a scalar float value, got tensor with shape \",\n            max_input_tensor.shape()));\n    const float min_input = context->input(1).flat<float>()(0);\n    const float max_input = context->input(2).flat<float>()(0);\n    MaxPoolingOp<Device, T>::Compute(context);\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));\n    output_min->flat<float>()(0) = min_input;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));\n    output_max->flat<float>()(0) = max_input;\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status KernelAndDeviceOp::Run(\n    ScopedStepContainer* step_container, const EagerKernelArgs& inputs,\n    std::vector<EagerKernelRet>* outputs,\n    CancellationManager* cancellation_manager,\n    const absl::optional<EagerRemoteFunctionParams>& remote_func_params) {\n  OpKernelContext::Params params;\n  params.device = device_;\n  params.frame_iter = FrameAndIter(0, 0);\n  params.inputs = inputs.GetTensorValues();\n  params.op_kernel = kernel_.get();\n  params.resource_manager = device_->resource_manager();\n  params.input_alloc_attrs = &input_alloc_attrs_;\n  params.output_attr_array = output_alloc_attrs_.data();\n  params.function_library = flr_;\n  params.slice_reader_cache = &slice_reader_cache_;\n  params.rendezvous = rendezvous_;\n  OpExecutionState* op_execution_state = nullptr;\n\n  CancellationManager default_cancellation_manager;\n  if (cancellation_manager) {\n    params.cancellation_manager = cancellation_manager;\n  } else if (kernel_->is_deferred()) {\n    op_execution_state = new OpExecutionState;\n    params.cancellation_manager = &op_execution_state->cancellation_manager;\n    params.inc_num_deferred_ops_function = [op_execution_state]() {\n      op_execution_state->Ref();\n    };\n    params.dec_num_deferred_ops_function = [op_execution_state]() {\n      op_execution_state->Unref();\n    };\n  } else {\n    params.cancellation_manager = &default_cancellation_manager;\n  }\n\n  params.log_memory = log_memory_;\n\n  params.runner = get_runner();\n\n  params.step_container =\n      step_container == nullptr ? &step_container_ : step_container;\n  auto step_container_cleanup = gtl::MakeCleanup([step_container, this] {\n    if (step_container == nullptr) {\n      this->step_container_.CleanUp();\n    }\n  });\n\n  params.collective_executor =\n      collective_executor_ ? collective_executor_->get() : nullptr;\n\n  OpKernelContext context(&params);\n\n  {\n    port::ScopedFlushDenormal flush;\n    port::ScopedSetRound round(FE_TONEAREST);\n    // 'AnnotatedTraceMe' will trace both scheduling time on host and execution\n    // time on device of the OpKernel.\n    profiler::AnnotatedTraceMe activity(\n        [&] { return kernel_->TraceString(context, /*verbose=*/false); },\n        profiler::TraceMeLevel::kInfo);\n    device_->Compute(kernel_.get(), &context);\n  }\n\n  // Clean up execution op_execution_state if deferred ops aren't running.\n  if (op_execution_state != nullptr) {\n    op_execution_state->Unref();\n  }\n\n  if (!context.status().ok()) return context.status();\n\n  if (outputs != nullptr) {\n    outputs->clear();\n    for (int i = 0; i < context.num_outputs(); ++i) {\n      const auto* output_tensor = context.mutable_output(i);\n      if (output_tensor != nullptr) {\n        outputs->push_back(Tensor(*output_tensor));\n      } else {\n        outputs->push_back(Tensor());\n      }\n    }\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor x = context->input(0);\n    const Tensor& scale = context->input(1);\n    const Tensor& offset = context->input(2);\n    const Tensor& estimated_mean = context->input(3);\n    const Tensor& estimated_variance = context->input(4);\n    const Tensor* side_input = has_side_input_ ? &context->input(5) : nullptr;\n\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(context, offset.dims() == 1,\n                errors::InvalidArgument(\"offset must be 1-dimensional\",\n                                        offset.shape().DebugString()));\n    OP_REQUIRES(context, estimated_mean.dims() == 1,\n                errors::InvalidArgument(\"estimated_mean must be 1-dimensional\",\n                                        estimated_mean.shape().DebugString()));\n    OP_REQUIRES(\n        context, estimated_variance.dims() == 1,\n        errors::InvalidArgument(\"estimated_variance must be 1-dimensional\",\n                                estimated_variance.shape().DebugString()));\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64 in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64 in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64 in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64 in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64 in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    const auto num_channels = GetTensorDim(x, tensor_format_, 'C');\n    OP_REQUIRES(\n        context, scale.NumElements() == num_channels,\n        errors::InvalidArgument(\"scale must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                scale.NumElements(), \" and \", num_channels));\n    OP_REQUIRES(\n        context, offset.NumElements() == num_channels,\n        errors::InvalidArgument(\"offset must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                offset.NumElements(), \" and \", num_channels));\n    if (estimated_mean.NumElements() != 0) {\n      OP_REQUIRES(context, estimated_mean.NumElements() == num_channels,\n                  errors::InvalidArgument(\n                      \"mean must be empty or have the same number of \"\n                      \"elements as the channels of x, got \",\n                      estimated_mean.NumElements(), \" and \", num_channels));\n    }\n    if (estimated_variance.NumElements() != 0) {\n      OP_REQUIRES(context, estimated_variance.NumElements() == num_channels,\n                  errors::InvalidArgument(\n                      \"variance must be empty or have the same number of \"\n                      \"elements as the channels of x, got \",\n                      estimated_variance.NumElements(), \" and \", num_channels));\n    }\n\n    if (has_side_input_) {\n      OP_REQUIRES(context, side_input->shape() == x.shape(),\n                  errors::InvalidArgument(\n                      \"side_input shape must be equal to input shape: \",\n                      side_input->shape().DebugString(),\n                      \" != \", x.shape().DebugString()));\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity) {\n      // NOTE(ezhulenev): This requirement is coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx.\n      OP_REQUIRES(\n          context, !is_training_ || num_channels % 4 == 0,\n          errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                  \"channel dimension to be a multiple of 4.\"));\n    }\n\n    Tensor* y = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, alloc_shape, &y));\n\n    Tensor* batch_mean = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {3}, 1, scale.shape(), &batch_mean));\n    Tensor* batch_var = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {4}, 2, scale.shape(), &batch_var));\n    Tensor* saved_mean = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(3, scale.shape(), &saved_mean));\n    Tensor* saved_maybe_inv_var = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(4, scale.shape(),\n                                                     &saved_maybe_inv_var));\n\n    if (is_training_) {\n      functor::FusedBatchNorm<Device, T, U, true>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    } else {\n      functor::FusedBatchNorm<Device, T, U, false>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, y->CopyFrom(*y, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // inputs must be at least a matrix\n    OP_REQUIRES(\n        ctx, sorted_inputs_t.shape().dims() >= 2,\n        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n\n    // For empty inputs, all values will be placed at the zeroth position.\n    if (sorted_inputs.size() == 0) {\n      functor::SetZeroFunctor<Device, OutType> set_zero;\n      set_zero(ctx->eigen_device<Device>(), output);\n      return;\n    }\n\n    OP_REQUIRES_OK(\n        ctx, functor::LowerBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "string SummarizeArray<bool>(int64_t limit, int64_t num_elts,\n                            const TensorShape& tensor_shape, const char* data,\n                            const bool print_v2) {\n  if (data == nullptr) {\n    return strings::StrCat(\"\");  // we already print type and shape\n  }\n  // We first convert all chars to be 0/1 to not get InvalidEnumValue sanitizer\n  // error\n  auto mutable_data = std::unique_ptr<char[]>(new char[num_elts]);\n  for (int64_t i = 0; i < num_elts; ++i)\n    mutable_data.get()[i] = data[i] ? 1 : 0;\n  bool* array = reinterpret_cast<bool*>(mutable_data.get());\n  return SummarizeArrayInternal<bool>(limit, num_elts, tensor_shape, array,\n                                      print_v2);\n}\n}  // namespace"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) final {\n    // Extract inputs and validate shapes and types.\n    const CSRSparseMatrix* input_matrix;\n    OP_REQUIRES_OK(ctx, ExtractVariantFromInput(ctx, 0, &input_matrix));\n    const Tensor& input_permutation_indices = ctx->input(1);\n\n    int64 num_rows;\n    int batch_size;\n    OP_REQUIRES_OK(ctx, ValidateInputs(*input_matrix, input_permutation_indices,\n                                       &batch_size, &num_rows));\n\n    // Allocate batch pointers.\n    Tensor batch_ptr(cpu_allocator(), DT_INT32, TensorShape({batch_size + 1}));\n    auto batch_ptr_vec = batch_ptr.vec<int32>();\n    batch_ptr_vec(0) = 0;\n\n    // Temporary vector of Eigen SparseMatrices to store the Sparse Cholesky\n    // factors.\n    // Note: we use column-compressed (CSC) SparseMatrix because SimplicialLLT\n    // returns the factors in column major format. Since our input should be\n    // symmetric, column major and row major is identical in storage. We just\n    // have to switch to reading the upper triangular part of the input, which\n    // corresponds to the lower triangular part in row major format.\n    std::vector<SparseMatrix> sparse_cholesky_factors(batch_size);\n\n    // TODO(anudhyan): Tune the cost per unit based on benchmarks.\n    const double nnz_per_row =\n        (input_matrix->total_nnz() / batch_size) / num_rows;\n    const int64 sparse_cholesky_cost_per_batch =\n        nnz_per_row * nnz_per_row * num_rows;\n    // Perform sparse Cholesky factorization of each batch in parallel.\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n    std::atomic<int64> invalid_input_index(-1);\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          sparse_cholesky_cost_per_batch,\n          [&](int64 batch_begin, int64 batch_end) {\n            for (int64 batch_index = batch_begin; batch_index < batch_end;\n                 ++batch_index) {\n              // Define an Eigen SparseMatrix Map to operate on the\n              // CSRSparseMatrix component without copying the data.\n              Eigen::Map<const SparseMatrix> sparse_matrix(\n                  num_rows, num_rows, input_matrix->nnz(batch_index),\n                  input_matrix->row_pointers_vec(batch_index).data(),\n                  input_matrix->col_indices_vec(batch_index).data(),\n                  input_matrix->values_vec<T>(batch_index).data());\n\n              Eigen::SimplicialLLT<SparseMatrix, Eigen::Upper,\n                                   Eigen::NaturalOrdering<int>>\n                  solver;\n              auto permutation_indices_flat =\n                  input_permutation_indices.flat<int32>().data();\n\n              // Invert the fill-in reducing ordering and apply it to the input\n              // sparse matrix.\n              Eigen::Map<\n                  Eigen::PermutationMatrix<Eigen::Dynamic, Eigen::Dynamic, int>>\n                  permutation(permutation_indices_flat + batch_index * num_rows,\n                              num_rows);\n              auto permutation_inverse = permutation.inverse();\n\n              SparseMatrix permuted_sparse_matrix;\n              permuted_sparse_matrix.template selfadjointView<Eigen::Upper>() =\n                  sparse_matrix.template selfadjointView<Eigen::Upper>()\n                      .twistedBy(permutation_inverse);\n\n              // Compute the Cholesky decomposition.\n              solver.compute(permuted_sparse_matrix);\n              if (solver.info() != Eigen::Success) {\n                invalid_input_index = batch_index;\n                return;\n              }\n\n              // Get the upper triangular factor, which would end up in the\n              // lower triangular part of the output CSRSparseMatrix when\n              // interpreted in row major format.\n              sparse_cholesky_factors[batch_index] =\n                  std::move(solver.matrixU());\n              // For now, batch_ptr contains the number of nonzeros in each\n              // batch.\n              batch_ptr_vec(batch_index + 1) =\n                  sparse_cholesky_factors[batch_index].nonZeros();\n            }\n          });\n\n    // Check for invalid input.\n    OP_REQUIRES(\n        ctx, invalid_input_index == -1,\n        errors::InvalidArgument(\n            \"Sparse Cholesky factorization failed for batch index \",\n            invalid_input_index.load(), \". The input might not be valid.\"));\n\n    // Compute a cumulative sum to obtain the batch pointers.\n    std::partial_sum(batch_ptr_vec.data(),\n                     batch_ptr_vec.data() + batch_size + 1,\n                     batch_ptr_vec.data());\n\n    // Allocate output Tensors.\n    const int64 total_nnz = batch_ptr_vec(batch_size);\n    Tensor output_row_ptr(cpu_allocator(), DT_INT32,\n                          TensorShape({(num_rows + 1) * batch_size}));\n    Tensor output_col_ind(cpu_allocator(), DT_INT32, TensorShape({total_nnz}));\n    Tensor output_values(cpu_allocator(), DataTypeToEnum<T>::value,\n                         TensorShape({total_nnz}));\n    auto output_row_ptr_ptr = output_row_ptr.flat<int32>().data();\n    auto output_col_ind_ptr = output_col_ind.flat<int32>().data();\n    auto output_values_ptr = output_values.flat<T>().data();\n\n    // Copy the output matrices from each batch into the CSRSparseMatrix\n    // Tensors.\n    // TODO(b/129906419): Factor out the copy from Eigen SparseMatrix to\n    // CSRSparseMatrix into common utils. This is also used in\n    // SparseMatrixSparseMatMul.\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          (3 * total_nnz) / batch_size /* cost per unit */,\n          [&](int64 batch_begin, int64 batch_end) {\n            for (int64 batch_index = batch_begin; batch_index < batch_end;\n                 ++batch_index) {\n              const SparseMatrix& cholesky_factor =\n                  sparse_cholesky_factors[batch_index];\n              const int64 nnz = cholesky_factor.nonZeros();\n\n              std::copy(cholesky_factor.outerIndexPtr(),\n                        cholesky_factor.outerIndexPtr() + num_rows + 1,\n                        output_row_ptr_ptr + batch_index * (num_rows + 1));\n              std::copy(cholesky_factor.innerIndexPtr(),\n                        cholesky_factor.innerIndexPtr() + nnz,\n                        output_col_ind_ptr + batch_ptr_vec(batch_index));\n              std::copy(cholesky_factor.valuePtr(),\n                        cholesky_factor.valuePtr() + nnz,\n                        output_values_ptr + batch_ptr_vec(batch_index));\n            }\n          });\n\n    // Create the CSRSparseMatrix instance from its component Tensors and\n    // prepare the Variant output Tensor.\n    CSRSparseMatrix output_csr_matrix;\n    OP_REQUIRES_OK(\n        ctx,\n        CSRSparseMatrix::CreateCSRSparseMatrix(\n            DataTypeToEnum<T>::value, input_matrix->dense_shape(), batch_ptr,\n            output_row_ptr, output_col_ind, output_values, &output_csr_matrix));\n    Tensor* output_csr_matrix_tensor;\n    AllocatorAttributes cpu_alloc;\n    cpu_alloc.set_on_host(true);\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, TensorShape({}), &output_csr_matrix_tensor,\n                                  cpu_alloc));\n    output_csr_matrix_tensor->scalar<Variant>()() =\n        std::move(output_csr_matrix);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status Examples::CreateSparseFeatureRepresentation(\n    const DeviceBase::CpuWorkerThreads& worker_threads, const int num_examples,\n    const int num_sparse_features, const ModelWeights& weights,\n    const OpInputList& sparse_example_indices_inputs,\n    const OpInputList& sparse_feature_indices_inputs,\n    const OpInputList& sparse_feature_values_inputs,\n    std::vector<Example>* const examples) {\n  mutex mu;\n  Status result;  // Guarded by mu\n  auto parse_partition = [&](const int64 begin, const int64 end) {\n    // The static_cast here is safe since begin and end can be at most\n    // num_examples which is an int.\n    for (int i = static_cast<int>(begin); i < end; ++i) {\n      auto example_indices =\n          sparse_example_indices_inputs[i].template flat<int64>();\n      auto feature_indices =\n          sparse_feature_indices_inputs[i].template flat<int64>();\n      if (example_indices.size() != feature_indices.size()) {\n        mutex_lock l(mu);\n        result = errors::InvalidArgument(\n            \"Found mismatched example_indices and feature_indices [\",\n            example_indices, \"] vs [\", feature_indices, \"]\");\n        return;\n      }\n\n      // Parse features for each example. Features for a particular example\n      // are at the offsets (start_id, end_id]\n      int start_id = -1;\n      int end_id = 0;\n      for (int example_id = 0; example_id < num_examples; ++example_id) {\n        start_id = end_id;\n        while (end_id < example_indices.size() &&\n               example_indices(end_id) == example_id) {\n          ++end_id;\n        }\n        Example::SparseFeatures* const sparse_features =\n            &(*examples)[example_id].sparse_features_[i];\n        if (start_id < example_indices.size() &&\n            example_indices(start_id) == example_id) {\n          sparse_features->indices.reset(new UnalignedInt64Vector(\n              &(feature_indices(start_id)), end_id - start_id));\n          if (sparse_feature_values_inputs.size() > i) {\n            auto feature_weights =\n                sparse_feature_values_inputs[i].flat<float>();\n            sparse_features->values.reset(new UnalignedFloatVector(\n                &(feature_weights(start_id)), end_id - start_id));\n          }\n          // If features are non empty.\n          if (end_id - start_id > 0) {\n            // TODO(sibyl-Aix6ihai): Write this efficiently using vectorized\n            // operations from eigen.\n            for (int64 k = 0; k < sparse_features->indices->size(); ++k) {\n              const int64 feature_index = (*sparse_features->indices)(k);\n              if (!weights.SparseIndexValid(i, feature_index)) {\n                mutex_lock l(mu);\n                result = errors::InvalidArgument(\n                    \"Found sparse feature indices out of valid range: \",\n                    (*sparse_features->indices)(k));\n                return;\n              }\n            }\n          }\n        } else {\n          // Add a Tensor that has size 0.\n          sparse_features->indices.reset(\n              new UnalignedInt64Vector(&(feature_indices(0)), 0));\n          // If values exist for this feature group.\n          if (sparse_feature_values_inputs.size() > i) {\n            auto feature_weights =\n                sparse_feature_values_inputs[i].flat<float>();\n            sparse_features->values.reset(\n                new UnalignedFloatVector(&(feature_weights(0)), 0));\n          }\n        }\n      }\n    }\n  };\n  // For each column, the cost of parsing it is O(num_examples). We use\n  // num_examples here, as empirically Shard() creates the right amount of\n  // threads based on the problem size.\n  // TODO(sibyl-Aix6ihai): Tune this as a function of dataset size.\n  const int64 kCostPerUnit = num_examples;\n  Shard(worker_threads.num_threads, worker_threads.workers, num_sparse_features,\n        kCostPerUnit, parse_partition);\n  return result;\n}\n"}, {"role": "assistant", "content": "False"}]}
