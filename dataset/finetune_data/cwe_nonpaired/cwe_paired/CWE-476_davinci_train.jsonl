{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    Tensor encoded_t = context->input(0);\n    auto* encoded = encoded_t.flat<Variant>()(0).get<CompositeTensorVariant>();\n\n    // Check that the encoded TypeSpec is compatible with the expected TypeSpec.\n    // For now, we just check that the class matches.\n    //\n    // TODO(b/173744905): Update this to do a generic compatibility check. This\n    // would require replacing the current design, where Python subclasses of\n    // TypeSpec can override is_compatible, with a design where compatibility\n    // can be deterministically determined from the metadata.\n    auto expected_class = metadata_.type_spec_proto().type_spec_class();\n    auto actual_class = encoded->metadata().type_spec_proto().type_spec_class();\n    OP_REQUIRES(\n        context, expected_class == actual_class,\n        errors::InvalidArgument(\n            \"Expected a \", TypeSpecProto::TypeSpecClass_Name(expected_class),\n            \" (based on `type_spec`), but `encoded` contains a \",\n            TypeSpecProto::TypeSpecClass_Name(actual_class)));\n\n    // Extract the component tensors.\n    OpOutputList components;\n    OP_REQUIRES_OK(context, context->output_list(\"components\", &components));\n    int num_components = encoded->flat_components().size();\n\n    OP_REQUIRES(context, component_dtypes_.size() == num_components,\n                errors::InvalidArgument(\"Encoded value has \", num_components,\n                                        \" tensor components; expected \",\n                                        component_dtypes_.size(),\n                                        \" components based on type_spec\"));\n\n    for (int i = 0; i < component_dtypes_.size(); i++) {\n      const Tensor& component = encoded->flat_components()[i];\n      OP_REQUIRES(context, component_dtypes_[i] == component.dtype(),\n                  errors::InvalidArgument(\"Tensor component \", i, \" had dtype \",\n                                          DataType_Name(component.dtype()),\n                                          \"; expected dtype \",\n                                          DataType_Name(component_dtypes_[i])));\n      components.set(i, component);\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLiteSVDFParams*>(node->builtin_data);\n  OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* weights_feature;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kWeightsFeatureTensor,\n                                          &weights_feature));\n  const TfLiteTensor* weights_time;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, kWeightsTimeTensor, &weights_time));\n  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);\n\n  TfLiteTensor* scratch;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, /*index=*/0, &scratch));\n\n  TfLiteTensor* state = GetVariableInput(context, node, kStateTensor);\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  switch (weights_feature->type) {\n    case kTfLiteFloat32: {\n      reference_ops::EvalFloatSVDF(\n          params, GetTensorShape(input), GetTensorData<float>(input),\n          GetTensorShape(weights_feature),\n          GetTensorData<float>(weights_feature), GetTensorShape(weights_time),\n          GetTensorData<float>(weights_time), GetTensorShape(bias),\n          GetTensorData<float>(bias), GetTensorData<float>(scratch),\n          GetTensorData<float>(state), GetTensorShape(output),\n          GetTensorData<float>(output));\n      return kTfLiteOk;\n    }\n    case kTfLiteUInt8:\n    case kTfLiteInt8: {\n      if (input->type == kTfLiteFloat32) {\n        TfLiteTensor* input_quantized;\n        TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/1,\n                                                    &input_quantized));\n        TfLiteTensor* scaling_factors;\n        TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/2,\n                                                    &scaling_factors));\n        TfLiteTensor* float_weights_time;\n        TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/3,\n                                                    &float_weights_time));\n        TfLiteTensor* zero_points;\n        TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/4,\n                                                    &zero_points));\n        TfLiteTensor* row_sums;\n        TF_LITE_ENSURE_OK(\n            context, GetTemporarySafe(context, node, /*index=*/5, &row_sums));\n        // Dequantize weights time.\n        // TODO(alanchiao): this dequantization initialization only needs to\n        // happen once per model and should theoretically be placed in either\n        // Init or Prepare. However, TFLite doesn't allocate float_weights_time\n        // until the Eval function.\n        // TODO(alanchiao): refactor logic out into dequantize function.\n        if (!op_data->float_weights_time_initialized) {\n          const float dequantization_scale = weights_time->params.scale;\n          const int8_t* weights_time_ptr = GetTensorData<int8_t>(weights_time);\n          float* float_weights_time_ptr =\n              GetTensorData<float>(float_weights_time);\n          for (int i = 0; i < NumElements(float_weights_time); ++i) {\n            float_weights_time_ptr[i] =\n                weights_time_ptr[i] * dequantization_scale;\n          }\n          op_data->float_weights_time_initialized = true;\n        }\n\n        int32_t* zero_points_ptr = nullptr;\n        int32_t* row_sums_ptr = nullptr;\n        if (params->asymmetric_quantize_inputs && row_sums != nullptr) {\n          zero_points_ptr = GetTensorData<int32_t>(zero_points);\n          row_sums_ptr = GetTensorData<int32_t>(row_sums);\n        }\n\n        reference_ops::EvalHybridSVDF(\n            params, GetTensorShape(input), GetTensorData<float>(input),\n            GetTensorShape(weights_feature),\n            GetTensorData<int8_t>(weights_feature),\n            weights_feature->params.scale, GetTensorShape(float_weights_time),\n            GetTensorData<float>(float_weights_time), GetTensorShape(bias),\n            GetTensorData<float>(bias), GetTensorData<float>(scratch),\n            GetTensorData<float>(scaling_factors),\n            GetTensorData<int8_t>(input_quantized), GetTensorData<float>(state),\n            GetTensorShape(output), GetTensorData<float>(output),\n            zero_points_ptr, row_sums_ptr, &op_data->compute_row_sums);\n        return kTfLiteOk;\n      }\n      auto* input_params = reinterpret_cast<TfLiteAffineQuantization*>(\n          input->quantization.params);\n      auto* output_params = reinterpret_cast<TfLiteAffineQuantization*>(\n          output->quantization.params);\n      TfLiteTensor* output_temp;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, /*index=*/1, &output_temp));\n\n      // Currently supports only ReLU.\n      // TODO(jianlijianli): support other activations.\n      TF_LITE_ENSURE_EQ(context, params->activation, kTfLiteActRelu);\n\n      reference_ops::EvalIntegerSVDF(\n          params, GetTensorShape(input), GetTensorData<int8_t>(input),\n          GetTensorShape(weights_feature),\n          GetTensorData<int8_t>(weights_feature), GetTensorShape(weights_time),\n          GetTensorData<int16_t>(weights_time), GetTensorShape(bias),\n          GetTensorData<int32_t>(bias), GetTensorData<int16_t>(state),\n          GetTensorShape(output), GetTensorData<int8_t>(output),\n          GetTensorData<int32_t>(scratch), GetTensorData<int32_t>(output_temp),\n          op_data->effective_scale_1_a, op_data->effective_scale_1_b,\n          op_data->effective_scale_2_a, op_data->effective_scale_2_b,\n          input_params->zero_point->data[0],\n          output_params->zero_point->data[0]);\n      return kTfLiteOk;\n    }\n    default:\n      context->ReportError(context, \"Type %s not currently supported.\",\n                           TfLiteTypeGetName(weights_feature->type));\n      return kTfLiteError;\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void SparseFillEmptyRowsOpImpl(OpKernelContext* context,\n                               AsyncOpKernel::DoneCallback done = nullptr) {\n  // Note that setting this empty lambda as the default parameter value directly\n  // can cause strange compiler/linker errors, so we do it like this instead.\n  if (!done) {\n    done = [] {};\n  }\n\n  const int kIndicesInput = 0;\n  const int kValuesInput = 1;\n  const int kDenseShapeInput = 2;\n  const int kDefaultValueInput = 3;\n\n  const Tensor& indices_t = context->input(kIndicesInput);\n  const Tensor& values_t = context->input(kValuesInput);\n  const Tensor& dense_shape_t = context->input(kDenseShapeInput);\n  const Tensor& default_value_t = context->input(kDefaultValueInput);\n\n  OP_REQUIRES_ASYNC(\n      context, TensorShapeUtils::IsVector(dense_shape_t.shape()),\n      errors::InvalidArgument(\"dense_shape must be a vector, saw: \",\n                              dense_shape_t.shape().DebugString()),\n      done);\n  OP_REQUIRES_ASYNC(context, TensorShapeUtils::IsMatrix(indices_t.shape()),\n                    errors::InvalidArgument(\"indices must be a matrix, saw: \",\n                                            indices_t.shape().DebugString()),\n                    done);\n  OP_REQUIRES_ASYNC(context, TensorShapeUtils::IsVector(values_t.shape()),\n                    errors::InvalidArgument(\"values must be a vector, saw: \",\n                                            values_t.shape().DebugString()),\n                    done);\n  OP_REQUIRES_ASYNC(\n      context, TensorShapeUtils::IsScalar(default_value_t.shape()),\n      errors::InvalidArgument(\"default_value must be a scalar, saw: \",\n                              default_value_t.shape().DebugString()),\n      done);\n  // TODO(ebrevdo): add shape checks between values, indices,\n  // dense_shape.  Also add check that dense rank > 0.\n\n  using FunctorType = functor::SparseFillEmptyRows<Device, T, Tindex>;\n  OP_REQUIRES_OK_ASYNC(context,\n                       FunctorType()(context, default_value_t, indices_t,\n                                     values_t, dense_shape_t, done),\n                       done);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    // Input tensor is of the following dimensions:\n    // [ batch, in_rows, in_cols, in_depth ]\n    const Tensor& input = context->input(0);\n\n    // Input filter is of the following dimensions:\n    // [ filter_rows, filter_cols, in_depth, out_depth]\n    const Tensor& filter = context->input(1);\n\n    // For 2D convolution, there should be 4 dimensions.\n    OP_REQUIRES(context, input.dims() == 4,\n                errors::InvalidArgument(\"input must be 4-dimensional\",\n                                        input.shape().DebugString()));\n    OP_REQUIRES(context, filter.dims() == 4,\n                errors::InvalidArgument(\"filter must be 4-dimensional: \",\n                                        filter.shape().DebugString()));\n\n    const float min_input = context->input(2).flat<float>()(0);\n    const float max_input = context->input(3).flat<float>()(0);\n    const float min_filter = context->input(4).flat<float>()(0);\n    const float max_filter = context->input(5).flat<float>()(0);\n    const int32_t offset_input =\n        FloatToQuantizedUnclamped<T1>(0.0f, min_input, max_input);\n    const int32_t offset_filter =\n        FloatToQuantizedUnclamped<T2>(0.0f, min_filter, max_filter);\n    const int32_t offset_output = 0;\n    const int32_t mult_output = 1;\n    const int32_t shift_output = 0;\n\n    // The last dimension for input is in_depth. It must be the same as the\n    // filter's in_depth.\n    const int64_t in_depth = input.dim_size(3);\n    OP_REQUIRES(context, in_depth == filter.dim_size(2),\n                errors::InvalidArgument(\n                    \"input and filter must have the same depth: \", in_depth,\n                    \" vs \", filter.dim_size(2)));\n\n    // The last dimension for filter is out_depth.\n    const int64_t out_depth = filter.dim_size(3);\n\n    // The second dimension for input is rows/height.\n    // The first dimension for filter is rows/height.\n    const int64_t input_rows = input.dim_size(1);\n    const int64_t filter_rows = filter.dim_size(0);\n\n    // The third dimension for input is columns/width.\n    // The second dimension for filter is columns/width.\n    const int64_t input_cols = input.dim_size(2);\n    const int64_t filter_cols = filter.dim_size(1);\n\n    // The first dimension for input is batch.\n    const int64_t batch = input.dim_size(0);\n\n    // For now we take the stride from the second dimension only (we\n    // assume row = col stride, and do not support striding on the\n    // batch or depth dimension).\n    const int stride = strides_[1];\n\n    int64_t out_rows = 0, out_cols = 0, pad_rows = 0, pad_cols = 0;\n    OP_REQUIRES_OK(context,\n                   GetWindowedOutputSize(input_rows, filter_rows, stride,\n                                         padding_, &out_rows, &pad_rows));\n    OP_REQUIRES_OK(context,\n                   GetWindowedOutputSize(input_cols, filter_cols, stride,\n                                         padding_, &out_cols, &pad_cols));\n    CHECK_GT(batch, 0);\n    CHECK_GT(out_rows, 0);\n    CHECK_GT(out_cols, 0);\n    CHECK_GT(out_depth, 0);\n    TensorShape out_shape({batch, out_rows, out_cols, out_depth});\n\n    // Output tensor is of the following dimensions:\n    // [ in_batch, out_rows, out_cols, out_depth ]\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n\n    // This will call different implementations (e.g. reference or optimized)\n    // depending on the template parameter.\n    ConvFunctor<T1, T2, T3> conv_functor;\n    conv_functor(context, input.flat<T1>().data(), batch, input_rows,\n                 input_cols, in_depth, offset_input, filter.flat<T2>().data(),\n                 filter_rows, filter_cols, out_depth, offset_filter, stride,\n                 padding_, output->flat<T3>().data(), out_rows, out_cols,\n                 shift_output, offset_output, mult_output);\n\n    float min_output_value;\n    float max_output_value;\n    QuantizationRangeForMultiplication<T1, T2, T3>(\n        min_input, max_input, min_filter, max_filter, &min_output_value,\n        &max_output_value);\n\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));\n    output_min->flat<float>()(0) = min_output_value;\n\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));\n    output_max->flat<float>()(0) = max_output_value;\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ModelWeights::Initialize(OpKernelContext* const context) {\n  OpInputList sparse_indices_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"sparse_indices\", &sparse_indices_inputs));\n  OpInputList sparse_weights_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"sparse_weights\", &sparse_weights_inputs));\n  OpInputList dense_weights_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"dense_weights\", &dense_weights_inputs));\n\n  OpOutputList sparse_weights_outputs;\n  TF_RETURN_IF_ERROR(context->output_list(\"out_delta_sparse_weights\",\n                                          &sparse_weights_outputs));\n\n  OpOutputList dense_weights_outputs;\n  TF_RETURN_IF_ERROR(\n      context->output_list(\"out_delta_dense_weights\", &dense_weights_outputs));\n\n  for (int i = 0; i < sparse_weights_inputs.size(); ++i) {\n    Tensor* delta_t;\n    TF_RETURN_IF_ERROR(sparse_weights_outputs.allocate(\n        i, sparse_weights_inputs[i].shape(), &delta_t));\n    // Convert the input vector to a row matrix in internal representation.\n    auto deltas = delta_t->shaped<float, 2>({1, delta_t->NumElements()});\n    deltas.setZero();\n    sparse_weights_.emplace_back(FeatureWeightsSparseStorage{\n        sparse_indices_inputs[i].flat<int64>(),\n        sparse_weights_inputs[i].shaped<float, 2>(\n            {1, sparse_weights_inputs[i].NumElements()}),\n        deltas});\n  }\n\n  // Reads in the weights, and allocates and initializes the delta weights.\n  const auto initialize_weights =\n      [&](const OpInputList& weight_inputs, OpOutputList* const weight_outputs,\n          std::vector<FeatureWeightsDenseStorage>* const feature_weights) {\n        for (int i = 0; i < weight_inputs.size(); ++i) {\n          Tensor* delta_t;\n          TF_RETURN_IF_ERROR(\n              weight_outputs->allocate(i, weight_inputs[i].shape(), &delta_t));\n          // Convert the input vector to a row matrix in internal\n          // representation.\n          auto deltas = delta_t->shaped<float, 2>({1, delta_t->NumElements()});\n          deltas.setZero();\n          feature_weights->emplace_back(FeatureWeightsDenseStorage{\n              weight_inputs[i].shaped<float, 2>(\n                  {1, weight_inputs[i].NumElements()}),\n              deltas});\n        }\n        return Status::OK();\n      };\n\n  return initialize_weights(dense_weights_inputs, &dense_weights_outputs,\n                            &dense_weights_);\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    Tensor encoded_t = context->input(0);\n    OP_REQUIRES(\n        context, encoded_t.flat<Variant>().size() > 0,\n        errors::InvalidArgument(\"Input `encoded` must not be an empty variant \"\n                                \"tensor, but got \",\n                                encoded_t.DebugString()));\n    auto* encoded = encoded_t.flat<Variant>()(0).get<CompositeTensorVariant>();\n\n    // Check that the encoded TypeSpec is compatible with the expected TypeSpec.\n    // For now, we just check that the class matches.\n    //\n    // TODO(b/173744905): Update this to do a generic compatibility check. This\n    // would require replacing the current design, where Python subclasses of\n    // TypeSpec can override is_compatible, with a design where compatibility\n    // can be deterministically determined from the metadata.\n    auto expected_class = metadata_.type_spec_proto().type_spec_class();\n    auto actual_class = encoded->metadata().type_spec_proto().type_spec_class();\n    OP_REQUIRES(\n        context, expected_class == actual_class,\n        errors::InvalidArgument(\n            \"Expected a \", TypeSpecProto::TypeSpecClass_Name(expected_class),\n            \" (based on `type_spec`), but `encoded` contains a \",\n            TypeSpecProto::TypeSpecClass_Name(actual_class)));\n\n    // Extract the component tensors.\n    OpOutputList components;\n    OP_REQUIRES_OK(context, context->output_list(\"components\", &components));\n    int num_components = encoded->flat_components().size();\n\n    OP_REQUIRES(context, component_dtypes_.size() == num_components,\n                errors::InvalidArgument(\"Encoded value has \", num_components,\n                                        \" tensor components; expected \",\n                                        component_dtypes_.size(),\n                                        \" components based on type_spec\"));\n\n    for (int i = 0; i < component_dtypes_.size(); i++) {\n      const Tensor& component = encoded->flat_components()[i];\n      OP_REQUIRES(context, component_dtypes_[i] == component.dtype(),\n                  errors::InvalidArgument(\"Tensor component \", i, \" had dtype \",\n                                          DataType_Name(component.dtype()),\n                                          \"; expected dtype \",\n                                          DataType_Name(component_dtypes_[i])));\n      components.set(i, component);\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& handle = ctx->input(0);\n    const string& name = handle.scalar<tstring>()();\n    OP_REQUIRES_OK(ctx, ctx->session_state()->DeleteTensor(name));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void MakeDataset(OpKernelContext* ctx, DatasetBase** output) override {\n    // Create a new SparseTensorSliceDatasetOp::Dataset, insert it in\n    // the step container, and return it as the output.\n    const Tensor* indices;\n    OP_REQUIRES_OK(ctx, ctx->input(\"indices\", &indices));\n    const Tensor* values;\n    OP_REQUIRES_OK(ctx, ctx->input(\"values\", &values));\n    const Tensor* dense_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"dense_shape\", &dense_shape));\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(indices->shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be a matrix but received shape \",\n                    indices->shape().DebugString()));\n\n    const auto num_indices = indices->NumElements();\n    const auto num_values = values->NumElements();\n    if (num_indices == 0 || num_values == 0) {\n      OP_REQUIRES(ctx, num_indices == num_values,\n                  errors::InvalidArgument(\n                      \"If indices or values are empty, the other one must also \"\n                      \"be. Got indices of shape \",\n                      indices->shape().DebugString(), \" and values of shape \",\n                      values->shape().DebugString()));\n    }\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(values->shape()),\n                errors::InvalidArgument(\n                    \"Input values should be a vector but received shape \",\n                    indices->shape().DebugString()));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(dense_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shape should be a vector but received shape \",\n                    dense_shape->shape().DebugString()));\n\n    // We currently ensure that `sparse_tensor` is ordered in the\n    // batch dimension.\n    // TODO(mrry): Investigate ways to avoid this unconditional check\n    // if we can be sure that the sparse tensor was produced in an\n    // appropriate order (e.g. by `tf.parse_example()` or a Dataset\n    // that batches elements into rows of a SparseTensor).\n    int64_t previous_batch_index = -1;\n    for (int64_t i = 0; i < indices->dim_size(0); ++i) {\n      int64_t next_batch_index = indices->matrix<int64_t>()(i, 0);\n      OP_REQUIRES(\n          ctx, next_batch_index >= previous_batch_index,\n          errors::Unimplemented(\"The SparseTensor must be ordered in the batch \"\n                                \"dimension; handling arbitrarily ordered input \"\n                                \"is not currently supported.\"));\n      previous_batch_index = next_batch_index;\n    }\n    gtl::InlinedVector<int64_t, 8> std_order(dense_shape->NumElements(), 0);\n    sparse::SparseTensor tensor;\n    OP_REQUIRES_OK(\n        ctx, sparse::SparseTensor::Create(\n                 *indices, *values, TensorShape(dense_shape->vec<int64_t>()),\n                 std_order, &tensor));\n    *output = new Dataset<T>(ctx, std::move(tensor));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext *ctx) override {\n    const Tensor *a_indices_t, *a_values_t, *a_shape_t, *b_indices_t,\n        *b_values_t, *b_shape_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_indices\", &a_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_shape\", &a_shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_indices\", &b_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_values\", &b_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_shape\", &b_shape_t));\n\n    // Validations.\n    OP_REQUIRES(\n        ctx,\n        TensorShapeUtils::IsMatrix(a_indices_t->shape()) &&\n            TensorShapeUtils::IsMatrix(b_indices_t->shape()),\n        errors::InvalidArgument(\"Inputs a_indices and b_indices should be \"\n                                \"matrices but received shapes: \",\n                                a_indices_t->shape().DebugString(), \", \",\n                                b_indices_t->shape().DebugString()));\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(a_values_t->shape()) &&\n                    TensorShapeUtils::IsVector(b_values_t->shape()),\n                errors::InvalidArgument(\n                    \"Inputs a_values and b_values should be vectors \"\n                    \"but received shapes: \",\n                    a_values_t->shape().DebugString(), \" and \",\n                    b_values_t->shape().DebugString()));\n\n    const int64_t a_nnz = a_indices_t->dim_size(0);\n    const int64_t b_nnz = b_indices_t->dim_size(0);\n\n    const auto a_values = a_values_t->vec<T>();\n    const auto b_values = b_values_t->vec<T>();\n\n    OP_REQUIRES(\n        ctx, a_values.size() == a_nnz && b_values.size() == b_nnz,\n        errors::InvalidArgument(\"Expected \", a_nnz, \" and \", b_nnz,\n                                \" non-empty input values, got \",\n                                a_values.size(), \" and \", b_values.size()));\n\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(a_shape_t->shape()) &&\n                    TensorShapeUtils::IsVector(b_shape_t->shape()),\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    a_shape_t->shape().DebugString(), \" and \",\n                    b_shape_t->shape().DebugString()));\n    const int num_dims = a_indices_t->dim_size(1);\n    OP_REQUIRES(\n        ctx, a_shape_t->NumElements() == num_dims,\n        errors::InvalidArgument(\"Second dimension of a_indices and length of \"\n                                \"a_shape must match, got \",\n                                num_dims, \" and \", a_shape_t->NumElements()));\n    OP_REQUIRES(ctx, num_dims > 0,\n                errors::InvalidArgument(\"Tensors must not be empty\"));\n    OP_REQUIRES(ctx, a_shape_t->IsSameSize(*b_shape_t),\n                errors::InvalidArgument(\n                    \"Operands do not have the same ranks; got shapes: \",\n                    a_shape_t->SummarizeValue(10), \" and \",\n                    b_shape_t->SummarizeValue(10)));\n    const auto a_shape = a_shape_t->flat<int64_t>();\n    const auto b_shape = b_shape_t->flat<int64_t>();\n    for (int i = 0; i < a_shape_t->NumElements(); ++i) {\n      OP_REQUIRES(ctx, a_shape(i) == b_shape(i),\n                  errors::InvalidArgument(\"Operands' shapes do not match: got \",\n                                          a_shape(i), \" and \", b_shape(i),\n                                          \" for dimension \", i));\n    }\n\n    const auto a_indices_mat = a_indices_t->matrix<int64_t>();\n    const auto b_indices_mat = b_indices_t->matrix<int64_t>();\n    std::vector<T> a_augmented_values, b_augmented_values;\n    std::vector<std::pair<bool, int64>> entries_to_copy;  // from_a?, idx\n    UnionSparseIndicesAndValues(a_indices_mat, a_values, a_nnz, b_indices_mat,\n                                b_values, b_nnz, num_dims, &a_augmented_values,\n                                &b_augmented_values, &entries_to_copy);\n\n    // Allocates and fills output tensors.\n    const int64_t sum_nnz = a_augmented_values.size();\n    Tensor *output_indices_t, *output_values_t;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({sum_nnz, num_dims}),\n                                        &output_indices_t));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(1, TensorShape({sum_nnz}), &output_values_t));\n    auto output_indices_mat = output_indices_t->matrix<int64_t>();\n\n    for (int64_t i = 0; i < sum_nnz; ++i) {\n      const bool from_a = entries_to_copy[i].first;\n      const int64_t idx = entries_to_copy[i].second;\n      output_indices_mat.chip<0>(i) =\n          from_a ? a_indices_mat.chip<0>(idx) : b_indices_mat.chip<0>(idx);\n    }\n\n    // Performs the functor operation using Eigen.\n    //\n    // Note that the two stack-allocated std::vector's may not be aligned. Using\n    // allocate_temp() would've given us aligned storage, but we do not know\n    // their sizes in advance, so we couldn't use allocate_temp() anyway.\n    //\n    // TODO(zongheng): measure if it's worthwhile to somehow force alignment.\n    using UnalignedTensorMap =\n        Eigen::TensorMap<Eigen::Tensor<const T, 1, Eigen::RowMajor>,\n                         Eigen::Unaligned>;\n    auto a_augmented_values_t =\n        UnalignedTensorMap(a_augmented_values.data(), sum_nnz);\n    auto b_augmented_values_t =\n        UnalignedTensorMap(b_augmented_values.data(), sum_nnz);\n    output_values_t->flat<T>().device(ctx->eigen_device<Device>()) =\n        a_augmented_values_t.binaryExpr(b_augmented_values_t,\n                                        typename Functor::func());\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Subgraph::Invoke() {\n  if (!consistent_) {\n    ReportError(\"Invoke called on model that is not consistent.\");\n    return kTfLiteError;\n  }\n\n  TfLiteStatus status = kTfLiteOk;\n  if (state_ == kStateUninvokable) {\n    ReportError(\"Invoke called on model that is not ready.\");\n    return kTfLiteError;\n  } else if (memory_planner_ && !memory_planner_->HasNonPersistentMemory()) {\n    ReportError(\"Non-persistent memory is not available.\");\n    return kTfLiteError;\n  }\n\n  // This is only needed for UseNNAPI(true);\n  if (should_apply_nnapi_delegate_ && !applied_nnapi_delegate_) {\n    TF_LITE_ENSURE_OK(&context_, ModifyGraphWithDelegate(NnApiDelegate()));\n    // only need to modify the graph once upon the first invocation.\n    applied_nnapi_delegate_ = true;\n  }\n\n  // Invocations are always done in node order.\n  // Note that calling Invoke repeatedly will cause the original memory plan to\n  // be reused, unless either ResizeInputTensor() or AllocateTensors() has been\n  // called.\n  for (int execution_plan_index = 0;\n       execution_plan_index < execution_plan_.size(); execution_plan_index++) {\n    if (execution_plan_index == next_execution_plan_index_to_prepare_) {\n      TF_LITE_ENSURE_STATUS(PrepareOpsAndTensors());\n      TF_LITE_ENSURE(&context_, next_execution_plan_index_to_prepare_ >=\n                                    execution_plan_index);\n    }\n    int node_index = execution_plan_[execution_plan_index];\n    TfLiteNode& node = nodes_and_registration_[node_index].first;\n    const TfLiteRegistration& registration =\n        nodes_and_registration_[node_index].second;\n\n    const char* op_name = nullptr;\n    if (profiler_) op_name = GetTFLiteOpName(registration);\n    TFLITE_SCOPED_TAGGED_OPERATOR_PROFILE(profiler_.get(), op_name, node_index);\n\n    // TODO(ycling): This is an extra loop through inputs to check if the data\n    // need to be copied from Delegate buffer to raw memory, which is often not\n    // needed. We may want to cache this in prepare to know if this needs to be\n    // done for a node or not.\n    for (int i = 0; i < node.inputs->size; ++i) {\n      int tensor_index = node.inputs->data[i];\n      if (tensor_index == kTfLiteOptionalTensor) {\n        continue;\n      }\n      TfLiteTensor* tensor = &tensors_[tensor_index];\n      if (tensor->delegate && tensor->delegate != node.delegate &&\n          tensor->data_is_stale) {\n        TF_LITE_ENSURE_STATUS(EnsureTensorDataIsReadable(tensor_index));\n      }\n    }\n\n    if (check_cancelled_func_ != nullptr &&\n        check_cancelled_func_(cancellation_data_)) {\n      ReportError(\"Client requested cancel during Invoke()\");\n      return kTfLiteError;\n    }\n\n    EnsureTensorsVectorCapacity();\n    tensor_resized_since_op_invoke_ = false;\n    if (OpInvoke(registration, &node) != kTfLiteOk) {\n      return ReportOpError(&context_, node, registration, node_index,\n                           \"failed to invoke\");\n    }\n\n    // Force execution prep for downstream ops if the latest op triggered the\n    // resize of a dynamic tensor.\n    if (tensor_resized_since_op_invoke_ &&\n        HasDynamicTensor(context_, node.outputs)) {\n      next_execution_plan_index_to_prepare_ = execution_plan_index + 1;\n\n      // This happens when an intermediate dynamic tensor is resized.\n      // We don't have to prepare all the ops, but we need to recompute\n      // the allocation plan.\n      if (next_execution_plan_index_to_plan_allocation_ >\n          next_execution_plan_index_to_prepare_) {\n        next_execution_plan_index_to_plan_allocation_ =\n            next_execution_plan_index_to_prepare_;\n        if (memory_planner_) {\n          TF_LITE_ENSURE_STATUS(memory_planner_->ResetAllocationsAfter(\n              next_execution_plan_index_to_plan_allocation_ - 1));\n        }\n      }\n    }\n  }\n\n  return status;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool SetOpAttrList(TFE_Context* ctx, TFE_Op* op, const char* key,\n                   PyObject* py_list, TF_AttrType type,\n                   tensorflow::gtl::FlatMap<string, int64_t>* attr_list_sizes,\n                   TF_Status* status) {\n  if (!PySequence_Check(py_list)) {\n    TF_SetStatus(\n        status, TF_INVALID_ARGUMENT,\n        tensorflow::strings::StrCat(\"Expecting sequence value for attr \", key,\n                                    \", got \", py_list->ob_type->tp_name)\n            .c_str());\n    return false;\n  }\n  const int num_values = PySequence_Size(py_list);\n  if (attr_list_sizes != nullptr) (*attr_list_sizes)[key] = num_values;\n\n#define PARSE_LIST(c_type, parse_fn)                                      \\\n  std::unique_ptr<c_type[]> values(new c_type[num_values]);               \\\n  for (int i = 0; i < num_values; ++i) {                                  \\\n    tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));   \\\n    if (!parse_fn(key, py_value.get(), status, &values[i])) return false; \\\n  }\n\n  if (type == TF_ATTR_STRING) {\n    std::unique_ptr<const void*[]> values(new const void*[num_values]);\n    std::unique_ptr<size_t[]> lengths(new size_t[num_values]);\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::StringPiece value;\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      if (!ParseStringValue(key, py_value.get(), status, &value)) return false;\n      values[i] = value.data();\n      lengths[i] = value.size();\n    }\n    TFE_OpSetAttrStringList(op, key, values.get(), lengths.get(), num_values);\n  } else if (type == TF_ATTR_INT) {\n    PARSE_LIST(int64_t, ParseInt64Value);\n    TFE_OpSetAttrIntList(op, key, values.get(), num_values);\n  } else if (type == TF_ATTR_FLOAT) {\n    PARSE_LIST(float, ParseFloatValue);\n    TFE_OpSetAttrFloatList(op, key, values.get(), num_values);\n  } else if (type == TF_ATTR_BOOL) {\n    PARSE_LIST(unsigned char, ParseBoolValue);\n    TFE_OpSetAttrBoolList(op, key, values.get(), num_values);\n  } else if (type == TF_ATTR_TYPE) {\n    PARSE_LIST(int, ParseTypeValue);\n    TFE_OpSetAttrTypeList(op, key,\n                          reinterpret_cast<const TF_DataType*>(values.get()),\n                          num_values);\n  } else if (type == TF_ATTR_SHAPE) {\n    // Make one pass through the input counting the total number of\n    // dims across all the input lists.\n    int total_dims = 0;\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      if (py_value.get() != Py_None) {\n        if (!PySequence_Check(py_value.get())) {\n          TF_SetStatus(\n              status, TF_INVALID_ARGUMENT,\n              tensorflow::strings::StrCat(\n                  \"Expecting None or sequence value for element\", i,\n                  \" of attr \", key, \", got \", py_value->ob_type->tp_name)\n                  .c_str());\n          return false;\n        }\n        const auto size = TensorShapeNumDims(py_value.get());\n        if (size >= 0) {\n          total_dims += size;\n        }\n      }\n    }\n    // Allocate a buffer that can fit all of the dims together.\n    std::unique_ptr<int64_t[]> buffer(new int64_t[total_dims]);\n    // Copy the input dims into the buffer and set dims to point to\n    // the start of each list's dims.\n    std::unique_ptr<const int64_t*[]> dims(new const int64_t*[num_values]);\n    std::unique_ptr<int[]> num_dims(new int[num_values]);\n    int64_t* offset = buffer.get();\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      if (py_value.get() == Py_None) {\n        dims[i] = nullptr;\n        num_dims[i] = -1;\n      } else {\n        const auto size = TensorShapeNumDims(py_value.get());\n        if (size == -1) {\n          dims[i] = nullptr;\n          num_dims[i] = -1;\n          continue;\n        }\n        dims[i] = offset;\n        num_dims[i] = size;\n        for (int j = 0; j < size; ++j) {\n          tensorflow::Safe_PyObjectPtr inner_py_value(\n              PySequence_ITEM(py_value.get(), j));\n          if (inner_py_value.get() == Py_None) {\n            *offset = -1;\n          } else if (!ParseDimensionValue(key, inner_py_value.get(), status,\n                                          offset)) {\n            return false;\n          }\n          ++offset;\n        }\n      }\n    }\n    TFE_OpSetAttrShapeList(op, key, dims.get(), num_dims.get(), num_values,\n                           status);\n    if (!status->status.ok()) return false;\n  } else if (type == TF_ATTR_FUNC) {\n    std::unique_ptr<const TFE_Op*[]> funcs(new const TFE_Op*[num_values]);\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      // Allow:\n      // (1) String function name, OR\n      // (2) A Python object with a .name attribute\n      //     (A crude test for being a\n      //     tensorflow.python.framework.function._DefinedFunction)\n      //     (which is what the various \"defun\" or \"Defun\" decorators do).\n      // And in the future also allow an object that can encapsulate\n      // the function name and its attribute values.\n      tensorflow::StringPiece func_name;\n      if (!ParseStringValue(key, py_value.get(), status, &func_name)) {\n        PyObject* name_attr = PyObject_GetAttrString(py_value.get(), \"name\");\n        if (name_attr == nullptr ||\n            !ParseStringValue(key, name_attr, status, &func_name)) {\n          TF_SetStatus(\n              status, TF_INVALID_ARGUMENT,\n              tensorflow::strings::StrCat(\n                  \"unable to set function value attribute from a \",\n                  py_value.get()->ob_type->tp_name,\n                  \" object. If you think this is an error, please file an \"\n                  \"issue at \"\n                  \"https://github.com/tensorflow/tensorflow/issues/new\")\n                  .c_str());\n          return false;\n        }\n      }\n      funcs[i] = TFE_NewOp(ctx, func_name.data(), status);\n      if (!status->status.ok()) return false;\n    }\n    TFE_OpSetAttrFunctionList(op, key, funcs.get(), num_values);\n    if (!status->status.ok()) return false;\n  } else {\n    TF_SetStatus(status, TF_UNIMPLEMENTED,\n                 tensorflow::strings::StrCat(\"Attr \", key,\n                                             \" has unhandled list type \", type)\n                     .c_str());\n    return false;\n  }\n#undef PARSE_LIST\n  return true;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ValidateInputs(const Tensor *a_indices, const Tensor *a_values,\n                      const Tensor *a_shape, const Tensor *b) {\n  if (!TensorShapeUtils::IsMatrix(a_indices->shape())) {\n    return errors::InvalidArgument(\n        \"Input a_indices should be a matrix but received shape: \",\n        a_indices->shape().DebugString());\n  }\n  if (!TensorShapeUtils::IsVector(a_values->shape()) ||\n      !TensorShapeUtils::IsVector(a_shape->shape())) {\n    return errors::InvalidArgument(\n        \"Inputs a_values and a_shape should be vectors \"\n        \"but received shapes: \",\n        a_values->shape().DebugString(), \" and \",\n        a_shape->shape().DebugString());\n  }\n  if (a_shape->NumElements() != b->dims()) {\n    return errors::InvalidArgument(\n        \"Two operands have different ranks; received: \", a_shape->NumElements(),\n        \" and \", b->dims());\n  }\n  const auto a_shape_flat = a_shape->flat<Index>();\n  for (int i = 0; i < b->dims(); ++i) {\n    if (a_shape_flat(i) != b->dim_size(i)) {\n      return errors::InvalidArgument(\n          \"Dimension \", i,\n          \" does not equal (no broadcasting is supported): sparse side \",\n          a_shape_flat(i), \" vs dense side \", b->dim_size(i));\n    }\n  }\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& prefix = context->input(0);\n    const Tensor& tensor_names = context->input(1);\n    const Tensor& shape_and_slices = context->input(2);\n    ValidateInputs(true /* is save op */, context, prefix, tensor_names,\n                   shape_and_slices);\n\n    const int kFixedInputs = 3;  // Prefix, tensor names, shape_and_slices.\n    const int num_tensors = static_cast<int>(tensor_names.NumElements());\n    const string& prefix_string = prefix.scalar<tstring>()();\n    const auto& tensor_names_flat = tensor_names.flat<tstring>();\n    const auto& shape_and_slices_flat = shape_and_slices.flat<tstring>();\n\n    BundleWriter writer(Env::Default(), prefix_string);\n    OP_REQUIRES_OK(context, writer.status());\n    VLOG(1) << \"BundleWriter, prefix_string: \" << prefix_string;\n\n    for (int i = 0; i < num_tensors; ++i) {\n      const string& tensor_name = tensor_names_flat(i);\n      const Tensor& tensor = context->input(i + kFixedInputs);\n      VLOG(2) << \"Starting save of \" << tensor_name;\n\n      if (!shape_and_slices_flat(i).empty()) {\n        const string& shape_spec = shape_and_slices_flat(i);\n        TensorShape shape;\n        TensorSlice slice(tensor.dims());\n        TensorShape slice_shape;\n\n        OP_REQUIRES_OK(context, checkpoint::ParseShapeAndSlice(\n                                    shape_spec, &shape, &slice, &slice_shape));\n        OP_REQUIRES(context, slice_shape.IsSameSize(tensor.shape()),\n                    errors::InvalidArgument(\"Slice in shape_and_slice \"\n                                            \"specification does not match the \"\n                                            \"shape of the tensor to  save: \",\n                                            shape_spec, \", tensor: \",\n                                            tensor.shape().DebugString()));\n\n        OP_REQUIRES_OK(context,\n                       writer.AddSlice(tensor_name, shape, slice, tensor));\n      } else {\n        OP_REQUIRES_OK(context, writer.Add(tensor_name, tensor));\n      }\n\n      if (VLOG_IS_ON(5)) {\n        if (tensor.dtype() == DT_FLOAT) {\n          const float* t_data = tensor.flat<float>().data();\n          float min = std::numeric_limits<float>::infinity();\n          float max = -std::numeric_limits<float>::infinity();\n          double avg = 0.0;\n          for (int i = 0; i < tensor.NumElements(); ++i) {\n            if (t_data[i] < min) min = t_data[i];\n            if (t_data[i] > max) max = t_data[i];\n            avg += t_data[i];\n          }\n          VLOG(5) << \" min \" << min << \" max \" << max << \" avg \"\n                  << avg / tensor.NumElements() << \" total elts \"\n                  << tensor.NumElements();\n        }\n      }\n\n      VLOG(2) << \"Done save of \" << tensor_name;\n    }\n    OP_REQUIRES_OK(context, writer.Finish());\n    VLOG(1) << \"Done BundleWriter, prefix_string: \" << prefix_string;\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "StatusOr<unsigned> GraphDefImporter::ArgNumType(const NamedAttrList &attrs,\n                                                const OpDef::ArgDef &arg_def,\n                                                SmallVectorImpl<Type> &types) {\n  // Check whether a type list attribute is specified.\n  if (!arg_def.type_list_attr().empty()) {\n    if (auto v = attrs.get(arg_def.type_list_attr()).dyn_cast<ArrayAttr>()) {\n      for (Attribute attr : v) {\n        if (auto dtype = attr.dyn_cast<TypeAttr>()) {\n          types.push_back(UnrankedTensorType::get(dtype.getValue()));\n        } else {\n          return InvalidArgument(\"Expected '\", arg_def.type_list_attr(),\n                                 \"' to be a list of types\");\n        }\n      }\n      return v.size();\n    }\n    return NotFound(\"Type attr not found: \", arg_def.type_list_attr());\n  }\n\n  unsigned num = 1;\n  // Check whether a number attribute is specified.\n  if (!arg_def.number_attr().empty()) {\n    if (auto v = attrs.get(arg_def.number_attr()).dyn_cast<IntegerAttr>()) {\n      num = v.getValue().getZExtValue();\n    } else {\n      return NotFound(\"Type attr not found: \", arg_def.number_attr());\n    }\n  }\n\n  // Check for a type or type attribute.\n  Type dtype;\n  if (arg_def.type() != DataType::DT_INVALID) {\n    TF_RETURN_IF_ERROR(ConvertDataType(arg_def.type(), b_, &dtype));\n  } else if (arg_def.type_attr().empty()) {\n    return InvalidArgument(\"Arg '\", arg_def.name(),\n                           \"' has invalid type and no type attribute\");\n  } else {\n    if (auto v = attrs.get(arg_def.type_attr()).dyn_cast<TypeAttr>()) {\n      dtype = v.getValue();\n    } else {\n      return NotFound(\"Type attr not found: \", arg_def.type_attr());\n    }\n  }\n  types.append(num, UnrankedTensorType::get(dtype));\n  return num;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor* hypothesis_indices;\n    const Tensor* hypothesis_values;\n    const Tensor* hypothesis_shape;\n    const Tensor* truth_indices;\n    const Tensor* truth_values;\n    const Tensor* truth_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_indices\", &hypothesis_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_values\", &hypothesis_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_shape\", &hypothesis_shape));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_indices\", &truth_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_values\", &truth_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_shape\", &truth_shape));\n\n    OP_REQUIRES_OK(\n        ctx, ValidateShapes(ctx, *hypothesis_indices, *hypothesis_values,\n                            *hypothesis_shape, *truth_indices, *truth_values,\n                            *truth_shape));\n\n    TensorShape hypothesis_st_shape;\n    OP_REQUIRES_OK(\n        ctx, TensorShapeUtils::MakeShape(hypothesis_shape->vec<int64>().data(),\n                                         hypothesis_shape->NumElements(),\n                                         &hypothesis_st_shape));\n    TensorShape truth_st_shape;\n    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n                            truth_shape->vec<int64>().data(),\n                            truth_shape->NumElements(), &truth_st_shape));\n\n    // Assume indices are sorted in row-major order.\n    std::vector<int64> sorted_order(truth_st_shape.dims());\n    std::iota(sorted_order.begin(), sorted_order.end(), 0);\n\n    sparse::SparseTensor hypothesis;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *hypothesis_indices, *hypothesis_values,\n                            hypothesis_st_shape, sorted_order, &hypothesis));\n\n    sparse::SparseTensor truth;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *truth_indices, *truth_values, truth_st_shape,\n                            sorted_order, &truth));\n\n    // Group dims 0, 1, ..., RANK - 1.  The very last dim is assumed\n    // to store the variable length sequences.\n    std::vector<int64> group_dims(truth_st_shape.dims() - 1);\n    std::iota(group_dims.begin(), group_dims.end(), 0);\n\n    TensorShape output_shape;\n    for (int d = 0; d < static_cast<int>(group_dims.size()); ++d) {\n      output_shape.AddDim(std::max(hypothesis_st_shape.dim_size(d),\n                                   truth_st_shape.dim_size(d)));\n    }\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"output\", output_shape, &output));\n    auto output_t = output->flat<float>();\n    output_t.setZero();\n\n    std::vector<int64> output_strides(output_shape.dims());\n    output_strides[output_shape.dims() - 1] = 1;\n    for (int d = output_shape.dims() - 2; d >= 0; --d) {\n      output_strides[d] = output_strides[d + 1] * output_shape.dim_size(d + 1);\n    }\n\n    auto hypothesis_grouper = hypothesis.group(group_dims);\n    auto truth_grouper = truth.group(group_dims);\n\n    auto hypothesis_iter = hypothesis_grouper.begin();\n    auto truth_iter = truth_grouper.begin();\n\n    auto cmp = std::equal_to<T>();\n\n    while (hypothesis_iter != hypothesis_grouper.end() &&\n           truth_iter != truth_grouper.end()) {\n      sparse::Group truth_i = *truth_iter;\n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64> g_truth = truth_i.group();\n      std::vector<int64> g_hypothesis = hypothesis_j.group();\n      auto truth_seq = truth_i.values<T>();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n\n      if (g_truth == g_hypothesis) {\n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64{0});\n        output_t(loc) =\n            gtl::LevenshteinDistance<T>(truth_seq, hypothesis_seq, cmp);\n        if (normalize_) output_t(loc) /= truth_seq.size();\n\n        ++hypothesis_iter;\n        ++truth_iter;\n      } else if (g_truth > g_hypothesis) {  // zero-length truth\n        auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                      output_strides.begin(), int64{0});\n        output_t(loc) = hypothesis_seq.size();\n        if (normalize_ && output_t(loc) != 0.0f) {\n          output_t(loc) = std::numeric_limits<float>::infinity();\n        }\n        ++hypothesis_iter;\n      } else {  // zero-length hypothesis\n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64{0});\n        output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n        ++truth_iter;\n      }\n    }\n    while (hypothesis_iter != hypothesis_grouper.end()) {  // zero-length truths\n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64> g_hypothesis = hypothesis_j.group();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                    output_strides.begin(), int64{0});\n      output_t(loc) = hypothesis_seq.size();\n      if (normalize_ && output_t(loc) != 0.0f) {\n        output_t(loc) = std::numeric_limits<float>::infinity();\n      }\n      ++hypothesis_iter;\n    }\n    while (truth_iter != truth_grouper.end()) {  // missing hypotheses\n      sparse::Group truth_i = *truth_iter;\n      std::vector<int64> g_truth = truth_i.group();\n      auto truth_seq = truth_i.values<T>();\n      auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                    output_strides.begin(), int64{0});\n      output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n      ++truth_iter;\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void CropInferMeta(const MetaTensor& x,\n                   const IntArray& shape,\n                   const IntArray& offsets,\n                   MetaTensor* out,\n                   MetaConfig config) {\n  PADDLE_ENFORCE_NE(\n      out,\n      nullptr,\n      errors::InvalidArgument(\"CropTensor should have output tensor out.\"));\n\n  auto x_dim = x.dims();\n  auto shape_dims = shape.GetData();\n  auto offsets_vec = offsets.GetData();\n\n  PADDLE_ENFORCE_EQ(shape_dims.size(),\n                    x_dim.size(),\n                    errors::InvalidArgument(\n                        \"The number of elements (%d) of attribute 'shape' for \"\n                        \"CropTensor must be equal to the number of \"\n                        \"dimensions (%d) of the input.\",\n                        shape_dims.size(),\n                        x_dim.size()));\n\n  if (config.is_runtime) {\n    out->share_lod(x);\n  }\n\n  auto out_dims = std::vector<int64_t>(shape.size(), -1);\n  for (int i = 0; i < static_cast<int>(shape_dims.size()); ++i) {\n    if (shape_dims[i] > 0) {\n      out_dims[i] = static_cast<int64_t>(shape_dims[i]);\n    } else {\n      if (shape_dims[i] == -1 && offsets_vec[i] != -1 && x_dim[i] != -1) {\n        out_dims[i] = x_dim[i] - static_cast<int64_t>(offsets_vec[i]);\n      }\n    }\n  }\n  out->set_dims(common::make_ddim(out_dims));\n  out->set_dtype(x.dtype());\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "const SegmentCommand* Binary::segment_from_offset(uint64_t offset) const {\n  const auto it_begin = std::begin(offset_seg_);\n  if (offset < it_begin->first) {\n    return nullptr;\n  }\n\n  auto it = offset_seg_.lower_bound(offset);\n  if (it->first == offset || it == it_begin) {\n    SegmentCommand* seg = it->second;\n    if (seg->file_offset() <= offset && offset < (seg->file_offset() + seg->file_size())) {\n      return seg;\n    }\n  }\n\n  const auto it_end = offset_seg_.crbegin();\n  if (it == std::end(offset_seg_) && offset >= it_end->first) {\n    SegmentCommand* seg = it_end->second;\n    if (seg->file_offset() <= offset && offset < (seg->file_offset() + seg->file_size())) {\n      return seg;\n    }\n  }\n\n  if (it == it_begin) {\n    /* It did not pass the first check */\n    return nullptr;\n  }\n\n  --it;\n\n  SegmentCommand* seg = it->second;\n  if (seg->file_offset() <= offset && offset < (seg->file_offset() + seg->file_size())) {\n    return seg;\n  }\n  return nullptr;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteTensor* GetVariableInput(TfLiteContext* context, const TfLiteNode* node,\n                               int index) {\n  TfLiteTensor* tensor = GetMutableInput(context, node, index);\n  return tensor->is_variable ? tensor : nullptr;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status GetTensorArray(OpKernelContext* ctx, TensorArray** tensor_array) {\n  string container;\n  string ta_handle;\n  if (ctx->input_dtype(0) != DT_RESOURCE) {\n    TF_RETURN_IF_ERROR(GetHandle(ctx, &container, &ta_handle));\n    ResourceMgr* rm = ctx->resource_manager();\n    if (rm == nullptr) return errors::Internal(\"No resource manager.\");\n    TF_RETURN_IF_ERROR(\n        ctx->step_container()->Lookup(rm, container + ta_handle, tensor_array));\n    return OkStatus();\n  } else {\n    return LookupResource(ctx, HandleFromInput(ctx, 0), tensor_array);\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void ValidateInputs(OpKernelContext* ctx,\n                      const CSRSparseMatrix& sparse_matrix,\n                      const Tensor& permutation_indices, int* batch_size,\n                      int64* num_rows) {\n    OP_REQUIRES(ctx, sparse_matrix.dtype() == DataTypeToEnum<T>::value,\n                errors::InvalidArgument(\n                    \"Asked for a CSRSparseMatrix of type \",\n                    DataTypeString(DataTypeToEnum<T>::value),\n                    \" but saw dtype: \", DataTypeString(sparse_matrix.dtype())));\n\n    const Tensor& dense_shape = sparse_matrix.dense_shape();\n    const int rank = dense_shape.dim_size(0);\n    OP_REQUIRES(ctx, rank == 2 || rank == 3,\n                errors::InvalidArgument(\"sparse matrix must have rank 2 or 3; \",\n                                        \"but dense_shape has size \", rank));\n    const int row_dim = (rank == 2) ? 0 : 1;\n    auto dense_shape_vec = dense_shape.vec<int64>();\n    *num_rows = dense_shape_vec(row_dim);\n    const int64 num_cols = dense_shape_vec(row_dim + 1);\n    OP_REQUIRES(ctx, *num_rows == num_cols,\n                errors::InvalidArgument(\"sparse matrix must be square; got: \",\n                                        *num_rows, \" != \", num_cols));\n    const TensorShape& perm_shape = permutation_indices.shape();\n    OP_REQUIRES(\n        ctx, perm_shape.dims() + 1 == rank,\n        errors::InvalidArgument(\n            \"sparse matrix must have the same rank as permutation; got: \", rank,\n            \" != \", perm_shape.dims(), \" + 1.\"));\n    OP_REQUIRES(\n        ctx, perm_shape.dim_size(rank - 2) == *num_rows,\n        errors::InvalidArgument(\n            \"permutation must have the same number of elements in each batch \"\n            \"as the number of rows in sparse matrix; got: \",\n            perm_shape.dim_size(rank - 2), \" != \", *num_rows));\n\n    *batch_size = sparse_matrix.batch_size();\n    if (*batch_size > 1) {\n      OP_REQUIRES(\n          ctx, perm_shape.dim_size(0) == *batch_size,\n          errors::InvalidArgument(\"permutation must have the same batch size \"\n                                  \"as sparse matrix; got: \",\n                                  perm_shape.dim_size(0), \" != \", *batch_size));\n    }\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void UncompressElementOp::Compute(OpKernelContext* ctx) {\n  Tensor tensor = ctx->input(0);\n  const Variant& variant = tensor.scalar<Variant>()();\n  const CompressedElement* compressed = variant.get<CompressedElement>();\n\n  std::vector<Tensor> components;\n  OP_REQUIRES_OK(ctx, UncompressElement(*compressed, &components));\n  OP_REQUIRES(ctx, components.size() == output_types_.size(),\n              errors::FailedPrecondition(\"Expected \", output_types_.size(),\n                                         \" outputs from uncompress, but got \",\n                                         components.size()));\n  for (int i = 0; i < components.size(); ++i) {\n    OP_REQUIRES(\n        ctx, components[i].dtype() == output_types_[i],\n        errors::FailedPrecondition(\"Expected a tensor of type \",\n                                   DataTypeString(output_types_[i]),\n                                   \" but got a tensor of type \",\n                                   DataTypeString(components[i].dtype())));\n    ctx->set_output(i, components[i]);\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ImmutableExecutorState::BuildControlFlowInfo(const Graph* g,\n                                                    ControlFlowInfo* cf_info) {\n  const int num_nodes = g->num_node_ids();\n  cf_info->frame_names.resize(num_nodes);\n  std::vector<Node*> parent_nodes;\n  parent_nodes.resize(num_nodes);\n  std::vector<bool> visited;\n  visited.resize(num_nodes);\n\n  string frame_name;\n  std::deque<Node*> ready;\n\n  // Initialize with the root nodes.\n  for (Node* n : g->nodes()) {\n    if (n->in_edges().empty()) {\n      visited[n->id()] = true;\n      cf_info->unique_frame_names.insert(frame_name);\n      ready.push_back(n);\n    }\n  }\n\n  while (!ready.empty()) {\n    Node* curr_node = ready.front();\n    int curr_id = curr_node->id();\n    ready.pop_front();\n\n    Node* parent = nullptr;\n    if (IsEnter(curr_node)) {\n      // Enter a child frame.\n      TF_RETURN_IF_ERROR(\n          GetNodeAttr(curr_node->attrs(), \"frame_name\", &frame_name));\n      parent = curr_node;\n    } else if (IsExit(curr_node)) {\n      // Exit to the parent frame.\n      parent = parent_nodes[curr_id];\n      frame_name = cf_info->frame_names[parent->id()];\n      parent = parent_nodes[parent->id()];\n    } else {\n      parent = parent_nodes[curr_id];\n      frame_name = cf_info->frame_names[curr_id];\n    }\n\n    for (const Edge* out_edge : curr_node->out_edges()) {\n      Node* out = out_edge->dst();\n      if (IsSink(out)) continue;\n      const int out_id = out->id();\n\n      // Add to ready queue if not visited.\n      bool is_visited = visited[out_id];\n      if (!is_visited) {\n        ready.push_back(out);\n        visited[out_id] = true;\n\n        // Process the node 'out'.\n        cf_info->frame_names[out_id] = frame_name;\n        parent_nodes[out_id] = parent;\n        cf_info->unique_frame_names.insert(frame_name);\n      }\n    }\n  }\n\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status TrySimplify(NodeDef* consumer, string* simplified_node_name) override {\n    NodeDef* producer;\n    TF_RETURN_IF_ERROR(GetInputNode(consumer->input(0), &producer));\n    const bool producer_is_cast = IsCastLike(*producer);\n    const bool can_optimize =\n        !IsCheckNumerics(*producer) &&\n        ((producer_is_cast && IsValuePreserving(*consumer)) ||\n         (IsValuePreserving(*producer) && IsCastLike(*consumer)));\n    if (!can_optimize || IsControlFlow(*producer) ||\n        IsInPreserveSet(*producer) ||\n        producer->device() != consumer->device()) {\n      return Status::OK();\n    }\n\n    const NodeDef* cast_like_node = producer_is_cast ? producer : consumer;\n    const OpDef* cast_like_op_def = nullptr;\n    TF_RETURN_IF_ERROR(OpRegistry::Global()->LookUpOpDef(cast_like_node->op(),\n                                                         &cast_like_op_def));\n    DataType cast_src_type;\n    TF_RETURN_IF_ERROR(InputTypeForNode(*cast_like_node, *cast_like_op_def, 0,\n                                        &cast_src_type));\n    DataType cast_dst_type;\n    TF_RETURN_IF_ERROR(OutputTypeForNode(*cast_like_node, *cast_like_op_def, 0,\n                                         &cast_dst_type));\n    if (!IsFixedSizeType(cast_src_type) || !IsFixedSizeType(cast_dst_type)) {\n      return Status::OK();\n    } else if (producer_is_cast &&\n               DataTypeSize(cast_dst_type) <= DataTypeSize(cast_src_type)) {\n      return Status::OK();\n    } else if (!producer_is_cast &&\n               DataTypeSize(cast_dst_type) >= DataTypeSize(cast_src_type)) {\n      return Status::OK();\n    }\n\n    // Check that nodes were not already optimized.\n    const string optimized_producer_name = OptimizedNodeName(\n        ParseNodeScopeAndName(producer->name()), DataTypeString(cast_dst_type));\n    const string optimized_consumer_name = OptimizedNodeName(\n        ParseNodeScopeAndName(consumer->name()), DataTypeString(cast_src_type));\n    const bool is_already_optimized =\n        ctx().node_map->NodeExists(optimized_consumer_name) ||\n        ctx().node_map->NodeExists(optimized_producer_name);\n    if (is_already_optimized) {\n      return Status::OK();\n    }\n\n    // Add copies of consumer and producer in reverse order.\n    NodeDef* input;\n    TF_RETURN_IF_ERROR(GetInputNode(producer->input(0), &input));\n    // Create new producer node.\n    NodeDef* new_producer = AddCopyNode(optimized_consumer_name, consumer);\n    new_producer->set_input(0, producer->input(0));\n    ctx().node_map->AddOutput(input->name(), new_producer->name());\n\n    // Create new consumer node.\n    NodeDef* new_consumer = AddCopyNode(optimized_producer_name, producer);\n    new_consumer->set_input(0, new_producer->name());\n\n    NodeDef* new_value_preserving =\n        producer_is_cast ? new_producer : new_consumer;\n    const DataType new_input_type =\n        producer_is_cast ? cast_src_type : cast_dst_type;\n    // Update the input type of the value-preserving node. The input and\n    // output types of the cast-like nodes remain the same.\n    TF_RETURN_IF_ERROR(SetInputType(new_input_type, new_value_preserving));\n    // Make sure there is a kernel registered for the value preserving op\n    // with the new input type.\n    TF_RETURN_IF_ERROR(IsKernelRegisteredForNode(*new_value_preserving));\n    ctx().node_map->AddOutput(new_producer->name(), new_consumer->name());\n\n    AddToOptimizationQueue(new_producer);\n    *simplified_node_name = new_consumer->name();\n\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    // Read ragged_splits inputs.\n    OpInputList ragged_nested_splits_in;\n    OP_REQUIRES_OK(context, context->input_list(\"rt_nested_splits\",\n                                                &ragged_nested_splits_in));\n    const int ragged_nested_splits_len = ragged_nested_splits_in.size();\n    RaggedTensorVariant batched_ragged_input;\n    // Read ragged_values input.\n    batched_ragged_input.set_values(context->input(ragged_nested_splits_len));\n    batched_ragged_input.mutable_nested_splits()->reserve(\n        ragged_nested_splits_len);\n    for (int i = 0; i < ragged_nested_splits_len; i++) {\n      batched_ragged_input.append_splits(ragged_nested_splits_in[i]);\n    }\n\n    if (!batched_input_) {\n      // Encode as a Scalar Variant Tensor.\n      Tensor* encoded_scalar;\n      OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}),\n                                                       &encoded_scalar));\n      encoded_scalar->scalar<Variant>()() = std::move(batched_ragged_input);\n      return;\n    }\n\n    // Unbatch the Ragged Tensor and encode the components.\n    std::vector<RaggedTensorVariant> unbatched_ragged_input;\n    OP_REQUIRES_OK(context, UnbatchRaggedZerothDim<VALUE_TYPE, SPLIT_TYPE>(\n                                batched_ragged_input, &unbatched_ragged_input));\n\n    // Bundle the encoded scalar Variant Tensors into a rank-1 Variant Tensor.\n    Tensor* encoded_vector;\n    int output_size = unbatched_ragged_input.size();\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({output_size}),\n                                            &encoded_vector));\n    auto encoded_vector_t = encoded_vector->vec<Variant>();\n    for (int i = 0; i < output_size; i++) {\n      encoded_vector_t(i) = unbatched_ragged_input[i];\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ValidateShapes(OpKernelContext* ctx, const Tensor& hypothesis_indices,\n                      const Tensor& hypothesis_values,\n                      const Tensor& hypothesis_shape,\n                      const Tensor& truth_indices, const Tensor& truth_values,\n                      const Tensor& truth_shape) {\n  if (!TensorShapeUtils::IsMatrix(hypothesis_indices.shape()))\n    return errors::InvalidArgument(\n        \"hypothesis_indices should be a matrix, but got shape: \",\n        hypothesis_indices.shape().DebugString());\n  if (!TensorShapeUtils::IsMatrix(truth_indices.shape()))\n    return errors::InvalidArgument(\n        \"truth_indices should be a matrix, but got shape: \",\n        truth_indices.shape().DebugString());\n  if (!TensorShapeUtils::IsVector(hypothesis_values.shape()))\n    return errors::InvalidArgument(\n        \"hypothesis_values should be a vector, but got shape: \",\n        hypothesis_values.shape().DebugString());\n  if (!TensorShapeUtils::IsVector(truth_values.shape()))\n    return errors::InvalidArgument(\n        \"truth_values should be a vector, but got shape: \",\n        truth_values.shape().DebugString());\n  if (!TensorShapeUtils::IsVector(hypothesis_shape.shape()))\n    return errors::InvalidArgument(\n        \"hypothesis_shape should be a vector, but got shape: \",\n        hypothesis_shape.shape().DebugString());\n  if (!TensorShapeUtils::IsVector(truth_shape.shape()))\n    return errors::InvalidArgument(\n        \"truth_shape should be a vector, but got shape: \",\n        truth_shape.shape().DebugString());\n  if (hypothesis_shape.NumElements() != hypothesis_indices.dim_size(1))\n    return errors::InvalidArgument(\n        \"Expected hypothesis_shape.NumElements == \"\n        \"#cols(hypothesis_indices), their shapes are: \",\n        hypothesis_shape.shape().DebugString(), \" and \",\n        hypothesis_indices.shape().DebugString());\n  if (truth_shape.NumElements() < 2)\n    return errors::InvalidArgument(\n        \"Input SparseTensors must have rank at least 2, but truth_shape \"\n        \"rank is: \",\n        truth_shape.NumElements());\n  if (truth_shape.NumElements() != truth_indices.dim_size(1))\n    return errors::InvalidArgument(\n        \"Expected truth_shape.NumElements == \"\n        \"#cols(truth_indices), their shapes are: \",\n        truth_shape.shape().DebugString(), \" and \",\n        truth_indices.shape().DebugString());\n  if (truth_shape.NumElements() != hypothesis_shape.NumElements())\n    return errors::InvalidArgument(\n        \"Expected truth and hypothesis to have matching ranks, but \"\n        \"their shapes are: \",\n        truth_shape.shape().DebugString(), \" and \",\n        hypothesis_shape.shape().DebugString());\n\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Subgraph::Invoke() {\n  if (!consistent_) {\n    ReportError(\"Invoke called on model that is not consistent.\");\n    return kTfLiteError;\n  }\n\n  TfLiteStatus status = kTfLiteOk;\n  if (state_ == kStateUninvokable) {\n    ReportError(\"Invoke called on model that is not ready.\");\n    return kTfLiteError;\n  } else if (memory_planner_ && !memory_planner_->HasNonPersistentMemory()) {\n    ReportError(\"Non-persistent memory is not available.\");\n    return kTfLiteError;\n  }\n  TFLITE_SCOPED_TAGGED_DEFAULT_PROFILE(profiler_.get(), \"Invoke\");\n\n  // Invocations are always done in node order.\n  // Note that calling Invoke repeatedly will cause the original memory plan to\n  // be reused, unless either ResizeInputTensor() or AllocateTensors() has been\n  // called.\n  for (int execution_plan_index = 0;\n       execution_plan_index < execution_plan_.size(); execution_plan_index++) {\n    if (execution_plan_index == next_execution_plan_index_to_prepare_) {\n      TF_LITE_ENSURE_STATUS(PrepareOpsAndTensors());\n      TF_LITE_ENSURE(&context_, next_execution_plan_index_to_prepare_ >=\n                                    execution_plan_index);\n    }\n    int node_index = execution_plan_[execution_plan_index];\n    TfLiteNode& node = nodes_and_registration_[node_index].first;\n    const TfLiteRegistration& registration =\n        nodes_and_registration_[node_index].second;\n\n    const char* op_name = nullptr;\n    if (profiler_) op_name = GetTFLiteOpName(registration);\n    TFLITE_SCOPED_TAGGED_OPERATOR_PROFILE(profiler_.get(), op_name, node_index);\n\n    for (int i = 0; i < node.inputs->size; ++i) {\n      int tensor_index = node.inputs->data[i];\n      if (tensor_index == kTfLiteOptionalTensor) {\n        continue;\n      }\n      TfLiteTensor* tensor = &tensors_[tensor_index];\n      if (tensor->delegate && tensor->delegate != node.delegate &&\n          tensor->data_is_stale) {\n        TF_LITE_ENSURE_STATUS(EnsureTensorDataIsReadable(tensor_index));\n      }\n      if (tensor->data.raw == nullptr && tensor->bytes > 0) {\n        if (registration.builtin_code == kTfLiteBuiltinReshape && i == 1) {\n          // In general, having a tensor here with no buffer will be an error.\n          // However, for the reshape operator, the second input tensor is only\n          // used for the shape, not for the data. Thus, null buffer is ok.\n          continue;\n        } else {\n          // In all other cases, we need to return an error as otherwise we will\n          // trigger a null pointer dereference (likely).\n          ReportError(\"Input tensor %d lacks data\", tensor_index);\n          return kTfLiteError;\n        }\n      }\n    }\n\n    if (check_cancelled_func_ != nullptr &&\n        check_cancelled_func_(cancellation_data_)) {\n      ReportError(\"Client requested cancel during Invoke()\");\n      return kTfLiteError;\n    }\n\n    EnsureTensorsVectorCapacity();\n    tensor_resized_since_op_invoke_ = false;\n    if (OpInvoke(registration, &node) != kTfLiteOk) {\n      return ReportOpError(&context_, node, registration, node_index,\n                           \"failed to invoke\");\n    }\n\n    // Force execution prep for downstream ops if the latest op triggered the\n    // resize of a dynamic tensor.\n    if (tensor_resized_since_op_invoke_ &&\n        HasDynamicTensor(context_, node.outputs)) {\n      next_execution_plan_index_to_prepare_ = execution_plan_index + 1;\n\n      // This happens when an intermediate dynamic tensor is resized.\n      // We don't have to prepare all the ops, but we need to recompute\n      // the allocation plan.\n      if (next_execution_plan_index_to_plan_allocation_ >\n          next_execution_plan_index_to_prepare_) {\n        next_execution_plan_index_to_plan_allocation_ =\n            next_execution_plan_index_to_prepare_;\n        if (memory_planner_) {\n          TF_LITE_ENSURE_STATUS(memory_planner_->ResetAllocationsAfter(\n              next_execution_plan_index_to_plan_allocation_ - 1));\n        }\n      }\n    }\n  }\n\n  return status;\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // inputs must be at least a matrix\n    OP_REQUIRES(\n        ctx, sorted_inputs_t.shape().dims() >= 2,\n        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n    OP_REQUIRES_OK(\n        ctx, functor::UpperBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status PyArrayDescr_to_TF_DataType(PyArray_Descr* descr,\n                                   TF_DataType* out_tf_datatype) {\n  PyObject* key;\n  PyObject* value;\n  Py_ssize_t pos = 0;\n  if (PyDict_Next(descr->fields, &pos, &key, &value)) {\n    // In Python 3, the keys of numpy custom struct types are unicode, unlike\n    // Python 2, where the keys are bytes.\n    const char* key_string =\n        PyBytes_Check(key) ? PyBytes_AsString(key)\n                           : PyBytes_AsString(PyUnicode_AsASCIIString(key));\n    if (!key_string) {\n      return errors::Internal(\"Corrupt numpy type descriptor\");\n    }\n    tensorflow::string key = key_string;\n    // The typenames here should match the field names in the custom struct\n    // types constructed in test_util.py.\n    // TODO(mrry,keveman): Investigate Numpy type registration to replace this\n    // hard-coding of names.\n    if (key == \"quint8\") {\n      *out_tf_datatype = TF_QUINT8;\n    } else if (key == \"qint8\") {\n      *out_tf_datatype = TF_QINT8;\n    } else if (key == \"qint16\") {\n      *out_tf_datatype = TF_QINT16;\n    } else if (key == \"quint16\") {\n      *out_tf_datatype = TF_QUINT16;\n    } else if (key == \"qint32\") {\n      *out_tf_datatype = TF_QINT32;\n    } else if (key == \"resource\") {\n      *out_tf_datatype = TF_RESOURCE;\n    } else {\n      return errors::Internal(\"Unsupported numpy data type\");\n    }\n    return Status::OK();\n  }\n  return errors::Internal(\"Unsupported numpy data type\");\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void LinearAlgebraOp<InputScalar, OutputScalar>::PrepareOutputs(\n    OpKernelContext* context, const TensorShapes& input_matrix_shapes,\n    const TensorShape& batch_shape, TensorOutputs* outputs,\n    TensorShapes* output_matrix_shapes) {\n  // Get shape for each of the matrix outputs produced by the derived class.\n  *output_matrix_shapes = GetOutputMatrixShapes(input_matrix_shapes);\n  const int num_outputs = output_matrix_shapes->size();\n\n  // Make sure the number of op outputs is what the derived class expects.\n  OP_REQUIRES(\n      context, num_outputs <= context->num_outputs(),\n      errors::Internal(\n          \"Derived class expected more outputs (%d) that the op has (%d).\",\n          num_outputs, context->num_outputs()));\n\n  // Allocate outputs.\n  std::set<int> unused_inputs;\n  for (int input_idx = 0; input_idx < context->num_inputs(); ++input_idx) {\n    unused_inputs.insert(input_idx);\n  }\n  for (int output_idx = 0; output_idx < context->num_outputs(); ++output_idx) {\n    TensorShape output_tensor_shape({});\n    if (output_idx < num_outputs) {\n      // This output is used, set up output shape and allocate it.\n      const TensorShape& output_matrix_shape =\n          output_matrix_shapes->at(output_idx);\n      OP_REQUIRES(context, output_matrix_shape.dims() <= 2,\n                  errors::InvalidArgument(\n                      \"Rank of matrix output no. %d must be 0, 1 or 2, got %d.\",\n                      output_idx, output_matrix_shape.dims()));\n\n      // The final output has the shape of the outer batch dimensions\n      // concatenated with the output_matrix_shape (if the output is not\n      // scalar).\n      output_tensor_shape = batch_shape;\n      output_tensor_shape.AppendShape(output_matrix_shape);\n    }\n    Tensor* out = nullptr;\n    // See if there is an input buffer we can reuse for this output.\n    bool reused_input = false;\n    if (EnableInputForwarding()) {\n      for (int input_idx : unused_inputs) {\n        if (context->forward_input_to_output_with_shape(\n                input_idx, output_idx, output_tensor_shape, &out)) {\n          reused_input = true;\n          unused_inputs.erase(input_idx);\n          break;\n        }\n      }\n    }\n    if (!reused_input) {\n      OP_REQUIRES_OK(context, context->allocate_output(\n                                  output_idx, output_tensor_shape, &out));\n    }\n    outputs->emplace_back(out);\n  }\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_grad_backprop = context->input(2);\n\n    // For maxpooling3d, tensor_in should have 5 dimensions.\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    OP_REQUIRES(context, tensor_out.dims() == 5,\n                errors::InvalidArgument(\"tensor_out must be 5-dimensional\"));\n    // For maxpooling3d, out_grad_backprop should have 5 dimensions.\n    OP_REQUIRES(\n        context, out_grad_backprop.dims() == 5,\n        errors::InvalidArgument(\"out_grad_backprop must be 5-dimensional\"));\n\n    Pool3dParameters params{context,  ksize_,       stride_,\n                            padding_, data_format_, tensor_in.shape()};\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {2}, 0, tensor_out.shape(), &output));\n\n    LaunchMaxPooling3dGradGradOp<Device, T>::launch(\n        context, params, tensor_in, tensor_out, out_grad_backprop, output);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void MakeDataset(OpKernelContext* ctx, DatasetBase** output) override {\n    // Create a new SparseTensorSliceDatasetOp::Dataset, insert it in\n    // the step container, and return it as the output.\n    const Tensor* indices;\n    OP_REQUIRES_OK(ctx, ctx->input(\"indices\", &indices));\n    const Tensor* values;\n    OP_REQUIRES_OK(ctx, ctx->input(\"values\", &values));\n    const Tensor* dense_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"dense_shape\", &dense_shape));\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(indices->shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be a matrix but received shape \",\n                    indices->shape().DebugString()));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(values->shape()),\n                errors::InvalidArgument(\n                    \"Input values should be a vector but received shape \",\n                    indices->shape().DebugString()));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(dense_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shape should be a vector but received shape \",\n                    dense_shape->shape().DebugString()));\n\n    // We currently ensure that `sparse_tensor` is ordered in the\n    // batch dimension.\n    // TODO(mrry): Investigate ways to avoid this unconditional check\n    // if we can be sure that the sparse tensor was produced in an\n    // appropriate order (e.g. by `tf.parse_example()` or a Dataset\n    // that batches elements into rows of a SparseTensor).\n    int64_t previous_batch_index = -1;\n    for (int64_t i = 0; i < indices->dim_size(0); ++i) {\n      int64_t next_batch_index = indices->matrix<int64>()(i, 0);\n      OP_REQUIRES(\n          ctx, next_batch_index >= previous_batch_index,\n          errors::Unimplemented(\"The SparseTensor must be ordered in the batch \"\n                                \"dimension; handling arbitrarily ordered input \"\n                                \"is not currently supported.\"));\n      previous_batch_index = next_batch_index;\n    }\n    gtl::InlinedVector<int64, 8> std_order(dense_shape->NumElements(), 0);\n    sparse::SparseTensor tensor;\n    OP_REQUIRES_OK(\n        ctx, sparse::SparseTensor::Create(\n                 *indices, *values, TensorShape(dense_shape->vec<int64>()),\n                 std_order, &tensor));\n    *output = new Dataset<T>(ctx, std::move(tensor));\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status GetFirstDimensionSize(OpKernelContext* context, INDEX_TYPE* result) {\n    const Tensor first_partition_tensor =\n        context->input(kFirstPartitionInputIndex);\n    const RowPartitionType first_partition_type = row_partition_types_[0];\n    switch (first_partition_type) {\n      case RowPartitionType::FIRST_DIM_SIZE:\n        *result = first_partition_tensor.scalar<INDEX_TYPE>()();\n        return Status::OK();\n      case RowPartitionType::VALUE_ROWIDS:\n        return errors::InvalidArgument(\n            \"Cannot handle VALUE_ROWIDS in first dimension.\");\n      case RowPartitionType::ROW_SPLITS:\n        *result = first_partition_tensor.shape().dim_size(0) - 1;\n        return Status::OK();\n      default:\n        return errors::InvalidArgument(\n            \"Cannot handle type \",\n            RowPartitionTypeToString(first_partition_type));\n    }\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n\n    // MatrixDiagPart and MatrixDiagPartV2 both use this OpKernel.\n    // MatrixDiagPart only has one input, so we have to check the number of\n    // inputs before reading additional parameters in MatrixDiagV2.\n    int32_t lower_diag_index = 0;\n    int32_t upper_diag_index = 0;\n    T padding_value(0);\n\n    // MatrixDiagPartV2-specific.\n    if (context->num_inputs() > kNumV1Inputs) {\n      auto& diag_index = context->input(1);\n      OP_REQUIRES(context,\n                  TensorShapeUtils::IsScalar(diag_index.shape()) ||\n                      TensorShapeUtils::IsVector(diag_index.shape()),\n                  errors::InvalidArgument(\n                      \"diag_index must be a scalar or vector, received shape: \",\n                      diag_index.shape().DebugString()));\n      OP_REQUIRES(context, diag_index.NumElements() > 0,\n                  errors::InvalidArgument(\n                      \"Expected diag_index to have at least 1 element\"));\n      lower_diag_index = diag_index.flat<int32>()(0);\n      upper_diag_index = lower_diag_index;\n      if (TensorShapeUtils::IsVector(diag_index.shape())) {\n        auto diag_index_size = diag_index.dim_size(0);\n        OP_REQUIRES(\n            context, 0 < diag_index_size && diag_index_size <= 2,\n            errors::InvalidArgument(\n                \"diag_index must have only one or two elements, received \",\n                diag_index_size, \" elements.\"));\n        if (diag_index_size > 1) {\n          upper_diag_index = diag_index.flat<int32>()(1);\n        }\n      }\n      padding_value = context->input(2).flat<T>()(0);\n    }\n    const TensorShape& input_shape = input.shape();\n\n    // Preliminary validation of sizes.\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrixOrHigher(input_shape),\n                errors::InvalidArgument(\n                    \"input must be at least 2-dim, received shape: \",\n                    input.shape().DebugString()));\n\n    // Make sure lower_diag_index and upper_diag_index is valid.\n    const int rank = input_shape.dims();\n    const Eigen::Index num_rows = input_shape.dim_size(rank - 2);\n    const Eigen::Index num_cols = input_shape.dim_size(rank - 1);\n    OP_REQUIRES(  // Checks lower_diag_index == 0 for when matrix shape = 0.\n        context,\n        (-num_rows < lower_diag_index && lower_diag_index < num_cols) ||\n            lower_diag_index == 0,\n        errors::InvalidArgument(\n            \"lower_diag_index is out of bound: \", lower_diag_index,\n            \". It must be between \", -num_rows, \" and \", num_cols));\n    OP_REQUIRES(context,\n                (-num_rows < upper_diag_index && upper_diag_index < num_cols) ||\n                    upper_diag_index == 0,\n                errors::InvalidArgument(\n                    \"upper_diag_index is out of bound: \", upper_diag_index,\n                    \" It must be between \", -num_rows, \" and \", num_cols));\n    OP_REQUIRES(\n        context, lower_diag_index <= upper_diag_index,\n        errors::InvalidArgument(\n            \"lower_diag_index must not be larger than upper_diag_index: \",\n            lower_diag_index, \" > \", upper_diag_index));\n\n    TensorShape output_shape;\n    for (int i = 0; i < rank - 2; ++i) {\n      output_shape.AddDim(input_shape.dim_size(i));\n    }\n    const Eigen::Index num_diags = upper_diag_index - lower_diag_index + 1;\n    if (num_diags > 1) output_shape.AddDim(num_diags);\n    const int32_t max_diag_len =\n        std::min(num_rows + std::min(upper_diag_index, 0),\n                 num_cols - std::max(lower_diag_index, 0));\n    output_shape.AddDim(max_diag_len);\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &output));\n    auto output_reshaped = output->flat<T>();\n    auto input_reshaped = input.flat_inner_dims<T, 3>();\n    functor::MatrixDiagPart<Device, T>::Compute(\n        context, context->eigen_device<Device>(), input_reshaped,\n        output_reshaped, lower_diag_index, upper_diag_index, max_diag_len,\n        padding_value, left_align_superdiagonal_, left_align_subdiagonal_);\n  }\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    Tensor encoded_t = context->input(0);\n    OP_REQUIRES(\n        context, encoded_t.flat<Variant>().size() > 0,\n        errors::InvalidArgument(\"Input `encoded` must not be an empty variant \"\n                                \"tensor, but got \",\n                                encoded_t.DebugString()));\n    auto* encoded = encoded_t.flat<Variant>()(0).get<CompositeTensorVariant>();\n\n    // Check that the encoded TypeSpec is compatible with the expected TypeSpec.\n    // For now, we just check that the class matches.\n    //\n    // TODO(b/173744905): Update this to do a generic compatibility check. This\n    // would require replacing the current design, where Python subclasses of\n    // TypeSpec can override is_compatible, with a design where compatibility\n    // can be deterministically determined from the metadata.\n    auto expected_class = metadata_.type_spec_proto().type_spec_class();\n    auto actual_class = encoded->metadata().type_spec_proto().type_spec_class();\n    OP_REQUIRES(\n        context, expected_class == actual_class,\n        errors::InvalidArgument(\n            \"Expected a \", TypeSpecProto::TypeSpecClass_Name(expected_class),\n            \" (based on `type_spec`), but `encoded` contains a \",\n            TypeSpecProto::TypeSpecClass_Name(actual_class)));\n\n    // Extract the component tensors.\n    OpOutputList components;\n    OP_REQUIRES_OK(context, context->output_list(\"components\", &components));\n    int num_components = encoded->flat_components().size();\n\n    OP_REQUIRES(context, component_dtypes_.size() == num_components,\n                errors::InvalidArgument(\"Encoded value has \", num_components,\n                                        \" tensor components; expected \",\n                                        component_dtypes_.size(),\n                                        \" components based on type_spec\"));\n\n    for (int i = 0; i < component_dtypes_.size(); i++) {\n      const Tensor& component = encoded->flat_components()[i];\n      OP_REQUIRES(context, component_dtypes_[i] == component.dtype(),\n                  errors::InvalidArgument(\"Tensor component \", i, \" had dtype \",\n                                          DataType_Name(component.dtype()),\n                                          \"; expected dtype \",\n                                          DataType_Name(component_dtypes_[i])));\n      components.set(i, component);\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLiteSVDFParams*>(node->builtin_data);\n  OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* weights_feature;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kWeightsFeatureTensor,\n                                          &weights_feature));\n  const TfLiteTensor* weights_time;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, kWeightsTimeTensor, &weights_time));\n  const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);\n\n  TfLiteTensor* scratch;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, /*index=*/0, &scratch));\n\n  TfLiteTensor* state = GetVariableInput(context, node, kStateTensor);\n  TF_LITE_ENSURE(context, state != nullptr);\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  switch (weights_feature->type) {\n    case kTfLiteFloat32: {\n      reference_ops::EvalFloatSVDF(\n          params, GetTensorShape(input), GetTensorData<float>(input),\n          GetTensorShape(weights_feature),\n          GetTensorData<float>(weights_feature), GetTensorShape(weights_time),\n          GetTensorData<float>(weights_time), GetTensorShape(bias),\n          GetTensorData<float>(bias), GetTensorData<float>(scratch),\n          GetTensorData<float>(state), GetTensorShape(output),\n          GetTensorData<float>(output));\n      return kTfLiteOk;\n    }\n    case kTfLiteUInt8:\n    case kTfLiteInt8: {\n      if (input->type == kTfLiteFloat32) {\n        TfLiteTensor* input_quantized;\n        TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/1,\n                                                    &input_quantized));\n        TfLiteTensor* scaling_factors;\n        TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/2,\n                                                    &scaling_factors));\n        TfLiteTensor* float_weights_time;\n        TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/3,\n                                                    &float_weights_time));\n        TfLiteTensor* zero_points;\n        TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/4,\n                                                    &zero_points));\n        TfLiteTensor* row_sums;\n        TF_LITE_ENSURE_OK(\n            context, GetTemporarySafe(context, node, /*index=*/5, &row_sums));\n        // Dequantize weights time.\n        // TODO(alanchiao): this dequantization initialization only needs to\n        // happen once per model and should theoretically be placed in either\n        // Init or Prepare. However, TFLite doesn't allocate float_weights_time\n        // until the Eval function.\n        // TODO(alanchiao): refactor logic out into dequantize function.\n        if (!op_data->float_weights_time_initialized) {\n          const float dequantization_scale = weights_time->params.scale;\n          const int8_t* weights_time_ptr = GetTensorData<int8_t>(weights_time);\n          float* float_weights_time_ptr =\n              GetTensorData<float>(float_weights_time);\n          for (int i = 0; i < NumElements(float_weights_time); ++i) {\n            float_weights_time_ptr[i] =\n                weights_time_ptr[i] * dequantization_scale;\n          }\n          op_data->float_weights_time_initialized = true;\n        }\n\n        int32_t* zero_points_ptr = nullptr;\n        int32_t* row_sums_ptr = nullptr;\n        if (params->asymmetric_quantize_inputs && row_sums != nullptr) {\n          zero_points_ptr = GetTensorData<int32_t>(zero_points);\n          row_sums_ptr = GetTensorData<int32_t>(row_sums);\n        }\n\n        reference_ops::EvalHybridSVDF(\n            params, GetTensorShape(input), GetTensorData<float>(input),\n            GetTensorShape(weights_feature),\n            GetTensorData<int8_t>(weights_feature),\n            weights_feature->params.scale, GetTensorShape(float_weights_time),\n            GetTensorData<float>(float_weights_time), GetTensorShape(bias),\n            GetTensorData<float>(bias), GetTensorData<float>(scratch),\n            GetTensorData<float>(scaling_factors),\n            GetTensorData<int8_t>(input_quantized), GetTensorData<float>(state),\n            GetTensorShape(output), GetTensorData<float>(output),\n            zero_points_ptr, row_sums_ptr, &op_data->compute_row_sums);\n        return kTfLiteOk;\n      }\n      auto* input_params = reinterpret_cast<TfLiteAffineQuantization*>(\n          input->quantization.params);\n      auto* output_params = reinterpret_cast<TfLiteAffineQuantization*>(\n          output->quantization.params);\n      TfLiteTensor* output_temp;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, /*index=*/1, &output_temp));\n\n      // Currently supports only ReLU.\n      // TODO(jianlijianli): support other activations.\n      TF_LITE_ENSURE_EQ(context, params->activation, kTfLiteActRelu);\n\n      reference_ops::EvalIntegerSVDF(\n          params, GetTensorShape(input), GetTensorData<int8_t>(input),\n          GetTensorShape(weights_feature),\n          GetTensorData<int8_t>(weights_feature), GetTensorShape(weights_time),\n          GetTensorData<int16_t>(weights_time), GetTensorShape(bias),\n          GetTensorData<int32_t>(bias), GetTensorData<int16_t>(state),\n          GetTensorShape(output), GetTensorData<int8_t>(output),\n          GetTensorData<int32_t>(scratch), GetTensorData<int32_t>(output_temp),\n          op_data->effective_scale_1_a, op_data->effective_scale_1_b,\n          op_data->effective_scale_2_a, op_data->effective_scale_2_b,\n          input_params->zero_point->data[0],\n          output_params->zero_point->data[0]);\n      return kTfLiteOk;\n    }\n    default:\n      context->ReportError(context, \"Type %s not currently supported.\",\n                           TfLiteTypeGetName(weights_feature->type));\n      return kTfLiteError;\n  }\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void SparseFillEmptyRowsOpImpl(OpKernelContext* context,\n                               AsyncOpKernel::DoneCallback done = nullptr) {\n  // Note that setting this empty lambda as the default parameter value directly\n  // can cause strange compiler/linker errors, so we do it like this instead.\n  if (!done) {\n    done = [] {};\n  }\n\n  const int kIndicesInput = 0;\n  const int kValuesInput = 1;\n  const int kDenseShapeInput = 2;\n  const int kDefaultValueInput = 3;\n\n  const Tensor& indices_t = context->input(kIndicesInput);\n  const Tensor& values_t = context->input(kValuesInput);\n  const Tensor& dense_shape_t = context->input(kDenseShapeInput);\n  const Tensor& default_value_t = context->input(kDefaultValueInput);\n\n  OP_REQUIRES_ASYNC(\n      context, TensorShapeUtils::IsVector(dense_shape_t.shape()),\n      errors::InvalidArgument(\"dense_shape must be a vector, saw: \",\n                              dense_shape_t.shape().DebugString()),\n      done);\n  OP_REQUIRES_ASYNC(context, TensorShapeUtils::IsMatrix(indices_t.shape()),\n                    errors::InvalidArgument(\"indices must be a matrix, saw: \",\n                                            indices_t.shape().DebugString()),\n                    done);\n  OP_REQUIRES_ASYNC(context, TensorShapeUtils::IsVector(values_t.shape()),\n                    errors::InvalidArgument(\"values must be a vector, saw: \",\n                                            values_t.shape().DebugString()),\n                    done);\n  OP_REQUIRES_ASYNC(\n      context, TensorShapeUtils::IsScalar(default_value_t.shape()),\n      errors::InvalidArgument(\"default_value must be a scalar, saw: \",\n                              default_value_t.shape().DebugString()),\n      done);\n  // TODO(ebrevdo): add shape checks between values, indices,\n  // Also add check that dense rank > 0.\n  OP_REQUIRES_ASYNC(context, dense_shape_t.NumElements() != 0,\n                    errors::InvalidArgument(\"Dense shape cannot be empty.\"),\n                    done);\n\n  using FunctorType = functor::SparseFillEmptyRows<Device, T, Tindex>;\n  OP_REQUIRES_OK_ASYNC(context,\n                       FunctorType()(context, default_value_t, indices_t,\n                                     values_t, dense_shape_t, done),\n                       done);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    // Input tensor is of the following dimensions:\n    // [ batch, in_rows, in_cols, in_depth ]\n    const Tensor& input = context->input(0);\n\n    // Input filter is of the following dimensions:\n    // [ filter_rows, filter_cols, in_depth, out_depth]\n    const Tensor& filter = context->input(1);\n\n    // For 2D convolution, there should be 4 dimensions.\n    OP_REQUIRES(context, input.dims() == 4,\n                errors::InvalidArgument(\"input must be rank 4 but is rank \",\n                                        input.shape().dims()));\n    OP_REQUIRES(context, filter.dims() == 4,\n                errors::InvalidArgument(\"filter must be rank 4 but is rank \",\n                                        filter.shape().dims()));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(2).shape()),\n                errors::InvalidArgument(\"min_input must be rank 0 but is rank \",\n                                        context->input(2).shape().dims()));\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(3).shape()),\n                errors::InvalidArgument(\"max_input must be rank 0 but is rank \",\n                                        context->input(3).shape().dims()));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(context->input(4).shape()),\n        errors::InvalidArgument(\"min_filter must be rank 0 but is rank \",\n                                context->input(4).shape().dims()));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(context->input(5).shape()),\n        errors::InvalidArgument(\"max_filter must be rank 0 but is rank \",\n                                context->input(5).shape().dims()));\n\n    const float min_input = context->input(2).flat<float>()(0);\n    const float max_input = context->input(3).flat<float>()(0);\n    const float min_filter = context->input(4).flat<float>()(0);\n    const float max_filter = context->input(5).flat<float>()(0);\n    const int32_t offset_input =\n        FloatToQuantizedUnclamped<T1>(0.0f, min_input, max_input);\n    const int32_t offset_filter =\n        FloatToQuantizedUnclamped<T2>(0.0f, min_filter, max_filter);\n    const int32_t offset_output = 0;\n    const int32_t mult_output = 1;\n    const int32_t shift_output = 0;\n\n    // The last dimension for input is in_depth. It must be the same as the\n    // filter's in_depth.\n    const int64_t in_depth = input.dim_size(3);\n    OP_REQUIRES(context, in_depth == filter.dim_size(2),\n                errors::InvalidArgument(\n                    \"input and filter must have the same depth: \", in_depth,\n                    \" vs \", filter.dim_size(2)));\n\n    // The last dimension for filter is out_depth.\n    const int64_t out_depth = filter.dim_size(3);\n\n    // The second dimension for input is rows/height.\n    // The first dimension for filter is rows/height.\n    const int64_t input_rows = input.dim_size(1);\n    const int64_t filter_rows = filter.dim_size(0);\n\n    // The third dimension for input is columns/width.\n    // The second dimension for filter is columns/width.\n    const int64_t input_cols = input.dim_size(2);\n    const int64_t filter_cols = filter.dim_size(1);\n\n    // The first dimension for input is batch.\n    const int64_t batch = input.dim_size(0);\n\n    // For now we take the stride from the second dimension only (we\n    // assume row = col stride, and do not support striding on the\n    // batch or depth dimension).\n    const int stride = strides_[1];\n\n    int64_t out_rows = 0, out_cols = 0, pad_rows = 0, pad_cols = 0;\n    OP_REQUIRES_OK(context,\n                   GetWindowedOutputSize(input_rows, filter_rows, stride,\n                                         padding_, &out_rows, &pad_rows));\n    OP_REQUIRES_OK(context,\n                   GetWindowedOutputSize(input_cols, filter_cols, stride,\n                                         padding_, &out_cols, &pad_cols));\n    CHECK_GT(batch, 0);\n    CHECK_GT(out_rows, 0);\n    CHECK_GT(out_cols, 0);\n    CHECK_GT(out_depth, 0);\n    TensorShape out_shape({batch, out_rows, out_cols, out_depth});\n\n    // Output tensor is of the following dimensions:\n    // [ in_batch, out_rows, out_cols, out_depth ]\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n\n    // This will call different implementations (e.g. reference or optimized)\n    // depending on the template parameter.\n    ConvFunctor<T1, T2, T3> conv_functor;\n    conv_functor(context, input.flat<T1>().data(), batch, input_rows,\n                 input_cols, in_depth, offset_input, filter.flat<T2>().data(),\n                 filter_rows, filter_cols, out_depth, offset_filter, stride,\n                 padding_, output->flat<T3>().data(), out_rows, out_cols,\n                 shift_output, offset_output, mult_output);\n\n    float min_output_value;\n    float max_output_value;\n    QuantizationRangeForMultiplication<T1, T2, T3>(\n        min_input, max_input, min_filter, max_filter, &min_output_value,\n        &max_output_value);\n\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));\n    output_min->flat<float>()(0) = min_output_value;\n\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));\n    output_max->flat<float>()(0) = max_output_value;\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ModelWeights::Initialize(OpKernelContext* const context) {\n  OpInputList sparse_indices_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"sparse_indices\", &sparse_indices_inputs));\n  OpInputList sparse_weights_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"sparse_weights\", &sparse_weights_inputs));\n  if (sparse_indices_inputs.size() != sparse_weights_inputs.size())\n    return errors::InvalidArgument(\n        \"sparse_indices and sparse_weights must have the same length, got \",\n        sparse_indices_inputs.size(), \" and \", sparse_weights_inputs.size());\n  OpInputList dense_weights_inputs;\n  TF_RETURN_IF_ERROR(\n      context->input_list(\"dense_weights\", &dense_weights_inputs));\n\n  OpOutputList sparse_weights_outputs;\n  TF_RETURN_IF_ERROR(context->output_list(\"out_delta_sparse_weights\",\n                                          &sparse_weights_outputs));\n  if (sparse_weights_outputs.size() != sparse_weights_inputs.size())\n    return errors::InvalidArgument(\n        \"out_delta_sparse_weights and sparse_weights must have the same \"\n        \"length, got \",\n        sparse_weights_outputs.size(), \" and \", sparse_weights_inputs.size());\n\n  OpOutputList dense_weights_outputs;\n  TF_RETURN_IF_ERROR(\n      context->output_list(\"out_delta_dense_weights\", &dense_weights_outputs));\n  if (dense_weights_outputs.size() != dense_weights_inputs.size())\n    return errors::InvalidArgument(\n        \"out_delta_dense_weights and dense_weights must have the same length, \"\n        \"got \",\n        dense_weights_outputs.size(), \" and \", dense_weights_inputs.size());\n\n  for (int i = 0; i < sparse_weights_inputs.size(); ++i) {\n    Tensor* delta_t;\n    TF_RETURN_IF_ERROR(sparse_weights_outputs.allocate(\n        i, sparse_weights_inputs[i].shape(), &delta_t));\n    // Convert the input vector to a row matrix in internal representation.\n    auto deltas = delta_t->shaped<float, 2>({1, delta_t->NumElements()});\n    deltas.setZero();\n    sparse_weights_.emplace_back(FeatureWeightsSparseStorage{\n        sparse_indices_inputs[i].flat<int64>(),\n        sparse_weights_inputs[i].shaped<float, 2>(\n            {1, sparse_weights_inputs[i].NumElements()}),\n        deltas});\n  }\n\n  // Reads in the weights, and allocates and initializes the delta weights.\n  const auto initialize_weights =\n      [&](const OpInputList& weight_inputs, OpOutputList* const weight_outputs,\n          std::vector<FeatureWeightsDenseStorage>* const feature_weights) {\n        for (int i = 0; i < weight_inputs.size(); ++i) {\n          Tensor* delta_t;\n          TF_RETURN_IF_ERROR(\n              weight_outputs->allocate(i, weight_inputs[i].shape(), &delta_t));\n          // Convert the input vector to a row matrix in internal\n          // representation.\n          auto deltas = delta_t->shaped<float, 2>({1, delta_t->NumElements()});\n          deltas.setZero();\n          feature_weights->emplace_back(FeatureWeightsDenseStorage{\n              weight_inputs[i].shaped<float, 2>(\n                  {1, weight_inputs[i].NumElements()}),\n              deltas});\n        }\n        return Status::OK();\n      };\n\n  return initialize_weights(dense_weights_inputs, &dense_weights_outputs,\n                            &dense_weights_);\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    Tensor encoded_t = context->input(0);\n    OP_REQUIRES(\n        context, encoded_t.flat<Variant>().size() > 0,\n        errors::InvalidArgument(\"Input `encoded` must not be an empty variant \"\n                                \"tensor, but got \",\n                                encoded_t.DebugString()));\n    auto* encoded = encoded_t.flat<Variant>()(0).get<CompositeTensorVariant>();\n    OP_REQUIRES(context, encoded != nullptr,\n                errors::InvalidArgument(\"The input `encoded` is not a valid \"\n                                        \"CompositeTensorVariant tensor, got \",\n                                        encoded_t.DebugString()));\n\n    // Check that the encoded TypeSpec is compatible with the expected TypeSpec.\n    // For now, we just check that the class matches.\n    //\n    // TODO(b/173744905): Update this to do a generic compatibility check. This\n    // would require replacing the current design, where Python subclasses of\n    // TypeSpec can override is_compatible, with a design where compatibility\n    // can be deterministically determined from the metadata.\n    auto expected_class = metadata_.type_spec_proto().type_spec_class();\n    auto actual_class = encoded->metadata().type_spec_proto().type_spec_class();\n    OP_REQUIRES(\n        context, expected_class == actual_class,\n        errors::InvalidArgument(\n            \"Expected a \", TypeSpecProto::TypeSpecClass_Name(expected_class),\n            \" (based on `type_spec`), but `encoded` contains a \",\n            TypeSpecProto::TypeSpecClass_Name(actual_class)));\n\n    // Extract the component tensors.\n    OpOutputList components;\n    OP_REQUIRES_OK(context, context->output_list(\"components\", &components));\n    int num_components = encoded->flat_components().size();\n\n    OP_REQUIRES(context, component_dtypes_.size() == num_components,\n                errors::InvalidArgument(\"Encoded value has \", num_components,\n                                        \" tensor components; expected \",\n                                        component_dtypes_.size(),\n                                        \" components based on type_spec\"));\n\n    for (int i = 0; i < component_dtypes_.size(); i++) {\n      const Tensor& component = encoded->flat_components()[i];\n      OP_REQUIRES(context, component_dtypes_[i] == component.dtype(),\n                  errors::InvalidArgument(\"Tensor component \", i, \" had dtype \",\n                                          DataType_Name(component.dtype()),\n                                          \"; expected dtype \",\n                                          DataType_Name(component_dtypes_[i])));\n      components.set(i, component);\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& handle = ctx->input(0);\n    const string& name = handle.scalar<tstring>()();\n    auto session_state = ctx->session_state();\n    OP_REQUIRES(ctx, session_state != nullptr,\n                errors::FailedPrecondition(\n                    \"DeleteSessionTensor called on null session state\"));\n    OP_REQUIRES_OK(ctx, session_state->DeleteTensor(name));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void MakeDataset(OpKernelContext* ctx, DatasetBase** output) override {\n    // Create a new SparseTensorSliceDatasetOp::Dataset, insert it in\n    // the step container, and return it as the output.\n    const Tensor* indices;\n    OP_REQUIRES_OK(ctx, ctx->input(\"indices\", &indices));\n    const Tensor* values;\n    OP_REQUIRES_OK(ctx, ctx->input(\"values\", &values));\n    const Tensor* dense_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"dense_shape\", &dense_shape));\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(indices->shape()),\n                errors::InvalidArgument(\"Input indices must be a matrix. Got: \",\n                                        indices->shape().DebugString()));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(values->shape()),\n                errors::InvalidArgument(\"Input values must be a vector. Got: \",\n                                        values->shape().DebugString()));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(dense_shape->shape()),\n                errors::InvalidArgument(\"Input shape must be a vector. Got: \",\n                                        dense_shape->shape().DebugString()));\n    OP_REQUIRES(\n        ctx, values->shape().dim_size(0) == indices->shape().dim_size(0),\n        errors::InvalidArgument(\n            \"Number of values must match first dimension of indices. \", \"Got \",\n            values->shape().dim_size(0),\n            \" values, indices shape: \", indices->shape().DebugString()));\n    OP_REQUIRES(\n        ctx, dense_shape->shape().dim_size(0) == indices->shape().dim_size(1),\n        errors::InvalidArgument(\n            \"Number of dimensions must match second dimension of indices. \",\n            \"Got \", dense_shape->shape().dim_size(0),\n            \" dimensions, indices shape: \", indices->shape().DebugString()));\n    OP_REQUIRES(ctx, dense_shape->NumElements() > 0,\n                errors::InvalidArgument(\n                    \"The shape argument requires at least one element.\"));\n\n    // We currently ensure that `sparse_tensor` is ordered in the\n    // batch dimension.\n    // TODO(mrry): Investigate ways to avoid this unconditional check\n    // if we can be sure that the sparse tensor was produced in an\n    // appropriate order (e.g. by `tf.parse_example()` or a Dataset\n    // that batches elements into rows of a SparseTensor).\n    int64_t previous_batch_index = -1;\n    for (int64_t i = 0; i < indices->dim_size(0); ++i) {\n      int64_t next_batch_index = indices->matrix<int64_t>()(i, 0);\n      OP_REQUIRES(\n          ctx, next_batch_index >= previous_batch_index,\n          errors::Unimplemented(\"The SparseTensor must be ordered in the batch \"\n                                \"dimension; handling arbitrarily ordered input \"\n                                \"is not currently supported.\"));\n      previous_batch_index = next_batch_index;\n    }\n    gtl::InlinedVector<int64_t, 8> std_order(dense_shape->NumElements(), 0);\n    sparse::SparseTensor tensor;\n    OP_REQUIRES_OK(\n        ctx, sparse::SparseTensor::Create(\n                 *indices, *values, TensorShape(dense_shape->vec<int64_t>()),\n                 std_order, &tensor));\n    *output = new Dataset<T>(ctx, std::move(tensor));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext *ctx) override {\n    const Tensor *a_indices_t, *a_values_t, *a_shape_t, *b_indices_t,\n        *b_values_t, *b_shape_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_indices\", &a_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_shape\", &a_shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_indices\", &b_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_values\", &b_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_shape\", &b_shape_t));\n\n    // Validations.\n    OP_REQUIRES_OK(ctx, sparse_utils::ValidateSparseTensor<int64_t>(\n                            *a_indices_t, *a_values_t, *a_shape_t,\n                            sparse_utils::IndexValidation::kUnordered));\n    OP_REQUIRES_OK(ctx, sparse_utils::ValidateSparseTensor<int64_t>(\n                            *b_indices_t, *b_values_t, *b_shape_t,\n                            sparse_utils::IndexValidation::kUnordered));\n\n    const int64_t a_nnz = a_indices_t->dim_size(0);\n    const int64_t b_nnz = b_indices_t->dim_size(0);\n\n    const auto a_values = a_values_t->vec<T>();\n    const auto b_values = b_values_t->vec<T>();\n\n    const int num_dims = a_indices_t->dim_size(1);\n    OP_REQUIRES(ctx, num_dims > 0,\n                errors::InvalidArgument(\"Tensors must not be empty\"));\n    OP_REQUIRES(ctx, a_shape_t->IsSameSize(*b_shape_t),\n                errors::InvalidArgument(\n                    \"Operands do not have the same ranks; got shapes: \",\n                    a_shape_t->SummarizeValue(10), \" and \",\n                    b_shape_t->SummarizeValue(10)));\n    const auto a_shape = a_shape_t->flat<int64_t>();\n    const auto b_shape = b_shape_t->flat<int64_t>();\n    for (int i = 0; i < a_shape_t->NumElements(); ++i) {\n      OP_REQUIRES(ctx, a_shape(i) == b_shape(i),\n                  errors::InvalidArgument(\"Operands' shapes do not match: got \",\n                                          a_shape(i), \" and \", b_shape(i),\n                                          \" for dimension \", i));\n    }\n\n    const auto a_indices_mat = a_indices_t->matrix<int64_t>();\n    const auto b_indices_mat = b_indices_t->matrix<int64_t>();\n    std::vector<T> a_augmented_values, b_augmented_values;\n    std::vector<std::pair<bool, int64_t>> entries_to_copy;  // from_a?, idx\n    UnionSparseIndicesAndValues(a_indices_mat, a_values, a_nnz, b_indices_mat,\n                                b_values, b_nnz, num_dims, &a_augmented_values,\n                                &b_augmented_values, &entries_to_copy);\n\n    // Allocates and fills output tensors.\n    const int64_t sum_nnz = a_augmented_values.size();\n    Tensor *output_indices_t, *output_values_t;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({sum_nnz, num_dims}),\n                                        &output_indices_t));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(1, TensorShape({sum_nnz}), &output_values_t));\n    auto output_indices_mat = output_indices_t->matrix<int64_t>();\n\n    for (int64_t i = 0; i < sum_nnz; ++i) {\n      const bool from_a = entries_to_copy[i].first;\n      const int64_t idx = entries_to_copy[i].second;\n      output_indices_mat.chip<0>(i) =\n          from_a ? a_indices_mat.chip<0>(idx) : b_indices_mat.chip<0>(idx);\n    }\n\n    // Performs the functor operation using Eigen.\n    //\n    // Note that the two stack-allocated std::vector's may not be aligned. Using\n    // allocate_temp() would've given us aligned storage, but we do not know\n    // their sizes in advance, so we couldn't use allocate_temp() anyway.\n    //\n    // TODO(zongheng): measure if it's worthwhile to somehow force alignment.\n    using UnalignedTensorMap =\n        Eigen::TensorMap<Eigen::Tensor<const T, 1, Eigen::RowMajor>,\n                         Eigen::Unaligned>;\n    auto a_augmented_values_t =\n        UnalignedTensorMap(a_augmented_values.data(), sum_nnz);\n    auto b_augmented_values_t =\n        UnalignedTensorMap(b_augmented_values.data(), sum_nnz);\n    output_values_t->flat<T>().device(ctx->eigen_device<Device>()) =\n        a_augmented_values_t.binaryExpr(b_augmented_values_t,\n                                        typename Functor::func());\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Subgraph::Invoke() {\n  if (!consistent_) {\n    ReportError(\"Invoke called on model that is not consistent.\");\n    return kTfLiteError;\n  }\n\n  TfLiteStatus status = kTfLiteOk;\n  if (state_ == kStateUninvokable) {\n    ReportError(\"Invoke called on model that is not ready.\");\n    return kTfLiteError;\n  } else if (memory_planner_ && !memory_planner_->HasNonPersistentMemory()) {\n    ReportError(\"Non-persistent memory is not available.\");\n    return kTfLiteError;\n  }\n\n  // This is only needed for UseNNAPI(true);\n  if (should_apply_nnapi_delegate_ && !applied_nnapi_delegate_) {\n    TF_LITE_ENSURE_OK(&context_, ModifyGraphWithDelegate(NnApiDelegate()));\n    // only need to modify the graph once upon the first invocation.\n    applied_nnapi_delegate_ = true;\n  }\n\n  // Invocations are always done in node order.\n  // Note that calling Invoke repeatedly will cause the original memory plan to\n  // be reused, unless either ResizeInputTensor() or AllocateTensors() has been\n  // called.\n  for (int execution_plan_index = 0;\n       execution_plan_index < execution_plan_.size(); execution_plan_index++) {\n    if (execution_plan_index == next_execution_plan_index_to_prepare_) {\n      TF_LITE_ENSURE_STATUS(PrepareOpsAndTensors());\n      TF_LITE_ENSURE(&context_, next_execution_plan_index_to_prepare_ >=\n                                    execution_plan_index);\n    }\n    int node_index = execution_plan_[execution_plan_index];\n    TfLiteNode& node = nodes_and_registration_[node_index].first;\n    const TfLiteRegistration& registration =\n        nodes_and_registration_[node_index].second;\n\n    const char* op_name = nullptr;\n    if (profiler_) op_name = GetTFLiteOpName(registration);\n    TFLITE_SCOPED_TAGGED_OPERATOR_PROFILE(profiler_.get(), op_name, node_index);\n\n    // TODO(ycling): This is an extra loop through inputs to check if the data\n    // need to be copied from Delegate buffer to raw memory, which is often not\n    // needed. We may want to cache this in prepare to know if this needs to be\n    // done for a node or not.\n    for (int i = 0; i < node.inputs->size; ++i) {\n      int tensor_index = node.inputs->data[i];\n      if (tensor_index == kTfLiteOptionalTensor) {\n        continue;\n      }\n      TfLiteTensor* tensor = &tensors_[tensor_index];\n      if (tensor->delegate && tensor->delegate != node.delegate &&\n          tensor->data_is_stale) {\n        TF_LITE_ENSURE_STATUS(EnsureTensorDataIsReadable(tensor_index));\n      }\n      if (tensor->data.raw == nullptr && tensor->bytes > 0) {\n        if (registration.builtin_code == kTfLiteBuiltinReshape && i == 1) {\n          // In general, having a tensor here with no buffer will be an error.\n          // However, for the reshape operator, the second input tensor is only\n          // used for the shape, not for the data. Thus, null buffer is ok.\n          continue;\n        } else {\n          // In all other cases, we need to return an error as otherwise we will\n          // trigger a null pointer dereference (likely).\n          ReportError(\"Input tensor %d lacks data\", tensor_index);\n          return kTfLiteError;\n        }\n      }\n    }\n\n    if (check_cancelled_func_ != nullptr &&\n        check_cancelled_func_(cancellation_data_)) {\n      ReportError(\"Client requested cancel during Invoke()\");\n      return kTfLiteError;\n    }\n\n    EnsureTensorsVectorCapacity();\n    tensor_resized_since_op_invoke_ = false;\n    if (OpInvoke(registration, &node) != kTfLiteOk) {\n      return ReportOpError(&context_, node, registration, node_index,\n                           \"failed to invoke\");\n    }\n\n    // Force execution prep for downstream ops if the latest op triggered the\n    // resize of a dynamic tensor.\n    if (tensor_resized_since_op_invoke_ &&\n        HasDynamicTensor(context_, node.outputs)) {\n      next_execution_plan_index_to_prepare_ = execution_plan_index + 1;\n\n      // This happens when an intermediate dynamic tensor is resized.\n      // We don't have to prepare all the ops, but we need to recompute\n      // the allocation plan.\n      if (next_execution_plan_index_to_plan_allocation_ >\n          next_execution_plan_index_to_prepare_) {\n        next_execution_plan_index_to_plan_allocation_ =\n            next_execution_plan_index_to_prepare_;\n        if (memory_planner_) {\n          TF_LITE_ENSURE_STATUS(memory_planner_->ResetAllocationsAfter(\n              next_execution_plan_index_to_plan_allocation_ - 1));\n        }\n      }\n    }\n  }\n\n  return status;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "bool SetOpAttrList(TFE_Context* ctx, TFE_Op* op, const char* key,\n                   PyObject* py_list, TF_AttrType type,\n                   tensorflow::gtl::FlatMap<string, int64_t>* attr_list_sizes,\n                   TF_Status* status) {\n  if (!PySequence_Check(py_list)) {\n    TF_SetStatus(\n        status, TF_INVALID_ARGUMENT,\n        tensorflow::strings::StrCat(\"Expecting sequence value for attr \", key,\n                                    \", got \", py_list->ob_type->tp_name)\n            .c_str());\n    return false;\n  }\n  const int num_values = PySequence_Size(py_list);\n  if (attr_list_sizes != nullptr) (*attr_list_sizes)[key] = num_values;\n\n#define PARSE_LIST(c_type, parse_fn)                                       \\\n  std::unique_ptr<c_type[]> values(new c_type[num_values]);                \\\n  for (int i = 0; i < num_values; ++i) {                                   \\\n    tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));    \\\n    if (py_value == nullptr) {                                             \\\n      TF_SetStatus(status, TF_INVALID_ARGUMENT,                            \\\n                   tensorflow::strings::StrCat(                            \\\n                       \"Expecting sequence of \" #c_type \" for attr \", key, \\\n                       \", got \", py_list->ob_type->tp_name)                \\\n                       .c_str());                                          \\\n      return false;                                                        \\\n    } else if (!parse_fn(key, py_value.get(), status, &values[i])) {       \\\n      return false;                                                        \\\n    }                                                                      \\\n  }\n\n  if (type == TF_ATTR_STRING) {\n    std::unique_ptr<const void*[]> values(new const void*[num_values]);\n    std::unique_ptr<size_t[]> lengths(new size_t[num_values]);\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::StringPiece value;\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      if (!ParseStringValue(key, py_value.get(), status, &value)) return false;\n      values[i] = value.data();\n      lengths[i] = value.size();\n    }\n    TFE_OpSetAttrStringList(op, key, values.get(), lengths.get(), num_values);\n  } else if (type == TF_ATTR_INT) {\n    PARSE_LIST(int64_t, ParseInt64Value);\n    TFE_OpSetAttrIntList(op, key, values.get(), num_values);\n  } else if (type == TF_ATTR_FLOAT) {\n    PARSE_LIST(float, ParseFloatValue);\n    TFE_OpSetAttrFloatList(op, key, values.get(), num_values);\n  } else if (type == TF_ATTR_BOOL) {\n    PARSE_LIST(unsigned char, ParseBoolValue);\n    TFE_OpSetAttrBoolList(op, key, values.get(), num_values);\n  } else if (type == TF_ATTR_TYPE) {\n    PARSE_LIST(int, ParseTypeValue);\n    TFE_OpSetAttrTypeList(op, key,\n                          reinterpret_cast<const TF_DataType*>(values.get()),\n                          num_values);\n  } else if (type == TF_ATTR_SHAPE) {\n    // Make one pass through the input counting the total number of\n    // dims across all the input lists.\n    int total_dims = 0;\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      if (py_value.get() != Py_None) {\n        if (!PySequence_Check(py_value.get())) {\n          TF_SetStatus(\n              status, TF_INVALID_ARGUMENT,\n              tensorflow::strings::StrCat(\n                  \"Expecting None or sequence value for element\", i,\n                  \" of attr \", key, \", got \", py_value->ob_type->tp_name)\n                  .c_str());\n          return false;\n        }\n        const auto size = TensorShapeNumDims(py_value.get());\n        if (size >= 0) {\n          total_dims += size;\n        }\n      }\n    }\n    // Allocate a buffer that can fit all of the dims together.\n    std::unique_ptr<int64_t[]> buffer(new int64_t[total_dims]);\n    // Copy the input dims into the buffer and set dims to point to\n    // the start of each list's dims.\n    std::unique_ptr<const int64_t*[]> dims(new const int64_t*[num_values]);\n    std::unique_ptr<int[]> num_dims(new int[num_values]);\n    int64_t* offset = buffer.get();\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      if (py_value.get() == Py_None) {\n        dims[i] = nullptr;\n        num_dims[i] = -1;\n      } else {\n        const auto size = TensorShapeNumDims(py_value.get());\n        if (size == -1) {\n          dims[i] = nullptr;\n          num_dims[i] = -1;\n          continue;\n        }\n        dims[i] = offset;\n        num_dims[i] = size;\n        for (int j = 0; j < size; ++j) {\n          tensorflow::Safe_PyObjectPtr inner_py_value(\n              PySequence_ITEM(py_value.get(), j));\n          if (inner_py_value.get() == Py_None) {\n            *offset = -1;\n          } else if (!ParseDimensionValue(key, inner_py_value.get(), status,\n                                          offset)) {\n            return false;\n          }\n          ++offset;\n        }\n      }\n    }\n    TFE_OpSetAttrShapeList(op, key, dims.get(), num_dims.get(), num_values,\n                           status);\n    if (!status->status.ok()) return false;\n  } else if (type == TF_ATTR_FUNC) {\n    std::unique_ptr<const TFE_Op*[]> funcs(new const TFE_Op*[num_values]);\n    for (int i = 0; i < num_values; ++i) {\n      tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));\n      // Allow:\n      // (1) String function name, OR\n      // (2) A Python object with a .name attribute\n      //     (A crude test for being a\n      //     tensorflow.python.framework.function._DefinedFunction)\n      //     (which is what the various \"defun\" or \"Defun\" decorators do).\n      // And in the future also allow an object that can encapsulate\n      // the function name and its attribute values.\n      tensorflow::StringPiece func_name;\n      if (!ParseStringValue(key, py_value.get(), status, &func_name)) {\n        PyObject* name_attr = PyObject_GetAttrString(py_value.get(), \"name\");\n        if (name_attr == nullptr ||\n            !ParseStringValue(key, name_attr, status, &func_name)) {\n          TF_SetStatus(\n              status, TF_INVALID_ARGUMENT,\n              tensorflow::strings::StrCat(\n                  \"unable to set function value attribute from a \",\n                  py_value.get()->ob_type->tp_name,\n                  \" object. If you think this is an error, please file an \"\n                  \"issue at \"\n                  \"https://github.com/tensorflow/tensorflow/issues/new\")\n                  .c_str());\n          return false;\n        }\n      }\n      funcs[i] = TFE_NewOp(ctx, func_name.data(), status);\n      if (!status->status.ok()) return false;\n    }\n    TFE_OpSetAttrFunctionList(op, key, funcs.get(), num_values);\n    if (!status->status.ok()) return false;\n  } else {\n    TF_SetStatus(status, TF_UNIMPLEMENTED,\n                 tensorflow::strings::StrCat(\"Attr \", key,\n                                             \" has unhandled list type \", type)\n                     .c_str());\n    return false;\n  }\n#undef PARSE_LIST\n  return true;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ValidateInputs(const Tensor *a_indices, const Tensor *a_values,\n                      const Tensor *a_shape, const Tensor *b) {\n  if (!TensorShapeUtils::IsMatrix(a_indices->shape())) {\n    return errors::InvalidArgument(\n        \"Input a_indices should be a matrix but received shape: \",\n        a_indices->shape().DebugString());\n  }\n  if (!TensorShapeUtils::IsVector(a_values->shape()) ||\n      !TensorShapeUtils::IsVector(a_shape->shape())) {\n    return errors::InvalidArgument(\n        \"Inputs a_values and a_shape should be vectors \"\n        \"but received shapes: \",\n        a_values->shape().DebugString(), \" and \",\n        a_shape->shape().DebugString());\n  }\n  int64_t nnz = a_indices->dim_size(0);\n  int64_t ndims = a_indices->dim_size(1);\n  if (a_values->dim_size(0) != nnz) {\n    return errors::InvalidArgument(\"Dimensions \", nnz, \" and \",\n                                   a_values->dim_size(0),\n                                   \" are not compatible\");\n  }\n  if (a_shape->dim_size(0) != ndims) {\n    return errors::InvalidArgument(\"Dimensions \", ndims, \" and \",\n                                   a_shape->dim_size(0), \" are not compatible\");\n  }\n  if (a_shape->NumElements() != b->dims()) {\n    return errors::InvalidArgument(\n        \"Two operands have different ranks; received: \", a_shape->NumElements(),\n        \" and \", b->dims());\n  }\n  const auto a_shape_flat = a_shape->flat<Index>();\n  for (int i = 0; i < b->dims(); ++i) {\n    if (a_shape_flat(i) != b->dim_size(i)) {\n      return errors::InvalidArgument(\n          \"Dimension \", i,\n          \" does not equal (no broadcasting is supported): sparse side \",\n          a_shape_flat(i), \" vs dense side \", b->dim_size(i));\n    }\n  }\n\n  // Check for invalid indices.\n  const auto a_indices_mat = a_indices->flat_inner_dims<Index>();\n\n  for (int64_t zidx = 0; zidx < nnz; ++zidx) {\n    for (int64_t didx = 0; didx < ndims; ++didx) {\n      const Index idx = a_indices_mat(zidx, didx);\n      if (idx < 0 || idx >= a_shape_flat(didx)) {\n        return errors::InvalidArgument(\n            \"Sparse tensor has an invalid index on dimension \", didx,\n            \": \"\n            \"a_indices(\",\n            zidx, \",\", didx, \") = \", idx,\n            \", dense tensor shape: \", a_shape_flat);\n      }\n    }\n  }\n\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& prefix = context->input(0);\n    const Tensor& tensor_names = context->input(1);\n    const Tensor& shape_and_slices = context->input(2);\n    ValidateInputs(true /* is save op */, context, prefix, tensor_names,\n                   shape_and_slices);\n    if (!context->status().ok()) return;\n\n    const int kFixedInputs = 3;  // Prefix, tensor names, shape_and_slices.\n    const int num_tensors = static_cast<int>(tensor_names.NumElements());\n    const string& prefix_string = prefix.scalar<tstring>()();\n    const auto& tensor_names_flat = tensor_names.flat<tstring>();\n    const auto& shape_and_slices_flat = shape_and_slices.flat<tstring>();\n\n    BundleWriter writer(Env::Default(), prefix_string);\n    OP_REQUIRES_OK(context, writer.status());\n    VLOG(1) << \"BundleWriter, prefix_string: \" << prefix_string;\n\n    for (int i = 0; i < num_tensors; ++i) {\n      const string& tensor_name = tensor_names_flat(i);\n      const Tensor& tensor = context->input(i + kFixedInputs);\n      VLOG(2) << \"Starting save of \" << tensor_name;\n\n      if (!shape_and_slices_flat(i).empty()) {\n        const string& shape_spec = shape_and_slices_flat(i);\n        TensorShape shape;\n        TensorSlice slice(tensor.dims());\n        TensorShape slice_shape;\n\n        OP_REQUIRES_OK(context, checkpoint::ParseShapeAndSlice(\n                                    shape_spec, &shape, &slice, &slice_shape));\n        OP_REQUIRES(context, slice_shape.IsSameSize(tensor.shape()),\n                    errors::InvalidArgument(\"Slice in shape_and_slice \"\n                                            \"specification does not match the \"\n                                            \"shape of the tensor to  save: \",\n                                            shape_spec, \", tensor: \",\n                                            tensor.shape().DebugString()));\n\n        OP_REQUIRES_OK(context,\n                       writer.AddSlice(tensor_name, shape, slice, tensor));\n      } else {\n        OP_REQUIRES_OK(context, writer.Add(tensor_name, tensor));\n      }\n\n      if (VLOG_IS_ON(5)) {\n        if (tensor.dtype() == DT_FLOAT) {\n          const float* t_data = tensor.flat<float>().data();\n          float min = std::numeric_limits<float>::infinity();\n          float max = -std::numeric_limits<float>::infinity();\n          double avg = 0.0;\n          for (int i = 0; i < tensor.NumElements(); ++i) {\n            if (t_data[i] < min) min = t_data[i];\n            if (t_data[i] > max) max = t_data[i];\n            avg += t_data[i];\n          }\n          VLOG(5) << \" min \" << min << \" max \" << max << \" avg \"\n                  << avg / tensor.NumElements() << \" total elts \"\n                  << tensor.NumElements();\n        }\n      }\n\n      VLOG(2) << \"Done save of \" << tensor_name;\n    }\n    OP_REQUIRES_OK(context, writer.Finish());\n    VLOG(1) << \"Done BundleWriter, prefix_string: \" << prefix_string;\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "StatusOr<unsigned> GraphDefImporter::ArgNumType(const NamedAttrList &attrs,\n                                                const OpDef::ArgDef &arg_def,\n                                                SmallVectorImpl<Type> &types) {\n  // Check whether a type list attribute is specified.\n  if (!arg_def.type_list_attr().empty()) {\n    if (auto v =\n            attrs.get(arg_def.type_list_attr()).dyn_cast_or_null<ArrayAttr>()) {\n      for (Attribute attr : v) {\n        if (auto dtype = attr.dyn_cast<TypeAttr>()) {\n          types.push_back(UnrankedTensorType::get(dtype.getValue()));\n        } else {\n          return InvalidArgument(\"Expected '\", arg_def.type_list_attr(),\n                                 \"' to be a list of types\");\n        }\n      }\n      return v.size();\n    }\n    return NotFound(\"Type attr not found: \", arg_def.type_list_attr());\n  }\n\n  unsigned num = 1;\n  // Check whether a number attribute is specified.\n  if (!arg_def.number_attr().empty()) {\n    if (auto v =\n            attrs.get(arg_def.number_attr()).dyn_cast_or_null<IntegerAttr>()) {\n      num = v.getValue().getZExtValue();\n    } else {\n      return NotFound(\"Type attr not found: \", arg_def.number_attr());\n    }\n  }\n\n  // Check for a type or type attribute.\n  Type dtype;\n  if (arg_def.type() != DataType::DT_INVALID) {\n    TF_RETURN_IF_ERROR(ConvertDataType(arg_def.type(), b_, &dtype));\n  } else if (arg_def.type_attr().empty()) {\n    return InvalidArgument(\"Arg '\", arg_def.name(),\n                           \"' has invalid type and no type attribute\");\n  } else {\n    if (auto v = attrs.get(arg_def.type_attr()).dyn_cast_or_null<TypeAttr>()) {\n      dtype = v.getValue();\n    } else {\n      return NotFound(\"Type attr not found: \", arg_def.type_attr());\n    }\n  }\n  types.append(num, UnrankedTensorType::get(dtype));\n  return num;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor* hypothesis_indices;\n    const Tensor* hypothesis_values;\n    const Tensor* hypothesis_shape;\n    const Tensor* truth_indices;\n    const Tensor* truth_values;\n    const Tensor* truth_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_indices\", &hypothesis_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_values\", &hypothesis_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_shape\", &hypothesis_shape));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_indices\", &truth_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_values\", &truth_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_shape\", &truth_shape));\n\n    OP_REQUIRES_OK(\n        ctx, ValidateShapes(ctx, *hypothesis_indices, *hypothesis_values,\n                            *hypothesis_shape, *truth_indices, *truth_values,\n                            *truth_shape));\n\n    TensorShape hypothesis_st_shape;\n    OP_REQUIRES_OK(\n        ctx, TensorShapeUtils::MakeShape(hypothesis_shape->vec<int64>().data(),\n                                         hypothesis_shape->NumElements(),\n                                         &hypothesis_st_shape));\n    TensorShape truth_st_shape;\n    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n                            truth_shape->vec<int64>().data(),\n                            truth_shape->NumElements(), &truth_st_shape));\n\n    // Assume indices are sorted in row-major order.\n    std::vector<int64> sorted_order(truth_st_shape.dims());\n    std::iota(sorted_order.begin(), sorted_order.end(), 0);\n\n    sparse::SparseTensor hypothesis;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *hypothesis_indices, *hypothesis_values,\n                            hypothesis_st_shape, sorted_order, &hypothesis));\n\n    sparse::SparseTensor truth;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *truth_indices, *truth_values, truth_st_shape,\n                            sorted_order, &truth));\n\n    // Group dims 0, 1, ..., RANK - 1.  The very last dim is assumed\n    // to store the variable length sequences.\n    std::vector<int64> group_dims(truth_st_shape.dims() - 1);\n    std::iota(group_dims.begin(), group_dims.end(), 0);\n\n    TensorShape output_shape;\n    for (int d = 0; d < static_cast<int>(group_dims.size()); ++d) {\n      output_shape.AddDim(std::max(hypothesis_st_shape.dim_size(d),\n                                   truth_st_shape.dim_size(d)));\n    }\n    const auto output_elements = output_shape.num_elements();\n    OP_REQUIRES(\n        ctx, output_elements > 0,\n        errors::InvalidArgument(\"Got output shape \", output_shape.DebugString(),\n                                \" which has 0 elements\"));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"output\", output_shape, &output));\n    auto output_t = output->flat<float>();\n    output_t.setZero();\n\n    std::vector<int64> output_strides(output_shape.dims());\n    output_strides[output_shape.dims() - 1] = 1;\n    for (int d = output_shape.dims() - 2; d >= 0; --d) {\n      output_strides[d] = output_strides[d + 1] * output_shape.dim_size(d + 1);\n    }\n\n    auto hypothesis_grouper = hypothesis.group(group_dims);\n    auto truth_grouper = truth.group(group_dims);\n\n    auto hypothesis_iter = hypothesis_grouper.begin();\n    auto truth_iter = truth_grouper.begin();\n\n    auto cmp = std::equal_to<T>();\n\n    while (hypothesis_iter != hypothesis_grouper.end() &&\n           truth_iter != truth_grouper.end()) {\n      sparse::Group truth_i = *truth_iter;\n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64> g_truth = truth_i.group();\n      std::vector<int64> g_hypothesis = hypothesis_j.group();\n      auto truth_seq = truth_i.values<T>();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n\n      if (g_truth == g_hypothesis) {\n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) =\n            gtl::LevenshteinDistance<T>(truth_seq, hypothesis_seq, cmp);\n        if (normalize_) output_t(loc) /= truth_seq.size();\n\n        ++hypothesis_iter;\n        ++truth_iter;\n      } else if (g_truth > g_hypothesis) {  // zero-length truth\n        auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                      output_strides.begin(), int64{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = hypothesis_seq.size();\n        if (normalize_ && output_t(loc) != 0.0f) {\n          output_t(loc) = std::numeric_limits<float>::infinity();\n        }\n        ++hypothesis_iter;\n      } else {  // zero-length hypothesis\n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n        ++truth_iter;\n      }\n    }\n    while (hypothesis_iter != hypothesis_grouper.end()) {  // zero-length truths\n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64> g_hypothesis = hypothesis_j.group();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                    output_strides.begin(), int64{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = hypothesis_seq.size();\n      if (normalize_ && output_t(loc) != 0.0f) {\n        output_t(loc) = std::numeric_limits<float>::infinity();\n      }\n      ++hypothesis_iter;\n    }\n    while (truth_iter != truth_grouper.end()) {  // missing hypotheses\n      sparse::Group truth_i = *truth_iter;\n      std::vector<int64> g_truth = truth_i.group();\n      auto truth_seq = truth_i.values<T>();\n      auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                    output_strides.begin(), int64{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n      ++truth_iter;\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void CropInferMeta(const MetaTensor& x,\n                   const IntArray& shape,\n                   const IntArray& offsets,\n                   MetaTensor* out,\n                   MetaConfig config) {\n  PADDLE_ENFORCE_NE(\n      out,\n      nullptr,\n      errors::InvalidArgument(\"CropTensor should have output tensor out.\"));\n\n  auto x_dim = x.dims();\n  auto shape_dims = shape.GetData();\n  auto offsets_vec = offsets.GetData();\n\n  PADDLE_ENFORCE_EQ(shape_dims.size(),\n                    x_dim.size(),\n                    errors::InvalidArgument(\n                        \"The number of elements (%d) of attribute 'shape' for \"\n                        \"CropTensor must be equal to the number of \"\n                        \"dimensions (%d) of the input.\",\n                        shape_dims.size(),\n                        x_dim.size()));\n\n  PADDLE_ENFORCE_EQ(\n      offsets_vec.size(),\n      x_dim.size(),\n      errors::InvalidArgument(\n          \"The number of elements (%d) of attribute 'offsets' for \"\n          \"CropTensor must be equal to the number of \"\n          \"dimensions (%d) of the input.\",\n          offsets_vec.size(),\n          x_dim.size()));\n\n  if (config.is_runtime) {\n    out->share_lod(x);\n  }\n\n  auto out_dims = std::vector<int64_t>(shape.size(), -1);\n  for (int i = 0; i < static_cast<int>(shape_dims.size()); ++i) {\n    if (shape_dims[i] > 0) {\n      out_dims[i] = static_cast<int64_t>(shape_dims[i]);\n    } else {\n      if (shape_dims[i] == -1 && offsets_vec[i] != -1 && x_dim[i] != -1) {\n        out_dims[i] = x_dim[i] - static_cast<int64_t>(offsets_vec[i]);\n      }\n    }\n  }\n  out->set_dims(common::make_ddim(out_dims));\n  out->set_dtype(x.dtype());\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "const SegmentCommand* Binary::segment_from_offset(uint64_t offset) const {\n  if (offset_seg_.empty()) {\n    return nullptr;\n  }\n\n  const auto it_begin = std::begin(offset_seg_);\n  if (offset < it_begin->first) {\n    return nullptr;\n  }\n\n  auto it = offset_seg_.lower_bound(offset);\n  if (it->first == offset || it == it_begin) {\n    SegmentCommand* seg = it->second;\n    if (seg->file_offset() <= offset && offset < (seg->file_offset() + seg->file_size())) {\n      return seg;\n    }\n  }\n\n  const auto it_end = offset_seg_.crbegin();\n  if (it == std::end(offset_seg_) && offset >= it_end->first) {\n    SegmentCommand* seg = it_end->second;\n    if (seg->file_offset() <= offset && offset < (seg->file_offset() + seg->file_size())) {\n      return seg;\n    }\n  }\n\n  if (it == it_begin) {\n    /* It did not pass the first check */\n    return nullptr;\n  }\n\n  --it;\n\n  SegmentCommand* seg = it->second;\n  if (seg->file_offset() <= offset && offset < (seg->file_offset() + seg->file_size())) {\n    return seg;\n  }\n  return nullptr;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteTensor* GetVariableInput(TfLiteContext* context, const TfLiteNode* node,\n                               int index) {\n  TfLiteTensor* tensor = GetMutableInput(context, node, index);\n  if (tensor == nullptr) return nullptr;\n  return tensor->is_variable ? tensor : nullptr;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status GetTensorArray(OpKernelContext* ctx, TensorArray** tensor_array) {\n  string container;\n  string ta_handle;\n  if (ctx->input_dtype(0) != DT_RESOURCE) {\n    TF_RETURN_IF_ERROR(GetHandle(ctx, &container, &ta_handle));\n    ResourceMgr* rm = ctx->resource_manager();\n    if (rm == nullptr) return errors::Internal(\"No resource manager.\");\n    ScopedStepContainer* sc = ctx->step_container();\n    if (sc == nullptr) return errors::Internal(\"No step container.\");\n    TF_RETURN_IF_ERROR(sc->Lookup(rm, container + ta_handle, tensor_array));\n    return OkStatus();\n  } else {\n    return LookupResource(ctx, HandleFromInput(ctx, 0), tensor_array);\n  }\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status ValidateInputs(const CSRSparseMatrix& sparse_matrix,\n                        const Tensor& permutation_indices, int* batch_size,\n                        int64* num_rows) {\n    if (sparse_matrix.dtype() != DataTypeToEnum<T>::value)\n      return errors::InvalidArgument(\n          \"Asked for a CSRSparseMatrix of type \",\n          DataTypeString(DataTypeToEnum<T>::value),\n          \" but saw dtype: \", DataTypeString(sparse_matrix.dtype()));\n\n    const Tensor& dense_shape = sparse_matrix.dense_shape();\n    const int rank = dense_shape.dim_size(0);\n    if (rank < 2 || rank > 3)\n      return errors::InvalidArgument(\"sparse matrix must have rank 2 or 3; \",\n                                     \"but dense_shape has size \", rank);\n    const int row_dim = (rank == 2) ? 0 : 1;\n    auto dense_shape_vec = dense_shape.vec<int64>();\n    *num_rows = dense_shape_vec(row_dim);\n    const int64 num_cols = dense_shape_vec(row_dim + 1);\n    if (*num_rows != num_cols)\n      return errors::InvalidArgument(\n          \"sparse matrix must be square; got: \", *num_rows, \" != \", num_cols);\n    const TensorShape& perm_shape = permutation_indices.shape();\n    if (perm_shape.dims() + 1 != rank)\n      return errors::InvalidArgument(\n          \"sparse matrix must have the same rank as permutation; got: \", rank,\n          \" != \", perm_shape.dims(), \" + 1.\");\n    if (perm_shape.dim_size(rank - 2) != *num_rows)\n      return errors::InvalidArgument(\n          \"permutation must have the same number of elements in each batch \"\n          \"as the number of rows in sparse matrix; got: \",\n          perm_shape.dim_size(rank - 2), \" != \", *num_rows);\n\n    *batch_size = sparse_matrix.batch_size();\n    if (*batch_size > 1) {\n      if (perm_shape.dim_size(0) != *batch_size)\n        return errors::InvalidArgument(\n            \"permutation must have the same batch size \"\n            \"as sparse matrix; got: \",\n            perm_shape.dim_size(0), \" != \", *batch_size);\n    }\n\n    return Status::OK();\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void UncompressElementOp::Compute(OpKernelContext* ctx) {\n  Tensor tensor = ctx->input(0);\n  const Variant& variant = tensor.scalar<Variant>()();\n  const CompressedElement* compressed = variant.get<CompressedElement>();\n  OP_REQUIRES(\n      ctx, compressed != nullptr,\n      errors::InvalidArgument(\n          \"Input does not contain a compressed element. Instead got tensor \",\n          tensor.DebugString()));\n\n  std::vector<Tensor> components;\n  OP_REQUIRES_OK(ctx, UncompressElement(*compressed, &components));\n  OP_REQUIRES(ctx, components.size() == output_types_.size(),\n              errors::FailedPrecondition(\"Expected \", output_types_.size(),\n                                         \" outputs from uncompress, but got \",\n                                         components.size()));\n  for (int i = 0; i < components.size(); ++i) {\n    OP_REQUIRES(\n        ctx, components[i].dtype() == output_types_[i],\n        errors::FailedPrecondition(\"Expected a tensor of type \",\n                                   DataTypeString(output_types_[i]),\n                                   \" but got a tensor of type \",\n                                   DataTypeString(components[i].dtype())));\n    ctx->set_output(i, components[i]);\n  }\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ImmutableExecutorState::BuildControlFlowInfo(const Graph* g,\n                                                    ControlFlowInfo* cf_info) {\n  const int num_nodes = g->num_node_ids();\n  cf_info->frame_names.resize(num_nodes);\n  std::vector<Node*> parent_nodes;\n  parent_nodes.resize(num_nodes);\n  std::vector<bool> visited;\n  visited.resize(num_nodes);\n\n  string frame_name;\n  std::deque<Node*> ready;\n\n  // Initialize with the root nodes.\n  for (Node* n : g->nodes()) {\n    if (n->in_edges().empty()) {\n      visited[n->id()] = true;\n      cf_info->unique_frame_names.insert(frame_name);\n      ready.push_back(n);\n    }\n  }\n\n  while (!ready.empty()) {\n    Node* curr_node = ready.front();\n    int curr_id = curr_node->id();\n    ready.pop_front();\n\n    Node* parent = nullptr;\n    if (IsEnter(curr_node)) {\n      // Enter a child frame.\n      TF_RETURN_IF_ERROR(\n          GetNodeAttr(curr_node->attrs(), \"frame_name\", &frame_name));\n      parent = curr_node;\n    } else if (IsExit(curr_node)) {\n      // Exit to the parent frame.\n      parent = parent_nodes[curr_id];\n      if (!parent) {\n        return errors::InvalidArgument(\n            \"Invalid Exit op: Cannot find a corresponding Enter op.\");\n      }\n      frame_name = cf_info->frame_names[parent->id()];\n      parent = parent_nodes[parent->id()];\n    } else {\n      parent = parent_nodes[curr_id];\n      frame_name = cf_info->frame_names[curr_id];\n    }\n\n    for (const Edge* out_edge : curr_node->out_edges()) {\n      Node* out = out_edge->dst();\n      if (IsSink(out)) continue;\n      const int out_id = out->id();\n\n      // Add to ready queue if not visited.\n      bool is_visited = visited[out_id];\n      if (!is_visited) {\n        ready.push_back(out);\n        visited[out_id] = true;\n\n        // Process the node 'out'.\n        cf_info->frame_names[out_id] = frame_name;\n        parent_nodes[out_id] = parent;\n        cf_info->unique_frame_names.insert(frame_name);\n      }\n    }\n  }\n\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status TrySimplify(NodeDef* consumer, string* simplified_node_name) override {\n    NodeDef* producer;\n\n    if (consumer->input_size() < 1) {\n      return errors::FailedPrecondition(\"Node \", simplified_node_name,\n                                        \" lacks inputs\");\n    }\n\n    TF_RETURN_IF_ERROR(GetInputNode(consumer->input(0), &producer));\n    const bool producer_is_cast = IsCastLike(*producer);\n    const bool can_optimize =\n        !IsCheckNumerics(*producer) &&\n        ((producer_is_cast && IsValuePreserving(*consumer)) ||\n         (IsValuePreserving(*producer) && IsCastLike(*consumer)));\n    if (!can_optimize || IsControlFlow(*producer) ||\n        IsInPreserveSet(*producer) ||\n        producer->device() != consumer->device()) {\n      return Status::OK();\n    }\n\n    const NodeDef* cast_like_node = producer_is_cast ? producer : consumer;\n    const OpDef* cast_like_op_def = nullptr;\n    TF_RETURN_IF_ERROR(OpRegistry::Global()->LookUpOpDef(cast_like_node->op(),\n                                                         &cast_like_op_def));\n    DataType cast_src_type;\n    TF_RETURN_IF_ERROR(InputTypeForNode(*cast_like_node, *cast_like_op_def, 0,\n                                        &cast_src_type));\n    DataType cast_dst_type;\n    TF_RETURN_IF_ERROR(OutputTypeForNode(*cast_like_node, *cast_like_op_def, 0,\n                                         &cast_dst_type));\n    if (!IsFixedSizeType(cast_src_type) || !IsFixedSizeType(cast_dst_type)) {\n      return Status::OK();\n    } else if (producer_is_cast &&\n               DataTypeSize(cast_dst_type) <= DataTypeSize(cast_src_type)) {\n      return Status::OK();\n    } else if (!producer_is_cast &&\n               DataTypeSize(cast_dst_type) >= DataTypeSize(cast_src_type)) {\n      return Status::OK();\n    }\n\n    // Check that nodes were not already optimized.\n    const string optimized_producer_name = OptimizedNodeName(\n        ParseNodeScopeAndName(producer->name()), DataTypeString(cast_dst_type));\n    const string optimized_consumer_name = OptimizedNodeName(\n        ParseNodeScopeAndName(consumer->name()), DataTypeString(cast_src_type));\n    const bool is_already_optimized =\n        ctx().node_map->NodeExists(optimized_consumer_name) ||\n        ctx().node_map->NodeExists(optimized_producer_name);\n    if (is_already_optimized) {\n      return Status::OK();\n    }\n\n    // Add copies of consumer and producer in reverse order.\n    NodeDef* input;\n    TF_RETURN_IF_ERROR(GetInputNode(producer->input(0), &input));\n    // Create new producer node.\n    NodeDef* new_producer = AddCopyNode(optimized_consumer_name, consumer);\n    new_producer->set_input(0, producer->input(0));\n    ctx().node_map->AddOutput(input->name(), new_producer->name());\n\n    // Create new consumer node.\n    NodeDef* new_consumer = AddCopyNode(optimized_producer_name, producer);\n    new_consumer->set_input(0, new_producer->name());\n\n    NodeDef* new_value_preserving =\n        producer_is_cast ? new_producer : new_consumer;\n    const DataType new_input_type =\n        producer_is_cast ? cast_src_type : cast_dst_type;\n    // Update the input type of the value-preserving node. The input and\n    // output types of the cast-like nodes remain the same.\n    TF_RETURN_IF_ERROR(SetInputType(new_input_type, new_value_preserving));\n    // Make sure there is a kernel registered for the value preserving op\n    // with the new input type.\n    TF_RETURN_IF_ERROR(IsKernelRegisteredForNode(*new_value_preserving));\n    ctx().node_map->AddOutput(new_producer->name(), new_consumer->name());\n\n    AddToOptimizationQueue(new_producer);\n    *simplified_node_name = new_consumer->name();\n\n    return Status::OK();\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    // Read ragged_splits inputs.\n    OpInputList ragged_nested_splits_in;\n    OP_REQUIRES_OK(context, context->input_list(\"rt_nested_splits\",\n                                                &ragged_nested_splits_in));\n    const int ragged_nested_splits_len = ragged_nested_splits_in.size();\n    RaggedTensorVariant batched_ragged_input;\n    // Read ragged_values input.\n    batched_ragged_input.set_values(context->input(ragged_nested_splits_len));\n    batched_ragged_input.mutable_nested_splits()->reserve(\n        ragged_nested_splits_len);\n    for (int i = 0; i < ragged_nested_splits_len; i++) {\n      batched_ragged_input.append_splits(ragged_nested_splits_in[i]);\n    }\n\n    if (!batched_input_) {\n      // Encode as a Scalar Variant Tensor.\n      Tensor* encoded_scalar;\n      OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}),\n                                                       &encoded_scalar));\n      encoded_scalar->scalar<Variant>()() = std::move(batched_ragged_input);\n      return;\n    }\n\n    // Unbatch the Ragged Tensor and encode the components.\n    std::vector<RaggedTensorVariant> unbatched_ragged_input;\n    auto batched_splits_top_vec =\n        batched_ragged_input.splits(0).vec<SPLIT_TYPE>();\n    int num_components = batched_splits_top_vec.size() - 1;\n    OP_REQUIRES(context, num_components >= 0,\n                errors::Internal(\"Invalid split argument.\"));\n    OP_REQUIRES_OK(context, UnbatchRaggedZerothDim<VALUE_TYPE, SPLIT_TYPE>(\n                                batched_ragged_input, &unbatched_ragged_input));\n\n    // Bundle the encoded scalar Variant Tensors into a rank-1 Variant Tensor.\n    Tensor* encoded_vector;\n    int output_size = unbatched_ragged_input.size();\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({output_size}),\n                                            &encoded_vector));\n    auto encoded_vector_t = encoded_vector->vec<Variant>();\n    for (int i = 0; i < output_size; i++) {\n      encoded_vector_t(i) = unbatched_ragged_input[i];\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status ValidateShapes(OpKernelContext* ctx, const Tensor& hypothesis_indices,\n                      const Tensor& hypothesis_values,\n                      const Tensor& hypothesis_shape,\n                      const Tensor& truth_indices, const Tensor& truth_values,\n                      const Tensor& truth_shape) {\n  if (!TensorShapeUtils::IsMatrix(hypothesis_indices.shape()))\n    return errors::InvalidArgument(\n        \"hypothesis_indices should be a matrix, but got shape: \",\n        hypothesis_indices.shape().DebugString());\n  if (!TensorShapeUtils::IsMatrix(truth_indices.shape()))\n    return errors::InvalidArgument(\n        \"truth_indices should be a matrix, but got shape: \",\n        truth_indices.shape().DebugString());\n  if (!TensorShapeUtils::IsVector(hypothesis_values.shape()))\n    return errors::InvalidArgument(\n        \"hypothesis_values should be a vector, but got shape: \",\n        hypothesis_values.shape().DebugString());\n  if (!TensorShapeUtils::IsVector(truth_values.shape()))\n    return errors::InvalidArgument(\n        \"truth_values should be a vector, but got shape: \",\n        truth_values.shape().DebugString());\n  if (!TensorShapeUtils::IsVector(hypothesis_shape.shape()))\n    return errors::InvalidArgument(\n        \"hypothesis_shape should be a vector, but got shape: \",\n        hypothesis_shape.shape().DebugString());\n  if (!TensorShapeUtils::IsVector(truth_shape.shape()))\n    return errors::InvalidArgument(\n        \"truth_shape should be a vector, but got shape: \",\n        truth_shape.shape().DebugString());\n  if (hypothesis_values.NumElements() != hypothesis_indices.dim_size(0))\n    return errors::InvalidArgument(\n        \"Expected hypothesis_values.NumElements == \"\n        \"#rows(hypothesis_indices), their shapes are: \",\n        hypothesis_values.shape().DebugString(), \" and \",\n        hypothesis_indices.shape().DebugString());\n  if (hypothesis_shape.NumElements() != hypothesis_indices.dim_size(1))\n    return errors::InvalidArgument(\n        \"Expected hypothesis_shape.NumElements == \"\n        \"#cols(hypothesis_indices), their shapes are: \",\n        hypothesis_shape.shape().DebugString(), \" and \",\n        hypothesis_indices.shape().DebugString());\n  if (truth_shape.NumElements() < 2)\n    return errors::InvalidArgument(\n        \"Input SparseTensors must have rank at least 2, but truth_shape \"\n        \"rank is: \",\n        truth_shape.NumElements());\n  if (truth_values.NumElements() != truth_indices.dim_size(0))\n    return errors::InvalidArgument(\n        \"Expected truth_values.NumElements == \"\n        \"#rows(truth_indices), their shapes are: \",\n        truth_values.shape().DebugString(), \" and \",\n        truth_indices.shape().DebugString());\n  if (truth_shape.NumElements() != truth_indices.dim_size(1))\n    return errors::InvalidArgument(\n        \"Expected truth_shape.NumElements == \"\n        \"#cols(truth_indices), their shapes are: \",\n        truth_shape.shape().DebugString(), \" and \",\n        truth_indices.shape().DebugString());\n  if (truth_shape.NumElements() != hypothesis_shape.NumElements())\n    return errors::InvalidArgument(\n        \"Expected truth and hypothesis to have matching ranks, but \"\n        \"their shapes are: \",\n        truth_shape.shape().DebugString(), \" and \",\n        hypothesis_shape.shape().DebugString());\n\n  return Status::OK();\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "TfLiteStatus Subgraph::Invoke() {\n  if (!consistent_) {\n    ReportError(\"Invoke called on model that is not consistent.\");\n    return kTfLiteError;\n  }\n\n  TfLiteStatus status = kTfLiteOk;\n  if (state_ == kStateUninvokable) {\n    ReportError(\"Invoke called on model that is not ready.\");\n    return kTfLiteError;\n  } else if (memory_planner_ && !memory_planner_->HasNonPersistentMemory()) {\n    ReportError(\"Non-persistent memory is not available.\");\n    return kTfLiteError;\n  }\n  TFLITE_SCOPED_TAGGED_DEFAULT_PROFILE(profiler_.get(), \"Invoke\");\n\n  // Invocations are always done in node order.\n  // Note that calling Invoke repeatedly will cause the original memory plan to\n  // be reused, unless either ResizeInputTensor() or AllocateTensors() has been\n  // called.\n  for (int execution_plan_index = 0;\n       execution_plan_index < execution_plan_.size(); execution_plan_index++) {\n    if (execution_plan_index == next_execution_plan_index_to_prepare_) {\n      TF_LITE_ENSURE_STATUS(PrepareOpsAndTensors());\n      TF_LITE_ENSURE(&context_, next_execution_plan_index_to_prepare_ >=\n                                    execution_plan_index);\n    }\n    int node_index = execution_plan_[execution_plan_index];\n    TfLiteNode& node = nodes_and_registration_[node_index].first;\n    const TfLiteRegistration& registration =\n        nodes_and_registration_[node_index].second;\n\n    const char* op_name = nullptr;\n    if (profiler_) op_name = GetTFLiteOpName(registration);\n    TFLITE_SCOPED_TAGGED_OPERATOR_PROFILE(profiler_.get(), op_name, node_index);\n\n    for (int i = 0; i < node.inputs->size; ++i) {\n      int tensor_index = node.inputs->data[i];\n      if (tensor_index == kTfLiteOptionalTensor) {\n        continue;\n      }\n      TfLiteTensor* tensor = &tensors_[tensor_index];\n      if (tensor->delegate && tensor->delegate != node.delegate &&\n          tensor->data_is_stale) {\n        TF_LITE_ENSURE_STATUS(EnsureTensorDataIsReadable(tensor_index));\n      }\n      if (tensor->data.raw == nullptr && tensor->bytes > 0) {\n        if (registration.builtin_code == kTfLiteBuiltinReshape && i == 1 &&\n            tensor->dims->size != 1) {\n          // In general, having a tensor here with no buffer will be an error.\n          // However, for the reshape operator, the second input tensor is\n          // sometimes only used for the shape, not for the data. Thus, null\n          // buffer is ok in this situation.\n          // The situation where null buffer is not ok for reshape operator is\n          // only when there are 2 inputs given to the node and the one\n          // corresponding to the shape (i == 1) is a vector that contains all\n          // dimensions. See `GetOutputShape()` function in\n          // `tensorflow/lite/kernels/reshape.cc`\n          continue;\n        } else {\n          // In all other cases, we need to return an error as otherwise we will\n          // trigger a null pointer dereference (likely).\n          ReportError(\"Input tensor %d lacks data\", tensor_index);\n          return kTfLiteError;\n        }\n      }\n    }\n\n    if (check_cancelled_func_ != nullptr &&\n        check_cancelled_func_(cancellation_data_)) {\n      ReportError(\"Client requested cancel during Invoke()\");\n      return kTfLiteError;\n    }\n\n    EnsureTensorsVectorCapacity();\n    tensor_resized_since_op_invoke_ = false;\n    if (OpInvoke(registration, &node) != kTfLiteOk) {\n      return ReportOpError(&context_, node, registration, node_index,\n                           \"failed to invoke\");\n    }\n\n    // Force execution prep for downstream ops if the latest op triggered the\n    // resize of a dynamic tensor.\n    if (tensor_resized_since_op_invoke_ &&\n        HasDynamicTensor(context_, node.outputs)) {\n      next_execution_plan_index_to_prepare_ = execution_plan_index + 1;\n\n      // This happens when an intermediate dynamic tensor is resized.\n      // We don't have to prepare all the ops, but we need to recompute\n      // the allocation plan.\n      if (next_execution_plan_index_to_plan_allocation_ >\n          next_execution_plan_index_to_prepare_) {\n        next_execution_plan_index_to_plan_allocation_ =\n            next_execution_plan_index_to_prepare_;\n        if (memory_planner_) {\n          TF_LITE_ENSURE_STATUS(memory_planner_->ResetAllocationsAfter(\n              next_execution_plan_index_to_plan_allocation_ - 1));\n        }\n      }\n    }\n  }\n\n  return status;\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // inputs must be at least a matrix\n    OP_REQUIRES(\n        ctx, sorted_inputs_t.shape().dims() >= 2,\n        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n\n    // For empty inputs, all values will be placed at the zeroth position.\n    if (sorted_inputs.size() == 0) {\n      functor::SetZeroFunctor<Device, OutType> set_zero;\n      set_zero(ctx->eigen_device<Device>(), output);\n      return;\n    }\n\n    OP_REQUIRES_OK(\n        ctx, functor::UpperBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "Status PyArrayDescr_to_TF_DataType(PyArray_Descr* descr,\n                                   TF_DataType* out_tf_datatype) {\n  PyObject* key;\n  PyObject* value;\n  Py_ssize_t pos = 0;\n\n  // Return an error if the fields attribute is null.\n  // Occurs with an improper conversion attempt to resource.\n  if (descr->fields == nullptr) {\n    return errors::Internal(\"Unexpected numpy data type\");\n  }\n\n  if (PyDict_Next(descr->fields, &pos, &key, &value)) {\n    // In Python 3, the keys of numpy custom struct types are unicode, unlike\n    // Python 2, where the keys are bytes.\n    const char* key_string =\n        PyBytes_Check(key) ? PyBytes_AsString(key)\n                           : PyBytes_AsString(PyUnicode_AsASCIIString(key));\n    if (!key_string) {\n      return errors::Internal(\"Corrupt numpy type descriptor\");\n    }\n    tensorflow::string key = key_string;\n    // The typenames here should match the field names in the custom struct\n    // types constructed in test_util.py.\n    // TODO(mrry,keveman): Investigate Numpy type registration to replace this\n    // hard-coding of names.\n    if (key == \"quint8\") {\n      *out_tf_datatype = TF_QUINT8;\n    } else if (key == \"qint8\") {\n      *out_tf_datatype = TF_QINT8;\n    } else if (key == \"qint16\") {\n      *out_tf_datatype = TF_QINT16;\n    } else if (key == \"quint16\") {\n      *out_tf_datatype = TF_QUINT16;\n    } else if (key == \"qint32\") {\n      *out_tf_datatype = TF_QINT32;\n    } else if (key == \"resource\") {\n      *out_tf_datatype = TF_RESOURCE;\n    } else {\n      return errors::Internal(\"Unsupported numpy data type\");\n    }\n    return Status::OK();\n  }\n  return errors::Internal(\"Unsupported numpy data type\");\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void LinearAlgebraOp<InputScalar, OutputScalar>::PrepareOutputs(\n    OpKernelContext* context, const TensorShapes& input_matrix_shapes,\n    const TensorShape& batch_shape, TensorOutputs* outputs,\n    TensorShapes* output_matrix_shapes) {\n  // Get shape for each of the matrix outputs produced by the derived class.\n  *output_matrix_shapes = GetOutputMatrixShapes(input_matrix_shapes);\n  const int num_outputs = output_matrix_shapes->size();\n\n  // Make sure the number of op outputs is what the derived class expects.\n  OP_REQUIRES(\n      context, num_outputs <= context->num_outputs(),\n      errors::Internal(\n          \"Derived class expected more outputs (%d) that the op has (%d).\",\n          num_outputs, context->num_outputs()));\n\n  // Allocate outputs.\n  std::set<int> unused_inputs;\n  for (int input_idx = 0; input_idx < context->num_inputs(); ++input_idx) {\n    unused_inputs.insert(input_idx);\n  }\n  for (int output_idx = 0; output_idx < context->num_outputs(); ++output_idx) {\n    TensorShape output_tensor_shape({});\n    if (output_idx < num_outputs) {\n      // This output is used, set up output shape and allocate it.\n      const TensorShape& output_matrix_shape =\n          output_matrix_shapes->at(output_idx);\n      OP_REQUIRES(context, output_matrix_shape.dims() <= 2,\n                  errors::InvalidArgument(\n                      \"Rank of matrix output no. %d must be 0, 1 or 2, got %d.\",\n                      output_idx, output_matrix_shape.dims()));\n\n      // The final output has the shape of the outer batch dimensions\n      // concatenated with the output_matrix_shape (if the output is not\n      // scalar).\n      output_tensor_shape = batch_shape;\n      output_tensor_shape.AppendShape(output_matrix_shape);\n    }\n    Tensor* out = nullptr;\n    // See if there is an input buffer we can reuse for this output.\n    bool reused_input = false;\n    if (EnableInputForwarding()) {\n      for (int input_idx : unused_inputs) {\n        if (context->forward_input_to_output_with_shape(\n                input_idx, output_idx, output_tensor_shape, &out)) {\n          reused_input = true;\n          unused_inputs.erase(input_idx);\n          break;\n        }\n      }\n    }\n    if (!reused_input) {\n      OP_REQUIRES_OK(context, context->allocate_output(\n                                  output_idx, output_tensor_shape, &out));\n    }\n    OP_REQUIRES(\n        context, out->dtype() == DataTypeToEnum<OutputScalar>::v(),\n        errors::InvalidArgument(\"Invalid output dtype \", out->dtype(), \" vs \",\n                                DataTypeToEnum<OutputScalar>::v()));\n\n    outputs->emplace_back(out);\n  }\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_grad_backprop = context->input(2);\n\n    // For maxpooling3d, tensor_in should have 5 dimensions.\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    OP_REQUIRES(context, tensor_out.dims() == 5,\n                errors::InvalidArgument(\"tensor_out must be 5-dimensional\"));\n    // For maxpooling3d, out_grad_backprop should have 5 dimensions.\n    OP_REQUIRES(\n        context, out_grad_backprop.dims() == 5,\n        errors::InvalidArgument(\"out_grad_backprop must be 5-dimensional\"));\n\n    Pool3dParameters params{context,  ksize_,       stride_,\n                            padding_, data_format_, tensor_in.shape()};\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {2}, 0, tensor_out.shape(), &output));\n\n    // Given access patterns in LaunchMaxPooling3dGradGradOp, these tensors must\n    // have elements.\n    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n                errors::InvalidArgument(\"received empty tensor tensor_in: \",\n                                        tensor_in.DebugString()));\n    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n                errors::InvalidArgument(\"received empty tensor tensor_out: \",\n                                        tensor_out.DebugString()));\n    OP_REQUIRES(\n        context, out_grad_backprop.NumElements() > 0,\n        errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\n                                out_grad_backprop.DebugString()));\n\n    LaunchMaxPooling3dGradGradOp<Device, T>::launch(\n        context, params, tensor_in, tensor_out, out_grad_backprop, output);\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void MakeDataset(OpKernelContext* ctx, DatasetBase** output) override {\n    // Create a new SparseTensorSliceDatasetOp::Dataset, insert it in\n    // the step container, and return it as the output.\n    const Tensor* indices;\n    OP_REQUIRES_OK(ctx, ctx->input(\"indices\", &indices));\n    const Tensor* values;\n    OP_REQUIRES_OK(ctx, ctx->input(\"values\", &values));\n    const Tensor* dense_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"dense_shape\", &dense_shape));\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(indices->shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be a matrix but received shape \",\n                    indices->shape().DebugString()));\n\n    const auto num_indices = indices->NumElements();\n    const auto num_values = values->NumElements();\n    if (num_indices == 0 || num_values == 0) {\n      OP_REQUIRES(ctx, num_indices == num_values,\n                  errors::InvalidArgument(\n                      \"If indices or values are empty, the other one must also \"\n                      \"be. Got indices of shape \",\n                      indices->shape().DebugString(), \" and values of shape \",\n                      values->shape().DebugString()));\n    }\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(values->shape()),\n                errors::InvalidArgument(\n                    \"Input values should be a vector but received shape \",\n                    indices->shape().DebugString()));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(dense_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shape should be a vector but received shape \",\n                    dense_shape->shape().DebugString()));\n\n    // We currently ensure that `sparse_tensor` is ordered in the\n    // batch dimension.\n    // TODO(mrry): Investigate ways to avoid this unconditional check\n    // if we can be sure that the sparse tensor was produced in an\n    // appropriate order (e.g. by `tf.parse_example()` or a Dataset\n    // that batches elements into rows of a SparseTensor).\n    int64_t previous_batch_index = -1;\n    for (int64_t i = 0; i < indices->dim_size(0); ++i) {\n      int64_t next_batch_index = indices->matrix<int64>()(i, 0);\n      OP_REQUIRES(\n          ctx, next_batch_index >= previous_batch_index,\n          errors::Unimplemented(\"The SparseTensor must be ordered in the batch \"\n                                \"dimension; handling arbitrarily ordered input \"\n                                \"is not currently supported.\"));\n      previous_batch_index = next_batch_index;\n    }\n    gtl::InlinedVector<int64, 8> std_order(dense_shape->NumElements(), 0);\n    sparse::SparseTensor tensor;\n    OP_REQUIRES_OK(\n        ctx, sparse::SparseTensor::Create(\n                 *indices, *values, TensorShape(dense_shape->vec<int64>()),\n                 std_order, &tensor));\n    *output = new Dataset<T>(ctx, std::move(tensor));\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  Status GetFirstDimensionSize(OpKernelContext* context, INDEX_TYPE* result) {\n    const Tensor first_partition_tensor =\n        context->input(kFirstPartitionInputIndex);\n    if (row_partition_types_.empty()) {\n      return errors::InvalidArgument(\"No row_partition_types given.\");\n    }\n    const RowPartitionType first_partition_type = row_partition_types_[0];\n    switch (first_partition_type) {\n      case RowPartitionType::FIRST_DIM_SIZE:\n        *result = first_partition_tensor.scalar<INDEX_TYPE>()();\n        return Status::OK();\n      case RowPartitionType::VALUE_ROWIDS:\n        return errors::InvalidArgument(\n            \"Cannot handle VALUE_ROWIDS in first dimension.\");\n      case RowPartitionType::ROW_SPLITS:\n        *result = first_partition_tensor.shape().dim_size(0) - 1;\n        return Status::OK();\n      default:\n        return errors::InvalidArgument(\n            \"Cannot handle type \",\n            RowPartitionTypeToString(first_partition_type));\n    }\n  }\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n\n    // MatrixDiagPart and MatrixDiagPartV2 both use this OpKernel.\n    // MatrixDiagPart only has one input, so we have to check the number of\n    // inputs before reading additional parameters in MatrixDiagV2.\n    int32_t lower_diag_index = 0;\n    int32_t upper_diag_index = 0;\n    T padding_value(0);\n\n    // MatrixDiagPartV2-specific.\n    if (context->num_inputs() > kNumV1Inputs) {\n      auto& diag_index = context->input(1);\n      OP_REQUIRES(context,\n                  TensorShapeUtils::IsScalar(diag_index.shape()) ||\n                      TensorShapeUtils::IsVector(diag_index.shape()),\n                  errors::InvalidArgument(\n                      \"diag_index must be a scalar or vector, received shape: \",\n                      diag_index.shape().DebugString()));\n      OP_REQUIRES(context, diag_index.NumElements() > 0,\n                  errors::InvalidArgument(\n                      \"Expected diag_index to have at least 1 element\"));\n      lower_diag_index = diag_index.flat<int32>()(0);\n      upper_diag_index = lower_diag_index;\n      if (TensorShapeUtils::IsVector(diag_index.shape())) {\n        auto diag_index_size = diag_index.dim_size(0);\n        OP_REQUIRES(\n            context, 0 < diag_index_size && diag_index_size <= 2,\n            errors::InvalidArgument(\n                \"diag_index must have only one or two elements, received \",\n                diag_index_size, \" elements.\"));\n        if (diag_index_size > 1) {\n          upper_diag_index = diag_index.flat<int32>()(1);\n        }\n      }\n      const Tensor& padding_in = context->input(2);\n      OP_REQUIRES(context, padding_in.NumElements() == 1,\n                  errors::InvalidArgument(\"Padding must be scalar.\"));\n      padding_value = padding_in.flat<T>()(0);\n    }\n    const TensorShape& input_shape = input.shape();\n\n    // Preliminary validation of sizes.\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrixOrHigher(input_shape),\n                errors::InvalidArgument(\n                    \"input must be at least 2-dim, received shape: \",\n                    input.shape().DebugString()));\n\n    // Make sure lower_diag_index and upper_diag_index is valid.\n    const int rank = input_shape.dims();\n    const Eigen::Index num_rows = input_shape.dim_size(rank - 2);\n    const Eigen::Index num_cols = input_shape.dim_size(rank - 1);\n    OP_REQUIRES(  // Checks lower_diag_index == 0 for when matrix shape = 0.\n        context,\n        (-num_rows < lower_diag_index && lower_diag_index < num_cols) ||\n            lower_diag_index == 0,\n        errors::InvalidArgument(\n            \"lower_diag_index is out of bound: \", lower_diag_index,\n            \". It must be between \", -num_rows, \" and \", num_cols));\n    OP_REQUIRES(context,\n                (-num_rows < upper_diag_index && upper_diag_index < num_cols) ||\n                    upper_diag_index == 0,\n                errors::InvalidArgument(\n                    \"upper_diag_index is out of bound: \", upper_diag_index,\n                    \" It must be between \", -num_rows, \" and \", num_cols));\n    OP_REQUIRES(\n        context, lower_diag_index <= upper_diag_index,\n        errors::InvalidArgument(\n            \"lower_diag_index must not be larger than upper_diag_index: \",\n            lower_diag_index, \" > \", upper_diag_index));\n\n    TensorShape output_shape;\n    for (int i = 0; i < rank - 2; ++i) {\n      output_shape.AddDim(input_shape.dim_size(i));\n    }\n    const Eigen::Index num_diags = upper_diag_index - lower_diag_index + 1;\n    if (num_diags > 1) output_shape.AddDim(num_diags);\n    const int32_t max_diag_len =\n        std::min(num_rows + std::min(upper_diag_index, 0),\n                 num_cols - std::max(lower_diag_index, 0));\n    output_shape.AddDim(max_diag_len);\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &output));\n    auto output_reshaped = output->flat<T>();\n    auto input_reshaped = input.flat_inner_dims<T, 3>();\n    functor::MatrixDiagPart<Device, T>::Compute(\n        context, context->eigen_device<Device>(), input_reshaped,\n        output_reshaped, lower_diag_index, upper_diag_index, max_diag_len,\n        padding_value, left_align_superdiagonal_, left_align_subdiagonal_);\n  }\n"}, {"role": "assistant", "content": "False"}]}
