{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _delete_artifact_mlflow_artifacts(artifact_path):\n    \"\"\"\n    A request handler for `DELETE /mlflow-artifacts/artifacts?path=<value>` to delete artifacts in\n    `path` (a relative path from the root artifact directory).\n    \"\"\"\n    validate_path_is_safe(artifact_path)\n    _get_request_message(DeleteArtifact())\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    artifact_repo.delete_artifacts(artifact_path)\n    response_message = DeleteArtifact.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def extract(self, fp: BinaryIO, parallel: bool, skip_notarget=True, q=None) -> None:\n        \"\"\"Extract worker method to handle 7zip folder and decompress each files.\"\"\"\n        if hasattr(self.header, \"main_streams\") and self.header.main_streams is not None:\n            src_end = self.src_start + self.header.main_streams.packinfo.packpositions[-1]\n            numfolders = self.header.main_streams.unpackinfo.numfolders\n            if numfolders == 1:\n                self.extract_single(\n                    fp,\n                    self.files,\n                    self.src_start,\n                    src_end,\n                    q,\n                    skip_notarget=skip_notarget,\n                )\n            else:\n                folders = self.header.main_streams.unpackinfo.folders\n                positions = self.header.main_streams.packinfo.packpositions\n                empty_files = [f for f in self.files if f.emptystream]\n                if not parallel:\n                    self.extract_single(fp, empty_files, 0, 0, q)\n                    for i in range(numfolders):\n                        if skip_notarget:\n                            if not any([self.target_filepath.get(f.id, None) for f in folders[i].files]):\n                                continue\n                        self.extract_single(\n                            fp,\n                            folders[i].files,\n                            self.src_start + positions[i],\n                            self.src_start + positions[i + 1],\n                            q,\n                            skip_notarget=skip_notarget,\n                        )\n                else:\n                    if getattr(fp, \"name\", None) is None:\n                        raise InternalError(\"Caught unknown variable status error\")\n                    filename: str = getattr(fp, \"name\", \"\")  # do not become \"\" but it is for type check.\n                    self.extract_single(open(filename, \"rb\"), empty_files, 0, 0, q)\n                    concurrent_tasks = []\n                    exc_q: queue.Queue = queue.Queue()\n                    for i in range(numfolders):\n                        if skip_notarget:\n                            if not any([self.target_filepath.get(f.id, None) for f in folders[i].files]):\n                                continue\n                        p = self.concurrent(\n                            target=self.extract_single,\n                            args=(\n                                filename,\n                                folders[i].files,\n                                self.src_start + positions[i],\n                                self.src_start + positions[i + 1],\n                                q,\n                                exc_q,\n                                skip_notarget,\n                            ),\n                        )\n                        p.start()\n                        concurrent_tasks.append(p)\n                    for p in concurrent_tasks:\n                        p.join()\n                    if exc_q.empty():\n                        pass\n                    else:\n                        exc_info = exc_q.get()\n                        raise exc_info[1].with_traceback(exc_info[2])\n        else:\n            empty_files = [f for f in self.files if f.emptystream]\n            self.extract_single(fp, empty_files, 0, 0, q)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_path_is_safe(path):\n    \"\"\"\n    Validates that the specified path is safe to join with a trusted prefix. This is a security\n    measure to prevent path traversal attacks.\n    A valid path should:\n        not contain separators other than '/'\n        not contain .. to navigate to parent dir in path\n        not be an absolute path\n    \"\"\"\n    from mlflow.utils.file_utils import local_file_uri_to_path\n\n    # We must decode path before validating it\n    path = _decode(path)\n\n    exc = MlflowException(\"Invalid path\", error_code=INVALID_PARAMETER_VALUE)\n    if \"#\" in path:\n        raise exc\n\n    if is_file_uri(path):\n        path = local_file_uri_to_path(path)\n    if (\n        any((s in path) for s in _OS_ALT_SEPS)\n        or \"..\" in path.split(\"/\")\n        or pathlib.PureWindowsPath(path).is_absolute()\n        or pathlib.PurePosixPath(path).is_absolute()\n        or (is_windows() and len(path) >= 2 and path[1] == \":\")\n    ):\n        raise exc\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def side_effect(old_cmd, command):\n    with tarfile.TarFile(_tar_file(old_cmd.script_parts)[0]) as archive:\n        for file in archive.getnames():\n            try:\n                os.remove(file)\n            except OSError:\n                # does not try to remove directories as we cannot know if they\n                # already existed before\n                pass\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def cors_after_request(resp):\n        # If CORS headers are set in a view decorator, pass\n        if resp.headers is not None and resp.headers.get(ACL_ORIGIN):\n            LOG.debug('CORS have been already evaluated, skipping')\n            return resp\n\n        for res_regex, res_options in resources:\n            if try_match(request.path, res_regex):\n                LOG.debug(\"Request to '%s' matches CORS resource '%s'. Using options: %s\",\n                      request.path, get_regexp_pattern(res_regex), res_options)\n                set_cors_headers(resp, res_options)\n                break\n        else:\n            LOG.debug('No CORS rule matches')\n        return resp\n    return cors_after_request"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def upload_artifact_handler():\n    args = request.args\n    run_uuid = args.get(\"run_uuid\")\n    if not run_uuid:\n        raise MlflowException(\n            message=\"Request must specify run_uuid.\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n    path = args.get(\"path\")\n    if not path:\n        raise MlflowException(\n            message=\"Request must specify path.\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n    validate_path_is_safe(path)\n\n    if request.content_length and request.content_length > 10 * 1024 * 1024:\n        raise MlflowException(\n            message=\"Artifact size is too large. Max size is 10MB.\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n\n    data = request.data\n    if not data:\n        raise MlflowException(\n            message=\"Request must specify data.\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n\n    run = _get_tracking_store().get_run(run_uuid)\n    artifact_dir = run.info.artifact_uri\n\n    basename = posixpath.basename(path)\n    dirname = posixpath.dirname(path)\n\n    def _log_artifact_to_repo(file, run, dirname, artifact_dir):\n        if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n            artifact_repo = _get_artifact_repo_mlflow_artifacts()\n            path_to_log = (\n                os.path.join(run.info.experiment_id, run.info.run_id, \"artifacts\", dirname)\n                if dirname\n                else os.path.join(run.info.experiment_id, run.info.run_id, \"artifacts\")\n            )\n        else:\n            artifact_repo = get_artifact_repository(artifact_dir)\n            path_to_log = dirname\n\n        artifact_repo.log_artifact(file, path_to_log)\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        dir_path = os.path.join(tmpdir, dirname) if dirname else tmpdir\n        file_path = os.path.join(dir_path, basename)\n\n        os.makedirs(dir_path, exist_ok=True)\n\n        with open(file_path, \"wb\") as f:\n            f.write(data)\n\n        _log_artifact_to_repo(file_path, run, dirname, artifact_dir)\n\n    return Response(mimetype=\"application/json\")\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def save(self, name, content, max_length=None):\n        \"\"\"\n        Save new content to the file specified by name. The content should be\n        a proper File object or any Python file-like object, ready to be read\n        from the beginning.\n        \"\"\"\n        # Get the proper name for the file, as it will actually be saved.\n        if name is None:\n            name = content.name\n\n        if not hasattr(content, 'chunks'):\n            content = File(content, name)\n\n        name = self.get_available_name(name, max_length=max_length)\n        return self._save(name, content)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def pascal_case(value: str) -> str:\n    return stringcase.pascalcase(value)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def _handle(self, request: Request) -> StreamResponse:\n        rel_url = request.match_info[\"filename\"]\n        try:\n            filename = Path(rel_url)\n            if filename.anchor:\n                # rel_url is an absolute name like\n                # /static/\\\\machine_name\\c$ or /static/D:\\path\n                # where the static dir is totally different\n                raise HTTPForbidden()\n            filepath = self._directory.joinpath(filename).resolve()\n            if not self._follow_symlinks:\n                filepath.relative_to(self._directory)\n        except (ValueError, FileNotFoundError) as error:\n            # relatively safe\n            raise HTTPNotFound() from error\n        except HTTPForbidden:\n            raise\n        except Exception as error:\n            # perm error or other kind!\n            request.app.logger.exception(error)\n            raise HTTPNotFound() from error\n\n        # on opening a dir, load its contents if allowed\n        if filepath.is_dir():\n            if self._show_index:\n                try:\n                    return Response(\n                        text=self._directory_as_html(filepath), content_type=\"text/html\"\n                    )\n                except PermissionError:\n                    raise HTTPForbidden()\n            else:\n                raise HTTPForbidden()\n        elif filepath.is_file():\n            return FileResponse(filepath, chunk_size=self._chunk_size)\n        else:\n            raise HTTPNotFound\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_mediastatic_content(url):\n    if url.startswith(settings.STATIC_URL):\n        local_path = settings.STATIC_ROOT / url[len(settings.STATIC_URL) :]\n    elif url.startswith(settings.MEDIA_URL):\n        local_path = settings.MEDIA_ROOT / url[len(settings.MEDIA_URL) :]\n    else:\n        raise FileNotFoundError()\n\n    with open(local_path, \"rb\") as f:\n        return f.read()\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        dir_name, file_name = os.path.split(name)\n        if '..' in pathlib.PurePath(dir_name).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n        validate_file_name(file_name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, add an underscore and a random 7\n        # character alphanumeric string (before the file extension, if one\n        # exists) to the filename until the generated filename doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n        return name\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_context_data(self, **kwargs):\n        template = self.kwargs['template']\n        templates = []\n        try:\n            default_engine = Engine.get_default()\n        except ImproperlyConfigured:\n            # Non-trivial TEMPLATES settings aren't supported (#24125).\n            pass\n        else:\n            # This doesn't account for template loaders (#24128).\n            for index, directory in enumerate(default_engine.dirs):\n                template_file = Path(directory) / template\n                if template_file.exists():\n                    template_contents = template_file.read_text()\n                else:\n                    template_contents = ''\n                templates.append({\n                    'file': template_file,\n                    'exists': template_file.exists(),\n                    'contents': template_contents,\n                    'order': index,\n                })\n        return super().get_context_data(**{\n            **kwargs,\n            'name': template,\n            'templates': templates,\n        })\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_obj_absolute_path(obj_path):\n    return os.path.join(DATAROOT, obj_path)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def lookup_path(\n        self, path: str\n    ) -> typing.Tuple[str, typing.Optional[os.stat_result]]:\n        for directory in self.all_directories:\n            joined_path = os.path.join(directory, path)\n            if self.follow_symlink:\n                full_path = os.path.abspath(joined_path)\n            else:\n                full_path = os.path.realpath(joined_path)\n            directory = os.path.realpath(directory)\n            if os.path.commonprefix([full_path, directory]) != directory:\n                # Don't allow misbehaving clients to break out of the static files\n                # directory.\n                continue\n            try:\n                return full_path, os.stat(full_path)\n            except (FileNotFoundError, NotADirectoryError):\n                continue\n        return \"\", None\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_context_data(self, **kwargs):\n        template = self.kwargs['template']\n        templates = []\n        try:\n            default_engine = Engine.get_default()\n        except ImproperlyConfigured:\n            # Non-trivial TEMPLATES settings aren't supported (#24125).\n            pass\n        else:\n            # This doesn't account for template loaders (#24128).\n            for index, directory in enumerate(default_engine.dirs):\n                template_file = Path(directory) / template\n                if template_file.exists():\n                    template_contents = template_file.read_text()\n                else:\n                    template_contents = ''\n                templates.append({\n                    'file': template_file,\n                    'exists': template_file.exists(),\n                    'contents': template_contents,\n                    'order': index,\n                })\n        return super().get_context_data(**{\n            **kwargs,\n            'name': template,\n            'templates': templates,\n        })\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, filename):\n        \"\"\"\n        Validate the filename by calling get_valid_name() and return a filename\n        to be passed to the save() method.\n        \"\"\"\n        # `filename` may include a path as returned by FileField.upload_to.\n        dirname, filename = os.path.split(filename)\n        if '..' in pathlib.PurePath(dirname).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dirname)\n        return os.path.normpath(os.path.join(dirname, self.get_valid_name(filename)))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void check_tensor(const TensorProto& tensor, const CheckerContext& ctx) {\n  enforce_has_field(tensor, data_type);\n  if (tensor.data_type() == TensorProto::UNDEFINED) {\n    fail_check(\"setting data_type field (tensor name: \", tensor.name(), \") to UNDEFINED is not allowed\");\n  }\n\n  int num_value_fields = 0;\n\n  const char* value_field = nullptr;\n\n#define check_data_field(field)             \\\n  bool has_##field = tensor.field().size(); \\\n  if (has_##field) {                        \\\n    ++num_value_fields;                     \\\n    value_field = #field;                   \\\n  }\n\n  check_data_field(float_data);\n  check_data_field(int32_data);\n  check_data_field(string_data);\n  check_data_field(int64_data);\n  check_data_field(raw_data);\n  check_data_field(double_data);\n  check_data_field(uint64_data);\n\n#undef check_data_field\n\n  bool stored_externally = tensor.has_data_location() && tensor.data_location() == TensorProto::EXTERNAL;\n  if (stored_externally) {\n    if (num_value_fields != 0) {\n      fail_check(\n          \"Data of TensorProto ( tensor name: \",\n          tensor.name(),\n          \") is stored externally and should not have data field.\",\n          value_field);\n    }\n\n    bool has_location = false;\n    for (const StringStringEntryProto& entry : tensor.external_data()) {\n      if (entry.has_key() && entry.has_value() && entry.key() == \"location\") {\n        has_location = true;\n        std::string data_path = path_join(ctx.get_model_dir(), entry.value());\n        // use stat to check whether the file exists\n        struct stat buffer;\n        if (stat((data_path).c_str(), &buffer) != 0) {\n          fail_check(\n              \"Data of TensorProto ( tensor name: \",\n              tensor.name(),\n              \") should be stored in \",\n              data_path,\n              \", but it doesn't exist or is not accessible.\");\n        }\n      }\n    }\n    if (!has_location) {\n      fail_check(\"TensorProto ( tensor name: \", tensor.name(), \") is stored externally but doesn't have a location.\");\n    }\n    return;\n  }\n  int64_t nelem = 1;\n  for (auto x : tensor.dims()) {\n    nelem *= x;\n  }\n  if (nelem == 0 && num_value_fields != 0) {\n    fail_check(\"TensorProto (tensor name: \", tensor.name(), \") is 0-element but contains data!\");\n  }\n  if (nelem != 0 && num_value_fields != 1) {\n    fail_check(\"TensorProto (tensor name: \", tensor.name(), \") should contain one and only one value field.\");\n  }\n  if (has_raw_data) {\n    if (tensor.data_type() == TensorProto::STRING) {\n      fail_check(\"STRING data (tensor name: \", tensor.name(), \") should not be stored in raw_data field\");\n    }\n    return;\n  } else {\n#define check_field(field)               \\\n  if (nelem != 0 && !has_##field) {      \\\n    fail_check(                          \\\n        \"values of data_type '\",         \\\n        tensor.data_type(),              \\\n        \"' should be stored in field '\", \\\n        #field,                          \\\n        \"' instead of '\",                \\\n        value_field,                     \\\n        \"'\");                            \\\n  }\n\n    switch (tensor.data_type()) {\n      case TensorProto::FLOAT:\n      case TensorProto::COMPLEX64:\n        check_field(float_data);\n        break;\n\n      case TensorProto::DOUBLE:\n      case TensorProto::COMPLEX128:\n        check_field(double_data);\n        break;\n\n      case TensorProto::INT32:\n      case TensorProto::UINT8:\n      case TensorProto::INT8:\n      case TensorProto::UINT16:\n      case TensorProto::INT16:\n      case TensorProto::BOOL:\n      case TensorProto::FLOAT16:\n      case TensorProto::BFLOAT16:\n        check_field(int32_data);\n        break;\n\n      case TensorProto::INT64:\n        check_field(int64_data);\n        break;\n\n      case TensorProto::UINT32:\n      case TensorProto::UINT64:\n        check_field(uint64_data);\n        break;\n\n      case TensorProto::STRING:\n        check_field(string_data);\n        break;\n\n      default:\n        fail_check(\"Unrecognized data_type (tensor name: \", tensor.name(), \"): \", tensor.data_type());\n    }\n  }\n\n#undef check_field\n}\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_object_element(dataset, seq_no, rel_path, download_link):\n    \"\"\"If rel_path and download_link are not None, we are called from scope.\n    Otherwise we are called from ID and need to run SQL query to fetch these attrs.\"\"\"\n\n    if rel_path is None:\n        query = \"SELECT rel_path, download_link FROM \" + \\\n        dataset + \\\n        \" WHERE sequence_no = %s\"\n\n        cnx = mysql.connector.connect(user=DB_USER,\n                                    password=DB_PASSWORD,\n                                    host=DB_HOST,\n                                    database=DB_DBNAME,\n                                    port=DB_PORT)\n        cursor = cnx.cursor()\n        cursor.execute(query, (seq_no,))\n\n        row = cursor.fetchone()\n\n        if not row:\n            return None\n\n        rel_path, download_link = row[0], row[1]\n\n    if LOCAL_OBJ_URI:\n        src_uri = 'file://' + os.path.join(DATAROOT, dataset, rel_path)\n    else:\n        src_uri = url_for('.get_object_src_http', dataset=dataset, rel_path=rel_path)\n\n    return '<object id={} src={} hyperfind.external-link={} />' \\\n        .format(\n        quoteattr(url_for('.get_object_id', dataset=dataset, seq_no=seq_no)),\n        quoteattr(src_uri),\n        quoteattr(download_link))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __call__(self, args: argparse.Namespace) -> None:\n        \"\"\"Perform the update.\"\"\"\n        manifest, path = dfetch.manifest.manifest.get_manifest()\n\n        exceptions: List[str] = []\n        with in_directory(os.path.dirname(path)):\n            for project in manifest.selected_projects(args.projects):\n                with catch_runtime_exceptions(exceptions) as exceptions:\n                    dfetch.project.make(project).update(force=args.force)\n\n                if not args.no_recommendations and os.path.isdir(project.destination):\n                    with in_directory(project.destination):\n                        check_child_manifests(manifest, project, path)\n\n        if exceptions:\n            raise RuntimeError(\"\\n\".join(exceptions))\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _download_artifact(artifact_path):\n    \"\"\"\n    A request handler for `GET /mlflow-artifacts/artifacts/<artifact_path>` to download an artifact\n    from `artifact_path` (a relative path from the root artifact directory).\n    \"\"\"\n    validate_path_is_safe(artifact_path)\n    tmp_dir = tempfile.TemporaryDirectory()\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    dst = artifact_repo.download_artifacts(artifact_path, tmp_dir.name)\n\n    # Ref: https://stackoverflow.com/a/24613980/6943581\n    file_handle = open(dst, \"rb\")  # noqa: SIM115\n\n    def stream_and_remove_file():\n        yield from file_handle\n        file_handle.close()\n        tmp_dir.cleanup()\n\n    file_sender_response = current_app.response_class(stream_and_remove_file())\n\n    return _response_with_file_attachment_headers(artifact_path, file_sender_response)\n"}, {"role": "assistant", "content": "True"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _delete_artifact_mlflow_artifacts(artifact_path):\n    \"\"\"\n    A request handler for `DELETE /mlflow-artifacts/artifacts?path=<value>` to delete artifacts in\n    `path` (a relative path from the root artifact directory).\n    \"\"\"\n    artifact_path = validate_path_is_safe(artifact_path)\n    _get_request_message(DeleteArtifact())\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    artifact_repo.delete_artifacts(artifact_path)\n    response_message = DeleteArtifact.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def extract(self, fp: BinaryIO, path: Optional[pathlib.Path], parallel: bool, skip_notarget=True, q=None) -> None:\n        \"\"\"Extract worker method to handle 7zip folder and decompress each files.\"\"\"\n        if hasattr(self.header, \"main_streams\") and self.header.main_streams is not None:\n            src_end = self.src_start + self.header.main_streams.packinfo.packpositions[-1]\n            numfolders = self.header.main_streams.unpackinfo.numfolders\n            if numfolders == 1:\n                self.extract_single(\n                    fp,\n                    self.files,\n                    path,\n                    self.src_start,\n                    src_end,\n                    q,\n                    skip_notarget=skip_notarget,\n                )\n            else:\n                folders = self.header.main_streams.unpackinfo.folders\n                positions = self.header.main_streams.packinfo.packpositions\n                empty_files = [f for f in self.files if f.emptystream]\n                if not parallel:\n                    self.extract_single(fp, empty_files, path, 0, 0, q)\n                    for i in range(numfolders):\n                        if skip_notarget:\n                            if not any([self.target_filepath.get(f.id, None) for f in folders[i].files]):\n                                continue\n                        self.extract_single(\n                            fp,\n                            folders[i].files,\n                            path,\n                            self.src_start + positions[i],\n                            self.src_start + positions[i + 1],\n                            q,\n                            skip_notarget=skip_notarget,\n                        )\n                else:\n                    if getattr(fp, \"name\", None) is None:\n                        raise InternalError(\"Caught unknown variable status error\")\n                    filename: str = getattr(fp, \"name\", \"\")  # do not become \"\" but it is for type check.\n                    self.extract_single(open(filename, \"rb\"), empty_files, path, 0, 0, q)\n                    concurrent_tasks = []\n                    exc_q: queue.Queue = queue.Queue()\n                    for i in range(numfolders):\n                        if skip_notarget:\n                            if not any([self.target_filepath.get(f.id, None) for f in folders[i].files]):\n                                continue\n                        p = self.concurrent(\n                            target=self.extract_single,\n                            args=(\n                                filename,\n                                folders[i].files,\n                                path,\n                                self.src_start + positions[i],\n                                self.src_start + positions[i + 1],\n                                q,\n                                exc_q,\n                                skip_notarget,\n                            ),\n                        )\n                        p.start()\n                        concurrent_tasks.append(p)\n                    for p in concurrent_tasks:\n                        p.join()\n                    if exc_q.empty():\n                        pass\n                    else:\n                        exc_info = exc_q.get()\n                        raise exc_info[1].with_traceback(exc_info[2])\n        else:\n            empty_files = [f for f in self.files if f.emptystream]\n            self.extract_single(fp, empty_files, path, 0, 0, q)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def validate_path_is_safe(path):\n    \"\"\"\n    Validates that the specified path is safe to join with a trusted prefix. This is a security\n    measure to prevent path traversal attacks.\n    A valid path should:\n        not contain separators other than '/'\n        not contain .. to navigate to parent dir in path\n        not be an absolute path\n    \"\"\"\n    from mlflow.utils.file_utils import local_file_uri_to_path\n\n    # We must decode path before validating it\n    path = _decode(path)\n\n    exc = MlflowException(\"Invalid path\", error_code=INVALID_PARAMETER_VALUE)\n    if \"#\" in path:\n        raise exc\n\n    if is_file_uri(path):\n        path = local_file_uri_to_path(path)\n    if (\n        any((s in path) for s in _OS_ALT_SEPS)\n        or \"..\" in path.split(\"/\")\n        or pathlib.PureWindowsPath(path).is_absolute()\n        or pathlib.PurePosixPath(path).is_absolute()\n        or (is_windows() and len(path) >= 2 and path[1] == \":\")\n    ):\n        raise exc\n\n    return path\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def side_effect(old_cmd, command):\n    with tarfile.TarFile(_tar_file(old_cmd.script_parts)[0]) as archive:\n        for file in archive.getnames():\n            if not os.path.abspath(file).startswith(os.getcwd()):\n                # it's unsafe to overwrite files outside of the current directory\n                continue\n\n            try:\n                os.remove(file)\n            except OSError:\n                # does not try to remove directories as we cannot know if they\n                # already existed before\n                pass\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def cors_after_request(resp):\n        # If CORS headers are set in a view decorator, pass\n        if resp.headers is not None and resp.headers.get(ACL_ORIGIN):\n            LOG.debug('CORS have been already evaluated, skipping')\n            return resp\n        normalized_path = unquote_plus(request.path)\n        for res_regex, res_options in resources:\n            if try_match(normalized_path, res_regex):\n                LOG.debug(\"Request to '%s' matches CORS resource '%s'. Using options: %s\",\n                      request.path, get_regexp_pattern(res_regex), res_options)\n                set_cors_headers(resp, res_options)\n                break\n        else:\n            LOG.debug('No CORS rule matches')\n        return resp\n    return cors_after_request"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def upload_artifact_handler():\n    args = request.args\n    run_uuid = args.get(\"run_uuid\")\n    if not run_uuid:\n        raise MlflowException(\n            message=\"Request must specify run_uuid.\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n    path = args.get(\"path\")\n    if not path:\n        raise MlflowException(\n            message=\"Request must specify path.\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n    path = validate_path_is_safe(path)\n\n    if request.content_length and request.content_length > 10 * 1024 * 1024:\n        raise MlflowException(\n            message=\"Artifact size is too large. Max size is 10MB.\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n\n    data = request.data\n    if not data:\n        raise MlflowException(\n            message=\"Request must specify data.\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n\n    run = _get_tracking_store().get_run(run_uuid)\n    artifact_dir = run.info.artifact_uri\n\n    basename = posixpath.basename(path)\n    dirname = posixpath.dirname(path)\n\n    def _log_artifact_to_repo(file, run, dirname, artifact_dir):\n        if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n            artifact_repo = _get_artifact_repo_mlflow_artifacts()\n            path_to_log = (\n                os.path.join(run.info.experiment_id, run.info.run_id, \"artifacts\", dirname)\n                if dirname\n                else os.path.join(run.info.experiment_id, run.info.run_id, \"artifacts\")\n            )\n        else:\n            artifact_repo = get_artifact_repository(artifact_dir)\n            path_to_log = dirname\n\n        artifact_repo.log_artifact(file, path_to_log)\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        dir_path = os.path.join(tmpdir, dirname) if dirname else tmpdir\n        file_path = os.path.join(dir_path, basename)\n\n        os.makedirs(dir_path, exist_ok=True)\n\n        with open(file_path, \"wb\") as f:\n            f.write(data)\n\n        _log_artifact_to_repo(file_path, run, dirname, artifact_dir)\n\n    return Response(mimetype=\"application/json\")\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def save(self, name, content, max_length=None):\n        \"\"\"\n        Save new content to the file specified by name. The content should be\n        a proper File object or any Python file-like object, ready to be read\n        from the beginning.\n        \"\"\"\n        # Get the proper name for the file, as it will actually be saved.\n        if name is None:\n            name = content.name\n\n        if not hasattr(content, 'chunks'):\n            content = File(content, name)\n\n        name = self.get_available_name(name, max_length=max_length)\n        name = self._save(name, content)\n        # Ensure that the name returned from the storage system is still valid.\n        validate_file_name(name, allow_relative_path=True)\n        return name\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def pascal_case(value: str) -> str:\n    return stringcase.pascalcase(_sanitize(value))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    async def _handle(self, request: Request) -> StreamResponse:\n        rel_url = request.match_info[\"filename\"]\n        try:\n            filename = Path(rel_url)\n            if filename.anchor:\n                # rel_url is an absolute name like\n                # /static/\\\\machine_name\\c$ or /static/D:\\path\n                # where the static dir is totally different\n                raise HTTPForbidden()\n            unresolved_path = self._directory.joinpath(filename)\n            if self._follow_symlinks:\n                normalized_path = Path(os.path.normpath(unresolved_path))\n                normalized_path.relative_to(self._directory)\n                filepath = normalized_path.resolve()\n            else:\n                filepath = unresolved_path.resolve()\n                filepath.relative_to(self._directory)\n        except (ValueError, FileNotFoundError) as error:\n            # relatively safe\n            raise HTTPNotFound() from error\n        except HTTPForbidden:\n            raise\n        except Exception as error:\n            # perm error or other kind!\n            request.app.logger.exception(error)\n            raise HTTPNotFound() from error\n\n        # on opening a dir, load its contents if allowed\n        if filepath.is_dir():\n            if self._show_index:\n                try:\n                    return Response(\n                        text=self._directory_as_html(filepath), content_type=\"text/html\"\n                    )\n                except PermissionError:\n                    raise HTTPForbidden()\n            else:\n                raise HTTPForbidden()\n        elif filepath.is_file():\n            return FileResponse(filepath, chunk_size=self._chunk_size)\n        else:\n            raise HTTPNotFound\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def get_mediastatic_content(url):\n    if url.startswith(settings.STATIC_URL):\n        local_path = settings.STATIC_ROOT / url[len(settings.STATIC_URL) :]\n    elif url.startswith(settings.MEDIA_URL):\n        local_path = settings.MEDIA_ROOT / url[len(settings.MEDIA_URL) :]\n    else:\n        raise FileNotFoundError()\n\n    # Prevent directory traversal, make sure the path is inside the media or static root\n    local_path = local_path.resolve(strict=True)\n    if not any(\n        path in local_path.parents\n        for path in (settings.MEDIA_ROOT, settings.STATIC_ROOT)\n    ):\n        raise FileNotFoundError()\n\n    with open(local_path, \"rb\") as f:\n        return f.read()\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        name = str(name).replace('\\\\', '/')\n        dir_name, file_name = os.path.split(name)\n        if '..' in pathlib.PurePath(dir_name).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n        validate_file_name(file_name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, add an underscore and a random 7\n        # character alphanumeric string (before the file extension, if one\n        # exists) to the filename until the generated filename doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n        return name\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_context_data(self, **kwargs):\n        template = self.kwargs['template']\n        templates = []\n        try:\n            default_engine = Engine.get_default()\n        except ImproperlyConfigured:\n            # Non-trivial TEMPLATES settings aren't supported (#24125).\n            pass\n        else:\n            # This doesn't account for template loaders (#24128).\n            for index, directory in enumerate(default_engine.dirs):\n                template_file = Path(safe_join(directory, template))\n                if template_file.exists():\n                    template_contents = template_file.read_text()\n                else:\n                    template_contents = ''\n                templates.append({\n                    'file': template_file,\n                    'exists': template_file.exists(),\n                    'contents': template_contents,\n                    'order': index,\n                })\n        return super().get_context_data(**{\n            **kwargs,\n            'name': template,\n            'templates': templates,\n        })\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_obj_absolute_path(obj_path):\n    return safe_join(DATAROOT, obj_path)\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def lookup_path(\n        self, path: str\n    ) -> typing.Tuple[str, typing.Optional[os.stat_result]]:\n        for directory in self.all_directories:\n            joined_path = os.path.join(directory, path)\n            if self.follow_symlink:\n                full_path = os.path.abspath(joined_path)\n            else:\n                full_path = os.path.realpath(joined_path)\n            directory = os.path.realpath(directory)\n            if os.path.commonpath([full_path, directory]) != directory:\n                # Don't allow misbehaving clients to break out of the static files\n                # directory.\n                continue\n            try:\n                return full_path, os.stat(full_path)\n            except (FileNotFoundError, NotADirectoryError):\n                continue\n        return \"\", None\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def get_context_data(self, **kwargs):\n        template = self.kwargs['template']\n        templates = []\n        try:\n            default_engine = Engine.get_default()\n        except ImproperlyConfigured:\n            # Non-trivial TEMPLATES settings aren't supported (#24125).\n            pass\n        else:\n            # This doesn't account for template loaders (#24128).\n            for index, directory in enumerate(default_engine.dirs):\n                template_file = Path(safe_join(directory, template))\n                if template_file.exists():\n                    template_contents = template_file.read_text()\n                else:\n                    template_contents = ''\n                templates.append({\n                    'file': template_file,\n                    'exists': template_file.exists(),\n                    'contents': template_contents,\n                    'order': index,\n                })\n        return super().get_context_data(**{\n            **kwargs,\n            'name': template,\n            'templates': templates,\n        })\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def generate_filename(self, filename):\n        \"\"\"\n        Validate the filename by calling get_valid_name() and return a filename\n        to be passed to the save() method.\n        \"\"\"\n        filename = str(filename).replace('\\\\', '/')\n        # `filename` may include a path as returned by FileField.upload_to.\n        dirname, filename = os.path.split(filename)\n        if '..' in pathlib.PurePath(dirname).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dirname)\n        return os.path.normpath(os.path.join(dirname, self.get_valid_name(filename)))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "void check_tensor(const TensorProto& tensor, const CheckerContext& ctx) {\n  enforce_has_field(tensor, data_type);\n  if (tensor.data_type() == TensorProto::UNDEFINED) {\n    fail_check(\"setting data_type field (tensor name: \", tensor.name(), \") to UNDEFINED is not allowed\");\n  }\n\n  int num_value_fields = 0;\n\n  const char* value_field = nullptr;\n\n#define check_data_field(field)             \\\n  bool has_##field = tensor.field().size(); \\\n  if (has_##field) {                        \\\n    ++num_value_fields;                     \\\n    value_field = #field;                   \\\n  }\n\n  check_data_field(float_data);\n  check_data_field(int32_data);\n  check_data_field(string_data);\n  check_data_field(int64_data);\n  check_data_field(raw_data);\n  check_data_field(double_data);\n  check_data_field(uint64_data);\n\n#undef check_data_field\n\n  bool stored_externally = tensor.has_data_location() && tensor.data_location() == TensorProto::EXTERNAL;\n  if (stored_externally) {\n    if (num_value_fields != 0) {\n      fail_check(\n          \"Data of TensorProto ( tensor name: \",\n          tensor.name(),\n          \") is stored externally and should not have data field.\",\n          value_field);\n    }\n\n    bool has_location = false;\n    for (const StringStringEntryProto& entry : tensor.external_data()) {\n      if (entry.has_key() && entry.has_value() && entry.key() == \"location\") {\n        has_location = true;\n        std::string relative_path = clean_relative_path(entry.value());\n        // Check that normalized relative path starts with \"../\" or \"..\\\" on windows.\n        if (relative_path.rfind(\"..\" + k_preferred_path_separator, 0) == 0) {\n          fail_check(\n              \"Data of TensorProto ( tensor name: \",\n              tensor.name(),\n              \") should be file inside the \",\n              ctx.get_model_dir(),\n              \", but the '\",\n              entry.value(),\n              \"' points outside the directory\");\n        }\n\n        std::string data_path = path_join(ctx.get_model_dir(), relative_path);\n        // use stat to check whether the file exists\n        struct stat buffer;\n        if (stat((data_path).c_str(), &buffer) != 0) {\n          fail_check(\n              \"Data of TensorProto ( tensor name: \",\n              tensor.name(),\n              \") should be stored in \",\n              data_path,\n              \", but it doesn't exist or is not accessible.\");\n        }\n#ifdef _WIN32\n#else // POSIX\n      //  Do not allow symlinks or directories.\n        if (!S_ISREG(buffer.st_mode)) {\n          fail_check(\n              \"Data of TensorProto ( tensor name: \",\n              tensor.name(),\n              \") should be stored in \",\n              data_path,\n              \", but it is not regular file.\");\n        }\n#endif\n      }\n    }\n    if (!has_location) {\n      fail_check(\"TensorProto ( tensor name: \", tensor.name(), \") is stored externally but doesn't have a location.\");\n    }\n    return;\n  }\n  int64_t nelem = 1;\n  for (auto x : tensor.dims()) {\n    nelem *= x;\n  }\n  if (nelem == 0 && num_value_fields != 0) {\n    fail_check(\"TensorProto (tensor name: \", tensor.name(), \") is 0-element but contains data!\");\n  }\n  if (nelem != 0 && num_value_fields != 1) {\n    fail_check(\"TensorProto (tensor name: \", tensor.name(), \") should contain one and only one value field.\");\n  }\n  if (has_raw_data) {\n    if (tensor.data_type() == TensorProto::STRING) {\n      fail_check(\"STRING data (tensor name: \", tensor.name(), \") should not be stored in raw_data field\");\n    }\n    return;\n  } else {\n#define check_field(field)               \\\n  if (nelem != 0 && !has_##field) {      \\\n    fail_check(                          \\\n        \"values of data_type '\",         \\\n        tensor.data_type(),              \\\n        \"' should be stored in field '\", \\\n        #field,                          \\\n        \"' instead of '\",                \\\n        value_field,                     \\\n        \"'\");                            \\\n  }\n\n    switch (tensor.data_type()) {\n      case TensorProto::FLOAT:\n      case TensorProto::COMPLEX64:\n        check_field(float_data);\n        break;\n\n      case TensorProto::DOUBLE:\n      case TensorProto::COMPLEX128:\n        check_field(double_data);\n        break;\n\n      case TensorProto::INT32:\n      case TensorProto::UINT8:\n      case TensorProto::INT8:\n      case TensorProto::UINT16:\n      case TensorProto::INT16:\n      case TensorProto::BOOL:\n      case TensorProto::FLOAT16:\n      case TensorProto::BFLOAT16:\n        check_field(int32_data);\n        break;\n\n      case TensorProto::INT64:\n        check_field(int64_data);\n        break;\n\n      case TensorProto::UINT32:\n      case TensorProto::UINT64:\n        check_field(uint64_data);\n        break;\n\n      case TensorProto::STRING:\n        check_field(string_data);\n        break;\n\n      default:\n        fail_check(\"Unrecognized data_type (tensor name: \", tensor.name(), \"): \", tensor.data_type());\n    }\n  }\n\n#undef check_field\n}\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _get_object_element(dataset, seq_no, rel_path, download_link):\n    \"\"\"If rel_path and download_link are not None, we are called from scope.\n    Otherwise we are called from ID and need to run SQL query to fetch these attrs.\"\"\"\n\n    if rel_path is None:\n        query = \"SELECT rel_path, download_link FROM \" + \\\n        dataset + \\\n        \" WHERE sequence_no = %s\"\n\n        cnx = mysql.connector.connect(user=DB_USER,\n                                    password=DB_PASSWORD,\n                                    host=DB_HOST,\n                                    database=DB_DBNAME,\n                                    port=DB_PORT)\n        cursor = cnx.cursor()\n        cursor.execute(query, (seq_no,))\n\n        row = cursor.fetchone()\n\n        if not row:\n            return None\n\n        rel_path, download_link = row[0], row[1]\n\n    if LOCAL_OBJ_URI:\n        src_uri = 'file://' + _get_obj_absolute_path(dataset, rel_path)\n    else:\n        src_uri = url_for('.get_object_src_http', dataset=dataset, rel_path=rel_path)\n\n    return '<object id={} src={} hyperfind.external-link={} />' \\\n        .format(\n        quoteattr(url_for('.get_object_id', dataset=dataset, seq_no=seq_no)),\n        quoteattr(src_uri),\n        quoteattr(download_link))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "    def __call__(self, args: argparse.Namespace) -> None:\n        \"\"\"Perform the update.\"\"\"\n        manifest, path = dfetch.manifest.manifest.get_manifest()\n\n        exceptions: List[str] = []\n        destinations: List[str] = [\n            os.path.realpath(project.destination) for project in manifest.projects\n        ]\n        with in_directory(os.path.dirname(path)):\n            for project in manifest.selected_projects(args.projects):\n                with catch_runtime_exceptions(exceptions) as exceptions:\n                    self._check_destination(project, destinations)\n                    dfetch.project.make(project).update(force=args.force)\n\n                    if not args.no_recommendations and os.path.isdir(\n                        project.destination\n                    ):\n                        with in_directory(project.destination):\n                            check_child_manifests(manifest, project, path)\n\n        if exceptions:\n            raise RuntimeError(\"\\n\".join(exceptions))\n"}, {"role": "assistant", "content": "False"}]}
{"messages": [{"role": "system", "content": "You are a security expert that is good at static program analysis"}, {"role": "user", "content": "def _download_artifact(artifact_path):\n    \"\"\"\n    A request handler for `GET /mlflow-artifacts/artifacts/<artifact_path>` to download an artifact\n    from `artifact_path` (a relative path from the root artifact directory).\n    \"\"\"\n    artifact_path = validate_path_is_safe(artifact_path)\n    tmp_dir = tempfile.TemporaryDirectory()\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    dst = artifact_repo.download_artifacts(artifact_path, tmp_dir.name)\n\n    # Ref: https://stackoverflow.com/a/24613980/6943581\n    file_handle = open(dst, \"rb\")  # noqa: SIM115\n\n    def stream_and_remove_file():\n        yield from file_handle\n        file_handle.close()\n        tmp_dir.cleanup()\n\n    file_sender_response = current_app.response_class(stream_and_remove_file())\n\n    return _response_with_file_attachment_headers(artifact_path, file_sender_response)\n"}, {"role": "assistant", "content": "False"}]}
